{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c77d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import funcs\n",
    "import coral\n",
    "import star\n",
    "import mcd\n",
    "import dann\n",
    "import base\n",
    "import plots\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.metrics import precision_score,f1_score,recall_score\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39742bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testbed data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"/home/ash/ic3/testbed_da/data\"\n",
    "\n",
    "# Classes\n",
    "class_subset = [\"bpsk\", \"qpsk\", \"4qam\", \"16qam\", \"apsk\"]\n",
    "\n",
    "# Split source, target\n",
    "# try selecting some of the mods, not all\n",
    "X = np.load(file_path + \"/SNR_X.npy\")\n",
    "Y = np.load(file_path + \"/SNR_Y.npy\")\n",
    "\n",
    "doppler_offset_source = 22\n",
    "doppler_offset_target = 20\n",
    "\n",
    "source_mask = (Y[:, 1] == doppler_offset_source)\n",
    "target_mask = (Y[:, 1] == doppler_offset_target)\n",
    "\n",
    "X_s = X[source_mask]\n",
    "Y_s = Y[source_mask]\n",
    "Y_s = Y_s[:,0]\n",
    "\n",
    "X_t = X[target_mask]\n",
    "Y_t = Y[target_mask]\n",
    "Y_t = Y_t[:,0]\n",
    "\n",
    "# Dataloaders\n",
    "S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, permute=False)\n",
    "T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, permute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4966eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1/5\n",
      "Epoch 1/25, Train Loss: 1.6701, Train Acc: 0.2169, Val Loss: 1.2823, Val Acc: 0.3950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25, Train Loss: 0.8740, Train Acc: 0.6359, Val Loss: 0.3925, Val Acc: 0.7947\n",
      "Epoch 3/25, Train Loss: 0.6579, Train Acc: 0.7342, Val Loss: 0.7910, Val Acc: 0.6519\n",
      "Epoch 4/25, Train Loss: 0.6705, Train Acc: 0.7388, Val Loss: 0.4692, Val Acc: 0.7896\n",
      "Epoch 5/25, Train Loss: 0.7206, Train Acc: 0.7153, Val Loss: 1.0143, Val Acc: 0.5559\n",
      "Epoch 6/25, Train Loss: 0.5522, Train Acc: 0.7360, Val Loss: 0.5082, Val Acc: 0.7561\n",
      "Epoch 7/25, Train Loss: 0.4773, Train Acc: 0.7644, Val Loss: 0.4275, Val Acc: 0.7627\n",
      "Early stopping!\n",
      "\n",
      "Run 2/5\n",
      "Epoch 1/25, Train Loss: 1.6480, Train Acc: 0.1963, Val Loss: 1.6161, Val Acc: 0.1921\n",
      "Epoch 2/25, Train Loss: 0.9896, Train Acc: 0.5438, Val Loss: 0.4962, Val Acc: 0.7664\n",
      "Epoch 3/25, Train Loss: 0.6757, Train Acc: 0.7389, Val Loss: 0.7182, Val Acc: 0.6807\n",
      "Epoch 4/25, Train Loss: 0.6445, Train Acc: 0.7169, Val Loss: 0.4567, Val Acc: 0.7932\n",
      "Epoch 5/25, Train Loss: 0.6652, Train Acc: 0.7457, Val Loss: 0.2932, Val Acc: 0.8337\n",
      "Epoch 6/25, Train Loss: 0.8034, Train Acc: 0.7314, Val Loss: 0.7162, Val Acc: 0.7969\n",
      "Epoch 7/25, Train Loss: 0.4032, Train Acc: 0.8035, Val Loss: 0.2822, Val Acc: 0.8240\n",
      "Epoch 8/25, Train Loss: 0.4246, Train Acc: 0.8039, Val Loss: 0.2563, Val Acc: 0.8586\n",
      "Epoch 9/25, Train Loss: 0.3636, Train Acc: 0.7960, Val Loss: 0.4019, Val Acc: 0.7888\n",
      "Epoch 10/25, Train Loss: 0.5622, Train Acc: 0.7646, Val Loss: 0.2883, Val Acc: 0.8125\n",
      "Epoch 11/25, Train Loss: 0.3598, Train Acc: 0.8052, Val Loss: 0.2951, Val Acc: 0.8577\n",
      "Epoch 12/25, Train Loss: 0.4088, Train Acc: 0.7953, Val Loss: 0.4068, Val Acc: 0.7627\n",
      "Epoch 13/25, Train Loss: 0.3874, Train Acc: 0.8174, Val Loss: 0.3331, Val Acc: 0.8105\n",
      "Early stopping!\n",
      "\n",
      "Run 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 1.6553, Train Acc: 0.2050, Val Loss: 1.4745, Val Acc: 0.3828\n",
      "Epoch 2/25, Train Loss: 0.6878, Train Acc: 0.6976, Val Loss: 0.3437, Val Acc: 0.7979\n",
      "Epoch 3/25, Train Loss: 0.8610, Train Acc: 0.6860, Val Loss: 0.6276, Val Acc: 0.7417\n",
      "Epoch 4/25, Train Loss: 1.3562, Train Acc: 0.5259, Val Loss: 0.3905, Val Acc: 0.7998\n",
      "Epoch 5/25, Train Loss: 0.6954, Train Acc: 0.6468, Val Loss: 0.3354, Val Acc: 0.8164\n",
      "Epoch 6/25, Train Loss: 0.5593, Train Acc: 0.7532, Val Loss: 0.4146, Val Acc: 0.7666\n",
      "Epoch 7/25, Train Loss: 0.5495, Train Acc: 0.7563, Val Loss: 0.3642, Val Acc: 0.7888\n",
      "Epoch 8/25, Train Loss: 0.5703, Train Acc: 0.7533, Val Loss: 0.3492, Val Acc: 0.8066\n",
      "Epoch 9/25, Train Loss: 0.4710, Train Acc: 0.7764, Val Loss: 0.2780, Val Acc: 0.8533\n",
      "Epoch 10/25, Train Loss: 0.4020, Train Acc: 0.7949, Val Loss: 0.3688, Val Acc: 0.7971\n",
      "Epoch 11/25, Train Loss: 0.4588, Train Acc: 0.7837, Val Loss: 0.2831, Val Acc: 0.8237\n",
      "Epoch 12/25, Train Loss: 0.4339, Train Acc: 0.7952, Val Loss: 0.2813, Val Acc: 0.8252\n",
      "Epoch 13/25, Train Loss: 0.3778, Train Acc: 0.8185, Val Loss: 0.2850, Val Acc: 0.8379\n",
      "Epoch 14/25, Train Loss: 0.4237, Train Acc: 0.8310, Val Loss: 0.2789, Val Acc: 0.8687\n",
      "Early stopping!\n",
      "\n",
      "Run 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 1.6236, Train Acc: 0.2281, Val Loss: 0.9807, Val Acc: 0.4143\n",
      "Epoch 2/25, Train Loss: 0.6728, Train Acc: 0.6615, Val Loss: 0.7075, Val Acc: 0.5918\n",
      "Epoch 3/25, Train Loss: 0.9606, Train Acc: 0.6764, Val Loss: 0.3150, Val Acc: 0.8118\n",
      "Epoch 4/25, Train Loss: 0.6256, Train Acc: 0.7357, Val Loss: 0.3056, Val Acc: 0.8298\n",
      "Epoch 5/25, Train Loss: 0.5482, Train Acc: 0.7515, Val Loss: 0.3662, Val Acc: 0.7925\n",
      "Epoch 6/25, Train Loss: 0.7030, Train Acc: 0.7548, Val Loss: 0.2790, Val Acc: 0.8318\n",
      "Epoch 7/25, Train Loss: 0.4461, Train Acc: 0.7888, Val Loss: 0.3516, Val Acc: 0.8093\n",
      "Epoch 8/25, Train Loss: 0.4816, Train Acc: 0.7801, Val Loss: 0.4017, Val Acc: 0.7800\n",
      "Epoch 9/25, Train Loss: 0.3977, Train Acc: 0.8146, Val Loss: 0.3322, Val Acc: 0.8269\n",
      "Epoch 10/25, Train Loss: 0.3896, Train Acc: 0.8058, Val Loss: 0.3120, Val Acc: 0.8293\n",
      "Epoch 11/25, Train Loss: 0.3980, Train Acc: 0.8119, Val Loss: 0.2803, Val Acc: 0.8455\n",
      "Early stopping!\n",
      "\n",
      "Run 5/5\n",
      "Epoch 1/25, Train Loss: 1.6587, Train Acc: 0.2021, Val Loss: 1.4735, Val Acc: 0.2029\n",
      "Epoch 2/25, Train Loss: 0.7767, Train Acc: 0.6498, Val Loss: 0.3574, Val Acc: 0.7903\n",
      "Epoch 3/25, Train Loss: 0.7453, Train Acc: 0.7085, Val Loss: 3.5254, Val Acc: 0.2566\n",
      "Epoch 4/25, Train Loss: 0.8788, Train Acc: 0.6777, Val Loss: 0.3132, Val Acc: 0.8098\n",
      "Epoch 5/25, Train Loss: 0.5338, Train Acc: 0.7550, Val Loss: 0.3233, Val Acc: 0.8201\n",
      "Epoch 6/25, Train Loss: 0.5771, Train Acc: 0.7507, Val Loss: 1.5227, Val Acc: 0.3972\n",
      "Epoch 7/25, Train Loss: 0.7244, Train Acc: 0.7225, Val Loss: 1.0319, Val Acc: 0.5154\n",
      "Epoch 8/25, Train Loss: 0.4063, Train Acc: 0.7905, Val Loss: 0.4656, Val Acc: 0.8005\n",
      "Epoch 9/25, Train Loss: 0.4912, Train Acc: 0.7818, Val Loss: 0.4700, Val Acc: 0.7488\n",
      "Early stopping!\n",
      "\n",
      "Source performance: 80.72 82.40 80.79 78.01\n",
      "Target performance: 58.58 54.93 58.10 53.09\n",
      "\n",
      "bpsk: 99.88\n",
      "qpsk: 40.27\n",
      "4qam: 36.25\n",
      "16qam: 14.11\n",
      "apsk: 100.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#%% Baseline - VTC24 code\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.001\n",
    "n_epochs = 25\n",
    "n_runs = 5\n",
    "n_classes = len(class_subset)\n",
    "\n",
    "model = base.CLDNN().to(device) # Model selection\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=.1)\n",
    "\n",
    "# Initialize lists to store performance metrics for each run\n",
    "accuracy_s_list, pr_s_list, re_s_list, f1_s_list = [], [], [], []\n",
    "accuracy_t_list, pr_t_list, re_t_list, f1_t_list = [], [], [], []\n",
    "class_accuracies_s = np.zeros((n_runs, n_classes))\n",
    "class_accuracies_t = np.zeros((n_runs, n_classes))\n",
    "\n",
    "def train_model():\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    trigger_times = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in S_train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        epoch_loss = running_loss / len(S_train_loader.dataset)\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in S_val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_loss = val_running_loss / len(S_val_loader.dataset)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, '\n",
    "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "def eva_model(model, loader, num_classes):\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            class_outputs = model(inputs)\n",
    "            _, preds = torch.max(class_outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Performance metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    \n",
    "    # Confusion matrix and per-class accuracy\n",
    "    conf_mat = confusion_matrix(true_labels, predictions)\n",
    "    class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    #plt.figure(figsize=(8,6),dpi=300)\n",
    "    #sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "    #            xticklabels=n_classes,\n",
    "    #            yticklabels=n_classes)\n",
    "    #plt.yticks(fontsize=14,rotation=360)\n",
    "    #plt.xticks(fontsize=14,rotation=90)\n",
    "    #plt.title('Confusion Matrix')\n",
    "    #plt.show()\n",
    "\n",
    "    return accuracy, precision, recall, f1, class_accuracy\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f'\\nRun {run+1}/{n_runs}')\n",
    "    # Model is reset per run. Change model type here too.\n",
    "    model = base.CLDNN().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    trained_model = train_model()\n",
    "\n",
    "    # Evaluate on source domain\n",
    "    accuracy_s, pr_s, re_s, f1_s, class_acc_s = eva_model(trained_model, S_val_loader, n_classes)\n",
    "    accuracy_s_list.append(accuracy_s)\n",
    "    pr_s_list.append(pr_s)\n",
    "    re_s_list.append(re_s)\n",
    "    f1_s_list.append(f1_s)\n",
    "    class_accuracies_s[run] = class_acc_s\n",
    "    \n",
    "    # Evaluate on target domain\n",
    "    accuracy_t, pr_t, re_t, f1_t, class_acc_t = eva_model(trained_model, T_val_loader, n_classes)\n",
    "    accuracy_t_list.append(accuracy_t)\n",
    "    pr_t_list.append(pr_t)\n",
    "    re_t_list.append(re_t)\n",
    "    f1_t_list.append(f1_t)\n",
    "    class_accuracies_t[run] = class_acc_t\n",
    "\n",
    "# Calculate mean and standard deviation of performance metrics\n",
    "mean_accuracy_s = np.mean(accuracy_s_list)\n",
    "mean_pr_s = np.mean(pr_s_list)\n",
    "mean_re_s = np.mean(re_s_list)\n",
    "mean_f1_s = np.mean(f1_s_list)\n",
    "\n",
    "mean_accuracy_t = np.mean(accuracy_t_list)\n",
    "mean_pr_t = np.mean(pr_t_list)\n",
    "mean_re_t = np.mean(re_t_list)\n",
    "mean_f1_t = np.mean(f1_t_list)\n",
    "\n",
    "mean_class_accuracies_s = np.mean(class_accuracies_s, axis=0)\n",
    "mean_class_accuracies_t = np.mean(class_accuracies_t, axis=0)\n",
    "\n",
    "print(f\"\\nSource performance: {mean_accuracy_s*100:.2f} {mean_pr_s*100:.2f} {mean_re_s*100:.2f} {mean_f1_s*100:.2f}\")\n",
    "print(f\"Target performance: {mean_accuracy_t*100:.2f} {mean_pr_t*100:.2f} {mean_re_t*100:.2f} {mean_f1_t*100:.2f}\\n\")\n",
    "\n",
    "for i, class_name in enumerate(class_subset):\n",
    "    print(f\"{class_name}: {mean_class_accuracies_t[i]*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba3d953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 3.0476, Domain Loss: 1.4031, Class Loss: 1.6445\n",
      "Epoch 2/20, Loss: 2.1135, Domain Loss: 1.3437, Class Loss: 0.7698\n",
      "Epoch 3/20, Loss: 1.7421, Domain Loss: 1.2444, Class Loss: 0.4977\n",
      "Epoch 4/20, Loss: 18.7626, Domain Loss: 16.4397, Class Loss: 2.3229\n",
      "Epoch 5/20, Loss: 3.2993, Domain Loss: 1.6707, Class Loss: 1.6286\n",
      "Epoch 6/20, Loss: 4.0438, Domain Loss: 2.4385, Class Loss: 1.6053\n",
      "Epoch 7/20, Loss: 5.3057, Domain Loss: 3.7736, Class Loss: 1.5321\n",
      "Epoch 8/20, Loss: 7.9767, Domain Loss: 6.4575, Class Loss: 1.5192\n",
      "Epoch 9/20, Loss: 7.0614, Domain Loss: 5.4612, Class Loss: 1.6003\n",
      "Epoch 10/20, Loss: 11.0660, Domain Loss: 9.4838, Class Loss: 1.5822\n",
      "Epoch 11/20, Loss: 11.8672, Domain Loss: 10.5908, Class Loss: 1.2763\n",
      "Epoch 12/20, Loss: 9.2498, Domain Loss: 8.0650, Class Loss: 1.1848\n",
      "Epoch 13/20, Loss: 6.2736, Domain Loss: 5.7919, Class Loss: 0.4817\n",
      "Epoch 14/20, Loss: 6.5256, Domain Loss: 5.2241, Class Loss: 1.3015\n",
      "Epoch 15/20, Loss: 3.4029, Domain Loss: 2.8394, Class Loss: 0.5635\n",
      "Epoch 16/20, Loss: 2.8901, Domain Loss: 2.4311, Class Loss: 0.4590\n",
      "Epoch 17/20, Loss: 2.5643, Domain Loss: 2.2136, Class Loss: 0.3507\n",
      "Epoch 18/20, Loss: 1.5688, Domain Loss: 1.2953, Class Loss: 0.2735\n",
      "Epoch 19/20, Loss: 1.6284, Domain Loss: 1.3145, Class Loss: 0.3139\n",
      "Epoch 20/20, Loss: 1.7002, Domain Loss: 1.4197, Class Loss: 0.2805\n",
      "51.17\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 3.0360, Domain Loss: 1.3921, Class Loss: 1.6439\n",
      "Epoch 2/20, Loss: 2.3880, Domain Loss: 1.3482, Class Loss: 1.0397\n",
      "Epoch 3/20, Loss: 1.8656, Domain Loss: 1.2838, Class Loss: 0.5818\n",
      "Epoch 4/20, Loss: 2.4242, Domain Loss: 1.6236, Class Loss: 0.8005\n",
      "Epoch 5/20, Loss: 2.1836, Domain Loss: 1.5494, Class Loss: 0.6342\n",
      "Epoch 6/20, Loss: 2.1768, Domain Loss: 1.5303, Class Loss: 0.6464\n",
      "Epoch 7/20, Loss: 2.0623, Domain Loss: 1.5645, Class Loss: 0.4979\n",
      "Epoch 8/20, Loss: 3.5566, Domain Loss: 1.9343, Class Loss: 1.6223\n",
      "Epoch 9/20, Loss: 1.7438, Domain Loss: 1.2741, Class Loss: 0.4697\n",
      "Epoch 10/20, Loss: 1.5366, Domain Loss: 1.2391, Class Loss: 0.2975\n",
      "Epoch 11/20, Loss: 1.7148, Domain Loss: 1.2672, Class Loss: 0.4476\n",
      "Epoch 12/20, Loss: 2.0469, Domain Loss: 1.4217, Class Loss: 0.6252\n",
      "Epoch 13/20, Loss: 3.1787, Domain Loss: 2.0380, Class Loss: 1.1407\n",
      "Epoch 14/20, Loss: 8.6981, Domain Loss: 7.0458, Class Loss: 1.6523\n",
      "Epoch 15/20, Loss: 5.2465, Domain Loss: 4.1915, Class Loss: 1.0551\n",
      "Epoch 16/20, Loss: 3.0045, Domain Loss: 2.2782, Class Loss: 0.7263\n",
      "Epoch 17/20, Loss: 2.4902, Domain Loss: 1.9247, Class Loss: 0.5655\n",
      "Epoch 18/20, Loss: 2.3383, Domain Loss: 1.8690, Class Loss: 0.4693\n",
      "Epoch 19/20, Loss: 2.1181, Domain Loss: 1.4209, Class Loss: 0.6971\n",
      "Epoch 20/20, Loss: 1.7250, Domain Loss: 1.3878, Class Loss: 0.3372\n",
      "58.30\n",
      "\n",
      "\n",
      "Epoch 1/20, Loss: 3.0374, Domain Loss: 1.3970, Class Loss: 1.6404\n",
      "Epoch 2/20, Loss: 2.4031, Domain Loss: 1.3671, Class Loss: 1.0360\n",
      "Epoch 3/20, Loss: 1.7037, Domain Loss: 1.2827, Class Loss: 0.4210\n",
      "Epoch 4/20, Loss: 1.7223, Domain Loss: 1.2906, Class Loss: 0.4317\n",
      "Epoch 5/20, Loss: 1.9070, Domain Loss: 1.3245, Class Loss: 0.5826\n",
      "Epoch 6/20, Loss: 10.3499, Domain Loss: 8.5833, Class Loss: 1.7666\n",
      "Epoch 7/20, Loss: 3.0169, Domain Loss: 1.4080, Class Loss: 1.6089\n",
      "Epoch 8/20, Loss: 2.9922, Domain Loss: 1.3853, Class Loss: 1.6070\n",
      "Epoch 9/20, Loss: 2.9891, Domain Loss: 1.3855, Class Loss: 1.6036\n",
      "Epoch 10/20, Loss: 2.9861, Domain Loss: 1.3855, Class Loss: 1.6005\n",
      "Epoch 11/20, Loss: 2.9866, Domain Loss: 1.3913, Class Loss: 1.5953\n",
      "Epoch 12/20, Loss: 3.0022, Domain Loss: 1.4138, Class Loss: 1.5884\n",
      "Epoch 13/20, Loss: 2.9826, Domain Loss: 1.4121, Class Loss: 1.5705\n",
      "Epoch 14/20, Loss: 2.9158, Domain Loss: 1.4095, Class Loss: 1.5063\n",
      "Epoch 15/20, Loss: 2.7075, Domain Loss: 1.4085, Class Loss: 1.2990\n",
      "Epoch 16/20, Loss: 2.4816, Domain Loss: 1.4089, Class Loss: 1.0727\n",
      "Epoch 17/20, Loss: 2.3580, Domain Loss: 1.4082, Class Loss: 0.9497\n",
      "Epoch 18/20, Loss: 2.3588, Domain Loss: 1.4211, Class Loss: 0.9376\n",
      "Epoch 19/20, Loss: 2.3585, Domain Loss: 1.5032, Class Loss: 0.8553\n",
      "Epoch 20/20, Loss: 2.6281, Domain Loss: 1.8313, Class Loss: 0.7967\n",
      "51.42\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 3.0401, Domain Loss: 1.3960, Class Loss: 1.6441\n",
      "Epoch 2/20, Loss: 3.4604, Domain Loss: 1.8772, Class Loss: 1.5832\n",
      "Epoch 3/20, Loss: 2.5151, Domain Loss: 1.4467, Class Loss: 1.0684\n",
      "Epoch 4/20, Loss: 1.9541, Domain Loss: 1.3989, Class Loss: 0.5552\n",
      "Epoch 5/20, Loss: 1.8629, Domain Loss: 1.3927, Class Loss: 0.4702\n",
      "Epoch 6/20, Loss: 1.6369, Domain Loss: 1.3402, Class Loss: 0.2967\n",
      "Epoch 7/20, Loss: 1.7275, Domain Loss: 1.2690, Class Loss: 0.4586\n",
      "Epoch 8/20, Loss: 1.7849, Domain Loss: 1.2646, Class Loss: 0.5203\n",
      "Epoch 9/20, Loss: 1.9551, Domain Loss: 1.3604, Class Loss: 0.5947\n",
      "Epoch 10/20, Loss: 1.7850, Domain Loss: 1.2843, Class Loss: 0.5008\n",
      "Epoch 11/20, Loss: 4.2093, Domain Loss: 2.9556, Class Loss: 1.2538\n",
      "Epoch 12/20, Loss: 1.7139, Domain Loss: 1.3934, Class Loss: 0.3205\n",
      "Epoch 13/20, Loss: 1.6789, Domain Loss: 1.3853, Class Loss: 0.2935\n",
      "Epoch 14/20, Loss: 1.6869, Domain Loss: 1.3845, Class Loss: 0.3024\n",
      "Epoch 15/20, Loss: 1.8686, Domain Loss: 1.3843, Class Loss: 0.4843\n",
      "Epoch 16/20, Loss: 1.7203, Domain Loss: 1.3813, Class Loss: 0.3389\n",
      "Epoch 17/20, Loss: 1.6657, Domain Loss: 1.3795, Class Loss: 0.2862\n",
      "Epoch 18/20, Loss: 1.6578, Domain Loss: 1.3770, Class Loss: 0.2808\n",
      "Epoch 19/20, Loss: 1.6630, Domain Loss: 1.3747, Class Loss: 0.2883\n",
      "Epoch 20/20, Loss: 1.6504, Domain Loss: 1.3715, Class Loss: 0.2790\n",
      "43.63\n",
      "\n",
      "\n",
      "Epoch 1/20, Loss: 3.0543, Domain Loss: 1.4110, Class Loss: 1.6433\n",
      "Epoch 2/20, Loss: 2.1001, Domain Loss: 1.3414, Class Loss: 0.7587\n",
      "Epoch 3/20, Loss: 2.0416, Domain Loss: 1.3313, Class Loss: 0.7102\n",
      "Epoch 4/20, Loss: 1.6996, Domain Loss: 1.3427, Class Loss: 0.3569\n",
      "Epoch 5/20, Loss: 3.3730, Domain Loss: 2.0056, Class Loss: 1.3674\n",
      "Epoch 6/20, Loss: 2.8066, Domain Loss: 1.7260, Class Loss: 1.0807\n",
      "Epoch 7/20, Loss: 2.4848, Domain Loss: 1.7549, Class Loss: 0.7299\n",
      "Epoch 8/20, Loss: 3.0723, Domain Loss: 2.1379, Class Loss: 0.9345\n",
      "Epoch 9/20, Loss: 1.5719, Domain Loss: 1.2387, Class Loss: 0.3332\n",
      "Epoch 10/20, Loss: 1.6854, Domain Loss: 1.2380, Class Loss: 0.4474\n",
      "Epoch 11/20, Loss: 1.5473, Domain Loss: 1.2235, Class Loss: 0.3238\n",
      "Epoch 12/20, Loss: 1.5697, Domain Loss: 1.2160, Class Loss: 0.3537\n",
      "Epoch 13/20, Loss: 1.5492, Domain Loss: 1.2276, Class Loss: 0.3217\n",
      "Epoch 14/20, Loss: 1.8059, Domain Loss: 1.2755, Class Loss: 0.5304\n",
      "Epoch 15/20, Loss: 1.5219, Domain Loss: 1.2164, Class Loss: 0.3056\n",
      "Epoch 16/20, Loss: 1.7455, Domain Loss: 1.3015, Class Loss: 0.4440\n",
      "Epoch 17/20, Loss: 1.5500, Domain Loss: 1.2407, Class Loss: 0.3093\n",
      "Epoch 18/20, Loss: 1.7303, Domain Loss: 1.3194, Class Loss: 0.4110\n",
      "Epoch 19/20, Loss: 1.5842, Domain Loss: 1.2427, Class Loss: 0.3415\n",
      "Epoch 20/20, Loss: 1.9997, Domain Loss: 1.3827, Class Loss: 0.6170\n",
      "59.50\n",
      "\n",
      "\n",
      "Source performance:\n",
      "81.76 81.90 81.83 79.04 \n",
      "Target performance:\n",
      "52.80 49.06 52.35 43.29 \n",
      "\n",
      "Per-class target performance: 99.88 48.68 11.56 1.61 100.00 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#%% DANN - VTC24 code\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.001\n",
    "n_epochs = 20\n",
    "n_runs = 5\n",
    "\n",
    "model = dann.DANN(dann.CLDNN_FA,dann.CLDNN_LP,dann.CLDNN_DC).to(device)\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_domain = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_dann():\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss, total_domain_loss, total_class_loss = 0, 0, 0\n",
    "        len_dataloader = min(len(S_train_loader), len(T_train_loader))\n",
    "        data_source_iter = iter(S_train_loader)\n",
    "        data_target_iter = iter(T_train_loader)\n",
    "\n",
    "        for i in range(len_dataloader):\n",
    "            p = float(i + epoch * len_dataloader) / n_epochs / len_dataloader\n",
    "            alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "            # Training model using source data\n",
    "            s_data, s_label = next(data_source_iter)\n",
    "            s_data, s_label = s_data.to(device), s_label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            class_output, domain_output = model(s_data, alpha)\n",
    "            err_s_label = criterion_class(class_output, s_label)\n",
    "            err_s_domain = criterion_domain(domain_output, torch.zeros(s_data.size(0), dtype=torch.long).to(device))\n",
    "\n",
    "            # Training model using target data\n",
    "            t_data, _ = next(data_target_iter)\n",
    "            t_data = t_data.to(device)\n",
    "            _, domain_output = model(t_data, alpha)\n",
    "            err_t_domain = criterion_domain(domain_output, torch.ones(t_data.size(0), dtype=torch.long).to(device))\n",
    "\n",
    "            # Combining the losses\n",
    "            loss = err_s_label + err_s_domain + err_t_domain\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_domain_loss += err_s_domain.item() + err_t_domain.item()\n",
    "            total_class_loss += err_s_label.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}, Loss: {total_loss/len_dataloader:.4f}, Domain Loss: {total_domain_loss/len_dataloader:.4f}, Class Loss: {total_class_loss/len_dataloader:.4f}')\n",
    "\n",
    "def evaluate_and_plot_confusion_matrix(model, loader, title, num_classes):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            class_outputs, _ = model(inputs, alpha=0)\n",
    "            _, preds = torch.max(class_outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    \n",
    "    # Confusion matrix & per-class accuracy\n",
    "    conf_mat = confusion_matrix(true_labels, predictions)\n",
    "    per_class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    #plt.figure(figsize=(8,6), dpi=300)\n",
    "    #sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "    #            xticklabels=class_subset,\n",
    "    #            yticklabels=class_subset)\n",
    "    #plt.yticks(fontsize=14,rotation=360)\n",
    "    #plt.xticks(fontsize=14,rotation=90)\n",
    "    #plt.title(f'Confusion Matrix - {title}')\n",
    "    #plt.show()\n",
    "    \n",
    "    return accuracy, precision, recall, f1, per_class_accuracy\n",
    "\n",
    "source_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'per_class_accuracy': []}\n",
    "target_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'per_class_accuracy': []}\n",
    "\n",
    "for _ in range(n_runs):\n",
    "    # Model is reset per run. Change model type here too.\n",
    "    model = dann.DANN(dann.CLDNN_FA,dann.CLDNN_LP,dann.CLDNN_DC).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_dann()\n",
    "\n",
    "    # Evaluate on source domain\n",
    "    accuracy_s, pr_s, re_s, f1_s, per_class_accuracy_s = evaluate_and_plot_confusion_matrix(model, S_val_loader, \"Source Domain\", 10)\n",
    "    source_metrics['accuracy'].append(accuracy_s)\n",
    "    source_metrics['precision'].append(pr_s)\n",
    "    source_metrics['recall'].append(re_s)\n",
    "    source_metrics['f1'].append(f1_s)\n",
    "    source_metrics['per_class_accuracy'].append(per_class_accuracy_s)\n",
    "\n",
    "    # Evaluate on target domain\n",
    "    accuracy_t, pr_t, re_t, f1_t, per_class_accuracy_t = evaluate_and_plot_confusion_matrix(model, T_val_loader, \"Target Domain\", 10)\n",
    "    print(f'{accuracy_t*100:.2f}\\n\\n')\n",
    "    target_metrics['accuracy'].append(accuracy_t)\n",
    "    target_metrics['precision'].append(pr_t)\n",
    "    target_metrics['recall'].append(re_t)\n",
    "    target_metrics['f1'].append(f1_t)\n",
    "    target_metrics['per_class_accuracy'].append(per_class_accuracy_t)\n",
    "\n",
    "# Calculate and print average metrics\n",
    "avg_source_metrics = {metric: np.mean(values) for metric, values in source_metrics.items() if metric != 'per_class_accuracy'}\n",
    "avg_target_metrics = {metric: np.mean(values) for metric, values in target_metrics.items() if metric != 'per_class_accuracy'}\n",
    "\n",
    "print(\"Source performance:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    value = avg_source_metrics.get(metric, 0)\n",
    "    print(f\"{value*100:.2f}\", end= ' ')\n",
    "    \n",
    "print(\"\\nTarget performance:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    value = avg_target_metrics.get(metric, 0)\n",
    "    print(f\"{value*100:.2f}\", end= ' ')\n",
    "\n",
    "avg_target_per_class_accuracy = np.mean(np.array(target_metrics['per_class_accuracy']), axis=0)\n",
    "print(\"\\n\\nPer-class target performance:\", end=' ')\n",
    "for acc in avg_target_per_class_accuracy:\n",
    "    print(f\"{acc*100:.2f}\", end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d588b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1/5\n",
      "Epoch [1/25], Class Loss: 3.2334, Discrepancy Loss: 0.0117\n",
      "Validation Loss: 3.2201\n",
      "Epoch [2/25], Class Loss: 3.2211, Discrepancy Loss: 0.0044\n",
      "Validation Loss: 3.2187\n",
      "Epoch [3/25], Class Loss: 3.2192, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2188\n",
      "Epoch [4/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2189\n",
      "Epoch [5/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2189\n",
      "Epoch [6/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2191\n",
      "Epoch [7/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2190\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 19.21%, Precision: 3.84%, Recall: 20.00%, F1 Score: 6.45%\n",
      "Target Domain Performance - Accuracy: 20.53%, Precision: 4.11%, Recall: 20.00%, F1 Score: 6.81%\n",
      "\n",
      "Run 2/5\n",
      "Epoch [1/25], Class Loss: 3.2307, Discrepancy Loss: 0.0075\n",
      "Validation Loss: 3.2186\n",
      "Epoch [2/25], Class Loss: 3.2202, Discrepancy Loss: 0.0023\n",
      "Validation Loss: 3.2187\n",
      "Epoch [3/25], Class Loss: 3.2191, Discrepancy Loss: 0.0004\n",
      "Validation Loss: 3.2189\n",
      "Epoch [4/25], Class Loss: 3.2191, Discrepancy Loss: 0.0003\n",
      "Validation Loss: 3.2190\n",
      "Epoch [5/25], Class Loss: 3.2190, Discrepancy Loss: 0.0002\n",
      "Validation Loss: 3.2190\n",
      "Epoch [6/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2191\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 19.21%, Precision: 3.84%, Recall: 20.00%, F1 Score: 6.45%\n",
      "Target Domain Performance - Accuracy: 20.53%, Precision: 4.11%, Recall: 20.00%, F1 Score: 6.81%\n",
      "\n",
      "Run 3/5\n",
      "Epoch [1/25], Class Loss: 3.2300, Discrepancy Loss: 0.0091\n",
      "Validation Loss: 3.2188\n",
      "Epoch [2/25], Class Loss: 3.2196, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2186\n",
      "Epoch [3/25], Class Loss: 3.2191, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2188\n",
      "Epoch [4/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2190\n",
      "Epoch [5/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2190\n",
      "Epoch [6/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2191\n",
      "Epoch [7/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2191\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 19.21%, Precision: 3.84%, Recall: 20.00%, F1 Score: 6.45%\n",
      "Target Domain Performance - Accuracy: 20.53%, Precision: 4.11%, Recall: 20.00%, F1 Score: 6.81%\n",
      "\n",
      "Run 4/5\n",
      "Epoch [1/25], Class Loss: 3.2318, Discrepancy Loss: 0.0113\n",
      "Validation Loss: 3.2190\n",
      "Epoch [2/25], Class Loss: 2.9298, Discrepancy Loss: 0.0331\n",
      "Validation Loss: 2.0918\n",
      "Epoch [3/25], Class Loss: 1.2239, Discrepancy Loss: 0.0173\n",
      "Validation Loss: 8.9856\n",
      "Epoch [4/25], Class Loss: 3.6364, Discrepancy Loss: 0.0523\n",
      "Validation Loss: 0.9925\n",
      "Epoch [5/25], Class Loss: 1.6803, Discrepancy Loss: 0.0172\n",
      "Validation Loss: 1.3283\n",
      "Epoch [6/25], Class Loss: 2.5414, Discrepancy Loss: 0.0637\n",
      "Validation Loss: 3.2652\n",
      "Epoch [7/25], Class Loss: 1.0643, Discrepancy Loss: 0.0410\n",
      "Validation Loss: 0.4831\n",
      "Epoch [8/25], Class Loss: 1.1304, Discrepancy Loss: 0.0342\n",
      "Validation Loss: 6.3049\n",
      "Epoch [9/25], Class Loss: 0.6558, Discrepancy Loss: 0.0243\n",
      "Validation Loss: 0.4006\n",
      "Epoch [10/25], Class Loss: 0.6019, Discrepancy Loss: 0.0175\n",
      "Validation Loss: 0.6011\n",
      "Epoch [11/25], Class Loss: 0.3367, Discrepancy Loss: 0.0114\n",
      "Validation Loss: 0.3059\n",
      "Epoch [12/25], Class Loss: 0.2988, Discrepancy Loss: 0.0086\n",
      "Validation Loss: 0.2893\n",
      "Epoch [13/25], Class Loss: 0.2829, Discrepancy Loss: 0.0092\n",
      "Validation Loss: 0.2691\n",
      "Epoch [14/25], Class Loss: 0.2791, Discrepancy Loss: 0.0081\n",
      "Validation Loss: 0.2854\n",
      "Epoch [15/25], Class Loss: 0.2694, Discrepancy Loss: 0.0073\n",
      "Validation Loss: 0.2963\n",
      "Epoch [16/25], Class Loss: 0.2702, Discrepancy Loss: 0.0080\n",
      "Validation Loss: 0.3326\n",
      "Epoch [17/25], Class Loss: 0.2625, Discrepancy Loss: 0.0074\n",
      "Validation Loss: 0.3752\n",
      "Epoch [18/25], Class Loss: 0.2681, Discrepancy Loss: 0.0076\n",
      "Validation Loss: 0.2670\n",
      "Epoch [19/25], Class Loss: 0.2606, Discrepancy Loss: 0.0065\n",
      "Validation Loss: 0.3897\n",
      "Epoch [20/25], Class Loss: 0.2735, Discrepancy Loss: 0.0069\n",
      "Validation Loss: 0.3477\n",
      "Epoch [21/25], Class Loss: 0.2207, Discrepancy Loss: 0.0065\n",
      "Validation Loss: 0.2534\n",
      "Epoch [22/25], Class Loss: 0.2119, Discrepancy Loss: 0.0061\n",
      "Validation Loss: 0.2622\n",
      "Epoch [23/25], Class Loss: 0.2106, Discrepancy Loss: 0.0060\n",
      "Validation Loss: 0.2603\n",
      "Epoch [24/25], Class Loss: 0.2085, Discrepancy Loss: 0.0058\n",
      "Validation Loss: 0.2514\n",
      "Epoch [25/25], Class Loss: 0.2070, Discrepancy Loss: 0.0057\n",
      "Validation Loss: 0.2485\n",
      "Source Domain Performance - Accuracy: 95.39%, Precision: 95.40%, Recall: 95.41%, F1 Score: 95.40%\n",
      "Target Domain Performance - Accuracy: 58.20%, Precision: 42.21%, Recall: 57.86%, F1 Score: 46.76%\n",
      "\n",
      "Run 5/5\n",
      "Epoch [1/25], Class Loss: 3.2292, Discrepancy Loss: 0.0096\n",
      "Validation Loss: 3.2192\n",
      "Epoch [2/25], Class Loss: 3.2204, Discrepancy Loss: 0.0026\n",
      "Validation Loss: 3.2191\n",
      "Epoch [3/25], Class Loss: 3.2191, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2190\n",
      "Epoch [4/25], Class Loss: 3.2191, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2191\n",
      "Epoch [5/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2190\n",
      "Epoch [6/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2191\n",
      "Epoch [7/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2191\n",
      "Epoch [8/25], Class Loss: 3.2190, Discrepancy Loss: 0.0000\n",
      "Validation Loss: 3.2192\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 19.21%, Precision: 3.84%, Recall: 20.00%, F1 Score: 6.45%\n",
      "Target Domain Performance - Accuracy: 20.53%, Precision: 4.11%, Recall: 20.00%, F1 Score: 6.81%\n",
      "\n",
      "Source performance: 34.45% 22.15% 35.08% 24.24%\n",
      "Target performance: 28.07% 11.73% 27.57% 14.80%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 19.98%\n",
      "qpsk: 17.33%\n",
      "4qam: 0.55%\n",
      "16qam: 0.00%\n",
      "apsk: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#%% MCD - Maximum classifier discrepancy for UDA\n",
    "    \n",
    "# Hyperparameters\n",
    "num_classes = n_classes\n",
    "learning_rate = 0.001\n",
    "n_epochs = 25\n",
    "n_runs = 5\n",
    "patience = 5  # For early stopping\n",
    "\n",
    "# Lists to store performance metrics for each run\n",
    "accuracy_s_list, pr_s_list, re_s_list, f1_s_list = [], [], [], []\n",
    "accuracy_t_list, pr_t_list, re_t_list, f1_t_list = [], [], [], []\n",
    "class_accuracies_s = []\n",
    "class_accuracies_t = []\n",
    "\n",
    "def discrepancy_loss(output1, output2):\n",
    "    return torch.mean(torch.abs(F.softmax(output1, dim=1) - F.softmax(output2, dim=1)))\n",
    "\n",
    "def evaluate_model(feature_extractor, classifier1, classifier2, loader, num_classes):\n",
    "    feature_extractor.eval()\n",
    "    classifier1.eval()\n",
    "    classifier2.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            features = feature_extractor(inputs)\n",
    "            outputs1 = classifier1(features)\n",
    "            outputs2 = classifier2(features)\n",
    "            outputs = (outputs1 + outputs2) / 2\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    conf_mat = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    #plt.figure(figsize=(8,6), dpi=300)\n",
    "    #sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "    #            xticklabels=class_subset,\n",
    "    #            yticklabels=class_subset)\n",
    "    #plt.yticks(fontsize=14, rotation=360)\n",
    "    #plt.xticks(fontsize=14, rotation=90)\n",
    "    #plt.title('Confusion Matrix')\n",
    "    #plt.show()\n",
    "    \n",
    "    class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "    return accuracy, precision, recall, f1, class_accuracy\n",
    "\n",
    "def train_model():\n",
    "    # Initialize models\n",
    "    feature_extractor = mcd.CLDNN_G().to(device)\n",
    "    classifier1 = mcd.CLDNN_C(output_dim=num_classes).to(device)\n",
    "    classifier2 = mcd.CLDNN_C(output_dim=num_classes).to(device)\n",
    "    \n",
    "    # Define criterion and optimizers\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_g = optim.Adam(feature_extractor.parameters(), lr=learning_rate)\n",
    "    optimizer_c1 = optim.Adam(classifier1.parameters(), lr=learning_rate)\n",
    "    optimizer_c2 = optim.Adam(classifier2.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    scheduler_g = optim.lr_scheduler.StepLR(optimizer_g, step_size=10, gamma=0.1)\n",
    "    scheduler_c1 = optim.lr_scheduler.StepLR(optimizer_c1, step_size=10, gamma=0.1)\n",
    "    scheduler_c2 = optim.lr_scheduler.StepLR(optimizer_c2, step_size=10, gamma=0.1)\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        feature_extractor.train()\n",
    "        classifier1.train()\n",
    "        classifier2.train()\n",
    "        \n",
    "        running_loss_s = 0.0\n",
    "        running_loss_dis = 0.0\n",
    "        \n",
    "        source_iter = iter(S_train_loader)\n",
    "        target_iter = iter(T_train_loader)\n",
    "        num_batches = min(len(S_train_loader), len(T_train_loader))\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get source batch\n",
    "            inputs_s, labels_s = next(source_iter)\n",
    "            inputs_s, labels_s = inputs_s.to(device), labels_s.to(device)\n",
    "            \n",
    "            # Get target batch\n",
    "            inputs_t, _ = next(target_iter)\n",
    "            inputs_t = inputs_t.to(device)\n",
    "            \n",
    "            # Combine source and target data\n",
    "            inputs = torch.cat([inputs_s, inputs_t], dim=0)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer_g.zero_grad()\n",
    "            optimizer_c1.zero_grad()\n",
    "            optimizer_c2.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            features = feature_extractor(inputs)\n",
    "            features_s = features[:inputs_s.size(0)]\n",
    "            features_t = features[inputs_s.size(0):]\n",
    "            \n",
    "            # Classification outputs for source data\n",
    "            outputs_s1 = classifier1(features_s)\n",
    "            outputs_s2 = classifier2(features_s)\n",
    "            \n",
    "            # Outputs for target data with gradient reversal\n",
    "            outputs_t1 = classifier1(features_t, reverse=True, lambda_=1.0)\n",
    "            outputs_t2 = classifier2(features_t, reverse=True, lambda_=1.0)\n",
    "            \n",
    "            # Compute losses\n",
    "            loss_s1 = criterion(outputs_s1, labels_s)\n",
    "            loss_s2 = criterion(outputs_s2, labels_s)\n",
    "            loss_s = loss_s1 + loss_s2\n",
    "            \n",
    "            loss_dis = discrepancy_loss(outputs_t1, outputs_t2)\n",
    "            \n",
    "            total_loss = loss_s + loss_dis\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(feature_extractor.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(classifier1.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(classifier2.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimization step\n",
    "            optimizer_g.step()\n",
    "            optimizer_c1.step()\n",
    "            optimizer_c2.step()\n",
    "            \n",
    "            running_loss_s += loss_s.item()\n",
    "            running_loss_dis += loss_dis.item()\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler_g.step()\n",
    "        scheduler_c1.step()\n",
    "        scheduler_c2.step()\n",
    "        \n",
    "        # Print average losses for the epoch\n",
    "        avg_loss_s = running_loss_s / num_batches\n",
    "        avg_loss_dis = running_loss_dis / num_batches\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Class Loss: {avg_loss_s:.4f}, Discrepancy Loss: {avg_loss_dis:.4f}')\n",
    "        \n",
    "        # Early stopping based on validation loss on source domain\n",
    "        feature_extractor.eval()\n",
    "        classifier1.eval()\n",
    "        classifier2.eval()\n",
    "        val_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs_s, labels_s in S_val_loader:\n",
    "                inputs_s, labels_s = inputs_s.to(device), labels_s.to(device)\n",
    "                features_s = feature_extractor(inputs_s)\n",
    "                outputs_s1 = classifier1(features_s)\n",
    "                outputs_s2 = classifier2(features_s)\n",
    "                loss_s1 = criterion(outputs_s1, labels_s)\n",
    "                loss_s2 = criterion(outputs_s2, labels_s)\n",
    "                loss_s = loss_s1 + loss_s2\n",
    "                val_loss += loss_s.item() * inputs_s.size(0)\n",
    "                total_samples += inputs_s.size(0)\n",
    "        val_loss = val_loss / total_samples\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "\n",
    "    return feature_extractor, classifier1, classifier2\n",
    "\n",
    "# Run multiple times and collect performance metrics\n",
    "for run in range(n_runs):\n",
    "    print(f'\\nRun {run+1}/{n_runs}')\n",
    "    feature_extractor, classifier1, classifier2 = train_model()\n",
    "    \n",
    "    # Evaluate on source domain\n",
    "    accuracy_s, pr_s, re_s, f1_s, class_acc_s = evaluate_model(feature_extractor, classifier1, classifier2, S_val_loader, num_classes)\n",
    "    print(f'Source Domain Performance - Accuracy: {accuracy_s*100:.2f}%, Precision: {pr_s*100:.2f}%, Recall: {re_s*100:.2f}%, F1 Score: {f1_s*100:.2f}%')\n",
    "    accuracy_s_list.append(accuracy_s)\n",
    "    pr_s_list.append(pr_s)\n",
    "    re_s_list.append(re_s)\n",
    "    f1_s_list.append(f1_s)\n",
    "    class_accuracies_s.append(class_acc_s)\n",
    "    \n",
    "    # Evaluate on target domain\n",
    "    accuracy_t, pr_t, re_t, f1_t, class_acc_t = evaluate_model(feature_extractor, classifier1, classifier2, T_val_loader, num_classes)\n",
    "    print(f'Target Domain Performance - Accuracy: {accuracy_t*100:.2f}%, Precision: {pr_t*100:.2f}%, Recall: {re_t*100:.2f}%, F1 Score: {f1_t*100:.2f}%')\n",
    "    accuracy_t_list.append(accuracy_t)\n",
    "    pr_t_list.append(pr_t)\n",
    "    re_t_list.append(re_t)\n",
    "    f1_t_list.append(f1_t)\n",
    "    class_accuracies_t.append(class_acc_t)\n",
    "\n",
    "# Calculate mean and standard deviation of performance metrics\n",
    "mean_accuracy_s = np.mean(accuracy_s_list)\n",
    "mean_pr_s = np.mean(pr_s_list)\n",
    "mean_re_s = np.mean(re_s_list)\n",
    "mean_f1_s = np.mean(f1_s_list)\n",
    "\n",
    "mean_accuracy_t = np.mean(accuracy_t_list)\n",
    "mean_pr_t = np.mean(pr_t_list)\n",
    "mean_re_t = np.mean(re_t_list)\n",
    "mean_f1_t = np.mean(f1_t_list)\n",
    "\n",
    "mean_class_accuracies_s = np.mean(class_accuracies_s, axis=0)\n",
    "mean_class_accuracies_t = np.mean(class_accuracies_t, axis=0)\n",
    "\n",
    "print(f\"\\nSource performance: {mean_accuracy_s*100:.2f}% {mean_pr_s*100:.2f}% {mean_re_s*100:.2f}% {mean_f1_s*100:.2f}%\")\n",
    "print(f\"Target performance: {mean_accuracy_t*100:.2f}% {mean_pr_t*100:.2f}% {mean_re_t*100:.2f}% {mean_f1_t*100:.2f}%\")\n",
    "\n",
    "print(\"\\nPer-Class Accuracy on Target Domain:\")\n",
    "for i, class_name in enumerate(class_subset):\n",
    "    print(f\"{class_name}: {mean_class_accuracies_t[i]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2fc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1/5\n",
      "Epoch [1/25], Class Loss: 8.7815, Discrepancy Loss: 0.0833\n",
      "Epoch [2/25], Class Loss: 0.5249, Discrepancy Loss: 0.0750\n",
      "Epoch [3/25], Class Loss: 0.4417, Discrepancy Loss: 0.0678\n",
      "Epoch [4/25], Class Loss: 0.3734, Discrepancy Loss: 0.0606\n",
      "Epoch [5/25], Class Loss: 0.3973, Discrepancy Loss: 0.0621\n",
      "Epoch [6/25], Class Loss: 0.6185, Discrepancy Loss: 0.0744\n",
      "Epoch [7/25], Class Loss: 0.3578, Discrepancy Loss: 0.0591\n",
      "Epoch [8/25], Class Loss: 0.3454, Discrepancy Loss: 0.0633\n",
      "Epoch [9/25], Class Loss: 0.3470, Discrepancy Loss: 0.0635\n",
      "Epoch [10/25], Class Loss: 0.3256, Discrepancy Loss: 0.0590\n",
      "Epoch [11/25], Class Loss: 0.2987, Discrepancy Loss: 0.0574\n",
      "Epoch [12/25], Class Loss: 0.2929, Discrepancy Loss: 0.0601\n",
      "Epoch [13/25], Class Loss: 0.2892, Discrepancy Loss: 0.0568\n",
      "Epoch [14/25], Class Loss: 0.2796, Discrepancy Loss: 0.0621\n",
      "Epoch [15/25], Class Loss: 0.2893, Discrepancy Loss: 0.0576\n",
      "Epoch [16/25], Class Loss: 0.2756, Discrepancy Loss: 0.0615\n",
      "Epoch [17/25], Class Loss: 0.2669, Discrepancy Loss: 0.0600\n",
      "Epoch [18/25], Class Loss: 0.2679, Discrepancy Loss: 0.0592\n",
      "Epoch [19/25], Class Loss: 0.2614, Discrepancy Loss: 0.0585\n",
      "Epoch [20/25], Class Loss: 0.2589, Discrepancy Loss: 0.0628\n",
      "Epoch [21/25], Class Loss: 0.2542, Discrepancy Loss: 0.0611\n",
      "Epoch [22/25], Class Loss: 0.2473, Discrepancy Loss: 0.0638\n",
      "Epoch [23/25], Class Loss: 0.2431, Discrepancy Loss: 0.0636\n",
      "Epoch [24/25], Class Loss: 0.2402, Discrepancy Loss: 0.0642\n",
      "Epoch [25/25], Class Loss: 0.2382, Discrepancy Loss: 0.0628\n",
      "Source Domain Performance - Accuracy: 90.26%, Precision: 90.99%, Recall: 90.32%, F1 Score: 90.12%\n",
      "Target Domain Performance - Accuracy: 74.90%, Precision: 80.49%, Recall: 74.37%, F1 Score: 69.89%\n",
      "\n",
      "Run 2/5\n",
      "Epoch [1/25], Class Loss: 9.0069, Discrepancy Loss: 0.1024\n",
      "Epoch [2/25], Class Loss: 0.6750, Discrepancy Loss: 0.0867\n",
      "Epoch [3/25], Class Loss: 0.5254, Discrepancy Loss: 0.0751\n",
      "Epoch [4/25], Class Loss: 0.3730, Discrepancy Loss: 0.0655\n",
      "Epoch [5/25], Class Loss: 0.3509, Discrepancy Loss: 0.0624\n",
      "Epoch [6/25], Class Loss: 0.3599, Discrepancy Loss: 0.0623\n",
      "Epoch [7/25], Class Loss: 0.3708, Discrepancy Loss: 0.0650\n",
      "Epoch [8/25], Class Loss: 0.3173, Discrepancy Loss: 0.0611\n",
      "Epoch [9/25], Class Loss: 0.3138, Discrepancy Loss: 0.0568\n",
      "Epoch [10/25], Class Loss: 0.2904, Discrepancy Loss: 0.0579\n",
      "Epoch [11/25], Class Loss: 0.2689, Discrepancy Loss: 0.0647\n",
      "Epoch [12/25], Class Loss: 0.2530, Discrepancy Loss: 0.0604\n",
      "Epoch [13/25], Class Loss: 0.2510, Discrepancy Loss: 0.0627\n",
      "Epoch [14/25], Class Loss: 0.2554, Discrepancy Loss: 0.0601\n",
      "Epoch [15/25], Class Loss: 0.2405, Discrepancy Loss: 0.0591\n",
      "Epoch [16/25], Class Loss: 0.2394, Discrepancy Loss: 0.0601\n",
      "Epoch [17/25], Class Loss: 0.2226, Discrepancy Loss: 0.0605\n",
      "Epoch [18/25], Class Loss: 0.2196, Discrepancy Loss: 0.0637\n",
      "Epoch [19/25], Class Loss: 0.2214, Discrepancy Loss: 0.0613\n",
      "Epoch [20/25], Class Loss: 0.2183, Discrepancy Loss: 0.0638\n",
      "Epoch [21/25], Class Loss: 0.2113, Discrepancy Loss: 0.0635\n",
      "Epoch [22/25], Class Loss: 0.2055, Discrepancy Loss: 0.0636\n",
      "Epoch [23/25], Class Loss: 0.2000, Discrepancy Loss: 0.0618\n",
      "Epoch [24/25], Class Loss: 0.1993, Discrepancy Loss: 0.0673\n",
      "Epoch [25/25], Class Loss: 0.1969, Discrepancy Loss: 0.0623\n",
      "Source Domain Performance - Accuracy: 89.55%, Precision: 90.29%, Recall: 89.61%, F1 Score: 89.39%\n",
      "Target Domain Performance - Accuracy: 83.76%, Precision: 85.44%, Recall: 83.55%, F1 Score: 83.70%\n",
      "\n",
      "Run 3/5\n",
      "Epoch [1/25], Class Loss: 8.2150, Discrepancy Loss: 0.0896\n",
      "Epoch [2/25], Class Loss: 0.8120, Discrepancy Loss: 0.0812\n",
      "Epoch [3/25], Class Loss: 0.4487, Discrepancy Loss: 0.0656\n",
      "Epoch [4/25], Class Loss: 0.3594, Discrepancy Loss: 0.0646\n",
      "Epoch [5/25], Class Loss: 0.4131, Discrepancy Loss: 0.0684\n",
      "Epoch [6/25], Class Loss: 0.4258, Discrepancy Loss: 0.0668\n",
      "Epoch [7/25], Class Loss: 0.3380, Discrepancy Loss: 0.0614\n",
      "Epoch [8/25], Class Loss: 0.3397, Discrepancy Loss: 0.0619\n",
      "Epoch [9/25], Class Loss: 0.3504, Discrepancy Loss: 0.0672\n",
      "Epoch [10/25], Class Loss: 0.3212, Discrepancy Loss: 0.0606\n",
      "Epoch [11/25], Class Loss: 0.3066, Discrepancy Loss: 0.0559\n",
      "Epoch [12/25], Class Loss: 0.2791, Discrepancy Loss: 0.0565\n",
      "Epoch [13/25], Class Loss: 0.2875, Discrepancy Loss: 0.0591\n",
      "Epoch [14/25], Class Loss: 0.2679, Discrepancy Loss: 0.0592\n",
      "Epoch [15/25], Class Loss: 0.2576, Discrepancy Loss: 0.0581\n",
      "Epoch [16/25], Class Loss: 0.2575, Discrepancy Loss: 0.0606\n",
      "Epoch [17/25], Class Loss: 0.2602, Discrepancy Loss: 0.0612\n",
      "Epoch [18/25], Class Loss: 0.2532, Discrepancy Loss: 0.0599\n",
      "Epoch [19/25], Class Loss: 0.2508, Discrepancy Loss: 0.0621\n",
      "Epoch [20/25], Class Loss: 0.2451, Discrepancy Loss: 0.0643\n",
      "Epoch [21/25], Class Loss: 0.2300, Discrepancy Loss: 0.0625\n",
      "Epoch [22/25], Class Loss: 0.2291, Discrepancy Loss: 0.0640\n",
      "Epoch [23/25], Class Loss: 0.2326, Discrepancy Loss: 0.0643\n",
      "Epoch [24/25], Class Loss: 0.2274, Discrepancy Loss: 0.0623\n",
      "Epoch [25/25], Class Loss: 0.2185, Discrepancy Loss: 0.0638\n",
      "Source Domain Performance - Accuracy: 91.02%, Precision: 92.12%, Recall: 91.08%, F1 Score: 90.84%\n",
      "Target Domain Performance - Accuracy: 76.54%, Precision: 81.47%, Recall: 76.04%, F1 Score: 71.94%\n",
      "\n",
      "Run 4/5\n",
      "Epoch [1/25], Class Loss: 12.2540, Discrepancy Loss: 0.0916\n",
      "Epoch [2/25], Class Loss: 0.6378, Discrepancy Loss: 0.0768\n",
      "Epoch [3/25], Class Loss: 0.4133, Discrepancy Loss: 0.0600\n",
      "Epoch [4/25], Class Loss: 0.4351, Discrepancy Loss: 0.0645\n",
      "Epoch [5/25], Class Loss: 0.3718, Discrepancy Loss: 0.0630\n",
      "Epoch [6/25], Class Loss: 0.3506, Discrepancy Loss: 0.0585\n",
      "Epoch [7/25], Class Loss: 0.3241, Discrepancy Loss: 0.0586\n",
      "Epoch [8/25], Class Loss: 0.3353, Discrepancy Loss: 0.0589\n",
      "Epoch [9/25], Class Loss: 0.3635, Discrepancy Loss: 0.0588\n",
      "Epoch [10/25], Class Loss: 0.4569, Discrepancy Loss: 0.0658\n",
      "Epoch [11/25], Class Loss: 0.3025, Discrepancy Loss: 0.0550\n",
      "Epoch [12/25], Class Loss: 0.3044, Discrepancy Loss: 0.0561\n",
      "Epoch [13/25], Class Loss: 0.2951, Discrepancy Loss: 0.0555\n",
      "Epoch [14/25], Class Loss: 0.2867, Discrepancy Loss: 0.0534\n",
      "Epoch [15/25], Class Loss: 0.2866, Discrepancy Loss: 0.0553\n",
      "Epoch [16/25], Class Loss: 0.2791, Discrepancy Loss: 0.0540\n",
      "Epoch [17/25], Class Loss: 0.2851, Discrepancy Loss: 0.0578\n",
      "Epoch [18/25], Class Loss: 0.2770, Discrepancy Loss: 0.0567\n",
      "Epoch [19/25], Class Loss: 0.2630, Discrepancy Loss: 0.0602\n",
      "Epoch [20/25], Class Loss: 0.2636, Discrepancy Loss: 0.0592\n",
      "Epoch [21/25], Class Loss: 0.2620, Discrepancy Loss: 0.0592\n",
      "Epoch [22/25], Class Loss: 0.2593, Discrepancy Loss: 0.0582\n",
      "Epoch [23/25], Class Loss: 0.2570, Discrepancy Loss: 0.0584\n",
      "Epoch [24/25], Class Loss: 0.2579, Discrepancy Loss: 0.0598\n",
      "Epoch [25/25], Class Loss: 0.2632, Discrepancy Loss: 0.0622\n",
      "Source Domain Performance - Accuracy: 89.06%, Precision: 90.82%, Recall: 89.14%, F1 Score: 88.66%\n",
      "Target Domain Performance - Accuracy: 78.17%, Precision: 82.46%, Recall: 77.59%, F1 Score: 72.29%\n",
      "\n",
      "Run 5/5\n",
      "Epoch [1/25], Class Loss: 9.0232, Discrepancy Loss: 0.0875\n",
      "Epoch [2/25], Class Loss: 0.7060, Discrepancy Loss: 0.0832\n",
      "Epoch [3/25], Class Loss: 0.4288, Discrepancy Loss: 0.0728\n",
      "Epoch [4/25], Class Loss: 0.3874, Discrepancy Loss: 0.0580\n",
      "Epoch [5/25], Class Loss: 0.3732, Discrepancy Loss: 0.0586\n",
      "Epoch [6/25], Class Loss: 0.3639, Discrepancy Loss: 0.0563\n",
      "Epoch [7/25], Class Loss: 0.3589, Discrepancy Loss: 0.0573\n",
      "Epoch [8/25], Class Loss: 0.3363, Discrepancy Loss: 0.0589\n",
      "Epoch [9/25], Class Loss: 0.3577, Discrepancy Loss: 0.0631\n",
      "Epoch [10/25], Class Loss: 0.3246, Discrepancy Loss: 0.0641\n",
      "Epoch [11/25], Class Loss: 0.2905, Discrepancy Loss: 0.0604\n",
      "Epoch [12/25], Class Loss: 0.2840, Discrepancy Loss: 0.0592\n",
      "Epoch [13/25], Class Loss: 0.2858, Discrepancy Loss: 0.0554\n",
      "Epoch [14/25], Class Loss: 0.2776, Discrepancy Loss: 0.0591\n",
      "Epoch [15/25], Class Loss: 0.2729, Discrepancy Loss: 0.0583\n",
      "Epoch [16/25], Class Loss: 0.2617, Discrepancy Loss: 0.0555\n",
      "Epoch [17/25], Class Loss: 0.2567, Discrepancy Loss: 0.0588\n",
      "Epoch [18/25], Class Loss: 0.2574, Discrepancy Loss: 0.0596\n",
      "Epoch [19/25], Class Loss: 0.2500, Discrepancy Loss: 0.0605\n",
      "Epoch [20/25], Class Loss: 0.2415, Discrepancy Loss: 0.0551\n",
      "Epoch [21/25], Class Loss: 0.2402, Discrepancy Loss: 0.0589\n",
      "Epoch [22/25], Class Loss: 0.2260, Discrepancy Loss: 0.0576\n",
      "Epoch [23/25], Class Loss: 0.2195, Discrepancy Loss: 0.0621\n",
      "Epoch [24/25], Class Loss: 0.2190, Discrepancy Loss: 0.0612\n",
      "Epoch [25/25], Class Loss: 0.2182, Discrepancy Loss: 0.0591\n",
      "Source Domain Performance - Accuracy: 90.21%, Precision: 92.16%, Recall: 90.28%, F1 Score: 89.89%\n",
      "Target Domain Performance - Accuracy: 79.86%, Precision: 86.30%, Recall: 79.41%, F1 Score: 77.31%\n",
      "\n",
      "Source performance: 90.02% 91.28% 90.08% 89.78%\n",
      "Target performance: 78.65% 83.23% 78.19% 75.03%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 99.88%\n",
      "qpsk: 88.06%\n",
      "4qam: 81.54%\n",
      "16qam: 21.49%\n",
      "apsk: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#%% STAR - Stochastic classifier for UDA\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = n_classes\n",
    "learning_rate = 0.001\n",
    "n_epochs = 25\n",
    "n_runs = 5\n",
    "patience = 5\n",
    "\n",
    "# Lists to store performance metrics for each run\n",
    "accuracy_s_list, pr_s_list, re_s_list, f1_s_list = [], [], [], []\n",
    "accuracy_t_list, pr_t_list, re_t_list, f1_t_list = [], [], [], []\n",
    "class_accuracies_s = []\n",
    "class_accuracies_t = []\n",
    "\n",
    "def discrepancy_loss(out1, out2):\n",
    "    return torch.mean(torch.abs(F.softmax(out1, dim=1) - F.softmax(out2, dim=1)))\n",
    "\n",
    "def evaluate_model(feature_extractor, classifier, loader, num_classes):\n",
    "    feature_extractor.eval()\n",
    "    classifier.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            features = feature_extractor(inputs)\n",
    "            outputs_list = classifier(features, only_mu=True)\n",
    "            outputs = outputs_list[0]\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    conf_mat = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    #plt.figure(figsize=(8,6), dpi=300)\n",
    "    #sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "    #            xticklabels=class_subset,\n",
    "    #            yticklabels=class_subset)\n",
    "    #plt.yticks(fontsize=14, rotation=360)\n",
    "    #plt.xticks(fontsize=14, rotation=90)\n",
    "    #plt.title('Confusion Matrix')\n",
    "    #plt.show()\n",
    "    \n",
    "    class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "    return accuracy, precision, recall, f1, class_accuracy\n",
    "\n",
    "def train_model():\n",
    "    # Initialize models\n",
    "    feature_extractor = star.CLDNN_G().to(device)\n",
    "    classifier = star.CLDNN_C(output_dim=num_classes).to(device)\n",
    "    \n",
    "    # Define criterion and optimizers\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_g = optim.Adam(feature_extractor.parameters(), lr=learning_rate)\n",
    "    optimizer_c = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate schedulers (optional)\n",
    "    scheduler_g = optim.lr_scheduler.StepLR(optimizer_g, step_size=10, gamma=0.1)\n",
    "    scheduler_c = optim.lr_scheduler.StepLR(optimizer_c, step_size=10, gamma=0.1)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        feature_extractor.train()\n",
    "        classifier.train()\n",
    "        \n",
    "        running_loss_s = 0.0\n",
    "        running_loss_dis = 0.0\n",
    "        \n",
    "        source_iter = iter(S_train_loader)\n",
    "        target_iter = iter(T_train_loader)\n",
    "        num_batches = min(len(S_train_loader), len(T_train_loader))\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            ##############################\n",
    "            # Step 1: Update G and C using source data\n",
    "            ##############################\n",
    "            # Unfreeze all parameters\n",
    "            for param in feature_extractor.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            optimizer_g.zero_grad()\n",
    "            optimizer_c.zero_grad()\n",
    "            \n",
    "            # Get source batch\n",
    "            inputs_s, labels_s = next(source_iter)\n",
    "            inputs_s, labels_s = inputs_s.to(device), labels_s.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            features_s = feature_extractor(inputs_s)\n",
    "            outputs_s_list = classifier(features_s)\n",
    "            \n",
    "            # Compute classification loss on source data\n",
    "            loss_s = 0\n",
    "            for outputs_s in outputs_s_list:\n",
    "                loss_s += criterion(outputs_s, labels_s)\n",
    "            loss_s /= len(outputs_s_list)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss_s.backward()\n",
    "            optimizer_g.step()\n",
    "            optimizer_c.step()\n",
    "            \n",
    "            ##############################\n",
    "            # Step 2: Update classifiers C using target data to maximize discrepancy\n",
    "            ##############################\n",
    "            # Freeze feature extractor\n",
    "            for param in feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Unfreeze classifier\n",
    "            for param in classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            optimizer_c.zero_grad()\n",
    "            \n",
    "            # Get target batch\n",
    "            inputs_t, _ = next(target_iter)\n",
    "            inputs_t = inputs_t.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                features_t = feature_extractor(inputs_t)\n",
    "            outputs_t_list = classifier(features_t)\n",
    "            \n",
    "            # Compute discrepancy loss between classifiers\n",
    "            loss_dis = 0\n",
    "            num_classifiers = classifier.num_classifiers_train\n",
    "            for i in range(num_classifiers):\n",
    "                for j in range(i + 1, num_classifiers):\n",
    "                    loss_dis += discrepancy_loss(outputs_t_list[i], outputs_t_list[j])\n",
    "            num_pairs = num_classifiers * (num_classifiers - 1) / 2\n",
    "            loss_dis = loss_dis / num_pairs\n",
    "            \n",
    "            # Maximize discrepancy by minimizing negative loss\n",
    "            loss_dis = -loss_dis\n",
    "            loss_dis.backward()\n",
    "            optimizer_c.step()\n",
    "            \n",
    "            ##############################\n",
    "            # Step 3: Update generator G using target data to minimize discrepancy\n",
    "            ##############################\n",
    "            # Unfreeze feature extractor\n",
    "            for param in feature_extractor.parameters():\n",
    "                param.requires_grad = True\n",
    "            # Freeze classifier\n",
    "            for param in classifier.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            optimizer_g.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            features_t = feature_extractor(inputs_t)\n",
    "            outputs_t_list = classifier(features_t)\n",
    "            \n",
    "            # Compute discrepancy loss between classifiers\n",
    "            loss_dis = 0\n",
    "            for i in range(num_classifiers):\n",
    "                for j in range(i + 1, num_classifiers):\n",
    "                    loss_dis += discrepancy_loss(outputs_t_list[i], outputs_t_list[j])\n",
    "            loss_dis = loss_dis / num_pairs\n",
    "            \n",
    "            # Minimize discrepancy\n",
    "            loss_dis.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            ##############################\n",
    "            # Reset requires_grad for next iteration\n",
    "            ##############################\n",
    "            # Unfreeze all parameters for next iteration\n",
    "            for param in feature_extractor.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            # Update running losses\n",
    "            running_loss_s += loss_s.item()\n",
    "            running_loss_dis += loss_dis.item()\n",
    "        \n",
    "        # Learning rate scheduler step\n",
    "        scheduler_g.step()\n",
    "        scheduler_c.step()\n",
    "        \n",
    "        # Print average losses for the epoch\n",
    "        avg_loss_s = running_loss_s / num_batches\n",
    "        avg_loss_dis = running_loss_dis / num_batches\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Class Loss: {avg_loss_s:.4f}, Discrepancy Loss: {avg_loss_dis:.4f}')\n",
    "        \n",
    "        # Early stopping or validation steps can be added here if necessary\n",
    "        \n",
    "    return feature_extractor, classifier\n",
    "\n",
    "# Run multiple times and collect performance metrics\n",
    "for run in range(n_runs):\n",
    "    print(f'\\nRun {run+1}/{n_runs}')\n",
    "    feature_extractor, classifier = train_model()\n",
    "    \n",
    "    # Evaluate on source domain\n",
    "    accuracy_s, pr_s, re_s, f1_s, class_acc_s = evaluate_model(feature_extractor, classifier, S_val_loader, num_classes)\n",
    "    print(f'Source Domain Performance - Accuracy: {accuracy_s*100:.2f}%, Precision: {pr_s*100:.2f}%, Recall: {re_s*100:.2f}%, F1 Score: {f1_s*100:.2f}%')\n",
    "    accuracy_s_list.append(accuracy_s)\n",
    "    pr_s_list.append(pr_s)\n",
    "    re_s_list.append(re_s)\n",
    "    f1_s_list.append(f1_s)\n",
    "    class_accuracies_s.append(class_acc_s)\n",
    "    \n",
    "    # Evaluate on target domain\n",
    "    accuracy_t, pr_t, re_t, f1_t, class_acc_t = evaluate_model(feature_extractor, classifier, T_val_loader, num_classes)\n",
    "    print(f'Target Domain Performance - Accuracy: {accuracy_t*100:.2f}%, Precision: {pr_t*100:.2f}%, Recall: {re_t*100:.2f}%, F1 Score: {f1_t*100:.2f}%')\n",
    "    accuracy_t_list.append(accuracy_t)\n",
    "    pr_t_list.append(pr_t)\n",
    "    re_t_list.append(re_t)\n",
    "    f1_t_list.append(f1_t)\n",
    "    class_accuracies_t.append(class_acc_t)\n",
    "\n",
    "\n",
    "# Calculate mean and standard deviation of performance metrics\n",
    "mean_accuracy_s = np.mean(accuracy_s_list)\n",
    "mean_pr_s = np.mean(pr_s_list)\n",
    "mean_re_s = np.mean(re_s_list)\n",
    "mean_f1_s = np.mean(f1_s_list)\n",
    "\n",
    "mean_accuracy_t = np.mean(accuracy_t_list)\n",
    "mean_pr_t = np.mean(pr_t_list)\n",
    "mean_re_t = np.mean(re_t_list)\n",
    "mean_f1_t = np.mean(f1_t_list)\n",
    "\n",
    "mean_class_accuracies_s = np.mean(class_accuracies_s, axis=0)\n",
    "mean_class_accuracies_t = np.mean(class_accuracies_t, axis=0)\n",
    "\n",
    "print(f\"\\nSource performance: {mean_accuracy_s*100:.2f}% {mean_pr_s*100:.2f}% {mean_re_s*100:.2f}% {mean_f1_s*100:.2f}%\")\n",
    "print(f\"Target performance: {mean_accuracy_t*100:.2f}% {mean_pr_t*100:.2f}% {mean_re_t*100:.2f}% {mean_f1_t*100:.2f}%\")\n",
    "\n",
    "print(\"\\nPer-Class Accuracy on Target Domain:\")\n",
    "for i, class_name in enumerate(class_subset):\n",
    "    print(f\"{class_name}: {mean_class_accuracies_t[i]*100:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e7ada4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1/5\n",
      "Epoch [1/25], Class Loss: 1.6197, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6064\n",
      "Epoch [2/25], Class Loss: 1.3407, CORAL Loss: 0.0046\n",
      "Validation Loss: 6.1286\n",
      "Epoch [3/25], Class Loss: 0.5916, CORAL Loss: 0.0110\n",
      "Validation Loss: 0.2970\n",
      "Epoch [4/25], Class Loss: 0.2980, CORAL Loss: 0.0120\n",
      "Validation Loss: 0.2902\n",
      "Epoch [5/25], Class Loss: 0.2891, CORAL Loss: 0.0062\n",
      "Validation Loss: 0.3060\n",
      "Epoch [6/25], Class Loss: 0.2795, CORAL Loss: 0.0041\n",
      "Validation Loss: 0.2859\n",
      "Epoch [7/25], Class Loss: 0.4918, CORAL Loss: 0.0091\n",
      "Validation Loss: 0.2930\n",
      "Epoch [8/25], Class Loss: 0.2812, CORAL Loss: 0.0081\n",
      "Validation Loss: 0.2937\n",
      "Epoch [9/25], Class Loss: 0.2807, CORAL Loss: 0.0048\n",
      "Validation Loss: 0.3113\n",
      "Epoch [10/25], Class Loss: 0.2736, CORAL Loss: 0.0032\n",
      "Validation Loss: 0.2592\n",
      "Epoch [11/25], Class Loss: 0.2547, CORAL Loss: 0.0024\n",
      "Validation Loss: 0.2519\n",
      "Epoch [12/25], Class Loss: 0.2538, CORAL Loss: 0.0027\n",
      "Validation Loss: 0.2530\n",
      "Epoch [13/25], Class Loss: 0.2553, CORAL Loss: 0.0028\n",
      "Validation Loss: 0.2593\n",
      "Epoch [14/25], Class Loss: 0.2551, CORAL Loss: 0.0021\n",
      "Validation Loss: 0.2693\n",
      "Epoch [15/25], Class Loss: 0.2527, CORAL Loss: 0.0018\n",
      "Validation Loss: 0.2489\n",
      "Epoch [16/25], Class Loss: 0.2564, CORAL Loss: 0.0022\n",
      "Validation Loss: 0.2489\n",
      "Epoch [17/25], Class Loss: 0.2522, CORAL Loss: 0.0020\n",
      "Validation Loss: 0.2525\n",
      "Epoch [18/25], Class Loss: 0.2585, CORAL Loss: 0.0020\n",
      "Validation Loss: 0.2486\n",
      "Epoch [19/25], Class Loss: 0.2550, CORAL Loss: 0.0020\n",
      "Validation Loss: 0.2539\n",
      "Epoch [20/25], Class Loss: 0.2571, CORAL Loss: 0.0020\n",
      "Validation Loss: 0.2689\n",
      "Epoch [21/25], Class Loss: 0.2494, CORAL Loss: 0.0020\n",
      "Validation Loss: 0.2482\n",
      "Epoch [22/25], Class Loss: 0.2469, CORAL Loss: 0.0020\n",
      "Validation Loss: 0.2479\n",
      "Epoch [23/25], Class Loss: 0.2466, CORAL Loss: 0.0019\n",
      "Validation Loss: 0.2474\n",
      "Epoch [24/25], Class Loss: 0.2451, CORAL Loss: 0.0021\n",
      "Validation Loss: 0.2480\n",
      "Epoch [25/25], Class Loss: 0.2450, CORAL Loss: 0.0023\n",
      "Validation Loss: 0.2493\n",
      "Source Domain Performance - Accuracy: 87.23%, Precision: 87.77%, Recall: 87.30%, F1 Score: 87.09%\n",
      "Target Domain Performance - Accuracy: 73.83%, Precision: 62.78%, Recall: 73.22%, F1 Score: 66.87%\n",
      "\n",
      "Run 2/5\n",
      "Epoch [1/25], Class Loss: 1.6186, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6102\n",
      "Epoch [2/25], Class Loss: 1.6098, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6097\n",
      "Epoch [3/25], Class Loss: 1.6096, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [4/25], Class Loss: 1.6095, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [5/25], Class Loss: 1.6095, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [6/25], Class Loss: 1.6095, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [7/25], Class Loss: 1.6095, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [8/25], Class Loss: 1.6095, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [9/25], Class Loss: 1.6095, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6095\n",
      "Epoch [10/25], Class Loss: 1.6095, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [11/25], Class Loss: 1.6094, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [12/25], Class Loss: 1.6094, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [13/25], Class Loss: 1.6094, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [14/25], Class Loss: 1.6094, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 19.21%, Precision: 3.84%, Recall: 20.00%, F1 Score: 6.45%\n",
      "Target Domain Performance - Accuracy: 20.53%, Precision: 4.11%, Recall: 20.00%, F1 Score: 6.81%\n",
      "\n",
      "Run 3/5\n",
      "Epoch [1/25], Class Loss: 1.6182, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6096\n",
      "Epoch [2/25], Class Loss: 1.6104, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6050\n",
      "Epoch [3/25], Class Loss: 1.3466, CORAL Loss: 0.0056\n",
      "Validation Loss: 0.4561\n",
      "Epoch [4/25], Class Loss: 0.4061, CORAL Loss: 0.0238\n",
      "Validation Loss: 0.3002\n",
      "Epoch [5/25], Class Loss: 0.3636, CORAL Loss: 0.0105\n",
      "Validation Loss: 0.3993\n",
      "Epoch [6/25], Class Loss: 0.2920, CORAL Loss: 0.0085\n",
      "Validation Loss: 0.2869\n",
      "Epoch [7/25], Class Loss: 0.2886, CORAL Loss: 0.0054\n",
      "Validation Loss: 0.2779\n",
      "Epoch [8/25], Class Loss: 0.2839, CORAL Loss: 0.0040\n",
      "Validation Loss: 0.2726\n",
      "Epoch [9/25], Class Loss: 0.6355, CORAL Loss: 0.0033\n",
      "Validation Loss: 0.3216\n",
      "Epoch [10/25], Class Loss: 0.4352, CORAL Loss: 0.0072\n",
      "Validation Loss: 0.2989\n",
      "Epoch [11/25], Class Loss: 0.2856, CORAL Loss: 0.0085\n",
      "Validation Loss: 0.2929\n",
      "Epoch [12/25], Class Loss: 0.2815, CORAL Loss: 0.0084\n",
      "Validation Loss: 0.2881\n",
      "Epoch [13/25], Class Loss: 0.2778, CORAL Loss: 0.0079\n",
      "Validation Loss: 0.2847\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 86.65%, Precision: 86.91%, Recall: 86.68%, F1 Score: 86.54%\n",
      "Target Domain Performance - Accuracy: 67.24%, Precision: 72.52%, Recall: 66.61%, F1 Score: 60.66%\n",
      "\n",
      "Run 4/5\n",
      "Epoch [1/25], Class Loss: 1.6258, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6098\n",
      "Epoch [2/25], Class Loss: 1.5452, CORAL Loss: 0.0005\n",
      "Validation Loss: 0.7137\n",
      "Epoch [3/25], Class Loss: 0.4773, CORAL Loss: 0.0142\n",
      "Validation Loss: 0.3072\n",
      "Epoch [4/25], Class Loss: 0.3548, CORAL Loss: 0.0084\n",
      "Validation Loss: 0.3364\n",
      "Epoch [5/25], Class Loss: 0.2934, CORAL Loss: 0.0050\n",
      "Validation Loss: 0.2733\n",
      "Epoch [6/25], Class Loss: 0.3682, CORAL Loss: 0.0066\n",
      "Validation Loss: 0.2875\n",
      "Epoch [7/25], Class Loss: 0.2804, CORAL Loss: 0.0044\n",
      "Validation Loss: 0.2820\n",
      "Epoch [8/25], Class Loss: 0.4112, CORAL Loss: 0.0021\n",
      "Validation Loss: 0.9499\n",
      "Epoch [9/25], Class Loss: 0.4413, CORAL Loss: 0.0053\n",
      "Validation Loss: 0.6619\n",
      "Epoch [10/25], Class Loss: 0.2919, CORAL Loss: 0.0041\n",
      "Validation Loss: 0.2633\n",
      "Epoch [11/25], Class Loss: 0.2586, CORAL Loss: 0.0021\n",
      "Validation Loss: 0.2561\n",
      "Epoch [12/25], Class Loss: 0.2556, CORAL Loss: 0.0017\n",
      "Validation Loss: 0.2565\n",
      "Epoch [13/25], Class Loss: 0.2563, CORAL Loss: 0.0017\n",
      "Validation Loss: 0.2494\n",
      "Epoch [14/25], Class Loss: 0.2540, CORAL Loss: 0.0016\n",
      "Validation Loss: 0.2546\n",
      "Epoch [15/25], Class Loss: 0.2563, CORAL Loss: 0.0017\n",
      "Validation Loss: 0.2476\n",
      "Epoch [16/25], Class Loss: 0.2545, CORAL Loss: 0.0015\n",
      "Validation Loss: 0.2468\n",
      "Epoch [17/25], Class Loss: 0.2541, CORAL Loss: 0.0016\n",
      "Validation Loss: 0.2521\n",
      "Epoch [18/25], Class Loss: 0.2556, CORAL Loss: 0.0016\n",
      "Validation Loss: 0.2639\n",
      "Epoch [19/25], Class Loss: 0.2512, CORAL Loss: 0.0014\n",
      "Validation Loss: 0.2510\n",
      "Epoch [20/25], Class Loss: 0.2527, CORAL Loss: 0.0015\n",
      "Validation Loss: 0.2494\n",
      "Epoch [21/25], Class Loss: 0.2483, CORAL Loss: 0.0014\n",
      "Validation Loss: 0.2450\n",
      "Epoch [22/25], Class Loss: 0.2463, CORAL Loss: 0.0015\n",
      "Validation Loss: 0.2443\n",
      "Epoch [23/25], Class Loss: 0.2465, CORAL Loss: 0.0015\n",
      "Validation Loss: 0.2441\n",
      "Epoch [24/25], Class Loss: 0.2472, CORAL Loss: 0.0016\n",
      "Validation Loss: 0.2447\n",
      "Epoch [25/25], Class Loss: 0.2455, CORAL Loss: 0.0017\n",
      "Validation Loss: 0.2443\n",
      "Source Domain Performance - Accuracy: 87.43%, Precision: 87.68%, Recall: 87.46%, F1 Score: 87.37%\n",
      "Target Domain Performance - Accuracy: 57.20%, Precision: 46.01%, Recall: 56.73%, F1 Score: 49.43%\n",
      "\n",
      "Run 5/5\n",
      "Epoch [1/25], Class Loss: 1.6171, CORAL Loss: 0.0000\n",
      "Validation Loss: 1.6165\n",
      "Epoch [2/25], Class Loss: 1.3646, CORAL Loss: 0.0036\n",
      "Validation Loss: 0.3606\n",
      "Epoch [3/25], Class Loss: 0.6292, CORAL Loss: 0.0168\n",
      "Validation Loss: 0.3002\n",
      "Epoch [4/25], Class Loss: 0.2969, CORAL Loss: 0.0111\n",
      "Validation Loss: 0.3136\n",
      "Epoch [5/25], Class Loss: 0.3726, CORAL Loss: 0.0055\n",
      "Validation Loss: 0.2964\n",
      "Epoch [6/25], Class Loss: 0.2846, CORAL Loss: 0.0075\n",
      "Validation Loss: 0.3218\n",
      "Epoch [7/25], Class Loss: 0.2816, CORAL Loss: 0.0043\n",
      "Validation Loss: 0.2735\n",
      "Epoch [8/25], Class Loss: 0.2724, CORAL Loss: 0.0031\n",
      "Validation Loss: 0.2947\n",
      "Epoch [9/25], Class Loss: 0.2776, CORAL Loss: 0.0026\n",
      "Validation Loss: 0.3621\n",
      "Epoch [10/25], Class Loss: 0.2784, CORAL Loss: 0.0023\n",
      "Validation Loss: 0.2981\n",
      "Epoch [11/25], Class Loss: 0.2599, CORAL Loss: 0.0020\n",
      "Validation Loss: 0.2681\n",
      "Epoch [12/25], Class Loss: 0.2570, CORAL Loss: 0.0030\n",
      "Validation Loss: 0.2575\n",
      "Epoch [13/25], Class Loss: 0.2556, CORAL Loss: 0.0027\n",
      "Validation Loss: 0.2548\n",
      "Epoch [14/25], Class Loss: 0.2543, CORAL Loss: 0.0028\n",
      "Validation Loss: 0.2510\n",
      "Epoch [15/25], Class Loss: 0.2522, CORAL Loss: 0.0022\n",
      "Validation Loss: 0.2497\n",
      "Epoch [16/25], Class Loss: 0.2514, CORAL Loss: 0.0028\n",
      "Validation Loss: 0.2483\n",
      "Epoch [17/25], Class Loss: 0.2574, CORAL Loss: 0.0023\n",
      "Validation Loss: 0.2621\n",
      "Epoch [18/25], Class Loss: 0.2552, CORAL Loss: 0.0025\n",
      "Validation Loss: 0.2486\n",
      "Epoch [19/25], Class Loss: 0.2517, CORAL Loss: 0.0027\n",
      "Validation Loss: 0.2516\n",
      "Epoch [20/25], Class Loss: 0.2541, CORAL Loss: 0.0023\n",
      "Validation Loss: 0.2730\n",
      "Epoch [21/25], Class Loss: 0.2465, CORAL Loss: 0.0018\n",
      "Validation Loss: 0.2495\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 86.87%, Precision: 87.16%, Recall: 86.93%, F1 Score: 86.80%\n",
      "Target Domain Performance - Accuracy: 76.42%, Precision: 65.34%, Recall: 75.76%, F1 Score: 69.44%\n"
     ]
    }
   ],
   "source": [
    "#%% CORAL - Deep coral\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = n_classes\n",
    "lr = 0.001\n",
    "n_epochs = 25\n",
    "n_runs = 5\n",
    "patience = 5\n",
    "lambda_coral = 1\n",
    "\n",
    "# Initialize lists to store performance metrics for each run\n",
    "accuracy_s_list, pr_s_list, re_s_list, f1_s_list = [], [], [], []\n",
    "accuracy_t_list, pr_t_list, re_t_list, f1_t_list = [], [], [], []\n",
    "class_accuracies_s = []\n",
    "class_accuracies_t = []\n",
    "\n",
    "def train_model():\n",
    "    feature_extractor = coral.CLDNN_G().to(device)\n",
    "    classifier = coral.CLDNN_C(output_dim=num_classes).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(list(feature_extractor.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        feature_extractor.train()\n",
    "        classifier.train()\n",
    "        \n",
    "        running_classification_loss = 0.0\n",
    "        running_coral_loss = 0.0\n",
    "        \n",
    "        source_iter = iter(S_train_loader)\n",
    "        target_iter = iter(T_train_loader)\n",
    "        num_batches = min(len(S_train_loader), len(T_train_loader))\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            try:\n",
    "                inputs_s, labels_s = next(source_iter)\n",
    "            except StopIteration:\n",
    "                source_iter = iter(S_train_loader)\n",
    "                inputs_s, labels_s = next(source_iter)\n",
    "            try:\n",
    "                inputs_t, _ = next(target_iter)\n",
    "            except StopIteration:\n",
    "                target_iter = iter(T_train_loader)\n",
    "                inputs_t, _ = next(target_iter)\n",
    "            \n",
    "            inputs_s, labels_s = inputs_s.to(device), labels_s.to(device)\n",
    "            inputs_t = inputs_t.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            features_s = feature_extractor(inputs_s)\n",
    "            features_t = feature_extractor(inputs_t)\n",
    "            outputs_s = classifier(features_s)\n",
    "            \n",
    "            classification_loss = criterion(outputs_s, labels_s)\n",
    "            \n",
    "            coral_loss_value = coral.coral(features_s, features_t)\n",
    "            \n",
    "            total_loss = classification_loss + lambda_coral * coral_loss_value\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_classification_loss += classification_loss.item()\n",
    "            running_coral_loss += coral_loss_value.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_classification_loss = running_classification_loss / num_batches\n",
    "        avg_coral_loss = running_coral_loss / num_batches\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Class Loss: {avg_classification_loss:.4f}, CORAL Loss: {avg_coral_loss:.4f}')\n",
    "        \n",
    "        feature_extractor.eval()\n",
    "        classifier.eval()\n",
    "        val_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs_s, labels_s in S_val_loader:\n",
    "                inputs_s, labels_s = inputs_s.to(device), labels_s.to(device)\n",
    "                features_s = feature_extractor(inputs_s)\n",
    "                outputs_s = classifier(features_s)\n",
    "                loss_s = criterion(outputs_s, labels_s)\n",
    "                val_loss += loss_s.item() * inputs_s.size(0)\n",
    "                total_samples += inputs_s.size(0)\n",
    "        val_loss = val_loss / total_samples\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            trigger_times = 0\n",
    "        else:\n",
    "            trigger_times += 1\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!')\n",
    "                break\n",
    "            \n",
    "    return feature_extractor, classifier\n",
    "\n",
    "def evaluate_model(feature_extractor, classifier, loader, num_classes):\n",
    "    feature_extractor.eval()\n",
    "    classifier.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            features = feature_extractor(inputs)\n",
    "            outputs = classifier(features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    conf_mat = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    #plt.figure(figsize=(8,6), dpi=300)\n",
    "    #sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "     #           xticklabels=class_subset,\n",
    "     #           yticklabels=class_subset)\n",
    "    #plt.yticks(fontsize=14, rotation=360)\n",
    "    #plt.xticks(fontsize=14, rotation=90)\n",
    "    #plt.title('Confusion Matrix')\n",
    "    #plt.show()\n",
    "    \n",
    "    class_accuracy = conf_mat.diagonal() / conf_mat.sum(axis=1)\n",
    "    return accuracy, precision, recall, f1, class_accuracy\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f'\\nRun {run+1}/{n_runs}')\n",
    "    feature_extractor, classifier = train_model()\n",
    "    \n",
    "    # Evaluate on source domain\n",
    "    accuracy_s, pr_s, re_s, f1_s, class_acc_s = evaluate_model(feature_extractor, classifier, S_val_loader, num_classes)\n",
    "    print(f'Source Domain Performance - Accuracy: {accuracy_s*100:.2f}%, Precision: {pr_s*100:.2f}%, Recall: {re_s*100:.2f}%, F1 Score: {f1_s*100:.2f}%')\n",
    "    accuracy_s_list.append(accuracy_s)\n",
    "    pr_s_list.append(pr_s)\n",
    "    re_s_list.append(re_s)\n",
    "    f1_s_list.append(f1_s)\n",
    "    class_accuracies_s.append(class_acc_s)\n",
    "    \n",
    "    # Evaluate on target domain\n",
    "    accuracy_t, pr_t, re_t, f1_t, class_acc_t = evaluate_model(feature_extractor, classifier, T_val_loader, num_classes)\n",
    "    print(f'Target Domain Performance - Accuracy: {accuracy_t*100:.2f}%, Precision: {pr_t*100:.2f}%, Recall: {re_t*100:.2f}%, F1 Score: {f1_t*100:.2f}%')\n",
    "    accuracy_t_list.append(accuracy_t)\n",
    "    pr_t_list.append(pr_t)\n",
    "    re_t_list.append(re_t)\n",
    "    f1_t_list.append(f1_t)\n",
    "    class_accuracies_t.append(class_acc_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
