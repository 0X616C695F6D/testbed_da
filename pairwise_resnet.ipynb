{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b5599b2-801f-4761-a8f7-f920da044951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ResNet for UDA\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import funcs\n",
    "import jan\n",
    "import coral\n",
    "import star\n",
    "import mcd\n",
    "import dann\n",
    "import base\n",
    "import plots\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Function\n",
    "\n",
    "#%% ResNet block\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        \"\"\"\n",
    "        A basic residual block for 1D convolutions.\n",
    "        \"\"\"\n",
    "        super(ResidualBlock1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "#%% Base\n",
    "class DeepResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A deep ResNet classifier for 1D signals.\n",
    "    \n",
    "    This network consists of:\n",
    "      - An initial convolution + BN + ReLU + maxpool block.\n",
    "      - Four residual layers (with increasing feature channels).\n",
    "      - Global average pooling to obtain a fixed–length feature vector.\n",
    "      - A bottleneck fully connected layer mapping to 512–dimensions.\n",
    "      - A small classifier head (MLP) mapping to the desired output classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim=7):\n",
    "        super(DeepResNet, self).__init__()\n",
    "        # Initial convolutional block\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=7,\n",
    "                               stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(in_channels=64, out_channels=64, blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(in_channels=64, out_channels=128, blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(in_channels=128, out_channels=256, blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(in_channels=256, out_channels=512, blocks=2, stride=2)\n",
    "        \n",
    "        # Global average pooling to obtain a fixed-length feature vector\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        # Bottleneck fully-connected layer: maps 512-dim to 512-dim features\n",
    "        self.fc_bottleneck = nn.Linear(512, 512)\n",
    "        \n",
    "        # Classifier head (MLP)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.bn_fc = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        \"\"\"\n",
    "        Creates a sequential layer composed of multiple residual blocks.\n",
    "        If the stride is not 1 or the number of channels change,\n",
    "        a downsampling layer is used.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock1D(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock1D(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 2, length)\n",
    "        x = self.conv1(x)    # (B, 64, L/2)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)  # (B, 64, L/4)\n",
    "        \n",
    "        x = self.layer1(x)   # (B, 64, ?)\n",
    "        x = self.layer2(x)   # (B, 128, ?)\n",
    "        x = self.layer3(x)   # (B, 256, ?)\n",
    "        x = self.layer4(x)   # (B, 512, ?)\n",
    "        \n",
    "        x = self.avgpool(x)  # (B, 512, 1)\n",
    "        x = x.squeeze(-1)    # (B, 512)\n",
    "        x = self.fc_bottleneck(x)  # (B, 512)\n",
    "        \n",
    "        # Classification head\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#%% DANN\n",
    "class GradReverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.lambda_, None\n",
    "\n",
    "def grad_reverse(x, lambda_=1.0):\n",
    "    return GradReverse.apply(x, lambda_)\n",
    "\n",
    "class DANN_F(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep ResNet feature extractor for DANN.\n",
    "    \n",
    "    This network accepts a 2–channel 1D signal and outputs a 512–dimensional feature vector.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DANN_F, self).__init__()\n",
    "        # Initial convolutional block\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=7,\n",
    "                               stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(in_channels=64, out_channels=64, blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(in_channels=64, out_channels=128, blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(in_channels=128, out_channels=256, blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(in_channels=256, out_channels=512, blocks=2, stride=2)\n",
    "        \n",
    "        # Global average pooling to produce a fixed-length vector\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        # Bottleneck fully-connected layer (optional, here keeping feature dim at 512)\n",
    "        self.fc_bottleneck = nn.Linear(512, 512)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        \"\"\"\n",
    "        Create a sequential layer composed of multiple residual blocks.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock1D(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock1D(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x: (batch_size, channels=2, length)\n",
    "        x = self.conv1(x)    # -> (B, 64, L/2)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)  # -> (B, 64, L/4)\n",
    "        \n",
    "        x = self.layer1(x)   # -> (B, 64, ?)\n",
    "        x = self.layer2(x)   # -> (B, 128, ?)\n",
    "        x = self.layer3(x)   # -> (B, 256, ?)\n",
    "        x = self.layer4(x)   # -> (B, 512, ?)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avgpool(x)  # -> (B, 512, 1)\n",
    "        x = x.squeeze(-1)    # -> (B, 512)\n",
    "        # Bottleneck transformation\n",
    "        features = self.fc_bottleneck(x)  # (B, 512)\n",
    "        return features\n",
    "\n",
    "class DANN_LP(nn.Module):\n",
    "    \"\"\"\n",
    "    Label predictor network that maps 512–dim features to the desired output classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim=7):\n",
    "        super(DANN_LP, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class DANN_DC(nn.Module):\n",
    "    \"\"\"\n",
    "    Domain classifier network for DANN.\n",
    "    \n",
    "    This network applies a gradient reversal layer (using ReverseLayerF) to the feature vector\n",
    "    before classifying it as either source or target.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DANN_DC, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "        \n",
    "    def forward(self, x, alpha):\n",
    "        # Reverse gradient during the backward pass\n",
    "        x = GradReverse.apply(x, alpha)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#%% CORAL\n",
    "def coral_loss(source, target):\n",
    "    \"\"\"\n",
    "    Computes the CORAL loss between two matrices.\n",
    "    Assumes input tensors are of shape (batch_size, feature_dim).\n",
    "    \"\"\"\n",
    "    d = source.size(1)\n",
    "    ns = source.size(0)\n",
    "    nt = target.size(0)\n",
    "    # Center the features\n",
    "    source_mean = torch.mean(source, dim=0, keepdim=True)\n",
    "    target_mean = torch.mean(target, dim=0, keepdim=True)\n",
    "    source_centered = source - source_mean\n",
    "    target_centered = target - target_mean\n",
    "    # Compute covariance matrices\n",
    "    cov_source = (source_centered.t() @ source_centered) / (ns - 1)\n",
    "    cov_target = (target_centered.t() @ target_centered) / (nt - 1)\n",
    "    # Frobenius norm between covariance matrices (scaled)\n",
    "    loss = torch.mean((cov_source - cov_target) ** 2)\n",
    "    loss = loss / (4 * d * d)\n",
    "    return loss\n",
    "        \n",
    "class CORAL_G(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep ResNet generator that extracts features at multiple depths for CORAL.\n",
    "    \n",
    "    Architecture:\n",
    "      - An initial convolution + batchnorm + ReLU + maxpool.\n",
    "      - Four layers (residual blocks) built with 1D convolutions.\n",
    "      - Intermediate features are extracted after layer1 (early), layer2 (middle),\n",
    "        and layer3 (late) via global average pooling.\n",
    "      - The final layer (layer4) is pooled and passed through a fully-connected\n",
    "        bottleneck to obtain a 512–dim feature for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CORAL_G, self).__init__()\n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=7,\n",
    "                               stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(in_channels=64, out_channels=64, blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(in_channels=64, out_channels=128, blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(in_channels=128, out_channels=256, blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(in_channels=256, out_channels=512, blocks=2, stride=2)\n",
    "        \n",
    "        # Global average pooling (adaptive to any sequence length)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        # Bottleneck fully-connected layer mapping 512->512 for classification.\n",
    "        self.fc_bottleneck = nn.Linear(512, 512)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        \"\"\"\n",
    "        Creates a sequential layer composed of multiple residual blocks.\n",
    "        If the stride is not 1 or the channel dimensions differ,\n",
    "        a downsampling layer is used.\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock1D(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock1D(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x: (batch_size, channels=2, length)\n",
    "        x = self.conv1(x)   # (B, 64, L/2)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x) # (B, 64, L/4)\n",
    "        \n",
    "        # Layer 1: Early features\n",
    "        x1 = self.layer1(x)           # (B, 64, L1)\n",
    "        early = self.avgpool(x1)      # (B, 64, 1)\n",
    "        early = early.squeeze(-1)     # (B, 64)\n",
    "        \n",
    "        # Layer 2: Middle features\n",
    "        x2 = self.layer2(x1)          # (B, 128, L2)\n",
    "        middle = self.avgpool(x2)     # (B, 128, 1)\n",
    "        middle = middle.squeeze(-1)   # (B, 128)\n",
    "        \n",
    "        # Layer 3: Late features for CORAL loss\n",
    "        x3 = self.layer3(x2)          # (B, 256, L3)\n",
    "        late = self.avgpool(x3)       # (B, 256, 1)\n",
    "        late = late.squeeze(-1)       # (B, 256)\n",
    "        \n",
    "        # Layer 4: Final block for classification\n",
    "        x4 = self.layer4(x3)          # (B, 512, L4)\n",
    "        pooled = self.avgpool(x4)     # (B, 512, 1)\n",
    "        pooled = pooled.squeeze(-1)   # (B, 512)\n",
    "        final = self.fc_bottleneck(pooled)  # (B, 512)\n",
    "        \n",
    "        # Return multi-level features for deep CORAL\n",
    "        return early, middle, late, final\n",
    "\n",
    "class CORAL_C(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier network that maps the 512–dim bottleneck features to the output classes.\n",
    "    \n",
    "    Architecture:\n",
    "      - A fully-connected layer (512 -> 256) with batch normalization, ReLU, and dropout.\n",
    "      - A final fully-connected layer mapping 256 -> output_dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim):\n",
    "        super(CORAL_C, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepCORAL:\n",
    "    def __init__(self, G, C, device, \n",
    "                 S_train_loader, S_val_loader, \n",
    "                 T_train_loader, T_val_loader,\n",
    "                 class_subset, n_classes, lr, n_epochs, n_runs, patience, \n",
    "                 lambda_coral=0.5, deep_weights=(1.0, 1.0, 1.0)):\n",
    "        \"\"\"\n",
    "        deep_weights: tuple of weights for the CORAL loss at (early, middle, late) layers.\n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.C = C\n",
    "        self.device = device\n",
    "        self.S_train_loader = S_train_loader\n",
    "        self.S_val_loader = S_val_loader\n",
    "        self.T_train_loader = T_train_loader\n",
    "        self.T_val_loader = T_val_loader\n",
    "        self.class_subset = class_subset\n",
    "        self.n_classes = n_classes\n",
    "        self.lr = lr\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_runs = n_runs\n",
    "        self.patience = patience\n",
    "        self.lambda_coral = lambda_coral\n",
    "        self.deep_weights = deep_weights\n",
    "        \n",
    "    def evaluate(self, netG, netC, loader):\n",
    "        netG.eval()\n",
    "        netC.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in loader:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                # Only need the classification branch (late_fc)\n",
    "                _, _, _, cls_feat = netG(data)\n",
    "                outputs = netC(cls_feat)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        return correct / total\n",
    "\n",
    "    def run(self):\n",
    "        best_s_acc_list = []\n",
    "        best_t_acc_list = []\n",
    "        for run in range(self.n_runs):\n",
    "            print(f\"Deep CORAL Run {run+1}/{self.n_runs}\")\n",
    "            # Instantiate new networks for each run\n",
    "            netG = self.G().to(self.device)\n",
    "            netC = self.C(self.n_classes).to(self.device)\n",
    "            optimizer = torch.optim.Adam(list(netG.parameters()) + list(netC.parameters()), lr=self.lr)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            best_val_acc = 0.0\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(self.n_epochs):\n",
    "                netG.train()\n",
    "                netC.train()\n",
    "                # Use zip to iterate over source and target batches in parallel.\n",
    "                for (s_data, s_labels), (t_data, _) in zip(self.S_train_loader, self.T_train_loader):\n",
    "                    s_data, s_labels = s_data.to(self.device), s_labels.to(self.device)\n",
    "                    t_data = t_data.to(self.device)\n",
    "                    \n",
    "                    # Forward pass for source: get all features\n",
    "                    s_early, s_middle, s_late, s_cls = netG(s_data)\n",
    "                    # Classification loss on source (using the classification feature)\n",
    "                    loss_cls = criterion(netC(s_cls), s_labels)\n",
    "                    \n",
    "                    # Forward pass for target: we only need the intermediate features\n",
    "                    t_early, t_middle, t_late, _ = netG(t_data)\n",
    "                    \n",
    "                    # Compute CORAL loss for each level\n",
    "                    loss_early = coral_loss(s_early, t_early)\n",
    "                    loss_middle = coral_loss(s_middle, t_middle)\n",
    "                    loss_late = coral_loss(s_late, t_late)\n",
    "                    loss_coral_total = (self.deep_weights[0] * loss_early +\n",
    "                                        self.deep_weights[1] * loss_middle +\n",
    "                                        self.deep_weights[2] * loss_late)\n",
    "                    \n",
    "                    total_loss = loss_cls + self.lambda_coral * loss_coral_total\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                # End-of-epoch evaluation on source validation set\n",
    "                s_acc = self.evaluate(netG, netC, self.S_val_loader)\n",
    "                t_acc = self.evaluate(netG, netC, self.T_val_loader)\n",
    "                print(f\"Epoch {epoch+1}: Source Val Acc = {s_acc:.4f}, Target Val Acc = {t_acc:.4f}\")\n",
    "                \n",
    "                # Early stopping on source validation accuracy\n",
    "                if s_acc > best_val_acc:\n",
    "                    best_val_acc = s_acc\n",
    "                    best_model_G = netG.state_dict()\n",
    "                    best_model_C = netC.state_dict()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "                    \n",
    "            # Load best model from this run\n",
    "            netG.load_state_dict(best_model_G)\n",
    "            netC.load_state_dict(best_model_C)\n",
    "            s_acc = self.evaluate(netG, netC, self.S_val_loader)\n",
    "            t_acc = self.evaluate(netG, netC, self.T_val_loader)\n",
    "            print(f\"Run {run+1} finished: Best Source Val Acc = {s_acc:.4f}, Target Val Acc = {t_acc:.4f}\\n\")\n",
    "            best_s_acc_list.append(s_acc)\n",
    "            best_t_acc_list.append(t_acc)\n",
    "            \n",
    "        avg_s_acc = np.mean(best_s_acc_list)\n",
    "        avg_t_acc = np.mean(best_t_acc_list)\n",
    "        print(f\"Deep CORAL: Average Source Val Acc = {avg_s_acc:.4f}, Average Target Val Acc = {avg_t_acc:.4f}\")\n",
    "        return avg_s_acc, avg_t_acc\n",
    "\n",
    "#%% STAR\n",
    "class STAR_G(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep ResNet feature extractor for STAR.\n",
    "    \n",
    "    This network accepts a 2–channel 1D signal and extracts a 512–dimensional\n",
    "    feature vector via several residual layers.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(STAR_G, self).__init__()\n",
    "        # Initial convolutional block\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=7,\n",
    "                               stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(64, 64, blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, blocks=2, stride=2)\n",
    "        \n",
    "        # Global average pooling to obtain a fixed-length feature vector\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        # Optional bottleneck fully-connected layer (maps 512->512)\n",
    "        self.fc_bottleneck = nn.Linear(512, 512)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock1D(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock1D(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, channels=2, length)\n",
    "        x = self.conv1(x)      # (B, 64, L/2)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)    # (B, 64, L/4)\n",
    "        \n",
    "        x = self.layer1(x)     # (B, 64, ?)\n",
    "        x = self.layer2(x)     # (B, 128, ?)\n",
    "        x = self.layer3(x)     # (B, 256, ?)\n",
    "        x = self.layer4(x)     # (B, 512, ?)\n",
    "        \n",
    "        x = self.avgpool(x)    # (B, 512, 1)\n",
    "        x = x.squeeze(-1)      # (B, 512)\n",
    "        x = self.fc_bottleneck(x)  # (B, 512)\n",
    "        return x\n",
    "\n",
    "class STAR_C(nn.Module):\n",
    "    \"\"\"\n",
    "    Stochastic classifier network for STAR.\n",
    "    \n",
    "    This network receives the 512–dim features from DeepResNet_STAR_G,\n",
    "    applies a fully connected layer with batch normalization and ReLU,\n",
    "    and then uses a learned weight distribution (mu2, sigma2) to sample\n",
    "    classifier weights. During training, it samples num_classifiers_train classifiers,\n",
    "    while during evaluation it can either use only the mean (only_mu=True) or\n",
    "    sample num_classifiers_test classifiers.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, num_classifiers_train=2, num_classifiers_test=20,\n",
    "                 init='kaiming_u', use_init=False):\n",
    "        super(STAR_C, self).__init__()\n",
    "        self.num_classifiers_train = num_classifiers_train\n",
    "        self.num_classifiers_test = num_classifiers_test\n",
    "        self.init = init\n",
    "\n",
    "        function_init = {\n",
    "            'kaiming_u': nn.init.kaiming_uniform_,\n",
    "            'kaiming_n': nn.init.kaiming_normal_,\n",
    "            'xavier': nn.init.xavier_normal_\n",
    "        }\n",
    "\n",
    "        # Change input dimension to 512 (from the DeepResNet feature extractor)\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.bn1_fc = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Learnable parameters for the classifier weight distribution\n",
    "        self.mu2 = nn.Parameter(torch.randn(output_dim, 128))\n",
    "        self.sigma2 = nn.Parameter(torch.zeros(output_dim, 128))\n",
    "\n",
    "        if use_init:\n",
    "            for item in [self.mu2, self.sigma2]:\n",
    "                function_init[self.init](item)\n",
    "\n",
    "        self.b2 = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, x, only_mu=True):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.bn1_fc(x))\n",
    "\n",
    "        sigma2_pos = torch.sigmoid(self.sigma2)\n",
    "        fc2_distribution = torch.distributions.Normal(self.mu2, sigma2_pos)\n",
    "\n",
    "        if self.training:\n",
    "            classifiers = []\n",
    "            for _ in range(self.num_classifiers_train):\n",
    "                fc2_w = fc2_distribution.rsample()\n",
    "                classifiers.append([fc2_w, self.b2])\n",
    "\n",
    "            outputs = []\n",
    "            for classifier in classifiers:\n",
    "                out = F.linear(x, classifier[0], classifier[1])\n",
    "                outputs.append(out)\n",
    "            return outputs\n",
    "        else:\n",
    "            if only_mu:\n",
    "                out = F.linear(x, self.mu2, self.b2)\n",
    "                return [out]\n",
    "            else:\n",
    "                classifiers = []\n",
    "                for _ in range(self.num_classifiers_test):\n",
    "                    fc2_w = fc2_distribution.rsample()\n",
    "                    classifiers.append([fc2_w, self.b2])\n",
    "                outputs = []\n",
    "                for classifier in classifiers:\n",
    "                    out = F.linear(x, classifier[0], classifier[1])\n",
    "                    outputs.append(out)\n",
    "                return outputs\n",
    "\n",
    "#%% MCD\n",
    "class MCD_G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCD_G, self).__init__()\n",
    "        # Initial convolutional block\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=7,\n",
    "                               stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(in_channels=64, out_channels=64, blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(in_channels=64, out_channels=128, blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(in_channels=128, out_channels=256, blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(in_channels=256, out_channels=512, blocks=2, stride=2)\n",
    "        \n",
    "        # Global average pooling to get a fixed-length feature vector\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        # Bottleneck fully-connected layer mapping 512 -> 512 (optional)\n",
    "        self.fc_bottleneck = nn.Linear(512, 512)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock1D(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock1D(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input x shape: (batch_size, 2, length)\n",
    "        x = self.conv1(x)      # -> (B, 64, L/2)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)    # -> (B, 64, L/4)\n",
    "        \n",
    "        x = self.layer1(x)     # -> (B, 64, ?)\n",
    "        x = self.layer2(x)     # -> (B, 128, ?)\n",
    "        x = self.layer3(x)     # -> (B, 256, ?)\n",
    "        x = self.layer4(x)     # -> (B, 512, ?)\n",
    "        \n",
    "        x = self.avgpool(x)    # -> (B, 512, 1)\n",
    "        x = x.squeeze(-1)      # -> (B, 512)\n",
    "        x = self.fc_bottleneck(x)  # -> (B, 512)\n",
    "        return x\n",
    "\n",
    "class MCD_C(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(MCD_C, self).__init__()\n",
    "        # Since our feature extractor now outputs a 512-dim vector,\n",
    "        # adjust the input dimension accordingly.\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x, reverse=False, lambda_=1.0):\n",
    "        # Optionally apply gradient reversal\n",
    "        if reverse:\n",
    "            x = grad_reverse(x, lambda_)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#%% JAN\n",
    "class JAN_G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(JAN_G, self).__init__()\n",
    "        # Initial convolutional block\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=7,\n",
    "                               stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(64, 64, blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, blocks=2, stride=2)\n",
    "        \n",
    "        # Global average pooling to obtain a fixed-length feature vector\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        # Bottleneck layer mapping 512 -> 512 dimensions\n",
    "        self.fc_bottleneck = nn.Linear(512, 512)\n",
    "    \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock1D(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock1D(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 2, length)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)  # shape: (batch, 512, 1)\n",
    "        x = x.squeeze(-1)    # shape: (batch, 512)\n",
    "        x = self.fc_bottleneck(x)  # shape: (batch, 512)\n",
    "        \n",
    "        # Debug: Ensure x is a valid tensor\n",
    "        if x is None:\n",
    "            raise ValueError(\"Generator output is None. Check your forward pass.\")\n",
    "        # You can also uncomment the next line to print the shape during debugging.\n",
    "        # print(\"Generator output shape:\", x.shape)\n",
    "        return x\n",
    "\n",
    "class C_JAN(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(C_JAN, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "        \n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        inter = torch.relu(self.fc1(x))\n",
    "        out = self.fc2(inter)\n",
    "        if return_intermediate:\n",
    "            return out, inter\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd6acf4-60eb-46de-822d-ac7fb1783d03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SNR level: 10\n",
      "Base\n",
      "\n",
      "Run 1/3\n",
      "Epoch 1/50, Train Loss: 0.2876, Train Acc: 0.8988, Val Loss: 0.6662, Val Acc: 0.7410\n",
      "Epoch 2/50, Train Loss: 0.0394, Train Acc: 0.9882, Val Loss: 1.8437, Val Acc: 0.6379\n",
      "Epoch 3/50, Train Loss: 0.0192, Train Acc: 0.9945, Val Loss: 0.0091, Val Acc: 0.9982\n",
      "Epoch 4/50, Train Loss: 0.0216, Train Acc: 0.9936, Val Loss: 0.2437, Val Acc: 0.9155\n",
      "Epoch 5/50, Train Loss: 0.0227, Train Acc: 0.9927, Val Loss: 0.0123, Val Acc: 0.9952\n",
      "Epoch 6/50, Train Loss: 0.0147, Train Acc: 0.9963, Val Loss: 0.0078, Val Acc: 0.9970\n",
      "Epoch 7/50, Train Loss: 0.0213, Train Acc: 0.9937, Val Loss: 1.5503, Val Acc: 0.7116\n",
      "Epoch 8/50, Train Loss: 0.0111, Train Acc: 0.9981, Val Loss: 0.0255, Val Acc: 0.9904\n",
      "Epoch 9/50, Train Loss: 0.0109, Train Acc: 0.9975, Val Loss: 2.2404, Val Acc: 0.6553\n",
      "Epoch 10/50, Train Loss: 0.0067, Train Acc: 0.9982, Val Loss: 0.0080, Val Acc: 0.9976\n",
      "Epoch 11/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0029, Val Acc: 0.9982\n",
      "Epoch 12/50, Train Loss: 0.0013, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0019, Val Acc: 0.9988\n",
      "Epoch 14/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0015, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0023, Train Acc: 0.9991, Val Loss: 0.0022, Val Acc: 0.9988\n",
      "Epoch 18/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000\n",
      "Epoch 19/50, Train Loss: 0.0035, Train Acc: 0.9993, Val Loss: 0.0008, Val Acc: 1.0000\n",
      "Epoch 20/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9994\n",
      "Epoch 21/50, Train Loss: 0.0050, Train Acc: 0.9988, Val Loss: 0.0007, Val Acc: 1.0000\n",
      "Epoch 22/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0021, Val Acc: 0.9982\n",
      "Epoch 23/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0011, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Run 2/3\n",
      "Epoch 1/50, Train Loss: 0.3030, Train Acc: 0.9039, Val Loss: 1.0889, Val Acc: 0.6421\n",
      "Epoch 2/50, Train Loss: 0.0614, Train Acc: 0.9798, Val Loss: 0.6136, Val Acc: 0.7590\n",
      "Epoch 3/50, Train Loss: 0.0306, Train Acc: 0.9903, Val Loss: 0.0432, Val Acc: 0.9838\n",
      "Epoch 4/50, Train Loss: 0.0226, Train Acc: 0.9936, Val Loss: 0.6852, Val Acc: 0.7866\n",
      "Epoch 5/50, Train Loss: 0.0129, Train Acc: 0.9966, Val Loss: 0.0095, Val Acc: 0.9964\n",
      "Epoch 6/50, Train Loss: 0.0089, Train Acc: 0.9976, Val Loss: 0.0144, Val Acc: 0.9952\n",
      "Epoch 7/50, Train Loss: 0.0050, Train Acc: 0.9990, Val Loss: 0.0114, Val Acc: 0.9952\n",
      "Epoch 8/50, Train Loss: 0.0104, Train Acc: 0.9966, Val Loss: 2.1940, Val Acc: 0.7044\n",
      "Epoch 9/50, Train Loss: 0.0129, Train Acc: 0.9961, Val Loss: 0.0365, Val Acc: 0.9844\n",
      "Epoch 10/50, Train Loss: 0.0152, Train Acc: 0.9948, Val Loss: 0.3048, Val Acc: 0.9131\n",
      "Early stopping!\n",
      "\n",
      "Run 3/3\n",
      "Epoch 1/50, Train Loss: 0.3225, Train Acc: 0.8874, Val Loss: 1.6049, Val Acc: 0.7440\n",
      "Epoch 2/50, Train Loss: 0.0518, Train Acc: 0.9837, Val Loss: 0.1712, Val Acc: 0.9382\n",
      "Epoch 3/50, Train Loss: 0.0163, Train Acc: 0.9960, Val Loss: 0.0203, Val Acc: 0.9946\n",
      "Epoch 4/50, Train Loss: 0.0184, Train Acc: 0.9946, Val Loss: 0.0076, Val Acc: 0.9964\n",
      "Epoch 5/50, Train Loss: 0.0352, Train Acc: 0.9894, Val Loss: 0.2200, Val Acc: 0.9263\n",
      "Epoch 6/50, Train Loss: 0.0106, Train Acc: 0.9967, Val Loss: 0.0103, Val Acc: 0.9970\n",
      "Epoch 7/50, Train Loss: 0.0094, Train Acc: 0.9970, Val Loss: 0.0717, Val Acc: 0.9814\n",
      "Epoch 8/50, Train Loss: 0.0134, Train Acc: 0.9943, Val Loss: 0.0519, Val Acc: 0.9814\n",
      "Epoch 9/50, Train Loss: 0.0082, Train Acc: 0.9972, Val Loss: 0.0041, Val Acc: 0.9988\n",
      "Epoch 10/50, Train Loss: 0.0121, Train Acc: 0.9964, Val Loss: 0.0194, Val Acc: 0.9940\n",
      "Epoch 11/50, Train Loss: 0.0035, Train Acc: 0.9990, Val Loss: 0.0030, Val Acc: 0.9988\n",
      "Epoch 12/50, Train Loss: 0.0023, Train Acc: 0.9996, Val Loss: 0.0027, Val Acc: 0.9988\n",
      "Epoch 13/50, Train Loss: 0.0019, Train Acc: 0.9994, Val Loss: 0.0028, Val Acc: 0.9982\n",
      "Epoch 14/50, Train Loss: 0.0012, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994\n",
      "Epoch 15/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0033, Val Acc: 0.9976\n",
      "Epoch 16/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9988\n",
      "Epoch 17/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9988\n",
      "Epoch 18/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988\n",
      "Epoch 19/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994\n",
      "Epoch 20/50, Train Loss: 0.0038, Train Acc: 0.9996, Val Loss: 0.0025, Val Acc: 0.9982\n",
      "Epoch 21/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9988\n",
      "Epoch 22/50, Train Loss: 0.0010, Train Acc: 0.9997, Val Loss: 0.0023, Val Acc: 0.9988\n",
      "Epoch 23/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9988\n",
      "Epoch 24/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9982\n",
      "Early stopping!\n",
      "\n",
      "Source performance: 97.04 97.49 96.89 96.91\n",
      "Target performance: 66.37 68.84 65.68 59.40\n",
      "\n",
      "bpsk: 99.92\n",
      "qpsk: 58.35\n",
      "16qam: 4.45\n",
      "8apsk: 100.00\n",
      "DANN\n",
      "Epoch 1/50, Loss: 2.2591, Domain Loss: 1.3775, Class Loss: 0.8816\n",
      "Epoch 2/50, Loss: 1.6344, Domain Loss: 1.3774, Class Loss: 0.2571\n",
      "Epoch 3/50, Loss: 1.5083, Domain Loss: 1.3894, Class Loss: 0.1190\n",
      "Epoch 4/50, Loss: 2.0160, Domain Loss: 1.9284, Class Loss: 0.0875\n",
      "Epoch 5/50, Loss: 2.9906, Domain Loss: 2.8552, Class Loss: 0.1354\n",
      "Epoch 6/50, Loss: 5.2653, Domain Loss: 4.7813, Class Loss: 0.4840\n",
      "Epoch 7/50, Loss: 5.6949, Domain Loss: 5.3192, Class Loss: 0.3757\n",
      "Epoch 8/50, Loss: 3.2667, Domain Loss: 3.0472, Class Loss: 0.2195\n",
      "Epoch 9/50, Loss: 3.2296, Domain Loss: 2.9825, Class Loss: 0.2472\n",
      "Epoch 10/50, Loss: 3.9330, Domain Loss: 3.4413, Class Loss: 0.4916\n",
      "Epoch 11/50, Loss: 2.2782, Domain Loss: 2.0638, Class Loss: 0.2144\n",
      "Epoch 12/50, Loss: 3.5459, Domain Loss: 3.0828, Class Loss: 0.4631\n",
      "Epoch 13/50, Loss: 4.3021, Domain Loss: 3.5818, Class Loss: 0.7203\n",
      "Epoch 14/50, Loss: 2.2993, Domain Loss: 1.9015, Class Loss: 0.3978\n",
      "Epoch 15/50, Loss: 1.8151, Domain Loss: 1.4611, Class Loss: 0.3539\n",
      "Epoch 16/50, Loss: 1.7307, Domain Loss: 1.4021, Class Loss: 0.3286\n",
      "Epoch 17/50, Loss: 1.6620, Domain Loss: 1.3825, Class Loss: 0.2796\n",
      "Epoch 18/50, Loss: 1.6482, Domain Loss: 1.4043, Class Loss: 0.2439\n",
      "Epoch 19/50, Loss: 1.9065, Domain Loss: 1.5440, Class Loss: 0.3625\n",
      "Epoch 20/50, Loss: 2.0647, Domain Loss: 1.7507, Class Loss: 0.3140\n",
      "Epoch 21/50, Loss: 4.9832, Domain Loss: 4.6094, Class Loss: 0.3738\n",
      "Epoch 22/50, Loss: 2.6221, Domain Loss: 2.1892, Class Loss: 0.4329\n",
      "Epoch 23/50, Loss: 1.8568, Domain Loss: 1.5408, Class Loss: 0.3160\n",
      "Epoch 24/50, Loss: 1.6451, Domain Loss: 1.4133, Class Loss: 0.2317\n",
      "Epoch 25/50, Loss: 1.5452, Domain Loss: 1.3730, Class Loss: 0.1722\n",
      "Epoch 26/50, Loss: 1.5272, Domain Loss: 1.3821, Class Loss: 0.1451\n",
      "Epoch 27/50, Loss: 1.4970, Domain Loss: 1.3947, Class Loss: 0.1023\n",
      "Epoch 28/50, Loss: 1.5150, Domain Loss: 1.4247, Class Loss: 0.0903\n",
      "Epoch 29/50, Loss: 1.5177, Domain Loss: 1.4374, Class Loss: 0.0803\n",
      "Epoch 30/50, Loss: 1.5268, Domain Loss: 1.4554, Class Loss: 0.0714\n",
      "Epoch 31/50, Loss: 1.5246, Domain Loss: 1.4400, Class Loss: 0.0846\n",
      "Epoch 32/50, Loss: 1.4362, Domain Loss: 1.3947, Class Loss: 0.0416\n",
      "Epoch 33/50, Loss: 1.5701, Domain Loss: 1.4575, Class Loss: 0.1126\n",
      "Epoch 34/50, Loss: 1.5376, Domain Loss: 1.4335, Class Loss: 0.1041\n",
      "Epoch 35/50, Loss: 1.5063, Domain Loss: 1.4286, Class Loss: 0.0776\n",
      "Epoch 36/50, Loss: 1.4803, Domain Loss: 1.4182, Class Loss: 0.0621\n",
      "Epoch 37/50, Loss: 1.4768, Domain Loss: 1.4367, Class Loss: 0.0401\n",
      "Epoch 38/50, Loss: 1.4984, Domain Loss: 1.4455, Class Loss: 0.0529\n",
      "Epoch 39/50, Loss: 1.5197, Domain Loss: 1.4703, Class Loss: 0.0494\n",
      "Epoch 40/50, Loss: 1.4804, Domain Loss: 1.4366, Class Loss: 0.0438\n",
      "Epoch 41/50, Loss: 1.4487, Domain Loss: 1.4009, Class Loss: 0.0479\n",
      "Epoch 42/50, Loss: 1.5219, Domain Loss: 1.4686, Class Loss: 0.0533\n",
      "Epoch 43/50, Loss: 1.4483, Domain Loss: 1.4168, Class Loss: 0.0314\n",
      "Epoch 44/50, Loss: 1.4429, Domain Loss: 1.4019, Class Loss: 0.0410\n",
      "Epoch 45/50, Loss: 1.4493, Domain Loss: 1.3923, Class Loss: 0.0571\n",
      "Epoch 46/50, Loss: 1.4693, Domain Loss: 1.4195, Class Loss: 0.0498\n",
      "Epoch 47/50, Loss: 1.4848, Domain Loss: 1.4407, Class Loss: 0.0440\n",
      "Epoch 48/50, Loss: 1.4640, Domain Loss: 1.4133, Class Loss: 0.0507\n",
      "Epoch 49/50, Loss: 1.4418, Domain Loss: 1.4205, Class Loss: 0.0212\n",
      "Epoch 50/50, Loss: 1.4227, Domain Loss: 1.4019, Class Loss: 0.0208\n",
      "73.14\n",
      "\n",
      "\n",
      "Epoch 1/50, Loss: 2.2326, Domain Loss: 1.3970, Class Loss: 0.8356\n",
      "Epoch 2/50, Loss: 1.6429, Domain Loss: 1.3734, Class Loss: 0.2695\n",
      "Epoch 3/50, Loss: 1.4766, Domain Loss: 1.3499, Class Loss: 0.1267\n",
      "Epoch 4/50, Loss: 1.4262, Domain Loss: 1.3515, Class Loss: 0.0747\n",
      "Epoch 5/50, Loss: 1.5051, Domain Loss: 1.4544, Class Loss: 0.0507\n",
      "Epoch 6/50, Loss: 2.5040, Domain Loss: 2.4017, Class Loss: 0.1022\n",
      "Epoch 7/50, Loss: 2.4998, Domain Loss: 2.2272, Class Loss: 0.2726\n",
      "Epoch 8/50, Loss: 2.0297, Domain Loss: 1.7770, Class Loss: 0.2527\n",
      "Epoch 9/50, Loss: 1.7504, Domain Loss: 1.6108, Class Loss: 0.1397\n",
      "Epoch 10/50, Loss: 3.1626, Domain Loss: 2.9785, Class Loss: 0.1842\n",
      "Epoch 11/50, Loss: 3.3210, Domain Loss: 2.9771, Class Loss: 0.3439\n",
      "Epoch 12/50, Loss: 2.3111, Domain Loss: 2.0746, Class Loss: 0.2365\n",
      "Epoch 13/50, Loss: 5.4497, Domain Loss: 5.0076, Class Loss: 0.4421\n",
      "Epoch 14/50, Loss: 8.2163, Domain Loss: 7.7969, Class Loss: 0.4193\n",
      "Epoch 15/50, Loss: 6.7229, Domain Loss: 6.2214, Class Loss: 0.5015\n",
      "Epoch 16/50, Loss: 3.2221, Domain Loss: 2.8842, Class Loss: 0.3379\n",
      "Epoch 17/50, Loss: 1.9817, Domain Loss: 1.7846, Class Loss: 0.1970\n",
      "Epoch 18/50, Loss: 1.7930, Domain Loss: 1.6516, Class Loss: 0.1414\n",
      "Epoch 19/50, Loss: 1.8846, Domain Loss: 1.7210, Class Loss: 0.1636\n",
      "Epoch 20/50, Loss: 1.7223, Domain Loss: 1.6072, Class Loss: 0.1150\n",
      "Epoch 21/50, Loss: 1.5903, Domain Loss: 1.4425, Class Loss: 0.1477\n",
      "Epoch 22/50, Loss: 1.4519, Domain Loss: 1.3616, Class Loss: 0.0903\n",
      "Epoch 23/50, Loss: 1.5144, Domain Loss: 1.3818, Class Loss: 0.1326\n",
      "Epoch 24/50, Loss: 1.4490, Domain Loss: 1.3788, Class Loss: 0.0702\n",
      "Epoch 25/50, Loss: 1.4433, Domain Loss: 1.3831, Class Loss: 0.0602\n",
      "Epoch 26/50, Loss: 1.4912, Domain Loss: 1.4104, Class Loss: 0.0809\n",
      "Epoch 27/50, Loss: 1.4612, Domain Loss: 1.3881, Class Loss: 0.0731\n",
      "Epoch 28/50, Loss: 1.4711, Domain Loss: 1.3950, Class Loss: 0.0761\n",
      "Epoch 29/50, Loss: 1.4430, Domain Loss: 1.3931, Class Loss: 0.0498\n",
      "Epoch 30/50, Loss: 1.4130, Domain Loss: 1.3911, Class Loss: 0.0219\n",
      "Epoch 31/50, Loss: 1.4602, Domain Loss: 1.4140, Class Loss: 0.0462\n",
      "Epoch 32/50, Loss: 1.4569, Domain Loss: 1.4169, Class Loss: 0.0400\n",
      "Epoch 33/50, Loss: 1.4494, Domain Loss: 1.4159, Class Loss: 0.0335\n",
      "Epoch 34/50, Loss: 1.4604, Domain Loss: 1.4196, Class Loss: 0.0408\n",
      "Epoch 35/50, Loss: 1.4456, Domain Loss: 1.4243, Class Loss: 0.0213\n",
      "Epoch 36/50, Loss: 1.4451, Domain Loss: 1.4136, Class Loss: 0.0314\n",
      "Epoch 37/50, Loss: 1.4408, Domain Loss: 1.3975, Class Loss: 0.0433\n",
      "Epoch 38/50, Loss: 1.4041, Domain Loss: 1.3919, Class Loss: 0.0122\n",
      "Epoch 39/50, Loss: 1.4352, Domain Loss: 1.4044, Class Loss: 0.0308\n",
      "Epoch 40/50, Loss: 1.4300, Domain Loss: 1.3947, Class Loss: 0.0352\n",
      "Epoch 41/50, Loss: 1.4121, Domain Loss: 1.3820, Class Loss: 0.0302\n",
      "Epoch 42/50, Loss: 1.4107, Domain Loss: 1.3888, Class Loss: 0.0219\n",
      "Epoch 43/50, Loss: 1.4237, Domain Loss: 1.4093, Class Loss: 0.0143\n",
      "Epoch 44/50, Loss: 1.4115, Domain Loss: 1.3856, Class Loss: 0.0259\n",
      "Epoch 45/50, Loss: 1.4105, Domain Loss: 1.3968, Class Loss: 0.0137\n",
      "Epoch 46/50, Loss: 1.4097, Domain Loss: 1.3989, Class Loss: 0.0108\n",
      "Epoch 47/50, Loss: 1.4131, Domain Loss: 1.4033, Class Loss: 0.0098\n",
      "Epoch 48/50, Loss: 1.4549, Domain Loss: 1.4178, Class Loss: 0.0371\n",
      "Epoch 49/50, Loss: 1.4199, Domain Loss: 1.3879, Class Loss: 0.0320\n",
      "Epoch 50/50, Loss: 1.4286, Domain Loss: 1.4037, Class Loss: 0.0250\n",
      "77.46\n",
      "\n",
      "\n",
      "Epoch 1/50, Loss: 2.1709, Domain Loss: 1.3827, Class Loss: 0.7882\n",
      "Epoch 2/50, Loss: 1.5092, Domain Loss: 1.3387, Class Loss: 0.1705\n",
      "Epoch 3/50, Loss: 1.5191, Domain Loss: 1.3392, Class Loss: 0.1799\n",
      "Epoch 4/50, Loss: 1.8524, Domain Loss: 1.7346, Class Loss: 0.1178\n",
      "Epoch 5/50, Loss: 2.7529, Domain Loss: 2.6425, Class Loss: 0.1103\n",
      "Epoch 6/50, Loss: 3.4699, Domain Loss: 3.3478, Class Loss: 0.1221\n",
      "Epoch 7/50, Loss: 2.8498, Domain Loss: 2.6524, Class Loss: 0.1974\n",
      "Epoch 8/50, Loss: 3.8061, Domain Loss: 3.6101, Class Loss: 0.1960\n",
      "Epoch 9/50, Loss: 3.6565, Domain Loss: 3.2982, Class Loss: 0.3582\n",
      "Epoch 10/50, Loss: 2.3860, Domain Loss: 2.1285, Class Loss: 0.2576\n",
      "Epoch 11/50, Loss: 2.2478, Domain Loss: 2.0033, Class Loss: 0.2444\n",
      "Epoch 12/50, Loss: 6.8277, Domain Loss: 5.6543, Class Loss: 1.1735\n",
      "Epoch 13/50, Loss: 6.1992, Domain Loss: 5.4307, Class Loss: 0.7685\n",
      "Epoch 14/50, Loss: 14.0686, Domain Loss: 13.0359, Class Loss: 1.0327\n",
      "Epoch 15/50, Loss: 5.2768, Domain Loss: 4.8066, Class Loss: 0.4702\n",
      "Epoch 16/50, Loss: 2.5595, Domain Loss: 2.0660, Class Loss: 0.4935\n",
      "Epoch 17/50, Loss: 3.0588, Domain Loss: 2.4263, Class Loss: 0.6325\n",
      "Epoch 18/50, Loss: 5.7455, Domain Loss: 5.0159, Class Loss: 0.7296\n",
      "Epoch 19/50, Loss: 6.1082, Domain Loss: 5.4932, Class Loss: 0.6149\n",
      "Epoch 20/50, Loss: 6.0225, Domain Loss: 5.2929, Class Loss: 0.7296\n",
      "Epoch 21/50, Loss: 5.4025, Domain Loss: 4.8023, Class Loss: 0.6001\n",
      "Epoch 22/50, Loss: 3.5533, Domain Loss: 3.0017, Class Loss: 0.5517\n",
      "Epoch 23/50, Loss: 3.2286, Domain Loss: 2.8461, Class Loss: 0.3826\n",
      "Epoch 24/50, Loss: 2.8786, Domain Loss: 2.4696, Class Loss: 0.4089\n",
      "Epoch 25/50, Loss: 2.0239, Domain Loss: 1.5949, Class Loss: 0.4291\n",
      "Epoch 26/50, Loss: 1.7426, Domain Loss: 1.4302, Class Loss: 0.3125\n",
      "Epoch 27/50, Loss: 1.6779, Domain Loss: 1.3816, Class Loss: 0.2963\n",
      "Epoch 28/50, Loss: 1.6833, Domain Loss: 1.3823, Class Loss: 0.3011\n",
      "Epoch 29/50, Loss: 1.6609, Domain Loss: 1.3665, Class Loss: 0.2944\n",
      "Epoch 30/50, Loss: 1.6228, Domain Loss: 1.3652, Class Loss: 0.2576\n",
      "Epoch 31/50, Loss: 1.6449, Domain Loss: 1.3991, Class Loss: 0.2458\n",
      "Epoch 32/50, Loss: 1.6188, Domain Loss: 1.4138, Class Loss: 0.2049\n",
      "Epoch 33/50, Loss: 1.6794, Domain Loss: 1.4432, Class Loss: 0.2362\n",
      "Epoch 34/50, Loss: 1.7001, Domain Loss: 1.4823, Class Loss: 0.2179\n",
      "Epoch 35/50, Loss: 1.7095, Domain Loss: 1.4990, Class Loss: 0.2105\n",
      "Epoch 36/50, Loss: 1.8518, Domain Loss: 1.6125, Class Loss: 0.2392\n",
      "Epoch 37/50, Loss: 1.7417, Domain Loss: 1.5068, Class Loss: 0.2349\n",
      "Epoch 38/50, Loss: 1.6195, Domain Loss: 1.4634, Class Loss: 0.1561\n",
      "Epoch 39/50, Loss: 1.5803, Domain Loss: 1.4265, Class Loss: 0.1538\n",
      "Epoch 40/50, Loss: 1.5595, Domain Loss: 1.4364, Class Loss: 0.1232\n",
      "Epoch 41/50, Loss: 1.5174, Domain Loss: 1.4156, Class Loss: 0.1018\n",
      "Epoch 42/50, Loss: 1.5155, Domain Loss: 1.4221, Class Loss: 0.0934\n",
      "Epoch 43/50, Loss: 1.5438, Domain Loss: 1.4227, Class Loss: 0.1211\n",
      "Epoch 44/50, Loss: 1.6264, Domain Loss: 1.5115, Class Loss: 0.1149\n",
      "Epoch 45/50, Loss: 1.5728, Domain Loss: 1.4497, Class Loss: 0.1231\n",
      "Epoch 46/50, Loss: 1.5906, Domain Loss: 1.5072, Class Loss: 0.0834\n",
      "Epoch 47/50, Loss: 1.5298, Domain Loss: 1.4174, Class Loss: 0.1124\n",
      "Epoch 48/50, Loss: 1.7980, Domain Loss: 1.5721, Class Loss: 0.2259\n",
      "Epoch 49/50, Loss: 1.7157, Domain Loss: 1.4695, Class Loss: 0.2461\n",
      "Epoch 50/50, Loss: 1.7173, Domain Loss: 1.4641, Class Loss: 0.2532\n",
      "83.93\n",
      "\n",
      "\n",
      "Source performance:\n",
      "85.27 88.23 84.94 83.54 \n",
      "Target performance:\n",
      "78.18 78.55 77.75 75.80 \n",
      "\n",
      "Deep CORALtarget performance: 100.00 71.32 45.55 94.14 \n",
      "Deep CORAL Run 1/3\n",
      "Epoch 1: Source Val Acc = 0.8783, Target Val Acc = 0.6499\n",
      "Epoch 2: Source Val Acc = 0.6757, Target Val Acc = 0.9436\n",
      "Epoch 3: Source Val Acc = 0.2602, Target Val Acc = 0.4742\n",
      "Epoch 4: Source Val Acc = 0.7902, Target Val Acc = 0.8747\n",
      "Epoch 5: Source Val Acc = 0.9940, Target Val Acc = 0.7650\n",
      "Epoch 6: Source Val Acc = 0.9454, Target Val Acc = 0.7998\n",
      "Epoch 7: Source Val Acc = 0.9892, Target Val Acc = 0.8088\n",
      "Epoch 8: Source Val Acc = 0.7956, Target Val Acc = 0.9287\n",
      "Epoch 9: Source Val Acc = 0.9988, Target Val Acc = 0.7314\n",
      "Epoch 10: Source Val Acc = 0.9964, Target Val Acc = 0.7950\n",
      "Epoch 11: Source Val Acc = 0.9934, Target Val Acc = 0.8106\n",
      "Epoch 12: Source Val Acc = 0.9706, Target Val Acc = 0.8429\n",
      "Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 0.7704\n",
      "Epoch 14: Source Val Acc = 0.5665, Target Val Acc = 0.7692\n",
      "Early stopping triggered.\n",
      "Run 1 finished: Best Source Val Acc = 0.5665, Target Val Acc = 0.7692\n",
      "\n",
      "Deep CORAL Run 2/3\n",
      "Epoch 1: Source Val Acc = 0.7620, Target Val Acc = 0.8807\n",
      "Epoch 2: Source Val Acc = 0.7920, Target Val Acc = 0.7224\n",
      "Epoch 3: Source Val Acc = 0.9892, Target Val Acc = 0.6307\n",
      "Epoch 4: Source Val Acc = 0.9958, Target Val Acc = 0.7254\n",
      "Epoch 5: Source Val Acc = 0.6625, Target Val Acc = 0.8261\n",
      "Epoch 6: Source Val Acc = 0.7728, Target Val Acc = 0.7926\n",
      "Epoch 7: Source Val Acc = 0.9718, Target Val Acc = 0.8135\n",
      "Epoch 8: Source Val Acc = 0.9263, Target Val Acc = 0.8231\n",
      "Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.6847\n",
      "Epoch 10: Source Val Acc = 0.9958, Target Val Acc = 0.7986\n",
      "Epoch 11: Source Val Acc = 0.9958, Target Val Acc = 0.7728\n",
      "Epoch 12: Source Val Acc = 0.7788, Target Val Acc = 0.6757\n",
      "Epoch 13: Source Val Acc = 0.9011, Target Val Acc = 0.6169\n",
      "Epoch 14: Source Val Acc = 0.9952, Target Val Acc = 0.6763\n",
      "Early stopping triggered.\n",
      "Run 2 finished: Best Source Val Acc = 0.9952, Target Val Acc = 0.6763\n",
      "\n",
      "Deep CORAL Run 3/3\n",
      "Epoch 1: Source Val Acc = 0.7626, Target Val Acc = 0.7458\n",
      "Epoch 2: Source Val Acc = 0.8957, Target Val Acc = 0.7758\n",
      "Epoch 3: Source Val Acc = 0.7734, Target Val Acc = 0.9113\n",
      "Epoch 4: Source Val Acc = 0.9940, Target Val Acc = 0.5558\n",
      "Epoch 5: Source Val Acc = 0.9850, Target Val Acc = 0.5995\n",
      "Epoch 6: Source Val Acc = 0.9838, Target Val Acc = 0.7602\n",
      "Epoch 7: Source Val Acc = 0.9916, Target Val Acc = 0.7812\n",
      "Epoch 8: Source Val Acc = 0.9928, Target Val Acc = 0.6811\n",
      "Epoch 9: Source Val Acc = 0.9886, Target Val Acc = 0.7680\n",
      "Early stopping triggered.\n",
      "Run 3 finished: Best Source Val Acc = 0.9886, Target Val Acc = 0.7680\n",
      "\n",
      "Deep CORAL: Average Source Val Acc = 0.8501, Average Target Val Acc = 0.7378\n",
      "STAR\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 0.7980, Discrepancy Loss: 0.0747\n",
      "Epoch [2/50], Class Loss: 0.2923, Discrepancy Loss: 0.0461\n",
      "Epoch [3/50], Class Loss: 0.0793, Discrepancy Loss: 0.0238\n",
      "Epoch [4/50], Class Loss: 0.0806, Discrepancy Loss: 0.0184\n",
      "Epoch [5/50], Class Loss: 0.4689, Discrepancy Loss: 0.0369\n",
      "Epoch [6/50], Class Loss: 0.2024, Discrepancy Loss: 0.0269\n",
      "Epoch [7/50], Class Loss: 0.0759, Discrepancy Loss: 0.0176\n",
      "Epoch [8/50], Class Loss: 0.0193, Discrepancy Loss: 0.0105\n",
      "Epoch [9/50], Class Loss: 0.0098, Discrepancy Loss: 0.0144\n",
      "Epoch [10/50], Class Loss: 0.0237, Discrepancy Loss: 0.0164\n",
      "Epoch [11/50], Class Loss: 0.0439, Discrepancy Loss: 0.0096\n",
      "Epoch [12/50], Class Loss: 0.0136, Discrepancy Loss: 0.0126\n",
      "Epoch [13/50], Class Loss: 0.0151, Discrepancy Loss: 0.0081\n",
      "Epoch [14/50], Class Loss: 0.0092, Discrepancy Loss: 0.0132\n",
      "Epoch [15/50], Class Loss: 0.0071, Discrepancy Loss: 0.0093\n",
      "Epoch [16/50], Class Loss: 0.0074, Discrepancy Loss: 0.0094\n",
      "Epoch [17/50], Class Loss: 0.0050, Discrepancy Loss: 0.0091\n",
      "Epoch [18/50], Class Loss: 0.0100, Discrepancy Loss: 0.0078\n",
      "Epoch [19/50], Class Loss: 0.0514, Discrepancy Loss: 0.0094\n",
      "Epoch [20/50], Class Loss: 0.0435, Discrepancy Loss: 0.0148\n",
      "Epoch [21/50], Class Loss: 0.0089, Discrepancy Loss: 0.0184\n",
      "Epoch [22/50], Class Loss: 0.0077, Discrepancy Loss: 0.0154\n",
      "Epoch [23/50], Class Loss: 0.0078, Discrepancy Loss: 0.0127\n",
      "Epoch [24/50], Class Loss: 0.0973, Discrepancy Loss: 0.0159\n",
      "Epoch [25/50], Class Loss: 0.0148, Discrepancy Loss: 0.0162\n",
      "Epoch [26/50], Class Loss: 0.0041, Discrepancy Loss: 0.0127\n",
      "Epoch [27/50], Class Loss: 0.0159, Discrepancy Loss: 0.0141\n",
      "Epoch [28/50], Class Loss: 0.0088, Discrepancy Loss: 0.0159\n",
      "Epoch [29/50], Class Loss: 0.0051, Discrepancy Loss: 0.0111\n",
      "Epoch [30/50], Class Loss: 0.0064, Discrepancy Loss: 0.0127\n",
      "Epoch [31/50], Class Loss: 0.0061, Discrepancy Loss: 0.0139\n",
      "Epoch [32/50], Class Loss: 0.0052, Discrepancy Loss: 0.0123\n",
      "Epoch [33/50], Class Loss: 0.0034, Discrepancy Loss: 0.0149\n",
      "Epoch [34/50], Class Loss: 0.0079, Discrepancy Loss: 0.0114\n",
      "Epoch [35/50], Class Loss: 0.0070, Discrepancy Loss: 0.0119\n",
      "Epoch [36/50], Class Loss: 0.0047, Discrepancy Loss: 0.0149\n",
      "Epoch [37/50], Class Loss: 0.0072, Discrepancy Loss: 0.0131\n",
      "Epoch [38/50], Class Loss: 0.0044, Discrepancy Loss: 0.0121\n",
      "Epoch [39/50], Class Loss: 0.0045, Discrepancy Loss: 0.0140\n",
      "Epoch [40/50], Class Loss: 0.0056, Discrepancy Loss: 0.0162\n",
      "Epoch [41/50], Class Loss: 0.2126, Discrepancy Loss: 0.0134\n",
      "Epoch [42/50], Class Loss: 0.0077, Discrepancy Loss: 0.0115\n",
      "Epoch [43/50], Class Loss: 0.0052, Discrepancy Loss: 0.0109\n",
      "Epoch [44/50], Class Loss: 0.0061, Discrepancy Loss: 0.0138\n",
      "Epoch [45/50], Class Loss: 0.0053, Discrepancy Loss: 0.0136\n",
      "Epoch [46/50], Class Loss: 0.0088, Discrepancy Loss: 0.0112\n",
      "Epoch [47/50], Class Loss: 0.0903, Discrepancy Loss: 0.0121\n",
      "Epoch [48/50], Class Loss: 0.0156, Discrepancy Loss: 0.0116\n",
      "Epoch [49/50], Class Loss: 0.1020, Discrepancy Loss: 0.0113\n",
      "Epoch [50/50], Class Loss: 0.0138, Discrepancy Loss: 0.0117\n",
      "Source Domain Performance - Accuracy: 95.92%, Precision: 96.03%, Recall: 95.88%, F1 Score: 95.84%\n",
      "Target Domain Performance - Accuracy: 85.49%, Precision: 88.23%, Recall: 85.28%, F1 Score: 84.33%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 0.9911, Discrepancy Loss: 0.0979\n",
      "Epoch [2/50], Class Loss: 0.4773, Discrepancy Loss: 0.0562\n",
      "Epoch [3/50], Class Loss: 0.2852, Discrepancy Loss: 0.0402\n",
      "Epoch [4/50], Class Loss: 0.1957, Discrepancy Loss: 0.0251\n",
      "Epoch [5/50], Class Loss: 0.1368, Discrepancy Loss: 0.0316\n",
      "Epoch [6/50], Class Loss: 0.0763, Discrepancy Loss: 0.0273\n",
      "Epoch [7/50], Class Loss: 0.0275, Discrepancy Loss: 0.0194\n",
      "Epoch [8/50], Class Loss: 0.0391, Discrepancy Loss: 0.0170\n",
      "Epoch [9/50], Class Loss: 0.1964, Discrepancy Loss: 0.0310\n",
      "Epoch [10/50], Class Loss: 0.0830, Discrepancy Loss: 0.0302\n",
      "Epoch [11/50], Class Loss: 0.0158, Discrepancy Loss: 0.0160\n",
      "Epoch [12/50], Class Loss: 0.0156, Discrepancy Loss: 0.0162\n",
      "Epoch [13/50], Class Loss: 0.0186, Discrepancy Loss: 0.0167\n",
      "Epoch [14/50], Class Loss: 0.0316, Discrepancy Loss: 0.0125\n",
      "Epoch [15/50], Class Loss: 0.0125, Discrepancy Loss: 0.0148\n",
      "Epoch [16/50], Class Loss: 0.0084, Discrepancy Loss: 0.0127\n",
      "Epoch [17/50], Class Loss: 0.0056, Discrepancy Loss: 0.0145\n",
      "Epoch [18/50], Class Loss: 0.0122, Discrepancy Loss: 0.0158\n",
      "Epoch [19/50], Class Loss: 0.0078, Discrepancy Loss: 0.0117\n",
      "Epoch [20/50], Class Loss: 0.0067, Discrepancy Loss: 0.0137\n",
      "Epoch [21/50], Class Loss: 0.0564, Discrepancy Loss: 0.0128\n",
      "Epoch [22/50], Class Loss: 0.0626, Discrepancy Loss: 0.0192\n",
      "Epoch [23/50], Class Loss: 0.0121, Discrepancy Loss: 0.0109\n",
      "Epoch [24/50], Class Loss: 0.0235, Discrepancy Loss: 0.0126\n",
      "Epoch [25/50], Class Loss: 0.0070, Discrepancy Loss: 0.0137\n",
      "Epoch [26/50], Class Loss: 0.0072, Discrepancy Loss: 0.0148\n",
      "Epoch [27/50], Class Loss: 0.0073, Discrepancy Loss: 0.0124\n",
      "Epoch [28/50], Class Loss: 0.0061, Discrepancy Loss: 0.0116\n",
      "Epoch [29/50], Class Loss: 0.0076, Discrepancy Loss: 0.0182\n",
      "Epoch [30/50], Class Loss: 0.0064, Discrepancy Loss: 0.0146\n",
      "Epoch [31/50], Class Loss: 0.0061, Discrepancy Loss: 0.0116\n",
      "Epoch [32/50], Class Loss: 0.0175, Discrepancy Loss: 0.0139\n",
      "Epoch [33/50], Class Loss: 0.0072, Discrepancy Loss: 0.0147\n",
      "Epoch [34/50], Class Loss: 0.0082, Discrepancy Loss: 0.0120\n",
      "Epoch [35/50], Class Loss: 0.0285, Discrepancy Loss: 0.0140\n",
      "Epoch [36/50], Class Loss: 0.0057, Discrepancy Loss: 0.0143\n",
      "Epoch [37/50], Class Loss: 0.0068, Discrepancy Loss: 0.0120\n",
      "Epoch [38/50], Class Loss: 0.0066, Discrepancy Loss: 0.0143\n",
      "Epoch [39/50], Class Loss: 0.0074, Discrepancy Loss: 0.0125\n",
      "Epoch [40/50], Class Loss: 0.0064, Discrepancy Loss: 0.0117\n",
      "Epoch [41/50], Class Loss: 0.0070, Discrepancy Loss: 0.0117\n",
      "Epoch [42/50], Class Loss: 0.0101, Discrepancy Loss: 0.0114\n",
      "Epoch [43/50], Class Loss: 0.0100, Discrepancy Loss: 0.0136\n",
      "Epoch [44/50], Class Loss: 0.0088, Discrepancy Loss: 0.0151\n",
      "Epoch [45/50], Class Loss: 0.0169, Discrepancy Loss: 0.0140\n",
      "Epoch [46/50], Class Loss: 0.0061, Discrepancy Loss: 0.0137\n",
      "Epoch [47/50], Class Loss: 0.0969, Discrepancy Loss: 0.0131\n",
      "Epoch [48/50], Class Loss: 0.0086, Discrepancy Loss: 0.0122\n",
      "Epoch [49/50], Class Loss: 0.0127, Discrepancy Loss: 0.0148\n",
      "Epoch [50/50], Class Loss: 0.0074, Discrepancy Loss: 0.0127\n",
      "Source Domain Performance - Accuracy: 97.84%, Precision: 97.86%, Recall: 97.79%, F1 Score: 97.80%\n",
      "Target Domain Performance - Accuracy: 85.91%, Precision: 89.14%, Recall: 85.71%, F1 Score: 84.85%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 1.0231, Discrepancy Loss: 0.0925\n",
      "Epoch [2/50], Class Loss: 0.1859, Discrepancy Loss: 0.0355\n",
      "Epoch [3/50], Class Loss: 0.1513, Discrepancy Loss: 0.0276\n",
      "Epoch [4/50], Class Loss: 0.1595, Discrepancy Loss: 0.0346\n",
      "Epoch [5/50], Class Loss: 0.0787, Discrepancy Loss: 0.0256\n",
      "Epoch [6/50], Class Loss: 0.4900, Discrepancy Loss: 0.0545\n",
      "Epoch [7/50], Class Loss: 0.1336, Discrepancy Loss: 0.0294\n",
      "Epoch [8/50], Class Loss: 0.1290, Discrepancy Loss: 0.0222\n",
      "Epoch [9/50], Class Loss: 0.1073, Discrepancy Loss: 0.0257\n",
      "Epoch [10/50], Class Loss: 0.1356, Discrepancy Loss: 0.0220\n",
      "Epoch [11/50], Class Loss: 0.0345, Discrepancy Loss: 0.0162\n",
      "Epoch [12/50], Class Loss: 0.0219, Discrepancy Loss: 0.0152\n",
      "Epoch [13/50], Class Loss: 0.0184, Discrepancy Loss: 0.0136\n",
      "Epoch [14/50], Class Loss: 0.0653, Discrepancy Loss: 0.0136\n",
      "Epoch [15/50], Class Loss: 0.0337, Discrepancy Loss: 0.0190\n",
      "Epoch [16/50], Class Loss: 0.0236, Discrepancy Loss: 0.0109\n",
      "Epoch [17/50], Class Loss: 0.0130, Discrepancy Loss: 0.0115\n",
      "Epoch [18/50], Class Loss: 0.0123, Discrepancy Loss: 0.0093\n",
      "Epoch [19/50], Class Loss: 0.0197, Discrepancy Loss: 0.0099\n",
      "Epoch [20/50], Class Loss: 0.0875, Discrepancy Loss: 0.0088\n",
      "Epoch [21/50], Class Loss: 0.0197, Discrepancy Loss: 0.0156\n",
      "Epoch [22/50], Class Loss: 0.0466, Discrepancy Loss: 0.0128\n",
      "Epoch [23/50], Class Loss: 0.0079, Discrepancy Loss: 0.0117\n",
      "Epoch [24/50], Class Loss: 0.0066, Discrepancy Loss: 0.0119\n",
      "Epoch [25/50], Class Loss: 0.0092, Discrepancy Loss: 0.0115\n",
      "Epoch [26/50], Class Loss: 0.0148, Discrepancy Loss: 0.0140\n",
      "Epoch [27/50], Class Loss: 0.0078, Discrepancy Loss: 0.0118\n",
      "Epoch [28/50], Class Loss: 0.0190, Discrepancy Loss: 0.0106\n",
      "Epoch [29/50], Class Loss: 0.0078, Discrepancy Loss: 0.0101\n",
      "Epoch [30/50], Class Loss: 0.0291, Discrepancy Loss: 0.0105\n",
      "Epoch [31/50], Class Loss: 0.0134, Discrepancy Loss: 0.0112\n",
      "Epoch [32/50], Class Loss: 0.0067, Discrepancy Loss: 0.0115\n",
      "Epoch [33/50], Class Loss: 0.0083, Discrepancy Loss: 0.0105\n",
      "Epoch [34/50], Class Loss: 0.0071, Discrepancy Loss: 0.0090\n",
      "Epoch [35/50], Class Loss: 0.0060, Discrepancy Loss: 0.0093\n",
      "Epoch [36/50], Class Loss: 0.1435, Discrepancy Loss: 0.0107\n",
      "Epoch [37/50], Class Loss: 0.0075, Discrepancy Loss: 0.0087\n",
      "Epoch [38/50], Class Loss: 0.0387, Discrepancy Loss: 0.0116\n",
      "Epoch [39/50], Class Loss: 0.0057, Discrepancy Loss: 0.0102\n",
      "Epoch [40/50], Class Loss: 0.0052, Discrepancy Loss: 0.0081\n",
      "Epoch [41/50], Class Loss: 0.0059, Discrepancy Loss: 0.0091\n",
      "Epoch [42/50], Class Loss: 0.0065, Discrepancy Loss: 0.0109\n",
      "Epoch [43/50], Class Loss: 0.0100, Discrepancy Loss: 0.0099\n",
      "Epoch [44/50], Class Loss: 0.0250, Discrepancy Loss: 0.0117\n",
      "Epoch [45/50], Class Loss: 0.0076, Discrepancy Loss: 0.0091\n",
      "Epoch [46/50], Class Loss: 0.0105, Discrepancy Loss: 0.0115\n",
      "Epoch [47/50], Class Loss: 0.0061, Discrepancy Loss: 0.0099\n",
      "Epoch [48/50], Class Loss: 0.0103, Discrepancy Loss: 0.0086\n",
      "Epoch [49/50], Class Loss: 0.0371, Discrepancy Loss: 0.0102\n",
      "Epoch [50/50], Class Loss: 0.0067, Discrepancy Loss: 0.0122\n",
      "Source Domain Performance - Accuracy: 96.94%, Precision: 96.99%, Recall: 96.92%, F1 Score: 96.88%\n",
      "Target Domain Performance - Accuracy: 88.43%, Precision: 89.70%, Recall: 88.24%, F1 Score: 87.78%\n",
      "\n",
      "Source performance: 96.90% 96.96% 96.86% 96.84%\n",
      "Target performance: 86.61% 89.03% 86.41% 85.65%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 93.93%\n",
      "16qam: 51.70%\n",
      "8apsk: 100.00%\n",
      "MCD\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 0.8307, Discrepancy Loss: 0.0174\n",
      "Validation Loss: 6.5882\n",
      "Epoch [2/50], Class Loss: 0.2566, Discrepancy Loss: 0.0061\n",
      "Validation Loss: 0.2362\n",
      "Epoch [3/50], Class Loss: 0.1750, Discrepancy Loss: 0.0090\n",
      "Validation Loss: 0.0300\n",
      "Epoch [4/50], Class Loss: 0.0702, Discrepancy Loss: 0.0082\n",
      "Validation Loss: 0.2457\n",
      "Epoch [5/50], Class Loss: 0.0470, Discrepancy Loss: 0.0062\n",
      "Validation Loss: 0.1970\n",
      "Epoch [6/50], Class Loss: 0.0701, Discrepancy Loss: 0.0054\n",
      "Validation Loss: 0.1957\n",
      "Epoch [7/50], Class Loss: 0.0368, Discrepancy Loss: 0.0087\n",
      "Validation Loss: 0.1891\n",
      "Epoch [8/50], Class Loss: 0.0738, Discrepancy Loss: 0.0096\n",
      "Validation Loss: 0.5764\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 95.32%, Precision: 95.43%, Recall: 95.22%, F1 Score: 95.22%\n",
      "Target Domain Performance - Accuracy: 54.44%, Precision: 60.92%, Recall: 53.30%, F1 Score: 44.53%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 0.9342, Discrepancy Loss: 0.0171\n",
      "Validation Loss: 12.6384\n",
      "Epoch [2/50], Class Loss: 0.2479, Discrepancy Loss: 0.0057\n",
      "Validation Loss: 0.9866\n",
      "Epoch [3/50], Class Loss: 0.1023, Discrepancy Loss: 0.0047\n",
      "Validation Loss: 0.0359\n",
      "Epoch [4/50], Class Loss: 0.0670, Discrepancy Loss: 0.0042\n",
      "Validation Loss: 0.0510\n",
      "Epoch [5/50], Class Loss: 0.0861, Discrepancy Loss: 0.0091\n",
      "Validation Loss: 5.1667\n",
      "Epoch [6/50], Class Loss: 0.0534, Discrepancy Loss: 0.0086\n",
      "Validation Loss: 0.0199\n",
      "Epoch [7/50], Class Loss: 0.0308, Discrepancy Loss: 0.0103\n",
      "Validation Loss: 1.4757\n",
      "Epoch [8/50], Class Loss: 0.0792, Discrepancy Loss: 0.0183\n",
      "Validation Loss: 4.3847\n",
      "Epoch [9/50], Class Loss: 0.1478, Discrepancy Loss: 0.0069\n",
      "Validation Loss: 0.4851\n",
      "Epoch [10/50], Class Loss: 0.0222, Discrepancy Loss: 0.0074\n",
      "Validation Loss: 0.1537\n",
      "Epoch [11/50], Class Loss: 0.0041, Discrepancy Loss: 0.0028\n",
      "Validation Loss: 0.0037\n",
      "Epoch [12/50], Class Loss: 0.0014, Discrepancy Loss: 0.0027\n",
      "Validation Loss: 0.0033\n",
      "Epoch [13/50], Class Loss: 0.0012, Discrepancy Loss: 0.0028\n",
      "Validation Loss: 0.0031\n",
      "Epoch [14/50], Class Loss: 0.0013, Discrepancy Loss: 0.0033\n",
      "Validation Loss: 0.0045\n",
      "Epoch [15/50], Class Loss: 0.0014, Discrepancy Loss: 0.0033\n",
      "Validation Loss: 0.0029\n",
      "Epoch [16/50], Class Loss: 0.0018, Discrepancy Loss: 0.0030\n",
      "Validation Loss: 0.0031\n",
      "Epoch [17/50], Class Loss: 0.0010, Discrepancy Loss: 0.0032\n",
      "Validation Loss: 0.0017\n",
      "Epoch [18/50], Class Loss: 0.0008, Discrepancy Loss: 0.0036\n",
      "Validation Loss: 0.0020\n",
      "Epoch [19/50], Class Loss: 0.0005, Discrepancy Loss: 0.0037\n",
      "Validation Loss: 0.0015\n",
      "Epoch [20/50], Class Loss: 0.0013, Discrepancy Loss: 0.0053\n",
      "Validation Loss: 0.0057\n",
      "Epoch [21/50], Class Loss: 0.0011, Discrepancy Loss: 0.0052\n",
      "Validation Loss: 0.0026\n",
      "Epoch [22/50], Class Loss: 0.0010, Discrepancy Loss: 0.0051\n",
      "Validation Loss: 0.0017\n",
      "Epoch [23/50], Class Loss: 0.0010, Discrepancy Loss: 0.0050\n",
      "Validation Loss: 0.0016\n",
      "Epoch [24/50], Class Loss: 0.0006, Discrepancy Loss: 0.0051\n",
      "Validation Loss: 0.0019\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
      "Target Domain Performance - Accuracy: 63.13%, Precision: 63.21%, Recall: 62.33%, F1 Score: 57.22%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 0.7688, Discrepancy Loss: 0.0271\n",
      "Validation Loss: 15.5404\n",
      "Epoch [2/50], Class Loss: 0.1135, Discrepancy Loss: 0.0070\n",
      "Validation Loss: 2.2751\n",
      "Epoch [3/50], Class Loss: 0.1008, Discrepancy Loss: 0.0160\n",
      "Validation Loss: 0.3945\n",
      "Epoch [4/50], Class Loss: 0.1729, Discrepancy Loss: 0.0088\n",
      "Validation Loss: 0.0827\n",
      "Epoch [5/50], Class Loss: 0.0588, Discrepancy Loss: 0.0303\n",
      "Validation Loss: 0.1135\n",
      "Epoch [6/50], Class Loss: 0.0360, Discrepancy Loss: 0.0121\n",
      "Validation Loss: 0.0300\n",
      "Epoch [7/50], Class Loss: 0.0221, Discrepancy Loss: 0.0087\n",
      "Validation Loss: 0.7115\n",
      "Epoch [8/50], Class Loss: 0.0695, Discrepancy Loss: 0.0236\n",
      "Validation Loss: 0.2298\n",
      "Epoch [9/50], Class Loss: 0.0288, Discrepancy Loss: 0.0258\n",
      "Validation Loss: 0.0373\n",
      "Epoch [10/50], Class Loss: 0.0226, Discrepancy Loss: 0.0122\n",
      "Validation Loss: 0.0357\n",
      "Epoch [11/50], Class Loss: 0.0548, Discrepancy Loss: 0.0265\n",
      "Validation Loss: 0.0090\n",
      "Epoch [12/50], Class Loss: 0.0281, Discrepancy Loss: 0.1245\n",
      "Validation Loss: 0.0905\n",
      "Epoch [13/50], Class Loss: 0.0188, Discrepancy Loss: 0.0714\n",
      "Validation Loss: 0.0085\n",
      "Epoch [14/50], Class Loss: 0.0067, Discrepancy Loss: 0.0258\n",
      "Validation Loss: 0.0066\n",
      "Epoch [15/50], Class Loss: 0.0058, Discrepancy Loss: 0.0128\n",
      "Validation Loss: 0.0073\n",
      "Epoch [16/50], Class Loss: 0.0110, Discrepancy Loss: 0.0134\n",
      "Validation Loss: 0.0069\n",
      "Epoch [17/50], Class Loss: 0.0026, Discrepancy Loss: 0.0128\n",
      "Validation Loss: 0.0055\n",
      "Epoch [18/50], Class Loss: 0.0024, Discrepancy Loss: 0.0168\n",
      "Validation Loss: 0.0066\n",
      "Epoch [19/50], Class Loss: 0.0042, Discrepancy Loss: 0.0145\n",
      "Validation Loss: 0.0038\n",
      "Epoch [20/50], Class Loss: 0.0017, Discrepancy Loss: 0.0095\n",
      "Validation Loss: 0.0047\n",
      "Epoch [21/50], Class Loss: 0.0018, Discrepancy Loss: 0.0107\n",
      "Validation Loss: 0.0051\n",
      "Epoch [22/50], Class Loss: 0.0023, Discrepancy Loss: 0.0102\n",
      "Validation Loss: 0.0045\n",
      "Epoch [23/50], Class Loss: 0.0020, Discrepancy Loss: 0.0103\n",
      "Validation Loss: 0.0056\n",
      "Epoch [24/50], Class Loss: 0.0024, Discrepancy Loss: 0.0106\n",
      "Validation Loss: 0.0058\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%\n",
      "Target Domain Performance - Accuracy: 55.82%, Precision: 60.01%, Recall: 54.74%, F1 Score: 46.89%\n",
      "\n",
      "Source performance: 98.42% 98.46% 98.39% 98.39%\n",
      "Target performance: 57.79% 61.38% 56.79% 49.55%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 26.10%\n",
      "16qam: 1.05%\n",
      "8apsk: 100.00%\n",
      "JAN\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 0.4809, JMMD Loss: 0.0751\n",
      "Validation Loss: 0.4473\n",
      "Epoch [2/50], Class Loss: 0.2015, JMMD Loss: 0.0757\n",
      "Validation Loss: 2.9412\n",
      "Epoch [3/50], Class Loss: 0.1017, JMMD Loss: 0.0720\n",
      "Validation Loss: 3.3033\n",
      "Epoch [4/50], Class Loss: 0.0793, JMMD Loss: 0.0635\n",
      "Validation Loss: 0.0249\n",
      "Epoch [5/50], Class Loss: 0.0188, JMMD Loss: 0.0531\n",
      "Validation Loss: 0.1277\n",
      "Epoch [6/50], Class Loss: 0.0564, JMMD Loss: 0.0739\n",
      "Validation Loss: 0.0806\n",
      "Epoch [7/50], Class Loss: 0.0463, JMMD Loss: 0.0657\n",
      "Validation Loss: 0.1991\n",
      "Epoch [8/50], Class Loss: 0.0303, JMMD Loss: 0.0717\n",
      "Validation Loss: 0.0093\n",
      "Epoch [9/50], Class Loss: 0.0085, JMMD Loss: 0.0600\n",
      "Validation Loss: 1.2388\n",
      "Epoch [10/50], Class Loss: 0.0701, JMMD Loss: 0.0627\n",
      "Validation Loss: 0.6097\n",
      "Epoch [11/50], Class Loss: 0.0147, JMMD Loss: 0.0704\n",
      "Validation Loss: 0.0309\n",
      "Epoch [12/50], Class Loss: 0.0191, JMMD Loss: 0.0678\n",
      "Validation Loss: 0.0462\n",
      "Epoch [13/50], Class Loss: 0.0089, JMMD Loss: 0.0718\n",
      "Validation Loss: 0.0151\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.40%, Precision: 99.39%, Recall: 99.39%, F1 Score: 99.39%\n",
      "Target Domain Performance - Accuracy: 77.04%, Precision: 78.19%, Recall: 76.66%, F1 Score: 73.81%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 0.4713, JMMD Loss: 0.0930\n",
      "Validation Loss: 3.4185\n",
      "Epoch [2/50], Class Loss: 0.2191, JMMD Loss: 0.0999\n",
      "Validation Loss: 0.3345\n",
      "Epoch [3/50], Class Loss: 0.1288, JMMD Loss: 0.0704\n",
      "Validation Loss: 5.1267\n",
      "Epoch [4/50], Class Loss: 0.0797, JMMD Loss: 0.0898\n",
      "Validation Loss: 0.4657\n",
      "Epoch [5/50], Class Loss: 0.0424, JMMD Loss: 0.0755\n",
      "Validation Loss: 1.8189\n",
      "Epoch [6/50], Class Loss: 0.0660, JMMD Loss: 0.0925\n",
      "Validation Loss: 0.0146\n",
      "Epoch [7/50], Class Loss: 0.0311, JMMD Loss: 0.0865\n",
      "Validation Loss: 0.0249\n",
      "Epoch [8/50], Class Loss: 0.0066, JMMD Loss: 0.0699\n",
      "Validation Loss: 0.1857\n",
      "Epoch [9/50], Class Loss: 0.0158, JMMD Loss: 0.0714\n",
      "Validation Loss: 0.0069\n",
      "Epoch [10/50], Class Loss: 0.0102, JMMD Loss: 0.0637\n",
      "Validation Loss: 0.7378\n",
      "Epoch [11/50], Class Loss: 0.0119, JMMD Loss: 0.0653\n",
      "Validation Loss: 0.0319\n",
      "Epoch [12/50], Class Loss: 0.0043, JMMD Loss: 0.0668\n",
      "Validation Loss: 0.0240\n",
      "Epoch [13/50], Class Loss: 0.0040, JMMD Loss: 0.0664\n",
      "Validation Loss: 0.0482\n",
      "Epoch [14/50], Class Loss: 0.0029, JMMD Loss: 0.0642\n",
      "Validation Loss: 0.0453\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 98.62%, Precision: 98.60%, Recall: 98.62%, F1 Score: 98.59%\n",
      "Target Domain Performance - Accuracy: 86.27%, Precision: 89.73%, Recall: 86.09%, F1 Score: 85.10%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 0.4554, JMMD Loss: 0.0839\n",
      "Validation Loss: 0.7410\n",
      "Epoch [2/50], Class Loss: 0.1151, JMMD Loss: 0.0642\n",
      "Validation Loss: 1.0918\n",
      "Epoch [3/50], Class Loss: 0.0535, JMMD Loss: 0.0815\n",
      "Validation Loss: 1.9230\n",
      "Epoch [4/50], Class Loss: 0.0305, JMMD Loss: 0.0826\n",
      "Validation Loss: 0.5004\n",
      "Epoch [5/50], Class Loss: 0.0492, JMMD Loss: 0.0622\n",
      "Validation Loss: 0.1241\n",
      "Epoch [6/50], Class Loss: 0.0435, JMMD Loss: 0.0591\n",
      "Validation Loss: 0.0967\n",
      "Epoch [7/50], Class Loss: 0.1835, JMMD Loss: 0.0869\n",
      "Validation Loss: 0.1498\n",
      "Epoch [8/50], Class Loss: 0.0830, JMMD Loss: 0.0772\n",
      "Validation Loss: 0.1121\n",
      "Epoch [9/50], Class Loss: 0.0161, JMMD Loss: 0.0698\n",
      "Validation Loss: 0.0283\n",
      "Epoch [10/50], Class Loss: 0.0206, JMMD Loss: 0.0712\n",
      "Validation Loss: 0.0059\n",
      "Epoch [11/50], Class Loss: 0.0111, JMMD Loss: 0.0694\n",
      "Validation Loss: 0.0250\n",
      "Epoch [12/50], Class Loss: 0.0139, JMMD Loss: 0.0688\n",
      "Validation Loss: 0.0398\n",
      "Epoch [13/50], Class Loss: 0.0089, JMMD Loss: 0.0727\n",
      "Validation Loss: 0.0315\n",
      "Epoch [14/50], Class Loss: 0.0074, JMMD Loss: 0.0739\n",
      "Validation Loss: 0.0554\n",
      "Epoch [15/50], Class Loss: 0.0053, JMMD Loss: 0.0799\n",
      "Validation Loss: 0.0161\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.34%, Precision: 99.33%, Recall: 99.33%, F1 Score: 99.33%\n",
      "Target Domain Performance - Accuracy: 78.90%, Precision: 80.73%, Recall: 78.56%, F1 Score: 76.03%\n",
      "\n",
      "Source performance: 99.12% 99.11% 99.11% 99.10%\n",
      "Target performance: 80.74% 82.89% 80.43% 78.31%\n",
      "\n",
      "Per-Class Accuracy on Target Domain (Mean over runs):\n",
      "  Class 0: 100.00%\n",
      "  Class 1: 89.78%\n",
      "  Class 2: 31.96%\n",
      "  Class 3: 100.00%\n",
      "\n",
      "SNR level: 14\n",
      "Base\n",
      "\n",
      "Run 1/3\n",
      "Epoch 1/50, Train Loss: 0.4061, Train Acc: 0.8622, Val Loss: 3.5266, Val Acc: 0.5731\n",
      "Epoch 2/50, Train Loss: 0.0497, Train Acc: 0.9847, Val Loss: 0.1622, Val Acc: 0.9406\n",
      "Epoch 3/50, Train Loss: 0.0317, Train Acc: 0.9928, Val Loss: 0.0922, Val Acc: 0.9664\n",
      "Epoch 4/50, Train Loss: 0.0196, Train Acc: 0.9940, Val Loss: 0.2570, Val Acc: 0.8987\n",
      "Epoch 5/50, Train Loss: 0.0128, Train Acc: 0.9967, Val Loss: 1.8901, Val Acc: 0.6589\n",
      "Epoch 6/50, Train Loss: 0.0090, Train Acc: 0.9972, Val Loss: 0.0482, Val Acc: 0.9808\n",
      "Epoch 7/50, Train Loss: 0.0083, Train Acc: 0.9982, Val Loss: 3.4290, Val Acc: 0.6301\n",
      "Epoch 8/50, Train Loss: 0.0170, Train Acc: 0.9960, Val Loss: 0.0039, Val Acc: 0.9988\n",
      "Epoch 9/50, Train Loss: 0.0068, Train Acc: 0.9990, Val Loss: 0.0066, Val Acc: 0.9988\n",
      "Epoch 10/50, Train Loss: 0.0044, Train Acc: 0.9991, Val Loss: 0.0043, Val Acc: 0.9988\n",
      "Epoch 11/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0020, Val Acc: 0.9988\n",
      "Epoch 12/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0013, Val Acc: 0.9994\n",
      "Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0009, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0026, Train Acc: 0.9996, Val Loss: 0.0008, Val Acc: 1.0000\n",
      "Epoch 18/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994\n",
      "Epoch 19/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000\n",
      "Epoch 20/50, Train Loss: 0.0021, Train Acc: 0.9997, Val Loss: 0.0011, Val Acc: 0.9994\n",
      "Epoch 21/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994\n",
      "Epoch 22/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0014, Val Acc: 0.9994\n",
      "Epoch 23/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0013, Val Acc: 0.9994\n",
      "Epoch 24/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994\n",
      "Early stopping!\n",
      "\n",
      "Run 2/3\n",
      "Epoch 1/50, Train Loss: 0.3510, Train Acc: 0.8811, Val Loss: 1.8390, Val Acc: 0.5354\n",
      "Epoch 2/50, Train Loss: 0.0577, Train Acc: 0.9819, Val Loss: 0.1080, Val Acc: 0.9628\n",
      "Epoch 3/50, Train Loss: 0.0367, Train Acc: 0.9882, Val Loss: 0.3241, Val Acc: 0.8765\n",
      "Epoch 4/50, Train Loss: 0.0148, Train Acc: 0.9961, Val Loss: 0.0093, Val Acc: 0.9982\n",
      "Epoch 5/50, Train Loss: 0.0168, Train Acc: 0.9952, Val Loss: 0.1775, Val Acc: 0.9365\n",
      "Epoch 6/50, Train Loss: 0.0157, Train Acc: 0.9948, Val Loss: 0.0174, Val Acc: 0.9946\n",
      "Epoch 7/50, Train Loss: 0.0078, Train Acc: 0.9975, Val Loss: 1.2012, Val Acc: 0.7920\n",
      "Epoch 8/50, Train Loss: 0.0104, Train Acc: 0.9972, Val Loss: 0.0054, Val Acc: 0.9976\n",
      "Epoch 9/50, Train Loss: 0.0047, Train Acc: 0.9984, Val Loss: 0.0293, Val Acc: 0.9910\n",
      "Epoch 10/50, Train Loss: 0.0165, Train Acc: 0.9949, Val Loss: 0.0050, Val Acc: 0.9988\n",
      "Epoch 11/50, Train Loss: 0.0035, Train Acc: 0.9993, Val Loss: 0.0019, Val Acc: 0.9994\n",
      "Epoch 12/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994\n",
      "Epoch 13/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 0.9994\n",
      "Epoch 14/50, Train Loss: 0.0041, Train Acc: 0.9997, Val Loss: 0.0011, Val Acc: 0.9994\n",
      "Epoch 15/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0014, Val Acc: 0.9994\n",
      "Epoch 16/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9988\n",
      "Epoch 17/50, Train Loss: 0.0027, Train Acc: 0.9997, Val Loss: 0.0010, Val Acc: 0.9994\n",
      "Epoch 18/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0014, Val Acc: 0.9994\n",
      "Epoch 19/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994\n",
      "Epoch 20/50, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0007, Val Acc: 1.0000\n",
      "Epoch 21/50, Train Loss: 0.0017, Train Acc: 0.9994, Val Loss: 0.0011, Val Acc: 0.9994\n",
      "Epoch 22/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000\n",
      "Epoch 23/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 0.9994\n",
      "Epoch 24/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 0.9994\n",
      "Epoch 25/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000\n",
      "Epoch 26/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000\n",
      "Epoch 27/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 0.9994\n",
      "Epoch 28/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 1.0000\n",
      "Epoch 29/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 0.9994\n",
      "Epoch 30/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 0.9994\n",
      "Early stopping!\n",
      "\n",
      "Run 3/3\n",
      "Epoch 1/50, Train Loss: 0.3218, Train Acc: 0.8946, Val Loss: 1.6991, Val Acc: 0.6361\n",
      "Epoch 2/50, Train Loss: 0.0552, Train Acc: 0.9844, Val Loss: 0.4711, Val Acc: 0.7872\n",
      "Epoch 3/50, Train Loss: 0.0202, Train Acc: 0.9943, Val Loss: 1.6864, Val Acc: 0.6559\n",
      "Epoch 4/50, Train Loss: 0.0189, Train Acc: 0.9954, Val Loss: 0.0311, Val Acc: 0.9868\n",
      "Epoch 5/50, Train Loss: 0.0205, Train Acc: 0.9940, Val Loss: 0.5080, Val Acc: 0.8573\n",
      "Epoch 6/50, Train Loss: 0.0119, Train Acc: 0.9966, Val Loss: 2.1492, Val Acc: 0.5659\n",
      "Epoch 7/50, Train Loss: 0.0080, Train Acc: 0.9981, Val Loss: 0.0061, Val Acc: 0.9982\n",
      "Epoch 8/50, Train Loss: 0.0067, Train Acc: 0.9976, Val Loss: 0.3391, Val Acc: 0.8927\n",
      "Epoch 9/50, Train Loss: 0.0208, Train Acc: 0.9951, Val Loss: 0.6000, Val Acc: 0.8333\n",
      "Epoch 10/50, Train Loss: 0.0042, Train Acc: 0.9993, Val Loss: 0.0011, Val Acc: 1.0000\n",
      "Epoch 11/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0028, Val Acc: 0.9994\n",
      "Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0022, Val Acc: 0.9994\n",
      "Epoch 14/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0023, Val Acc: 0.9994\n",
      "Epoch 15/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0011, Val Acc: 0.9994\n",
      "Epoch 16/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9994\n",
      "Early stopping!\n",
      "\n",
      "Source performance: 99.94 99.94 99.95 99.94\n",
      "Target performance: 70.76 84.05 71.18 64.31\n",
      "\n",
      "bpsk: 100.00\n",
      "qpsk: 82.51\n",
      "16qam: 2.21\n",
      "8apsk: 100.00\n",
      "DANN\n",
      "Epoch 1/50, Loss: 2.2247, Domain Loss: 1.3825, Class Loss: 0.8421\n",
      "Epoch 2/50, Loss: 1.6921, Domain Loss: 1.3226, Class Loss: 0.3694\n",
      "Epoch 3/50, Loss: 1.5200, Domain Loss: 1.3536, Class Loss: 0.1664\n",
      "Epoch 4/50, Loss: 2.4223, Domain Loss: 2.2974, Class Loss: 0.1249\n",
      "Epoch 5/50, Loss: 9.7011, Domain Loss: 9.5074, Class Loss: 0.1936\n",
      "Epoch 6/50, Loss: 13.1054, Domain Loss: 12.7323, Class Loss: 0.3731\n",
      "Epoch 7/50, Loss: 5.3534, Domain Loss: 5.1279, Class Loss: 0.2255\n",
      "Epoch 8/50, Loss: 5.2452, Domain Loss: 4.8515, Class Loss: 0.3937\n",
      "Epoch 9/50, Loss: 22.3432, Domain Loss: 20.9781, Class Loss: 1.3651\n",
      "Epoch 10/50, Loss: 42.5459, Domain Loss: 41.5911, Class Loss: 0.9547\n",
      "Epoch 11/50, Loss: 16.4308, Domain Loss: 15.9170, Class Loss: 0.5138\n",
      "Epoch 12/50, Loss: 5.3004, Domain Loss: 4.8766, Class Loss: 0.4238\n",
      "Epoch 13/50, Loss: 7.3353, Domain Loss: 4.9705, Class Loss: 2.3648\n",
      "Epoch 14/50, Loss: 5.0400, Domain Loss: 4.4245, Class Loss: 0.6154\n",
      "Epoch 15/50, Loss: 3.5688, Domain Loss: 3.1017, Class Loss: 0.4671\n",
      "Epoch 16/50, Loss: 3.9687, Domain Loss: 3.4948, Class Loss: 0.4739\n",
      "Epoch 17/50, Loss: 3.9282, Domain Loss: 3.4612, Class Loss: 0.4670\n",
      "Epoch 18/50, Loss: 3.5059, Domain Loss: 3.1394, Class Loss: 0.3665\n",
      "Epoch 19/50, Loss: 4.5830, Domain Loss: 3.8132, Class Loss: 0.7698\n",
      "Epoch 20/50, Loss: 3.5741, Domain Loss: 2.9780, Class Loss: 0.5961\n",
      "Epoch 21/50, Loss: 2.7821, Domain Loss: 2.3786, Class Loss: 0.4035\n",
      "Epoch 22/50, Loss: 2.3616, Domain Loss: 2.0087, Class Loss: 0.3529\n",
      "Epoch 23/50, Loss: 2.3532, Domain Loss: 1.9833, Class Loss: 0.3699\n",
      "Epoch 24/50, Loss: 2.3788, Domain Loss: 2.0898, Class Loss: 0.2890\n",
      "Epoch 25/50, Loss: 2.3857, Domain Loss: 1.9780, Class Loss: 0.4077\n",
      "Epoch 26/50, Loss: 2.1312, Domain Loss: 1.8293, Class Loss: 0.3019\n",
      "Epoch 27/50, Loss: 3.3204, Domain Loss: 2.9403, Class Loss: 0.3801\n",
      "Epoch 28/50, Loss: 2.8458, Domain Loss: 2.5656, Class Loss: 0.2802\n",
      "Epoch 29/50, Loss: 2.0114, Domain Loss: 1.7792, Class Loss: 0.2322\n",
      "Epoch 30/50, Loss: 1.7422, Domain Loss: 1.5468, Class Loss: 0.1953\n",
      "Epoch 31/50, Loss: 1.7585, Domain Loss: 1.5585, Class Loss: 0.2000\n",
      "Epoch 32/50, Loss: 1.8738, Domain Loss: 1.6716, Class Loss: 0.2022\n",
      "Epoch 33/50, Loss: 2.0618, Domain Loss: 1.8578, Class Loss: 0.2040\n",
      "Epoch 34/50, Loss: 2.2502, Domain Loss: 1.9874, Class Loss: 0.2628\n",
      "Epoch 35/50, Loss: 2.0611, Domain Loss: 1.8397, Class Loss: 0.2214\n",
      "Epoch 36/50, Loss: 1.8687, Domain Loss: 1.6346, Class Loss: 0.2341\n",
      "Epoch 37/50, Loss: 1.6473, Domain Loss: 1.4636, Class Loss: 0.1836\n",
      "Epoch 38/50, Loss: 1.5882, Domain Loss: 1.4482, Class Loss: 0.1399\n",
      "Epoch 39/50, Loss: 1.5161, Domain Loss: 1.4104, Class Loss: 0.1057\n",
      "Epoch 40/50, Loss: 1.4843, Domain Loss: 1.3857, Class Loss: 0.0987\n",
      "Epoch 41/50, Loss: 1.4361, Domain Loss: 1.3753, Class Loss: 0.0608\n",
      "Epoch 42/50, Loss: 1.4633, Domain Loss: 1.3984, Class Loss: 0.0650\n",
      "Epoch 43/50, Loss: 1.4599, Domain Loss: 1.3906, Class Loss: 0.0692\n",
      "Epoch 44/50, Loss: 1.4351, Domain Loss: 1.3872, Class Loss: 0.0479\n",
      "Epoch 45/50, Loss: 1.4157, Domain Loss: 1.3829, Class Loss: 0.0328\n",
      "Epoch 46/50, Loss: 1.4163, Domain Loss: 1.3723, Class Loss: 0.0440\n",
      "Epoch 47/50, Loss: 1.4188, Domain Loss: 1.3876, Class Loss: 0.0312\n",
      "Epoch 48/50, Loss: 1.4659, Domain Loss: 1.4079, Class Loss: 0.0580\n",
      "Epoch 49/50, Loss: 1.4454, Domain Loss: 1.3925, Class Loss: 0.0528\n",
      "Epoch 50/50, Loss: 1.4168, Domain Loss: 1.3829, Class Loss: 0.0339\n",
      "79.26\n",
      "\n",
      "\n",
      "Epoch 1/50, Loss: 2.2454, Domain Loss: 1.3611, Class Loss: 0.8843\n",
      "Epoch 2/50, Loss: 1.6618, Domain Loss: 1.3009, Class Loss: 0.3609\n",
      "Epoch 3/50, Loss: 1.5942, Domain Loss: 1.3474, Class Loss: 0.2468\n",
      "Epoch 4/50, Loss: 1.8415, Domain Loss: 1.6894, Class Loss: 0.1521\n",
      "Epoch 5/50, Loss: 1.6449, Domain Loss: 1.5338, Class Loss: 0.1111\n",
      "Epoch 6/50, Loss: 1.6561, Domain Loss: 1.5634, Class Loss: 0.0927\n",
      "Epoch 7/50, Loss: 3.9283, Domain Loss: 3.7800, Class Loss: 0.1483\n",
      "Epoch 8/50, Loss: 4.1939, Domain Loss: 3.9167, Class Loss: 0.2772\n",
      "Epoch 9/50, Loss: 4.4693, Domain Loss: 4.1233, Class Loss: 0.3460\n",
      "Epoch 10/50, Loss: 23.6813, Domain Loss: 22.2164, Class Loss: 1.4649\n",
      "Epoch 11/50, Loss: 42.4301, Domain Loss: 41.6682, Class Loss: 0.7619\n",
      "Epoch 12/50, Loss: 25.2097, Domain Loss: 24.5841, Class Loss: 0.6256\n",
      "Epoch 13/50, Loss: 12.3258, Domain Loss: 11.7400, Class Loss: 0.5857\n",
      "Epoch 14/50, Loss: 9.8643, Domain Loss: 9.1844, Class Loss: 0.6798\n",
      "Epoch 15/50, Loss: 9.9488, Domain Loss: 9.0218, Class Loss: 0.9270\n",
      "Epoch 16/50, Loss: 8.8117, Domain Loss: 8.1962, Class Loss: 0.6156\n",
      "Epoch 17/50, Loss: 3.6304, Domain Loss: 3.1214, Class Loss: 0.5090\n",
      "Epoch 18/50, Loss: 2.5891, Domain Loss: 2.1415, Class Loss: 0.4476\n",
      "Epoch 19/50, Loss: 2.5297, Domain Loss: 2.0548, Class Loss: 0.4749\n",
      "Epoch 20/50, Loss: 2.4812, Domain Loss: 2.0049, Class Loss: 0.4764\n",
      "Epoch 21/50, Loss: 2.3721, Domain Loss: 1.9503, Class Loss: 0.4218\n",
      "Epoch 22/50, Loss: 2.1376, Domain Loss: 1.7290, Class Loss: 0.4086\n",
      "Epoch 23/50, Loss: 1.9161, Domain Loss: 1.5324, Class Loss: 0.3837\n",
      "Epoch 24/50, Loss: 1.8204, Domain Loss: 1.4275, Class Loss: 0.3929\n",
      "Epoch 25/50, Loss: 1.6706, Domain Loss: 1.3405, Class Loss: 0.3301\n",
      "Epoch 26/50, Loss: 1.7921, Domain Loss: 1.4601, Class Loss: 0.3320\n",
      "Epoch 27/50, Loss: 2.4354, Domain Loss: 2.0234, Class Loss: 0.4120\n",
      "Epoch 28/50, Loss: 1.7626, Domain Loss: 1.4977, Class Loss: 0.2649\n",
      "Epoch 29/50, Loss: 1.7993, Domain Loss: 1.5447, Class Loss: 0.2546\n",
      "Epoch 30/50, Loss: 1.9461, Domain Loss: 1.6125, Class Loss: 0.3336\n",
      "Epoch 31/50, Loss: 1.8838, Domain Loss: 1.6060, Class Loss: 0.2779\n",
      "Epoch 32/50, Loss: 1.7582, Domain Loss: 1.5273, Class Loss: 0.2309\n",
      "Epoch 33/50, Loss: 1.6953, Domain Loss: 1.4619, Class Loss: 0.2334\n",
      "Epoch 34/50, Loss: 1.6515, Domain Loss: 1.4730, Class Loss: 0.1785\n",
      "Epoch 35/50, Loss: 1.6305, Domain Loss: 1.4150, Class Loss: 0.2155\n",
      "Epoch 36/50, Loss: 1.5837, Domain Loss: 1.4275, Class Loss: 0.1562\n",
      "Epoch 37/50, Loss: 1.5704, Domain Loss: 1.4264, Class Loss: 0.1440\n",
      "Epoch 38/50, Loss: 1.5730, Domain Loss: 1.4443, Class Loss: 0.1286\n",
      "Epoch 39/50, Loss: 1.5066, Domain Loss: 1.4174, Class Loss: 0.0893\n",
      "Epoch 40/50, Loss: 1.5157, Domain Loss: 1.4242, Class Loss: 0.0916\n",
      "Epoch 41/50, Loss: 1.6196, Domain Loss: 1.5058, Class Loss: 0.1139\n",
      "Epoch 42/50, Loss: 1.6478, Domain Loss: 1.5155, Class Loss: 0.1323\n",
      "Epoch 43/50, Loss: 1.6273, Domain Loss: 1.5107, Class Loss: 0.1166\n",
      "Epoch 44/50, Loss: 1.6419, Domain Loss: 1.5321, Class Loss: 0.1097\n",
      "Epoch 45/50, Loss: 1.6653, Domain Loss: 1.5691, Class Loss: 0.0962\n",
      "Epoch 46/50, Loss: 1.6752, Domain Loss: 1.5908, Class Loss: 0.0843\n",
      "Epoch 47/50, Loss: 1.6956, Domain Loss: 1.5861, Class Loss: 0.1096\n",
      "Epoch 48/50, Loss: 1.6394, Domain Loss: 1.5398, Class Loss: 0.0996\n",
      "Epoch 49/50, Loss: 1.5912, Domain Loss: 1.5064, Class Loss: 0.0849\n",
      "Epoch 50/50, Loss: 1.5037, Domain Loss: 1.4485, Class Loss: 0.0552\n",
      "81.53\n",
      "\n",
      "\n",
      "Epoch 1/50, Loss: 2.2511, Domain Loss: 1.3815, Class Loss: 0.8696\n",
      "Epoch 2/50, Loss: 1.6021, Domain Loss: 1.3093, Class Loss: 0.2928\n",
      "Epoch 3/50, Loss: 1.5134, Domain Loss: 1.2888, Class Loss: 0.2246\n",
      "Epoch 4/50, Loss: 1.6932, Domain Loss: 1.4628, Class Loss: 0.2304\n",
      "Epoch 5/50, Loss: 7.1592, Domain Loss: 6.9264, Class Loss: 0.2329\n",
      "Epoch 6/50, Loss: 11.7453, Domain Loss: 11.4915, Class Loss: 0.2539\n",
      "Epoch 7/50, Loss: 14.0777, Domain Loss: 13.6821, Class Loss: 0.3956\n",
      "Epoch 8/50, Loss: 13.8806, Domain Loss: 12.2368, Class Loss: 1.6438\n",
      "Epoch 9/50, Loss: 14.2200, Domain Loss: 13.5656, Class Loss: 0.6543\n",
      "Epoch 10/50, Loss: 12.8910, Domain Loss: 11.6591, Class Loss: 1.2319\n",
      "Epoch 11/50, Loss: 20.5432, Domain Loss: 19.2226, Class Loss: 1.3206\n",
      "Epoch 12/50, Loss: 11.8690, Domain Loss: 11.2131, Class Loss: 0.6558\n",
      "Epoch 13/50, Loss: 9.2078, Domain Loss: 8.5930, Class Loss: 0.6147\n",
      "Epoch 14/50, Loss: 13.4251, Domain Loss: 12.8632, Class Loss: 0.5619\n",
      "Epoch 15/50, Loss: 21.7008, Domain Loss: 20.8598, Class Loss: 0.8410\n",
      "Epoch 16/50, Loss: 6.3190, Domain Loss: 5.8344, Class Loss: 0.4847\n",
      "Epoch 17/50, Loss: 6.5888, Domain Loss: 5.3849, Class Loss: 1.2039\n",
      "Epoch 18/50, Loss: 6.8215, Domain Loss: 5.6082, Class Loss: 1.2133\n",
      "Epoch 19/50, Loss: 3.4322, Domain Loss: 2.8925, Class Loss: 0.5397\n",
      "Epoch 20/50, Loss: 2.4808, Domain Loss: 2.0615, Class Loss: 0.4192\n",
      "Epoch 21/50, Loss: 2.5431, Domain Loss: 2.1271, Class Loss: 0.4159\n",
      "Epoch 22/50, Loss: 1.8601, Domain Loss: 1.6128, Class Loss: 0.2472\n",
      "Epoch 23/50, Loss: 1.7776, Domain Loss: 1.5256, Class Loss: 0.2520\n",
      "Epoch 24/50, Loss: 1.6506, Domain Loss: 1.4552, Class Loss: 0.1954\n",
      "Epoch 25/50, Loss: 1.6221, Domain Loss: 1.4183, Class Loss: 0.2038\n",
      "Epoch 26/50, Loss: 1.5908, Domain Loss: 1.4210, Class Loss: 0.1698\n",
      "Epoch 27/50, Loss: 1.6853, Domain Loss: 1.4895, Class Loss: 0.1958\n",
      "Epoch 28/50, Loss: 1.6594, Domain Loss: 1.4676, Class Loss: 0.1918\n",
      "Epoch 29/50, Loss: 1.6082, Domain Loss: 1.4771, Class Loss: 0.1311\n",
      "Epoch 30/50, Loss: 1.5725, Domain Loss: 1.4004, Class Loss: 0.1721\n",
      "Epoch 31/50, Loss: 1.5781, Domain Loss: 1.4384, Class Loss: 0.1398\n",
      "Epoch 32/50, Loss: 1.5589, Domain Loss: 1.4296, Class Loss: 0.1292\n",
      "Epoch 33/50, Loss: 1.5610, Domain Loss: 1.4280, Class Loss: 0.1330\n",
      "Epoch 34/50, Loss: 1.5401, Domain Loss: 1.4070, Class Loss: 0.1330\n",
      "Epoch 35/50, Loss: 1.5621, Domain Loss: 1.4472, Class Loss: 0.1149\n",
      "Epoch 36/50, Loss: 1.5539, Domain Loss: 1.4425, Class Loss: 0.1114\n",
      "Epoch 37/50, Loss: 1.5114, Domain Loss: 1.4286, Class Loss: 0.0828\n",
      "Epoch 38/50, Loss: 1.5642, Domain Loss: 1.4444, Class Loss: 0.1198\n",
      "Epoch 39/50, Loss: 1.5137, Domain Loss: 1.4351, Class Loss: 0.0786\n",
      "Epoch 40/50, Loss: 1.5178, Domain Loss: 1.4348, Class Loss: 0.0830\n",
      "Epoch 41/50, Loss: 1.4994, Domain Loss: 1.4217, Class Loss: 0.0776\n",
      "Epoch 42/50, Loss: 1.4810, Domain Loss: 1.4180, Class Loss: 0.0630\n",
      "Epoch 43/50, Loss: 1.5046, Domain Loss: 1.4364, Class Loss: 0.0683\n",
      "Epoch 44/50, Loss: 1.5605, Domain Loss: 1.4804, Class Loss: 0.0801\n",
      "Epoch 45/50, Loss: 1.5554, Domain Loss: 1.4834, Class Loss: 0.0720\n",
      "Epoch 46/50, Loss: 1.5764, Domain Loss: 1.4888, Class Loss: 0.0876\n",
      "Epoch 47/50, Loss: 1.5582, Domain Loss: 1.4943, Class Loss: 0.0639\n",
      "Epoch 48/50, Loss: 1.5807, Domain Loss: 1.5272, Class Loss: 0.0536\n",
      "Epoch 49/50, Loss: 1.6137, Domain Loss: 1.5755, Class Loss: 0.0382\n",
      "Epoch 50/50, Loss: 1.6916, Domain Loss: 1.6384, Class Loss: 0.0532\n",
      "78.24\n",
      "\n",
      "\n",
      "Source performance:\n",
      "82.25 86.71 82.77 80.86 \n",
      "Target performance:\n",
      "79.68 83.43 79.87 78.83 \n",
      "\n",
      "Per-class target performance: 100.00 76.60 68.79 74.10 Deep CORAL\n",
      "Deep CORAL Run 1/3\n",
      "Epoch 1: Source Val Acc = 0.7206, Target Val Acc = 0.5701\n",
      "Epoch 2: Source Val Acc = 0.8393, Target Val Acc = 0.5869\n",
      "Epoch 3: Source Val Acc = 0.9616, Target Val Acc = 0.8207\n",
      "Epoch 4: Source Val Acc = 0.9970, Target Val Acc = 0.7680\n",
      "Epoch 5: Source Val Acc = 0.7860, Target Val Acc = 0.9215\n",
      "Epoch 6: Source Val Acc = 0.5084, Target Val Acc = 0.7602\n",
      "Epoch 7: Source Val Acc = 0.9442, Target Val Acc = 0.8261\n",
      "Epoch 8: Source Val Acc = 0.9826, Target Val Acc = 0.8207\n",
      "Epoch 9: Source Val Acc = 0.8891, Target Val Acc = 0.8909\n",
      "Early stopping triggered.\n",
      "Run 1 finished: Best Source Val Acc = 0.8891, Target Val Acc = 0.8909\n",
      "\n",
      "Deep CORAL Run 2/3\n",
      "Epoch 1: Source Val Acc = 0.7242, Target Val Acc = 0.6115\n",
      "Epoch 2: Source Val Acc = 0.7764, Target Val Acc = 0.7392\n",
      "Epoch 3: Source Val Acc = 0.9562, Target Val Acc = 0.5342\n",
      "Epoch 4: Source Val Acc = 0.9940, Target Val Acc = 0.6865\n",
      "Epoch 5: Source Val Acc = 0.9988, Target Val Acc = 0.7566\n",
      "Epoch 6: Source Val Acc = 0.5210, Target Val Acc = 0.7494\n",
      "Epoch 7: Source Val Acc = 0.7236, Target Val Acc = 0.8309\n",
      "Epoch 8: Source Val Acc = 0.9928, Target Val Acc = 0.8495\n",
      "Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.8297\n",
      "Epoch 10: Source Val Acc = 0.9982, Target Val Acc = 0.7920\n",
      "Early stopping triggered.\n",
      "Run 2 finished: Best Source Val Acc = 0.9982, Target Val Acc = 0.7920\n",
      "\n",
      "Deep CORAL Run 3/3\n",
      "Epoch 1: Source Val Acc = 0.7122, Target Val Acc = 0.7926\n",
      "Epoch 2: Source Val Acc = 0.9173, Target Val Acc = 0.8147\n",
      "Epoch 3: Source Val Acc = 0.8237, Target Val Acc = 0.8663\n",
      "Epoch 4: Source Val Acc = 0.6948, Target Val Acc = 0.7842\n",
      "Epoch 5: Source Val Acc = 0.9808, Target Val Acc = 0.8010\n",
      "Epoch 6: Source Val Acc = 0.9844, Target Val Acc = 0.6247\n",
      "Epoch 7: Source Val Acc = 0.7560, Target Val Acc = 0.5504\n",
      "Epoch 8: Source Val Acc = 0.9952, Target Val Acc = 0.9137\n",
      "Epoch 9: Source Val Acc = 0.7782, Target Val Acc = 0.8621\n",
      "Epoch 10: Source Val Acc = 0.9946, Target Val Acc = 0.9029\n",
      "Epoch 11: Source Val Acc = 0.9898, Target Val Acc = 0.9059\n",
      "Epoch 12: Source Val Acc = 1.0000, Target Val Acc = 0.7806\n",
      "Epoch 13: Source Val Acc = 0.9946, Target Val Acc = 0.9005\n",
      "Epoch 14: Source Val Acc = 0.9994, Target Val Acc = 0.7746\n",
      "Epoch 15: Source Val Acc = 0.9562, Target Val Acc = 0.5288\n",
      "Epoch 16: Source Val Acc = 0.9994, Target Val Acc = 0.6235\n",
      "Epoch 17: Source Val Acc = 0.8555, Target Val Acc = 0.9574\n",
      "Early stopping triggered.\n",
      "Run 3 finished: Best Source Val Acc = 0.8555, Target Val Acc = 0.9574\n",
      "\n",
      "Deep CORAL: Average Source Val Acc = 0.9143, Average Target Val Acc = 0.8801\n",
      "STAR\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 0.9483, Discrepancy Loss: 0.0763\n",
      "Epoch [2/50], Class Loss: 0.2911, Discrepancy Loss: 0.0485\n",
      "Epoch [3/50], Class Loss: 0.1299, Discrepancy Loss: 0.0380\n",
      "Epoch [4/50], Class Loss: 0.1891, Discrepancy Loss: 0.0406\n",
      "Epoch [5/50], Class Loss: 0.1754, Discrepancy Loss: 0.0470\n",
      "Epoch [6/50], Class Loss: 0.0915, Discrepancy Loss: 0.0386\n",
      "Epoch [7/50], Class Loss: 0.0741, Discrepancy Loss: 0.0200\n",
      "Epoch [8/50], Class Loss: 0.0125, Discrepancy Loss: 0.0152\n",
      "Epoch [9/50], Class Loss: 0.0087, Discrepancy Loss: 0.0137\n",
      "Epoch [10/50], Class Loss: 0.0286, Discrepancy Loss: 0.0132\n",
      "Epoch [11/50], Class Loss: 0.0349, Discrepancy Loss: 0.0136\n",
      "Epoch [12/50], Class Loss: 0.0173, Discrepancy Loss: 0.0177\n",
      "Epoch [13/50], Class Loss: 0.0036, Discrepancy Loss: 0.0108\n",
      "Epoch [14/50], Class Loss: 0.0036, Discrepancy Loss: 0.0120\n",
      "Epoch [15/50], Class Loss: 0.0111, Discrepancy Loss: 0.0106\n",
      "Epoch [16/50], Class Loss: 0.0125, Discrepancy Loss: 0.0134\n",
      "Epoch [17/50], Class Loss: 0.0038, Discrepancy Loss: 0.0107\n",
      "Epoch [18/50], Class Loss: 0.0034, Discrepancy Loss: 0.0102\n",
      "Epoch [19/50], Class Loss: 0.0021, Discrepancy Loss: 0.0108\n",
      "Epoch [20/50], Class Loss: 0.0026, Discrepancy Loss: 0.0095\n",
      "Epoch [21/50], Class Loss: 0.0158, Discrepancy Loss: 0.0110\n",
      "Epoch [22/50], Class Loss: 0.0055, Discrepancy Loss: 0.0096\n",
      "Epoch [23/50], Class Loss: 0.0008, Discrepancy Loss: 0.0102\n",
      "Epoch [24/50], Class Loss: 0.0659, Discrepancy Loss: 0.0094\n",
      "Epoch [25/50], Class Loss: 0.0032, Discrepancy Loss: 0.0115\n",
      "Epoch [26/50], Class Loss: 0.0052, Discrepancy Loss: 0.0105\n",
      "Epoch [27/50], Class Loss: 0.0636, Discrepancy Loss: 0.0091\n",
      "Epoch [28/50], Class Loss: 0.0016, Discrepancy Loss: 0.0123\n",
      "Epoch [29/50], Class Loss: 0.0016, Discrepancy Loss: 0.0101\n",
      "Epoch [30/50], Class Loss: 0.0055, Discrepancy Loss: 0.0098\n",
      "Epoch [31/50], Class Loss: 0.0155, Discrepancy Loss: 0.0093\n",
      "Epoch [32/50], Class Loss: 0.0042, Discrepancy Loss: 0.0105\n",
      "Epoch [33/50], Class Loss: 0.0024, Discrepancy Loss: 0.0106\n",
      "Epoch [34/50], Class Loss: 0.0023, Discrepancy Loss: 0.0110\n",
      "Epoch [35/50], Class Loss: 0.0092, Discrepancy Loss: 0.0092\n",
      "Epoch [36/50], Class Loss: 0.0019, Discrepancy Loss: 0.0088\n",
      "Epoch [37/50], Class Loss: 0.0052, Discrepancy Loss: 0.0118\n",
      "Epoch [38/50], Class Loss: 0.0019, Discrepancy Loss: 0.0100\n",
      "Epoch [39/50], Class Loss: 0.0021, Discrepancy Loss: 0.0102\n",
      "Epoch [40/50], Class Loss: 0.0151, Discrepancy Loss: 0.0114\n",
      "Epoch [41/50], Class Loss: 0.0425, Discrepancy Loss: 0.0115\n",
      "Epoch [42/50], Class Loss: 0.0022, Discrepancy Loss: 0.0124\n",
      "Epoch [43/50], Class Loss: 0.0027, Discrepancy Loss: 0.0111\n",
      "Epoch [44/50], Class Loss: 0.0051, Discrepancy Loss: 0.0100\n",
      "Epoch [45/50], Class Loss: 0.0057, Discrepancy Loss: 0.0099\n",
      "Epoch [46/50], Class Loss: 0.0061, Discrepancy Loss: 0.0104\n",
      "Epoch [47/50], Class Loss: 0.0990, Discrepancy Loss: 0.0114\n",
      "Epoch [48/50], Class Loss: 0.0016, Discrepancy Loss: 0.0108\n",
      "Epoch [49/50], Class Loss: 0.0371, Discrepancy Loss: 0.0090\n",
      "Epoch [50/50], Class Loss: 0.0042, Discrepancy Loss: 0.0097\n",
      "Source Domain Performance - Accuracy: 98.32%, Precision: 98.53%, Recall: 98.26%, F1 Score: 98.35%\n",
      "Target Domain Performance - Accuracy: 90.41%, Precision: 92.52%, Recall: 90.54%, F1 Score: 90.14%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 1.4131, Discrepancy Loss: 0.0979\n",
      "Epoch [2/50], Class Loss: 0.4619, Discrepancy Loss: 0.0849\n",
      "Epoch [3/50], Class Loss: 0.2663, Discrepancy Loss: 0.0554\n",
      "Epoch [4/50], Class Loss: 0.1540, Discrepancy Loss: 0.0412\n",
      "Epoch [5/50], Class Loss: 0.0372, Discrepancy Loss: 0.0295\n",
      "Epoch [6/50], Class Loss: 0.0657, Discrepancy Loss: 0.0337\n",
      "Epoch [7/50], Class Loss: 0.0454, Discrepancy Loss: 0.0245\n",
      "Epoch [8/50], Class Loss: 0.2042, Discrepancy Loss: 0.0330\n",
      "Epoch [9/50], Class Loss: 0.0503, Discrepancy Loss: 0.0263\n",
      "Epoch [10/50], Class Loss: 0.0327, Discrepancy Loss: 0.0248\n",
      "Epoch [11/50], Class Loss: 0.0152, Discrepancy Loss: 0.0213\n",
      "Epoch [12/50], Class Loss: 0.0197, Discrepancy Loss: 0.0213\n",
      "Epoch [13/50], Class Loss: 0.0109, Discrepancy Loss: 0.0202\n",
      "Epoch [14/50], Class Loss: 0.0244, Discrepancy Loss: 0.0195\n",
      "Epoch [15/50], Class Loss: 0.0157, Discrepancy Loss: 0.0197\n",
      "Epoch [16/50], Class Loss: 0.0428, Discrepancy Loss: 0.0237\n",
      "Epoch [17/50], Class Loss: 0.0367, Discrepancy Loss: 0.0245\n",
      "Epoch [18/50], Class Loss: 0.0070, Discrepancy Loss: 0.0233\n",
      "Epoch [19/50], Class Loss: 0.0082, Discrepancy Loss: 0.0178\n",
      "Epoch [20/50], Class Loss: 0.0082, Discrepancy Loss: 0.0200\n",
      "Epoch [21/50], Class Loss: 0.1621, Discrepancy Loss: 0.0198\n",
      "Epoch [22/50], Class Loss: 0.0069, Discrepancy Loss: 0.0201\n",
      "Epoch [23/50], Class Loss: 0.0091, Discrepancy Loss: 0.0188\n",
      "Epoch [24/50], Class Loss: 0.0052, Discrepancy Loss: 0.0180\n",
      "Epoch [25/50], Class Loss: 0.0095, Discrepancy Loss: 0.0199\n",
      "Epoch [26/50], Class Loss: 0.0072, Discrepancy Loss: 0.0201\n",
      "Epoch [27/50], Class Loss: 0.0319, Discrepancy Loss: 0.0194\n",
      "Epoch [28/50], Class Loss: 0.0056, Discrepancy Loss: 0.0190\n",
      "Epoch [29/50], Class Loss: 0.0064, Discrepancy Loss: 0.0191\n",
      "Epoch [30/50], Class Loss: 0.0173, Discrepancy Loss: 0.0185\n",
      "Epoch [31/50], Class Loss: 0.0090, Discrepancy Loss: 0.0167\n",
      "Epoch [32/50], Class Loss: 0.0053, Discrepancy Loss: 0.0176\n",
      "Epoch [33/50], Class Loss: 0.0028, Discrepancy Loss: 0.0201\n",
      "Epoch [34/50], Class Loss: 0.0072, Discrepancy Loss: 0.0168\n",
      "Epoch [35/50], Class Loss: 0.0035, Discrepancy Loss: 0.0188\n",
      "Epoch [36/50], Class Loss: 0.0162, Discrepancy Loss: 0.0151\n",
      "Epoch [37/50], Class Loss: 0.0077, Discrepancy Loss: 0.0135\n",
      "Epoch [38/50], Class Loss: 0.0080, Discrepancy Loss: 0.0176\n",
      "Epoch [39/50], Class Loss: 0.0032, Discrepancy Loss: 0.0179\n",
      "Epoch [40/50], Class Loss: 0.0044, Discrepancy Loss: 0.0161\n",
      "Epoch [41/50], Class Loss: 0.0038, Discrepancy Loss: 0.0186\n",
      "Epoch [42/50], Class Loss: 0.0060, Discrepancy Loss: 0.0175\n",
      "Epoch [43/50], Class Loss: 0.0057, Discrepancy Loss: 0.0171\n",
      "Epoch [44/50], Class Loss: 0.0052, Discrepancy Loss: 0.0184\n",
      "Epoch [45/50], Class Loss: 0.0131, Discrepancy Loss: 0.0171\n",
      "Epoch [46/50], Class Loss: 0.0038, Discrepancy Loss: 0.0167\n",
      "Epoch [47/50], Class Loss: 0.0040, Discrepancy Loss: 0.0205\n",
      "Epoch [48/50], Class Loss: 0.0044, Discrepancy Loss: 0.0193\n",
      "Epoch [49/50], Class Loss: 0.0039, Discrepancy Loss: 0.0172\n",
      "Epoch [50/50], Class Loss: 0.0250, Discrepancy Loss: 0.0154\n",
      "Source Domain Performance - Accuracy: 99.82%, Precision: 99.83%, Recall: 99.82%, F1 Score: 99.83%\n",
      "Target Domain Performance - Accuracy: 87.77%, Precision: 90.27%, Recall: 87.94%, F1 Score: 87.36%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 1.1031, Discrepancy Loss: 0.1141\n",
      "Epoch [2/50], Class Loss: 0.3835, Discrepancy Loss: 0.0575\n",
      "Epoch [3/50], Class Loss: 0.3576, Discrepancy Loss: 0.0579\n",
      "Epoch [4/50], Class Loss: 0.2955, Discrepancy Loss: 0.0690\n",
      "Epoch [5/50], Class Loss: 0.0794, Discrepancy Loss: 0.0396\n",
      "Epoch [6/50], Class Loss: 0.0586, Discrepancy Loss: 0.0375\n",
      "Epoch [7/50], Class Loss: 0.1395, Discrepancy Loss: 0.0268\n",
      "Epoch [8/50], Class Loss: 0.1601, Discrepancy Loss: 0.0397\n",
      "Epoch [9/50], Class Loss: 0.0366, Discrepancy Loss: 0.0266\n",
      "Epoch [10/50], Class Loss: 0.1036, Discrepancy Loss: 0.0351\n",
      "Epoch [11/50], Class Loss: 0.0366, Discrepancy Loss: 0.0230\n",
      "Epoch [12/50], Class Loss: 0.0141, Discrepancy Loss: 0.0238\n",
      "Epoch [13/50], Class Loss: 0.0119, Discrepancy Loss: 0.0235\n",
      "Epoch [14/50], Class Loss: 0.1305, Discrepancy Loss: 0.0266\n",
      "Epoch [15/50], Class Loss: 0.0376, Discrepancy Loss: 0.0292\n",
      "Epoch [16/50], Class Loss: 0.0131, Discrepancy Loss: 0.0242\n",
      "Epoch [17/50], Class Loss: 0.0067, Discrepancy Loss: 0.0248\n",
      "Epoch [18/50], Class Loss: 0.0841, Discrepancy Loss: 0.0181\n",
      "Epoch [19/50], Class Loss: 0.1026, Discrepancy Loss: 0.0241\n",
      "Epoch [20/50], Class Loss: 0.1297, Discrepancy Loss: 0.0300\n",
      "Epoch [21/50], Class Loss: 0.0289, Discrepancy Loss: 0.0325\n",
      "Epoch [22/50], Class Loss: 0.0223, Discrepancy Loss: 0.0263\n",
      "Epoch [23/50], Class Loss: 0.0210, Discrepancy Loss: 0.0329\n",
      "Epoch [24/50], Class Loss: 0.0220, Discrepancy Loss: 0.0332\n",
      "Epoch [25/50], Class Loss: 0.0117, Discrepancy Loss: 0.0288\n",
      "Epoch [26/50], Class Loss: 0.0231, Discrepancy Loss: 0.0242\n",
      "Epoch [27/50], Class Loss: 0.0095, Discrepancy Loss: 0.0286\n",
      "Epoch [28/50], Class Loss: 0.0176, Discrepancy Loss: 0.0247\n",
      "Epoch [29/50], Class Loss: 0.0114, Discrepancy Loss: 0.0269\n",
      "Epoch [30/50], Class Loss: 0.0183, Discrepancy Loss: 0.0211\n",
      "Epoch [31/50], Class Loss: 0.0110, Discrepancy Loss: 0.0276\n",
      "Epoch [32/50], Class Loss: 0.0070, Discrepancy Loss: 0.0293\n",
      "Epoch [33/50], Class Loss: 0.0102, Discrepancy Loss: 0.0251\n",
      "Epoch [34/50], Class Loss: 0.0912, Discrepancy Loss: 0.0263\n",
      "Epoch [35/50], Class Loss: 0.0213, Discrepancy Loss: 0.0260\n",
      "Epoch [36/50], Class Loss: 0.0251, Discrepancy Loss: 0.0251\n",
      "Epoch [37/50], Class Loss: 0.1174, Discrepancy Loss: 0.0290\n",
      "Epoch [38/50], Class Loss: 0.0105, Discrepancy Loss: 0.0276\n",
      "Epoch [39/50], Class Loss: 0.0202, Discrepancy Loss: 0.0264\n",
      "Epoch [40/50], Class Loss: 0.0102, Discrepancy Loss: 0.0260\n",
      "Epoch [41/50], Class Loss: 0.0075, Discrepancy Loss: 0.0238\n",
      "Epoch [42/50], Class Loss: 0.0219, Discrepancy Loss: 0.0256\n",
      "Epoch [43/50], Class Loss: 0.0079, Discrepancy Loss: 0.0291\n",
      "Epoch [44/50], Class Loss: 0.0243, Discrepancy Loss: 0.0269\n",
      "Epoch [45/50], Class Loss: 0.0098, Discrepancy Loss: 0.0207\n",
      "Epoch [46/50], Class Loss: 0.0072, Discrepancy Loss: 0.0282\n",
      "Epoch [47/50], Class Loss: 0.0140, Discrepancy Loss: 0.0223\n",
      "Epoch [48/50], Class Loss: 0.0089, Discrepancy Loss: 0.0256\n",
      "Epoch [49/50], Class Loss: 0.0181, Discrepancy Loss: 0.0250\n",
      "Epoch [50/50], Class Loss: 0.0060, Discrepancy Loss: 0.0279\n",
      "Source Domain Performance - Accuracy: 99.10%, Precision: 99.17%, Recall: 99.09%, F1 Score: 99.13%\n",
      "Target Domain Performance - Accuracy: 87.77%, Precision: 91.47%, Recall: 87.94%, F1 Score: 87.09%\n",
      "\n",
      "Source performance: 99.08% 99.18% 99.06% 99.10%\n",
      "Target performance: 88.65% 91.42% 88.81% 88.20%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 98.35%\n",
      "16qam: 57.37%\n",
      "8apsk: 99.52%\n",
      "MCD\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 0.5794, Discrepancy Loss: 0.0166\n",
      "Validation Loss: 7.3358\n",
      "Epoch [2/50], Class Loss: 0.0836, Discrepancy Loss: 0.0154\n",
      "Validation Loss: 22.4868\n",
      "Epoch [3/50], Class Loss: 0.0630, Discrepancy Loss: 0.0120\n",
      "Validation Loss: 0.0535\n",
      "Epoch [4/50], Class Loss: 0.0422, Discrepancy Loss: 0.0104\n",
      "Validation Loss: 11.1788\n",
      "Epoch [5/50], Class Loss: 0.0320, Discrepancy Loss: 0.0185\n",
      "Validation Loss: 0.0887\n",
      "Epoch [6/50], Class Loss: 0.0353, Discrepancy Loss: 0.0159\n",
      "Validation Loss: 0.0159\n",
      "Epoch [7/50], Class Loss: 0.0175, Discrepancy Loss: 0.0160\n",
      "Validation Loss: 0.0194\n",
      "Epoch [8/50], Class Loss: 0.0265, Discrepancy Loss: 0.0373\n",
      "Validation Loss: 0.3120\n",
      "Epoch [9/50], Class Loss: 0.0344, Discrepancy Loss: 0.0249\n",
      "Validation Loss: 0.0849\n",
      "Epoch [10/50], Class Loss: 0.0172, Discrepancy Loss: 0.0375\n",
      "Validation Loss: 0.3016\n",
      "Epoch [11/50], Class Loss: 0.0048, Discrepancy Loss: 0.0117\n",
      "Validation Loss: 0.0150\n",
      "Epoch [12/50], Class Loss: 0.0022, Discrepancy Loss: 0.0051\n",
      "Validation Loss: 0.0141\n",
      "Epoch [13/50], Class Loss: 0.0011, Discrepancy Loss: 0.0059\n",
      "Validation Loss: 0.0149\n",
      "Epoch [14/50], Class Loss: 0.0013, Discrepancy Loss: 0.0089\n",
      "Validation Loss: 0.0136\n",
      "Epoch [15/50], Class Loss: 0.0014, Discrepancy Loss: 0.0151\n",
      "Validation Loss: 0.0095\n",
      "Epoch [16/50], Class Loss: 0.0048, Discrepancy Loss: 0.0297\n",
      "Validation Loss: 0.0186\n",
      "Epoch [17/50], Class Loss: 0.0048, Discrepancy Loss: 0.0274\n",
      "Validation Loss: 0.0092\n",
      "Epoch [18/50], Class Loss: 0.0038, Discrepancy Loss: 0.0180\n",
      "Validation Loss: 0.0106\n",
      "Epoch [19/50], Class Loss: 0.0031, Discrepancy Loss: 0.0156\n",
      "Validation Loss: 0.0079\n",
      "Epoch [20/50], Class Loss: 0.0036, Discrepancy Loss: 0.0272\n",
      "Validation Loss: 0.0108\n",
      "Epoch [21/50], Class Loss: 0.0059, Discrepancy Loss: 0.0358\n",
      "Validation Loss: 0.0143\n",
      "Epoch [22/50], Class Loss: 0.0062, Discrepancy Loss: 0.0340\n",
      "Validation Loss: 0.0115\n",
      "Epoch [23/50], Class Loss: 0.0058, Discrepancy Loss: 0.0332\n",
      "Validation Loss: 0.0111\n",
      "Epoch [24/50], Class Loss: 0.0063, Discrepancy Loss: 0.0320\n",
      "Validation Loss: 0.0122\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.88%, Precision: 99.89%, Recall: 99.89%, F1 Score: 99.89%\n",
      "Target Domain Performance - Accuracy: 69.12%, Precision: 76.96%, Recall: 69.56%, F1 Score: 67.75%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 0.6796, Discrepancy Loss: 0.0203\n",
      "Validation Loss: 5.7380\n",
      "Epoch [2/50], Class Loss: 0.0705, Discrepancy Loss: 0.0099\n",
      "Validation Loss: 0.1048\n",
      "Epoch [3/50], Class Loss: 0.0427, Discrepancy Loss: 0.0176\n",
      "Validation Loss: 6.2838\n",
      "Epoch [4/50], Class Loss: 0.0548, Discrepancy Loss: 0.0390\n",
      "Validation Loss: 0.2472\n",
      "Epoch [5/50], Class Loss: 0.0760, Discrepancy Loss: 0.0399\n",
      "Validation Loss: 1.3314\n",
      "Epoch [6/50], Class Loss: 0.0195, Discrepancy Loss: 0.0489\n",
      "Validation Loss: 0.0612\n",
      "Epoch [7/50], Class Loss: 0.0322, Discrepancy Loss: 0.0610\n",
      "Validation Loss: 0.0367\n",
      "Epoch [8/50], Class Loss: 0.0325, Discrepancy Loss: 0.0288\n",
      "Validation Loss: 0.0100\n",
      "Epoch [9/50], Class Loss: 0.0362, Discrepancy Loss: 0.0549\n",
      "Validation Loss: 0.0229\n",
      "Epoch [10/50], Class Loss: 0.0337, Discrepancy Loss: 0.0907\n",
      "Validation Loss: 0.0337\n",
      "Epoch [11/50], Class Loss: 0.0013, Discrepancy Loss: 0.0347\n",
      "Validation Loss: 0.0026\n",
      "Epoch [12/50], Class Loss: 0.0009, Discrepancy Loss: 0.0096\n",
      "Validation Loss: 0.0015\n",
      "Epoch [13/50], Class Loss: 0.0007, Discrepancy Loss: 0.0097\n",
      "Validation Loss: 0.0069\n",
      "Epoch [14/50], Class Loss: 0.0005, Discrepancy Loss: 0.0113\n",
      "Validation Loss: 0.0028\n",
      "Epoch [15/50], Class Loss: 0.0004, Discrepancy Loss: 0.0158\n",
      "Validation Loss: 0.0035\n",
      "Epoch [16/50], Class Loss: 0.0173, Discrepancy Loss: 0.0208\n",
      "Validation Loss: 0.0040\n",
      "Epoch [17/50], Class Loss: 0.0151, Discrepancy Loss: 0.0285\n",
      "Validation Loss: 0.0081\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.88%, Precision: 99.89%, Recall: 99.89%, F1 Score: 99.89%\n",
      "Target Domain Performance - Accuracy: 68.82%, Precision: 76.90%, Recall: 69.27%, F1 Score: 66.88%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 0.6410, Discrepancy Loss: 0.0250\n",
      "Validation Loss: 4.4110\n",
      "Epoch [2/50], Class Loss: 0.1981, Discrepancy Loss: 0.0136\n",
      "Validation Loss: 12.7830\n",
      "Epoch [3/50], Class Loss: 0.0548, Discrepancy Loss: 0.0154\n",
      "Validation Loss: 2.5948\n",
      "Epoch [4/50], Class Loss: 0.1176, Discrepancy Loss: 0.0162\n",
      "Validation Loss: 21.0848\n",
      "Epoch [5/50], Class Loss: 0.0596, Discrepancy Loss: 0.0084\n",
      "Validation Loss: 0.0128\n",
      "Epoch [6/50], Class Loss: 0.0186, Discrepancy Loss: 0.0045\n",
      "Validation Loss: 0.0204\n",
      "Epoch [7/50], Class Loss: 0.0242, Discrepancy Loss: 0.0062\n",
      "Validation Loss: 5.4673\n",
      "Epoch [8/50], Class Loss: 0.0705, Discrepancy Loss: 0.0089\n",
      "Validation Loss: 0.0268\n",
      "Epoch [9/50], Class Loss: 0.0527, Discrepancy Loss: 0.0108\n",
      "Validation Loss: 0.1877\n",
      "Epoch [10/50], Class Loss: 0.0479, Discrepancy Loss: 0.0089\n",
      "Validation Loss: 11.6683\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 50.48%, Precision: 64.10%, Recall: 51.55%, F1 Score: 43.34%\n",
      "Target Domain Performance - Accuracy: 76.50%, Precision: 87.80%, Recall: 76.50%, F1 Score: 70.10%\n",
      "\n",
      "Source performance: 83.41% 87.96% 83.77% 81.04%\n",
      "Target performance: 71.48% 80.56% 71.78% 68.24%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 59.50%\n",
      "16qam: 58.71%\n",
      "8apsk: 68.90%\n",
      "JAN\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 0.3545, JMMD Loss: 0.0706\n",
      "Validation Loss: 1.5770\n",
      "Epoch [2/50], Class Loss: 0.1424, JMMD Loss: 0.0789\n",
      "Validation Loss: 3.6838\n",
      "Epoch [3/50], Class Loss: 0.0312, JMMD Loss: 0.0885\n",
      "Validation Loss: 0.0112\n",
      "Epoch [4/50], Class Loss: 0.0486, JMMD Loss: 0.0913\n",
      "Validation Loss: 0.0406\n",
      "Epoch [5/50], Class Loss: 0.0297, JMMD Loss: 0.0925\n",
      "Validation Loss: 1.1227\n",
      "Epoch [6/50], Class Loss: 0.0250, JMMD Loss: 0.0908\n",
      "Validation Loss: 0.0453\n",
      "Epoch [7/50], Class Loss: 0.0416, JMMD Loss: 0.1242\n",
      "Validation Loss: 1.0295\n",
      "Epoch [8/50], Class Loss: 0.0098, JMMD Loss: 0.1040\n",
      "Validation Loss: 0.0998\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 95.80%, Precision: 96.30%, Recall: 95.78%, F1 Score: 95.90%\n",
      "Target Domain Performance - Accuracy: 94.90%, Precision: 94.94%, Recall: 94.95%, F1 Score: 94.91%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 0.3696, JMMD Loss: 0.0545\n",
      "Validation Loss: 0.7790\n",
      "Epoch [2/50], Class Loss: 0.0677, JMMD Loss: 0.0915\n",
      "Validation Loss: 0.1549\n",
      "Epoch [3/50], Class Loss: 0.0377, JMMD Loss: 0.0985\n",
      "Validation Loss: 0.4311\n",
      "Epoch [4/50], Class Loss: 0.1319, JMMD Loss: 0.0578\n",
      "Validation Loss: 0.3998\n",
      "Epoch [5/50], Class Loss: 0.0313, JMMD Loss: 0.0725\n",
      "Validation Loss: 0.0251\n",
      "Epoch [6/50], Class Loss: 0.0907, JMMD Loss: 0.0979\n",
      "Validation Loss: 6.9293\n",
      "Epoch [7/50], Class Loss: 0.2293, JMMD Loss: 0.1129\n",
      "Validation Loss: 3.1328\n",
      "Epoch [8/50], Class Loss: 0.0962, JMMD Loss: 0.1154\n",
      "Validation Loss: 0.1032\n",
      "Epoch [9/50], Class Loss: 0.0372, JMMD Loss: 0.1167\n",
      "Validation Loss: 0.0808\n",
      "Epoch [10/50], Class Loss: 0.0183, JMMD Loss: 0.1000\n",
      "Validation Loss: 0.1326\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 95.02%, Precision: 95.65%, Recall: 95.32%, F1 Score: 95.26%\n",
      "Target Domain Performance - Accuracy: 86.63%, Precision: 87.39%, Recall: 86.79%, F1 Score: 86.68%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 0.3917, JMMD Loss: 0.0798\n",
      "Validation Loss: 2.1318\n",
      "Epoch [2/50], Class Loss: 0.1850, JMMD Loss: 0.0945\n",
      "Validation Loss: 0.0962\n",
      "Epoch [3/50], Class Loss: 0.0329, JMMD Loss: 0.1010\n",
      "Validation Loss: 0.2090\n",
      "Epoch [4/50], Class Loss: 0.0623, JMMD Loss: 0.0893\n",
      "Validation Loss: 3.4949\n",
      "Epoch [5/50], Class Loss: 0.0611, JMMD Loss: 0.1038\n",
      "Validation Loss: 0.5628\n",
      "Epoch [6/50], Class Loss: 0.1501, JMMD Loss: 0.1048\n",
      "Validation Loss: 1.1873\n",
      "Epoch [7/50], Class Loss: 0.0216, JMMD Loss: 0.1022\n",
      "Validation Loss: 0.1057\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 96.04%, Precision: 96.53%, Recall: 96.00%, F1 Score: 96.13%\n",
      "Target Domain Performance - Accuracy: 89.27%, Precision: 90.85%, Recall: 89.41%, F1 Score: 89.14%\n",
      "\n",
      "Source performance: 95.62% 96.16% 95.70% 95.76%\n",
      "Target performance: 90.27% 91.06% 90.39% 90.24%\n",
      "\n",
      "Per-Class Accuracy on Target Domain (Mean over runs):\n",
      "  Class 0: 100.00%\n",
      "  Class 1: 94.72%\n",
      "  Class 2: 72.58%\n",
      "  Class 3: 94.24%\n",
      "\n",
      "SNR level: 18\n",
      "Base\n",
      "\n",
      "Run 1/3\n",
      "Epoch 1/50, Train Loss: 0.5185, Train Acc: 0.7854, Val Loss: 2.1184, Val Acc: 0.5594\n",
      "Epoch 2/50, Train Loss: 0.1256, Train Acc: 0.9592, Val Loss: 0.0380, Val Acc: 0.9868\n",
      "Epoch 3/50, Train Loss: 0.0434, Train Acc: 0.9865, Val Loss: 0.9690, Val Acc: 0.7902\n",
      "Epoch 4/50, Train Loss: 0.0185, Train Acc: 0.9945, Val Loss: 1.3059, Val Acc: 0.7662\n",
      "Epoch 5/50, Train Loss: 0.0172, Train Acc: 0.9955, Val Loss: 2.8517, Val Acc: 0.7602\n",
      "Epoch 6/50, Train Loss: 0.0076, Train Acc: 0.9979, Val Loss: 2.4995, Val Acc: 0.6385\n",
      "Epoch 7/50, Train Loss: 0.0075, Train Acc: 0.9985, Val Loss: 1.3041, Val Acc: 0.7824\n",
      "Early stopping!\n",
      "\n",
      "Run 2/3\n",
      "Epoch 1/50, Train Loss: 0.5514, Train Acc: 0.7683, Val Loss: 1.3401, Val Acc: 0.6547\n",
      "Epoch 2/50, Train Loss: 0.1033, Train Acc: 0.9651, Val Loss: 0.9866, Val Acc: 0.7506\n",
      "Epoch 3/50, Train Loss: 0.0559, Train Acc: 0.9835, Val Loss: 0.9515, Val Acc: 0.7584\n",
      "Epoch 4/50, Train Loss: 0.0234, Train Acc: 0.9936, Val Loss: 0.1486, Val Acc: 0.9418\n",
      "Epoch 5/50, Train Loss: 0.0245, Train Acc: 0.9924, Val Loss: 0.6273, Val Acc: 0.8669\n",
      "Epoch 6/50, Train Loss: 0.0349, Train Acc: 0.9910, Val Loss: 0.0507, Val Acc: 0.9814\n",
      "Epoch 7/50, Train Loss: 0.0108, Train Acc: 0.9970, Val Loss: 0.9645, Val Acc: 0.7548\n",
      "Epoch 8/50, Train Loss: 0.0026, Train Acc: 0.9999, Val Loss: 0.0196, Val Acc: 0.9934\n",
      "Epoch 9/50, Train Loss: 0.0063, Train Acc: 0.9987, Val Loss: 0.1040, Val Acc: 0.9616\n",
      "Epoch 10/50, Train Loss: 0.0032, Train Acc: 0.9993, Val Loss: 0.7455, Val Acc: 0.8333\n",
      "Epoch 11/50, Train Loss: 0.0020, Train Acc: 0.9996, Val Loss: 0.0051, Val Acc: 0.9982\n",
      "Epoch 12/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988\n",
      "Epoch 13/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9988\n",
      "Epoch 14/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0020, Val Acc: 0.9994\n",
      "Epoch 15/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988\n",
      "Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988\n",
      "Epoch 17/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0020, Val Acc: 0.9988\n",
      "Epoch 18/50, Train Loss: 0.0030, Train Acc: 0.9996, Val Loss: 0.0014, Val Acc: 1.0000\n",
      "Epoch 19/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0026, Val Acc: 0.9994\n",
      "Epoch 20/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9994\n",
      "Epoch 21/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0016, Val Acc: 0.9994\n",
      "Epoch 22/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0015, Val Acc: 0.9994\n",
      "Epoch 23/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0018, Val Acc: 0.9994\n",
      "Early stopping!\n",
      "\n",
      "Run 3/3\n",
      "Epoch 1/50, Train Loss: 0.4530, Train Acc: 0.8260, Val Loss: 0.7225, Val Acc: 0.8207\n",
      "Epoch 2/50, Train Loss: 0.0942, Train Acc: 0.9702, Val Loss: 0.1252, Val Acc: 0.9586\n",
      "Epoch 3/50, Train Loss: 0.0314, Train Acc: 0.9918, Val Loss: 0.0264, Val Acc: 0.9898\n",
      "Epoch 4/50, Train Loss: 0.0160, Train Acc: 0.9961, Val Loss: 0.1343, Val Acc: 0.9574\n",
      "Epoch 5/50, Train Loss: 0.0058, Train Acc: 0.9985, Val Loss: 1.0733, Val Acc: 0.7290\n",
      "Epoch 6/50, Train Loss: 0.0213, Train Acc: 0.9930, Val Loss: 0.4770, Val Acc: 0.8663\n",
      "Epoch 7/50, Train Loss: 0.0191, Train Acc: 0.9943, Val Loss: 0.0324, Val Acc: 0.9892\n",
      "Epoch 8/50, Train Loss: 0.0118, Train Acc: 0.9966, Val Loss: 0.1529, Val Acc: 0.9472\n",
      "Early stopping!\n",
      "\n",
      "Source performance: 90.97 94.42 90.62 88.66\n",
      "Target performance: 82.29 81.73 81.97 78.10\n",
      "\n",
      "bpsk: 100.00\n",
      "qpsk: 99.92\n",
      "16qam: 67.09\n",
      "8apsk: 60.86\n",
      "DANN\n",
      "Epoch 1/50, Loss: 2.2977, Domain Loss: 1.3732, Class Loss: 0.9245\n",
      "Epoch 2/50, Loss: 1.8645, Domain Loss: 1.3089, Class Loss: 0.5556\n",
      "Epoch 3/50, Loss: 1.8766, Domain Loss: 1.3622, Class Loss: 0.5143\n",
      "Epoch 4/50, Loss: 2.3981, Domain Loss: 1.9366, Class Loss: 0.4615\n",
      "Epoch 5/50, Loss: 4.3750, Domain Loss: 3.9346, Class Loss: 0.4404\n",
      "Epoch 6/50, Loss: 2.8872, Domain Loss: 2.4036, Class Loss: 0.4836\n",
      "Epoch 7/50, Loss: 9.5499, Domain Loss: 8.8962, Class Loss: 0.6537\n",
      "Epoch 8/50, Loss: 18.6224, Domain Loss: 17.7161, Class Loss: 0.9063\n",
      "Epoch 9/50, Loss: 20.4417, Domain Loss: 19.4695, Class Loss: 0.9722\n",
      "Epoch 10/50, Loss: 9.6910, Domain Loss: 8.9815, Class Loss: 0.7095\n",
      "Epoch 11/50, Loss: 10.2864, Domain Loss: 9.7081, Class Loss: 0.5784\n",
      "Epoch 12/50, Loss: 3.6280, Domain Loss: 3.0506, Class Loss: 0.5774\n",
      "Epoch 13/50, Loss: 2.7403, Domain Loss: 2.0670, Class Loss: 0.6733\n",
      "Epoch 14/50, Loss: 3.2227, Domain Loss: 2.5958, Class Loss: 0.6269\n",
      "Epoch 15/50, Loss: 3.0394, Domain Loss: 2.4651, Class Loss: 0.5743\n",
      "Epoch 16/50, Loss: 2.3797, Domain Loss: 1.8568, Class Loss: 0.5229\n",
      "Epoch 17/50, Loss: 2.0472, Domain Loss: 1.5331, Class Loss: 0.5142\n",
      "Epoch 18/50, Loss: 1.8659, Domain Loss: 1.4030, Class Loss: 0.4630\n",
      "Epoch 19/50, Loss: 1.8333, Domain Loss: 1.4021, Class Loss: 0.4311\n",
      "Epoch 20/50, Loss: 1.8040, Domain Loss: 1.3917, Class Loss: 0.4124\n",
      "Epoch 21/50, Loss: 1.6940, Domain Loss: 1.3682, Class Loss: 0.3259\n",
      "Epoch 22/50, Loss: 1.6917, Domain Loss: 1.3690, Class Loss: 0.3227\n",
      "Epoch 23/50, Loss: 1.6618, Domain Loss: 1.3722, Class Loss: 0.2896\n",
      "Epoch 24/50, Loss: 1.6562, Domain Loss: 1.3742, Class Loss: 0.2819\n",
      "Epoch 25/50, Loss: 1.5921, Domain Loss: 1.3645, Class Loss: 0.2276\n",
      "Epoch 26/50, Loss: 1.6236, Domain Loss: 1.3961, Class Loss: 0.2275\n",
      "Epoch 27/50, Loss: 1.5695, Domain Loss: 1.3746, Class Loss: 0.1950\n",
      "Epoch 28/50, Loss: 1.5780, Domain Loss: 1.3818, Class Loss: 0.1962\n",
      "Epoch 29/50, Loss: 1.5145, Domain Loss: 1.3849, Class Loss: 0.1296\n",
      "Epoch 30/50, Loss: 1.5066, Domain Loss: 1.3953, Class Loss: 0.1113\n",
      "Epoch 31/50, Loss: 2.3490, Domain Loss: 1.5481, Class Loss: 0.8009\n",
      "Epoch 32/50, Loss: 1.8548, Domain Loss: 1.4940, Class Loss: 0.3608\n",
      "Epoch 33/50, Loss: 1.8007, Domain Loss: 1.4832, Class Loss: 0.3175\n",
      "Epoch 34/50, Loss: 1.6197, Domain Loss: 1.4081, Class Loss: 0.2115\n",
      "Epoch 35/50, Loss: 1.5564, Domain Loss: 1.3889, Class Loss: 0.1675\n",
      "Epoch 36/50, Loss: 1.4881, Domain Loss: 1.3791, Class Loss: 0.1090\n",
      "Epoch 37/50, Loss: 1.4955, Domain Loss: 1.3966, Class Loss: 0.0989\n",
      "Epoch 38/50, Loss: 1.4807, Domain Loss: 1.4001, Class Loss: 0.0806\n",
      "Epoch 39/50, Loss: 1.5305, Domain Loss: 1.3976, Class Loss: 0.1328\n",
      "Epoch 40/50, Loss: 1.5364, Domain Loss: 1.4150, Class Loss: 0.1215\n",
      "Epoch 41/50, Loss: 1.4634, Domain Loss: 1.3904, Class Loss: 0.0731\n",
      "Epoch 42/50, Loss: 1.4775, Domain Loss: 1.3976, Class Loss: 0.0799\n",
      "Epoch 43/50, Loss: 1.4757, Domain Loss: 1.3935, Class Loss: 0.0822\n",
      "Epoch 44/50, Loss: 1.4705, Domain Loss: 1.4095, Class Loss: 0.0610\n",
      "Epoch 45/50, Loss: 1.4860, Domain Loss: 1.4267, Class Loss: 0.0593\n",
      "Epoch 46/50, Loss: 1.4771, Domain Loss: 1.4250, Class Loss: 0.0520\n",
      "Epoch 47/50, Loss: 1.4691, Domain Loss: 1.4251, Class Loss: 0.0441\n",
      "Epoch 48/50, Loss: 1.4884, Domain Loss: 1.4419, Class Loss: 0.0465\n",
      "Epoch 49/50, Loss: 1.5277, Domain Loss: 1.4544, Class Loss: 0.0733\n",
      "Epoch 50/50, Loss: 1.4959, Domain Loss: 1.4269, Class Loss: 0.0691\n",
      "75.90\n",
      "\n",
      "\n",
      "Epoch 1/50, Loss: 2.3334, Domain Loss: 1.3789, Class Loss: 0.9546\n",
      "Epoch 2/50, Loss: 1.9300, Domain Loss: 1.3229, Class Loss: 0.6072\n",
      "Epoch 3/50, Loss: 1.8036, Domain Loss: 1.3210, Class Loss: 0.4826\n",
      "Epoch 4/50, Loss: 1.8936, Domain Loss: 1.5608, Class Loss: 0.3328\n",
      "Epoch 5/50, Loss: 3.4362, Domain Loss: 3.1045, Class Loss: 0.3317\n",
      "Epoch 6/50, Loss: 3.8062, Domain Loss: 3.4925, Class Loss: 0.3136\n",
      "Epoch 7/50, Loss: 9.0974, Domain Loss: 8.7486, Class Loss: 0.3488\n",
      "Epoch 8/50, Loss: 3.5563, Domain Loss: 3.3221, Class Loss: 0.2342\n",
      "Epoch 9/50, Loss: 3.5619, Domain Loss: 2.9994, Class Loss: 0.5626\n",
      "Epoch 10/50, Loss: 2.7551, Domain Loss: 2.3248, Class Loss: 0.4303\n",
      "Epoch 11/50, Loss: 1.9114, Domain Loss: 1.6728, Class Loss: 0.2387\n",
      "Epoch 12/50, Loss: 1.6513, Domain Loss: 1.4697, Class Loss: 0.1816\n",
      "Epoch 13/50, Loss: 1.8054, Domain Loss: 1.6514, Class Loss: 0.1540\n",
      "Epoch 14/50, Loss: 1.7702, Domain Loss: 1.6445, Class Loss: 0.1257\n",
      "Epoch 15/50, Loss: 1.6015, Domain Loss: 1.4839, Class Loss: 0.1176\n",
      "Epoch 16/50, Loss: 1.6900, Domain Loss: 1.5492, Class Loss: 0.1408\n",
      "Epoch 17/50, Loss: 1.9294, Domain Loss: 1.8061, Class Loss: 0.1233\n",
      "Epoch 18/50, Loss: 3.5215, Domain Loss: 3.2991, Class Loss: 0.2224\n",
      "Epoch 19/50, Loss: 2.6183, Domain Loss: 2.2429, Class Loss: 0.3754\n",
      "Epoch 20/50, Loss: 2.3121, Domain Loss: 1.8569, Class Loss: 0.4552\n",
      "Epoch 21/50, Loss: 2.3469, Domain Loss: 1.9304, Class Loss: 0.4165\n",
      "Epoch 22/50, Loss: 3.2166, Domain Loss: 2.7295, Class Loss: 0.4871\n",
      "Epoch 23/50, Loss: 2.7245, Domain Loss: 2.3630, Class Loss: 0.3615\n",
      "Epoch 24/50, Loss: 2.1609, Domain Loss: 1.8416, Class Loss: 0.3193\n",
      "Epoch 25/50, Loss: 4.3067, Domain Loss: 2.5294, Class Loss: 1.7773\n",
      "Epoch 26/50, Loss: 4.7716, Domain Loss: 3.2621, Class Loss: 1.5096\n",
      "Epoch 27/50, Loss: 2.5666, Domain Loss: 1.9607, Class Loss: 0.6058\n",
      "Epoch 28/50, Loss: 2.7533, Domain Loss: 2.1436, Class Loss: 0.6097\n",
      "Epoch 29/50, Loss: 2.5857, Domain Loss: 2.0382, Class Loss: 0.5475\n",
      "Epoch 30/50, Loss: 1.9491, Domain Loss: 1.5383, Class Loss: 0.4108\n",
      "Epoch 31/50, Loss: 3.0264, Domain Loss: 2.2206, Class Loss: 0.8058\n",
      "Epoch 32/50, Loss: 2.9641, Domain Loss: 2.3547, Class Loss: 0.6095\n",
      "Epoch 33/50, Loss: 2.2766, Domain Loss: 1.7121, Class Loss: 0.5645\n",
      "Epoch 34/50, Loss: 1.9841, Domain Loss: 1.5155, Class Loss: 0.4686\n",
      "Epoch 35/50, Loss: 2.1821, Domain Loss: 1.6641, Class Loss: 0.5180\n",
      "Epoch 36/50, Loss: 2.3956, Domain Loss: 1.8639, Class Loss: 0.5318\n",
      "Epoch 37/50, Loss: 2.0215, Domain Loss: 1.5474, Class Loss: 0.4741\n",
      "Epoch 38/50, Loss: 2.2030, Domain Loss: 1.6155, Class Loss: 0.5875\n",
      "Epoch 39/50, Loss: 1.8945, Domain Loss: 1.4294, Class Loss: 0.4651\n",
      "Epoch 40/50, Loss: 2.2757, Domain Loss: 1.8212, Class Loss: 0.4546\n",
      "Epoch 41/50, Loss: 2.1386, Domain Loss: 1.7481, Class Loss: 0.3905\n",
      "Epoch 42/50, Loss: 1.7969, Domain Loss: 1.5430, Class Loss: 0.2538\n",
      "Epoch 43/50, Loss: 1.6642, Domain Loss: 1.4506, Class Loss: 0.2137\n",
      "Epoch 44/50, Loss: 1.5956, Domain Loss: 1.4133, Class Loss: 0.1823\n",
      "Epoch 45/50, Loss: 1.5321, Domain Loss: 1.4061, Class Loss: 0.1260\n",
      "Epoch 46/50, Loss: 1.5652, Domain Loss: 1.4296, Class Loss: 0.1356\n",
      "Epoch 47/50, Loss: 1.5411, Domain Loss: 1.4187, Class Loss: 0.1224\n",
      "Epoch 48/50, Loss: 1.4867, Domain Loss: 1.3976, Class Loss: 0.0891\n",
      "Epoch 49/50, Loss: 1.5547, Domain Loss: 1.4463, Class Loss: 0.1084\n",
      "Epoch 50/50, Loss: 1.4835, Domain Loss: 1.3974, Class Loss: 0.0861\n",
      "79.08\n",
      "\n",
      "\n",
      "Epoch 1/50, Loss: 2.3068, Domain Loss: 1.3811, Class Loss: 0.9256\n",
      "Epoch 2/50, Loss: 1.9313, Domain Loss: 1.3336, Class Loss: 0.5978\n",
      "Epoch 3/50, Loss: 1.7874, Domain Loss: 1.3608, Class Loss: 0.4267\n",
      "Epoch 4/50, Loss: 1.8194, Domain Loss: 1.5290, Class Loss: 0.2904\n",
      "Epoch 5/50, Loss: 7.0313, Domain Loss: 6.7593, Class Loss: 0.2720\n",
      "Epoch 6/50, Loss: 4.7228, Domain Loss: 4.5340, Class Loss: 0.1888\n",
      "Epoch 7/50, Loss: 3.1158, Domain Loss: 2.9529, Class Loss: 0.1630\n",
      "Epoch 8/50, Loss: 7.4254, Domain Loss: 7.1086, Class Loss: 0.3168\n",
      "Epoch 9/50, Loss: 12.5682, Domain Loss: 11.7346, Class Loss: 0.8336\n",
      "Epoch 10/50, Loss: 26.7108, Domain Loss: 25.7099, Class Loss: 1.0009\n",
      "Epoch 11/50, Loss: 12.0353, Domain Loss: 11.3684, Class Loss: 0.6670\n",
      "Epoch 12/50, Loss: 5.6625, Domain Loss: 5.0781, Class Loss: 0.5844\n",
      "Epoch 13/50, Loss: 3.0534, Domain Loss: 2.4640, Class Loss: 0.5894\n",
      "Epoch 14/50, Loss: 2.5769, Domain Loss: 2.0368, Class Loss: 0.5400\n",
      "Epoch 15/50, Loss: 2.2583, Domain Loss: 1.7145, Class Loss: 0.5438\n",
      "Epoch 16/50, Loss: 2.3464, Domain Loss: 1.8199, Class Loss: 0.5265\n",
      "Epoch 17/50, Loss: 1.8589, Domain Loss: 1.4373, Class Loss: 0.4216\n",
      "Epoch 18/50, Loss: 1.9168, Domain Loss: 1.5146, Class Loss: 0.4022\n",
      "Epoch 19/50, Loss: 2.7695, Domain Loss: 2.2474, Class Loss: 0.5222\n",
      "Epoch 20/50, Loss: 2.4858, Domain Loss: 2.0368, Class Loss: 0.4490\n",
      "Epoch 21/50, Loss: 2.7379, Domain Loss: 2.1736, Class Loss: 0.5643\n",
      "Epoch 22/50, Loss: 2.2267, Domain Loss: 1.7237, Class Loss: 0.5030\n",
      "Epoch 23/50, Loss: 2.2280, Domain Loss: 1.7461, Class Loss: 0.4819\n",
      "Epoch 24/50, Loss: 2.2654, Domain Loss: 1.8416, Class Loss: 0.4238\n",
      "Epoch 25/50, Loss: 2.1027, Domain Loss: 1.6524, Class Loss: 0.4503\n",
      "Epoch 26/50, Loss: 2.0537, Domain Loss: 1.6400, Class Loss: 0.4137\n",
      "Epoch 27/50, Loss: 1.8685, Domain Loss: 1.5153, Class Loss: 0.3533\n",
      "Epoch 28/50, Loss: 1.8450, Domain Loss: 1.4901, Class Loss: 0.3549\n",
      "Epoch 29/50, Loss: 1.6995, Domain Loss: 1.4049, Class Loss: 0.2945\n",
      "Epoch 30/50, Loss: 1.7060, Domain Loss: 1.4292, Class Loss: 0.2768\n",
      "Epoch 31/50, Loss: 1.6935, Domain Loss: 1.3435, Class Loss: 0.3500\n",
      "Epoch 32/50, Loss: 1.6187, Domain Loss: 1.3669, Class Loss: 0.2519\n",
      "Epoch 33/50, Loss: 1.7083, Domain Loss: 1.4319, Class Loss: 0.2764\n",
      "Epoch 34/50, Loss: 1.5954, Domain Loss: 1.4113, Class Loss: 0.1841\n",
      "Epoch 35/50, Loss: 1.5212, Domain Loss: 1.3935, Class Loss: 0.1277\n",
      "Epoch 36/50, Loss: 1.4682, Domain Loss: 1.3908, Class Loss: 0.0774\n",
      "Epoch 37/50, Loss: 1.4552, Domain Loss: 1.3851, Class Loss: 0.0701\n",
      "Epoch 38/50, Loss: 1.4730, Domain Loss: 1.3681, Class Loss: 0.1049\n",
      "Epoch 39/50, Loss: 1.5113, Domain Loss: 1.3917, Class Loss: 0.1195\n",
      "Epoch 40/50, Loss: 1.4290, Domain Loss: 1.3701, Class Loss: 0.0588\n",
      "Epoch 41/50, Loss: 1.4334, Domain Loss: 1.3713, Class Loss: 0.0621\n",
      "Epoch 42/50, Loss: 1.4504, Domain Loss: 1.3845, Class Loss: 0.0660\n",
      "Epoch 43/50, Loss: 1.4739, Domain Loss: 1.4086, Class Loss: 0.0653\n",
      "Epoch 44/50, Loss: 1.4707, Domain Loss: 1.4149, Class Loss: 0.0558\n",
      "Epoch 45/50, Loss: 1.4870, Domain Loss: 1.4259, Class Loss: 0.0611\n",
      "Epoch 46/50, Loss: 1.5062, Domain Loss: 1.4585, Class Loss: 0.0476\n",
      "Epoch 47/50, Loss: 1.5368, Domain Loss: 1.4427, Class Loss: 0.0941\n",
      "Epoch 48/50, Loss: 1.4614, Domain Loss: 1.4081, Class Loss: 0.0533\n",
      "Epoch 49/50, Loss: 1.4770, Domain Loss: 1.4245, Class Loss: 0.0525\n",
      "Epoch 50/50, Loss: 1.5084, Domain Loss: 1.4444, Class Loss: 0.0639\n",
      "75.12\n",
      "\n",
      "\n",
      "Source performance:\n",
      "85.73 89.05 85.52 84.67 \n",
      "Target performance:\n",
      "76.70 79.09 76.33 75.35 \n",
      "\n",
      "Deep CORALtarget performance: 99.33 91.13 66.00 48.85 \n",
      "Deep CORAL Run 1/3\n",
      "Epoch 1: Source Val Acc = 0.7212, Target Val Acc = 0.7278\n",
      "Epoch 2: Source Val Acc = 0.9598, Target Val Acc = 0.7842\n",
      "Epoch 3: Source Val Acc = 0.9448, Target Val Acc = 0.8657\n",
      "Epoch 4: Source Val Acc = 0.5827, Target Val Acc = 0.6313\n",
      "Epoch 5: Source Val Acc = 0.9586, Target Val Acc = 0.8663\n",
      "Epoch 6: Source Val Acc = 0.7206, Target Val Acc = 0.5803\n",
      "Epoch 7: Source Val Acc = 0.9616, Target Val Acc = 0.8034\n",
      "Epoch 8: Source Val Acc = 0.9700, Target Val Acc = 0.7980\n",
      "Epoch 9: Source Val Acc = 0.9766, Target Val Acc = 0.8645\n",
      "Epoch 10: Source Val Acc = 0.9904, Target Val Acc = 0.8645\n",
      "Epoch 11: Source Val Acc = 0.9988, Target Val Acc = 0.9257\n",
      "Epoch 12: Source Val Acc = 0.9952, Target Val Acc = 0.8837\n",
      "Epoch 13: Source Val Acc = 1.0000, Target Val Acc = 0.9454\n",
      "Epoch 14: Source Val Acc = 0.9994, Target Val Acc = 0.9412\n",
      "Epoch 15: Source Val Acc = 1.0000, Target Val Acc = 0.9466\n",
      "Epoch 16: Source Val Acc = 0.9994, Target Val Acc = 0.9436\n",
      "Epoch 17: Source Val Acc = 1.0000, Target Val Acc = 0.9424\n",
      "Epoch 18: Source Val Acc = 0.9994, Target Val Acc = 0.9460\n",
      "Early stopping triggered.\n",
      "Run 1 finished: Best Source Val Acc = 0.9994, Target Val Acc = 0.9460\n",
      "\n",
      "Deep CORAL Run 2/3\n",
      "Epoch 1: Source Val Acc = 0.7422, Target Val Acc = 0.6433\n",
      "Epoch 2: Source Val Acc = 0.8219, Target Val Acc = 0.7212\n",
      "Epoch 3: Source Val Acc = 0.7566, Target Val Acc = 0.7224\n",
      "Epoch 4: Source Val Acc = 0.6763, Target Val Acc = 0.5084\n",
      "Epoch 5: Source Val Acc = 0.9371, Target Val Acc = 0.8028\n",
      "Epoch 6: Source Val Acc = 0.7638, Target Val Acc = 0.7356\n",
      "Epoch 7: Source Val Acc = 0.9173, Target Val Acc = 0.8321\n",
      "Epoch 8: Source Val Acc = 0.8867, Target Val Acc = 0.7872\n",
      "Epoch 9: Source Val Acc = 0.9269, Target Val Acc = 0.8369\n",
      "Epoch 10: Source Val Acc = 0.9988, Target Val Acc = 0.9113\n",
      "Epoch 11: Source Val Acc = 0.9790, Target Val Acc = 0.8825\n",
      "Epoch 12: Source Val Acc = 0.9436, Target Val Acc = 0.8231\n",
      "Epoch 13: Source Val Acc = 0.6715, Target Val Acc = 0.6571\n",
      "Epoch 14: Source Val Acc = 0.9982, Target Val Acc = 0.9041\n",
      "Epoch 15: Source Val Acc = 0.9269, Target Val Acc = 0.8243\n",
      "Early stopping triggered.\n",
      "Run 2 finished: Best Source Val Acc = 0.9269, Target Val Acc = 0.8243\n",
      "\n",
      "Deep CORAL Run 3/3\n",
      "Epoch 1: Source Val Acc = 0.7098, Target Val Acc = 0.6445\n",
      "Epoch 2: Source Val Acc = 0.7554, Target Val Acc = 0.6529\n",
      "Epoch 3: Source Val Acc = 0.7656, Target Val Acc = 0.7374\n",
      "Epoch 4: Source Val Acc = 0.9400, Target Val Acc = 0.8082\n",
      "Epoch 5: Source Val Acc = 0.7722, Target Val Acc = 0.7620\n",
      "Epoch 6: Source Val Acc = 0.9442, Target Val Acc = 0.7806\n",
      "Epoch 7: Source Val Acc = 0.9868, Target Val Acc = 0.8759\n",
      "Epoch 8: Source Val Acc = 0.9910, Target Val Acc = 0.8939\n",
      "Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.8885\n",
      "Epoch 10: Source Val Acc = 0.8801, Target Val Acc = 0.7830\n",
      "Epoch 11: Source Val Acc = 0.9982, Target Val Acc = 0.9059\n",
      "Epoch 12: Source Val Acc = 0.8945, Target Val Acc = 0.8369\n",
      "Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 0.9053\n",
      "Epoch 14: Source Val Acc = 0.9994, Target Val Acc = 0.9113\n",
      "Epoch 15: Source Val Acc = 0.9994, Target Val Acc = 0.9299\n",
      "Epoch 16: Source Val Acc = 0.5546, Target Val Acc = 0.6457\n",
      "Epoch 17: Source Val Acc = 0.9706, Target Val Acc = 0.8561\n",
      "Epoch 18: Source Val Acc = 0.9844, Target Val Acc = 0.8453\n",
      "Epoch 19: Source Val Acc = 0.9832, Target Val Acc = 0.8891\n",
      "Early stopping triggered.\n",
      "Run 3 finished: Best Source Val Acc = 0.9832, Target Val Acc = 0.8891\n",
      "\n",
      "Deep CORAL: Average Source Val Acc = 0.9698, Average Target Val Acc = 0.8865\n",
      "STAR\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 1.5615, Discrepancy Loss: 0.1029\n",
      "Epoch [2/50], Class Loss: 0.9666, Discrepancy Loss: 0.0818\n",
      "Epoch [3/50], Class Loss: 0.5711, Discrepancy Loss: 0.0636\n",
      "Epoch [4/50], Class Loss: 0.3416, Discrepancy Loss: 0.0474\n",
      "Epoch [5/50], Class Loss: 0.4846, Discrepancy Loss: 0.0524\n",
      "Epoch [6/50], Class Loss: 0.3425, Discrepancy Loss: 0.0431\n",
      "Epoch [7/50], Class Loss: 0.1235, Discrepancy Loss: 0.0325\n",
      "Epoch [8/50], Class Loss: 0.0715, Discrepancy Loss: 0.0218\n",
      "Epoch [9/50], Class Loss: 0.1945, Discrepancy Loss: 0.0302\n",
      "Epoch [10/50], Class Loss: 0.0401, Discrepancy Loss: 0.0238\n",
      "Epoch [11/50], Class Loss: 0.1056, Discrepancy Loss: 0.0292\n",
      "Epoch [12/50], Class Loss: 0.0179, Discrepancy Loss: 0.0238\n",
      "Epoch [13/50], Class Loss: 0.0101, Discrepancy Loss: 0.0221\n",
      "Epoch [14/50], Class Loss: 0.0110, Discrepancy Loss: 0.0170\n",
      "Epoch [15/50], Class Loss: 0.0071, Discrepancy Loss: 0.0187\n",
      "Epoch [16/50], Class Loss: 0.0068, Discrepancy Loss: 0.0217\n",
      "Epoch [17/50], Class Loss: 0.0135, Discrepancy Loss: 0.0164\n",
      "Epoch [18/50], Class Loss: 0.0203, Discrepancy Loss: 0.0208\n",
      "Epoch [19/50], Class Loss: 0.0039, Discrepancy Loss: 0.0158\n",
      "Epoch [20/50], Class Loss: 0.0056, Discrepancy Loss: 0.0165\n",
      "Epoch [21/50], Class Loss: 0.0051, Discrepancy Loss: 0.0169\n",
      "Epoch [22/50], Class Loss: 0.0050, Discrepancy Loss: 0.0181\n",
      "Epoch [23/50], Class Loss: 0.0089, Discrepancy Loss: 0.0157\n",
      "Epoch [24/50], Class Loss: 0.0035, Discrepancy Loss: 0.0194\n",
      "Epoch [25/50], Class Loss: 0.0073, Discrepancy Loss: 0.0183\n",
      "Epoch [26/50], Class Loss: 0.0043, Discrepancy Loss: 0.0158\n",
      "Epoch [27/50], Class Loss: 0.0058, Discrepancy Loss: 0.0138\n",
      "Epoch [28/50], Class Loss: 0.0044, Discrepancy Loss: 0.0135\n",
      "Epoch [29/50], Class Loss: 0.0049, Discrepancy Loss: 0.0149\n",
      "Epoch [30/50], Class Loss: 0.0030, Discrepancy Loss: 0.0145\n",
      "Epoch [31/50], Class Loss: 0.0042, Discrepancy Loss: 0.0176\n",
      "Epoch [32/50], Class Loss: 0.0742, Discrepancy Loss: 0.0132\n",
      "Epoch [33/50], Class Loss: 0.0128, Discrepancy Loss: 0.0173\n",
      "Epoch [34/50], Class Loss: 0.0041, Discrepancy Loss: 0.0136\n",
      "Epoch [35/50], Class Loss: 0.0039, Discrepancy Loss: 0.0158\n",
      "Epoch [36/50], Class Loss: 0.0024, Discrepancy Loss: 0.0180\n",
      "Epoch [37/50], Class Loss: 0.0029, Discrepancy Loss: 0.0174\n",
      "Epoch [38/50], Class Loss: 0.0049, Discrepancy Loss: 0.0169\n",
      "Epoch [39/50], Class Loss: 0.0022, Discrepancy Loss: 0.0174\n",
      "Epoch [40/50], Class Loss: 0.0036, Discrepancy Loss: 0.0196\n",
      "Epoch [41/50], Class Loss: 0.0087, Discrepancy Loss: 0.0153\n",
      "Epoch [42/50], Class Loss: 0.0174, Discrepancy Loss: 0.0166\n",
      "Epoch [43/50], Class Loss: 0.0027, Discrepancy Loss: 0.0186\n",
      "Epoch [44/50], Class Loss: 0.0180, Discrepancy Loss: 0.0158\n",
      "Epoch [45/50], Class Loss: 0.0029, Discrepancy Loss: 0.0145\n",
      "Epoch [46/50], Class Loss: 0.0026, Discrepancy Loss: 0.0153\n",
      "Epoch [47/50], Class Loss: 0.0057, Discrepancy Loss: 0.0177\n",
      "Epoch [48/50], Class Loss: 0.0077, Discrepancy Loss: 0.0164\n",
      "Epoch [49/50], Class Loss: 0.1712, Discrepancy Loss: 0.0160\n",
      "Epoch [50/50], Class Loss: 0.0156, Discrepancy Loss: 0.0167\n",
      "Source Domain Performance - Accuracy: 98.08%, Precision: 98.14%, Recall: 98.06%, F1 Score: 98.04%\n",
      "Target Domain Performance - Accuracy: 89.81%, Precision: 90.83%, Recall: 89.39%, F1 Score: 89.15%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 1.2161, Discrepancy Loss: 0.1141\n",
      "Epoch [2/50], Class Loss: 0.9240, Discrepancy Loss: 0.0921\n",
      "Epoch [3/50], Class Loss: 0.6935, Discrepancy Loss: 0.0847\n",
      "Epoch [4/50], Class Loss: 0.4735, Discrepancy Loss: 0.0785\n",
      "Epoch [5/50], Class Loss: 0.4633, Discrepancy Loss: 0.0781\n",
      "Epoch [6/50], Class Loss: 0.3014, Discrepancy Loss: 0.0639\n",
      "Epoch [7/50], Class Loss: 0.2591, Discrepancy Loss: 0.0551\n",
      "Epoch [8/50], Class Loss: 0.5624, Discrepancy Loss: 0.0891\n",
      "Epoch [9/50], Class Loss: 0.1701, Discrepancy Loss: 0.0504\n",
      "Epoch [10/50], Class Loss: 0.1972, Discrepancy Loss: 0.0452\n",
      "Epoch [11/50], Class Loss: 0.0686, Discrepancy Loss: 0.0356\n",
      "Epoch [12/50], Class Loss: 0.0900, Discrepancy Loss: 0.0389\n",
      "Epoch [13/50], Class Loss: 0.1393, Discrepancy Loss: 0.0326\n",
      "Epoch [14/50], Class Loss: 0.0552, Discrepancy Loss: 0.0387\n",
      "Epoch [15/50], Class Loss: 0.0576, Discrepancy Loss: 0.0325\n",
      "Epoch [16/50], Class Loss: 0.0488, Discrepancy Loss: 0.0389\n",
      "Epoch [17/50], Class Loss: 0.0403, Discrepancy Loss: 0.0350\n",
      "Epoch [18/50], Class Loss: 0.0444, Discrepancy Loss: 0.0341\n",
      "Epoch [19/50], Class Loss: 0.0290, Discrepancy Loss: 0.0289\n",
      "Epoch [20/50], Class Loss: 0.0706, Discrepancy Loss: 0.0292\n",
      "Epoch [21/50], Class Loss: 0.0622, Discrepancy Loss: 0.0242\n",
      "Epoch [22/50], Class Loss: 0.0555, Discrepancy Loss: 0.0289\n",
      "Epoch [23/50], Class Loss: 0.0252, Discrepancy Loss: 0.0260\n",
      "Epoch [24/50], Class Loss: 0.0914, Discrepancy Loss: 0.0260\n",
      "Epoch [25/50], Class Loss: 0.0280, Discrepancy Loss: 0.0270\n",
      "Epoch [26/50], Class Loss: 0.0261, Discrepancy Loss: 0.0244\n",
      "Epoch [27/50], Class Loss: 0.0186, Discrepancy Loss: 0.0284\n",
      "Epoch [28/50], Class Loss: 0.0165, Discrepancy Loss: 0.0263\n",
      "Epoch [29/50], Class Loss: 0.0175, Discrepancy Loss: 0.0263\n",
      "Epoch [30/50], Class Loss: 0.0166, Discrepancy Loss: 0.0265\n",
      "Epoch [31/50], Class Loss: 0.0161, Discrepancy Loss: 0.0240\n",
      "Epoch [32/50], Class Loss: 0.0182, Discrepancy Loss: 0.0211\n",
      "Epoch [33/50], Class Loss: 0.0210, Discrepancy Loss: 0.0230\n",
      "Epoch [34/50], Class Loss: 0.0941, Discrepancy Loss: 0.0234\n",
      "Epoch [35/50], Class Loss: 0.0205, Discrepancy Loss: 0.0210\n",
      "Epoch [36/50], Class Loss: 0.0167, Discrepancy Loss: 0.0253\n",
      "Epoch [37/50], Class Loss: 0.0399, Discrepancy Loss: 0.0247\n",
      "Epoch [38/50], Class Loss: 0.0197, Discrepancy Loss: 0.0276\n",
      "Epoch [39/50], Class Loss: 0.0462, Discrepancy Loss: 0.0227\n",
      "Epoch [40/50], Class Loss: 0.0189, Discrepancy Loss: 0.0192\n",
      "Epoch [41/50], Class Loss: 0.0141, Discrepancy Loss: 0.0253\n",
      "Epoch [42/50], Class Loss: 0.0163, Discrepancy Loss: 0.0237\n",
      "Epoch [43/50], Class Loss: 0.0224, Discrepancy Loss: 0.0213\n",
      "Epoch [44/50], Class Loss: 0.0258, Discrepancy Loss: 0.0236\n",
      "Epoch [45/50], Class Loss: 0.0191, Discrepancy Loss: 0.0251\n",
      "Epoch [46/50], Class Loss: 0.0296, Discrepancy Loss: 0.0240\n",
      "Epoch [47/50], Class Loss: 0.0625, Discrepancy Loss: 0.0255\n",
      "Epoch [48/50], Class Loss: 0.0245, Discrepancy Loss: 0.0237\n",
      "Epoch [49/50], Class Loss: 0.0304, Discrepancy Loss: 0.0234\n",
      "Epoch [50/50], Class Loss: 0.0390, Discrepancy Loss: 0.0232\n",
      "Source Domain Performance - Accuracy: 95.98%, Precision: 96.36%, Recall: 95.94%, F1 Score: 95.89%\n",
      "Target Domain Performance - Accuracy: 87.47%, Precision: 87.15%, Recall: 87.11%, F1 Score: 86.95%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 1.3587, Discrepancy Loss: 0.1099\n",
      "Epoch [2/50], Class Loss: 0.7863, Discrepancy Loss: 0.0840\n",
      "Epoch [3/50], Class Loss: 0.6891, Discrepancy Loss: 0.0844\n",
      "Epoch [4/50], Class Loss: 0.3147, Discrepancy Loss: 0.0580\n",
      "Epoch [5/50], Class Loss: 0.8022, Discrepancy Loss: 0.0948\n",
      "Epoch [6/50], Class Loss: 0.1791, Discrepancy Loss: 0.0574\n",
      "Epoch [7/50], Class Loss: 0.2340, Discrepancy Loss: 0.0374\n",
      "Epoch [8/50], Class Loss: 0.2027, Discrepancy Loss: 0.0406\n",
      "Epoch [9/50], Class Loss: 0.0511, Discrepancy Loss: 0.0279\n",
      "Epoch [10/50], Class Loss: 0.4341, Discrepancy Loss: 0.0580\n",
      "Epoch [11/50], Class Loss: 0.2211, Discrepancy Loss: 0.0619\n",
      "Epoch [12/50], Class Loss: 0.1443, Discrepancy Loss: 0.0479\n",
      "Epoch [13/50], Class Loss: 0.1449, Discrepancy Loss: 0.0405\n",
      "Epoch [14/50], Class Loss: 0.0893, Discrepancy Loss: 0.0358\n",
      "Epoch [15/50], Class Loss: 0.0754, Discrepancy Loss: 0.0330\n",
      "Epoch [16/50], Class Loss: 0.0863, Discrepancy Loss: 0.0343\n",
      "Epoch [17/50], Class Loss: 0.0488, Discrepancy Loss: 0.0302\n",
      "Epoch [18/50], Class Loss: 0.0285, Discrepancy Loss: 0.0306\n",
      "Epoch [19/50], Class Loss: 0.0426, Discrepancy Loss: 0.0293\n",
      "Epoch [20/50], Class Loss: 0.0418, Discrepancy Loss: 0.0240\n",
      "Epoch [21/50], Class Loss: 0.0233, Discrepancy Loss: 0.0243\n",
      "Epoch [22/50], Class Loss: 0.0455, Discrepancy Loss: 0.0239\n",
      "Epoch [23/50], Class Loss: 0.0281, Discrepancy Loss: 0.0241\n",
      "Epoch [24/50], Class Loss: 0.0175, Discrepancy Loss: 0.0220\n",
      "Epoch [25/50], Class Loss: 0.1011, Discrepancy Loss: 0.0245\n",
      "Epoch [26/50], Class Loss: 0.0199, Discrepancy Loss: 0.0258\n",
      "Epoch [27/50], Class Loss: 0.0339, Discrepancy Loss: 0.0226\n",
      "Epoch [28/50], Class Loss: 0.0210, Discrepancy Loss: 0.0263\n",
      "Epoch [29/50], Class Loss: 0.0344, Discrepancy Loss: 0.0242\n",
      "Epoch [30/50], Class Loss: 0.0175, Discrepancy Loss: 0.0258\n",
      "Epoch [31/50], Class Loss: 0.0248, Discrepancy Loss: 0.0228\n",
      "Epoch [32/50], Class Loss: 0.0683, Discrepancy Loss: 0.0208\n",
      "Epoch [33/50], Class Loss: 0.0133, Discrepancy Loss: 0.0231\n",
      "Epoch [34/50], Class Loss: 0.0130, Discrepancy Loss: 0.0263\n",
      "Epoch [35/50], Class Loss: 0.0137, Discrepancy Loss: 0.0271\n",
      "Epoch [36/50], Class Loss: 0.0159, Discrepancy Loss: 0.0245\n",
      "Epoch [37/50], Class Loss: 0.0181, Discrepancy Loss: 0.0234\n",
      "Epoch [38/50], Class Loss: 0.0296, Discrepancy Loss: 0.0223\n",
      "Epoch [39/50], Class Loss: 0.0182, Discrepancy Loss: 0.0212\n",
      "Epoch [40/50], Class Loss: 0.0187, Discrepancy Loss: 0.0217\n",
      "Epoch [41/50], Class Loss: 0.0224, Discrepancy Loss: 0.0255\n",
      "Epoch [42/50], Class Loss: 0.0167, Discrepancy Loss: 0.0304\n",
      "Epoch [43/50], Class Loss: 0.0224, Discrepancy Loss: 0.0236\n",
      "Epoch [44/50], Class Loss: 0.0131, Discrepancy Loss: 0.0272\n",
      "Epoch [45/50], Class Loss: 0.1111, Discrepancy Loss: 0.0213\n",
      "Epoch [46/50], Class Loss: 0.0141, Discrepancy Loss: 0.0224\n",
      "Epoch [47/50], Class Loss: 0.0140, Discrepancy Loss: 0.0233\n",
      "Epoch [48/50], Class Loss: 0.0887, Discrepancy Loss: 0.0258\n",
      "Epoch [49/50], Class Loss: 0.0100, Discrepancy Loss: 0.0232\n",
      "Epoch [50/50], Class Loss: 0.0775, Discrepancy Loss: 0.0248\n",
      "Source Domain Performance - Accuracy: 96.94%, Precision: 97.11%, Recall: 96.90%, F1 Score: 96.89%\n",
      "Target Domain Performance - Accuracy: 86.45%, Precision: 85.97%, Recall: 86.07%, F1 Score: 85.86%\n",
      "\n",
      "Source performance: 97.00% 97.21% 96.97% 96.94%\n",
      "Target performance: 87.91% 87.98% 87.52% 87.32%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 99.43%\n",
      "16qam: 64.82%\n",
      "8apsk: 85.84%\n",
      "MCD\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 1.0264, Discrepancy Loss: 0.0383\n",
      "Validation Loss: 2.0680\n",
      "Epoch [2/50], Class Loss: 0.2711, Discrepancy Loss: 0.0313\n",
      "Validation Loss: 2.6256\n",
      "Epoch [3/50], Class Loss: 0.1111, Discrepancy Loss: 0.0393\n",
      "Validation Loss: 0.0843\n",
      "Epoch [4/50], Class Loss: 0.0757, Discrepancy Loss: 0.0584\n",
      "Validation Loss: 0.7885\n",
      "Epoch [5/50], Class Loss: 0.0692, Discrepancy Loss: 0.0381\n",
      "Validation Loss: 13.3109\n",
      "Epoch [6/50], Class Loss: 0.0269, Discrepancy Loss: 0.0643\n",
      "Validation Loss: 0.4425\n",
      "Epoch [7/50], Class Loss: 0.0297, Discrepancy Loss: 0.1337\n",
      "Validation Loss: 0.8370\n",
      "Epoch [8/50], Class Loss: 0.2492, Discrepancy Loss: 0.1291\n",
      "Validation Loss: 0.9140\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 88.91%, Precision: 92.08%, Recall: 88.84%, F1 Score: 88.17%\n",
      "Target Domain Performance - Accuracy: 53.78%, Precision: 59.76%, Recall: 52.23%, F1 Score: 44.93%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 0.9626, Discrepancy Loss: 0.0436\n",
      "Validation Loss: 4.9556\n",
      "Epoch [2/50], Class Loss: 0.3253, Discrepancy Loss: 0.0349\n",
      "Validation Loss: 0.3130\n",
      "Epoch [3/50], Class Loss: 0.1101, Discrepancy Loss: 0.0322\n",
      "Validation Loss: 23.5082\n",
      "Epoch [4/50], Class Loss: 0.1164, Discrepancy Loss: 0.0401\n",
      "Validation Loss: 4.1869\n",
      "Epoch [5/50], Class Loss: 0.0754, Discrepancy Loss: 0.0513\n",
      "Validation Loss: 0.5618\n",
      "Epoch [6/50], Class Loss: 0.0496, Discrepancy Loss: 0.0634\n",
      "Validation Loss: 12.6093\n",
      "Epoch [7/50], Class Loss: 0.0619, Discrepancy Loss: 0.0618\n",
      "Validation Loss: 4.4936\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 78.36%, Precision: 87.45%, Recall: 78.19%, F1 Score: 72.96%\n",
      "Target Domain Performance - Accuracy: 65.95%, Precision: 73.57%, Recall: 64.54%, F1 Score: 58.22%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 0.9989, Discrepancy Loss: 0.0449\n",
      "Validation Loss: 3.0554\n",
      "Epoch [2/50], Class Loss: 0.3064, Discrepancy Loss: 0.0453\n",
      "Validation Loss: 4.0880\n",
      "Epoch [3/50], Class Loss: 0.1704, Discrepancy Loss: 0.0295\n",
      "Validation Loss: 1.2366\n",
      "Epoch [4/50], Class Loss: 0.0801, Discrepancy Loss: 0.0291\n",
      "Validation Loss: 0.3716\n",
      "Epoch [5/50], Class Loss: 0.0587, Discrepancy Loss: 0.0285\n",
      "Validation Loss: 0.3897\n",
      "Epoch [6/50], Class Loss: 0.0315, Discrepancy Loss: 0.0646\n",
      "Validation Loss: 1.6742\n",
      "Epoch [7/50], Class Loss: 0.1256, Discrepancy Loss: 0.0893\n",
      "Validation Loss: 19.3489\n",
      "Epoch [8/50], Class Loss: 0.0664, Discrepancy Loss: 0.1090\n",
      "Validation Loss: 8.9002\n",
      "Epoch [9/50], Class Loss: 0.1002, Discrepancy Loss: 0.1158\n",
      "Validation Loss: 0.1143\n",
      "Epoch [10/50], Class Loss: 0.0627, Discrepancy Loss: 0.1080\n",
      "Validation Loss: 36.9839\n",
      "Epoch [11/50], Class Loss: 0.0315, Discrepancy Loss: 0.0555\n",
      "Validation Loss: 0.0055\n",
      "Epoch [12/50], Class Loss: 0.0042, Discrepancy Loss: 0.0591\n",
      "Validation Loss: 6.2343\n",
      "Epoch [13/50], Class Loss: 0.0198, Discrepancy Loss: 0.1137\n",
      "Validation Loss: 0.1348\n",
      "Epoch [14/50], Class Loss: 0.0259, Discrepancy Loss: 0.0980\n",
      "Validation Loss: 0.5362\n",
      "Epoch [15/50], Class Loss: 0.0086, Discrepancy Loss: 0.0457\n",
      "Validation Loss: 0.0291\n",
      "Epoch [16/50], Class Loss: 0.0122, Discrepancy Loss: 0.0839\n",
      "Validation Loss: 0.4058\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 91.85%, Precision: 93.48%, Recall: 92.36%, F1 Score: 91.97%\n",
      "Target Domain Performance - Accuracy: 37.35%, Precision: 55.94%, Recall: 38.37%, F1 Score: 33.02%\n",
      "\n",
      "Source performance: 86.37% 91.00% 86.46% 84.36%\n",
      "Target performance: 52.36% 63.09% 51.71% 45.39%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 70.11%\n",
      "qpsk: 31.20%\n",
      "16qam: 36.01%\n",
      "8apsk: 69.53%\n",
      "JAN\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 0.6138, JMMD Loss: 0.0552\n",
      "Validation Loss: 2.5582\n",
      "Epoch [2/50], Class Loss: 0.4238, JMMD Loss: 0.0520\n",
      "Validation Loss: 1.3560\n",
      "Epoch [3/50], Class Loss: 0.2601, JMMD Loss: 0.0885\n",
      "Validation Loss: 0.3007\n",
      "Epoch [4/50], Class Loss: 0.0590, JMMD Loss: 0.0931\n",
      "Validation Loss: 0.1236\n",
      "Epoch [5/50], Class Loss: 0.1348, JMMD Loss: 0.0943\n",
      "Validation Loss: 2.7738\n",
      "Epoch [6/50], Class Loss: 0.2431, JMMD Loss: 0.1352\n",
      "Validation Loss: 0.1699\n",
      "Epoch [7/50], Class Loss: 0.0526, JMMD Loss: 0.1358\n",
      "Validation Loss: 0.0210\n",
      "Epoch [8/50], Class Loss: 0.0499, JMMD Loss: 0.1202\n",
      "Validation Loss: 0.1569\n",
      "Epoch [9/50], Class Loss: 0.0378, JMMD Loss: 0.1172\n",
      "Validation Loss: 0.0979\n",
      "Epoch [10/50], Class Loss: 0.0229, JMMD Loss: 0.1042\n",
      "Validation Loss: 0.9657\n",
      "Epoch [11/50], Class Loss: 0.0159, JMMD Loss: 0.1022\n",
      "Validation Loss: 0.0236\n",
      "Epoch [12/50], Class Loss: 0.0077, JMMD Loss: 0.1012\n",
      "Validation Loss: 0.0113\n",
      "Epoch [13/50], Class Loss: 0.0130, JMMD Loss: 0.1126\n",
      "Validation Loss: 0.0244\n",
      "Epoch [14/50], Class Loss: 0.0103, JMMD Loss: 0.1046\n",
      "Validation Loss: 0.0587\n",
      "Epoch [15/50], Class Loss: 0.0081, JMMD Loss: 0.1045\n",
      "Validation Loss: 0.0165\n",
      "Epoch [16/50], Class Loss: 0.0066, JMMD Loss: 0.0993\n",
      "Validation Loss: 0.0205\n",
      "Epoch [17/50], Class Loss: 0.0079, JMMD Loss: 0.0912\n",
      "Validation Loss: 0.0324\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 98.98%, Precision: 98.98%, Recall: 98.97%, F1 Score: 98.95%\n",
      "Target Domain Performance - Accuracy: 91.01%, Precision: 90.83%, Recall: 90.86%, F1 Score: 90.73%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 0.5874, JMMD Loss: 0.0688\n",
      "Validation Loss: 4.0613\n",
      "Epoch [2/50], Class Loss: 0.4156, JMMD Loss: 0.0713\n",
      "Validation Loss: 0.3677\n",
      "Epoch [3/50], Class Loss: 0.1491, JMMD Loss: 0.1138\n",
      "Validation Loss: 0.8130\n",
      "Epoch [4/50], Class Loss: 0.0529, JMMD Loss: 0.1059\n",
      "Validation Loss: 0.0978\n",
      "Epoch [5/50], Class Loss: 0.0239, JMMD Loss: 0.0996\n",
      "Validation Loss: 0.7582\n",
      "Epoch [6/50], Class Loss: 0.0432, JMMD Loss: 0.0998\n",
      "Validation Loss: 0.1542\n",
      "Epoch [7/50], Class Loss: 0.0071, JMMD Loss: 0.0955\n",
      "Validation Loss: 0.0193\n",
      "Epoch [8/50], Class Loss: 0.1158, JMMD Loss: 0.0789\n",
      "Validation Loss: 0.0307\n",
      "Epoch [9/50], Class Loss: 0.0276, JMMD Loss: 0.0712\n",
      "Validation Loss: 0.0981\n",
      "Epoch [10/50], Class Loss: 0.0099, JMMD Loss: 0.0658\n",
      "Validation Loss: 0.0309\n",
      "Epoch [11/50], Class Loss: 0.0059, JMMD Loss: 0.0619\n",
      "Validation Loss: 0.0095\n",
      "Epoch [12/50], Class Loss: 0.0037, JMMD Loss: 0.0657\n",
      "Validation Loss: 0.0088\n",
      "Epoch [13/50], Class Loss: 0.0030, JMMD Loss: 0.0603\n",
      "Validation Loss: 0.0066\n",
      "Epoch [14/50], Class Loss: 0.0037, JMMD Loss: 0.0663\n",
      "Validation Loss: 0.0079\n",
      "Epoch [15/50], Class Loss: 0.0041, JMMD Loss: 0.0688\n",
      "Validation Loss: 0.0063\n",
      "Epoch [16/50], Class Loss: 0.0097, JMMD Loss: 0.0602\n",
      "Validation Loss: 0.0046\n",
      "Epoch [17/50], Class Loss: 0.0068, JMMD Loss: 0.0645\n",
      "Validation Loss: 0.0091\n",
      "Epoch [18/50], Class Loss: 0.0047, JMMD Loss: 0.0747\n",
      "Validation Loss: 0.0056\n",
      "Epoch [19/50], Class Loss: 0.0032, JMMD Loss: 0.0608\n",
      "Validation Loss: 0.0079\n",
      "Epoch [20/50], Class Loss: 0.0041, JMMD Loss: 0.0551\n",
      "Validation Loss: 0.0131\n",
      "Epoch [21/50], Class Loss: 0.0034, JMMD Loss: 0.0559\n",
      "Validation Loss: 0.0096\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.70%, Precision: 99.69%, Recall: 99.70%, F1 Score: 99.69%\n",
      "Target Domain Performance - Accuracy: 94.96%, Precision: 94.82%, Recall: 94.82%, F1 Score: 94.80%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 0.6165, JMMD Loss: 0.0625\n",
      "Validation Loss: 0.6056\n",
      "Epoch [2/50], Class Loss: 0.2736, JMMD Loss: 0.0903\n",
      "Validation Loss: 0.4506\n",
      "Epoch [3/50], Class Loss: 0.1453, JMMD Loss: 0.1055\n",
      "Validation Loss: 1.2659\n",
      "Epoch [4/50], Class Loss: 0.1269, JMMD Loss: 0.1078\n",
      "Validation Loss: 1.7302\n",
      "Epoch [5/50], Class Loss: 0.0966, JMMD Loss: 0.0816\n",
      "Validation Loss: 0.1065\n",
      "Epoch [6/50], Class Loss: 0.0536, JMMD Loss: 0.1133\n",
      "Validation Loss: 0.6366\n",
      "Epoch [7/50], Class Loss: 0.0390, JMMD Loss: 0.1019\n",
      "Validation Loss: 0.2208\n",
      "Epoch [8/50], Class Loss: 0.0538, JMMD Loss: 0.0946\n",
      "Validation Loss: 0.0638\n",
      "Epoch [9/50], Class Loss: 0.0529, JMMD Loss: 0.1051\n",
      "Validation Loss: 1.5649\n",
      "Epoch [10/50], Class Loss: 0.0401, JMMD Loss: 0.0811\n",
      "Validation Loss: 0.0195\n",
      "Epoch [11/50], Class Loss: 0.0094, JMMD Loss: 0.0855\n",
      "Validation Loss: 0.0137\n",
      "Epoch [12/50], Class Loss: 0.0203, JMMD Loss: 0.0901\n",
      "Validation Loss: 0.0257\n",
      "Epoch [13/50], Class Loss: 0.0110, JMMD Loss: 0.1053\n",
      "Validation Loss: 0.0088\n",
      "Epoch [14/50], Class Loss: 0.0058, JMMD Loss: 0.0890\n",
      "Validation Loss: 0.0060\n",
      "Epoch [15/50], Class Loss: 0.0051, JMMD Loss: 0.0875\n",
      "Validation Loss: 0.0051\n",
      "Epoch [16/50], Class Loss: 0.0053, JMMD Loss: 0.0856\n",
      "Validation Loss: 0.0052\n",
      "Epoch [17/50], Class Loss: 0.0043, JMMD Loss: 0.0823\n",
      "Validation Loss: 0.0069\n",
      "Epoch [18/50], Class Loss: 0.0043, JMMD Loss: 0.0831\n",
      "Validation Loss: 0.0100\n",
      "Epoch [19/50], Class Loss: 0.0046, JMMD Loss: 0.0736\n",
      "Validation Loss: 0.0097\n",
      "Epoch [20/50], Class Loss: 0.0058, JMMD Loss: 0.0886\n",
      "Validation Loss: 0.0273\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 98.92%, Precision: 98.92%, Recall: 98.91%, F1 Score: 98.89%\n",
      "Target Domain Performance - Accuracy: 90.65%, Precision: 90.93%, Recall: 90.31%, F1 Score: 90.19%\n",
      "\n",
      "Source performance: 99.20% 99.20% 99.19% 99.18%\n",
      "Target performance: 92.21% 92.19% 92.00% 91.90%\n",
      "\n",
      "Per-Class Accuracy on Target Domain (Mean over runs):\n",
      "  Class 0: 100.00%\n",
      "  Class 1: 99.67%\n",
      "  Class 2: 80.40%\n",
      "  Class 3: 87.91%\n",
      "\n",
      "SNR level: 22\n",
      "Base\n",
      "\n",
      "Run 1/3\n",
      "Epoch 1/50, Train Loss: 0.4004, Train Acc: 0.8394, Val Loss: 2.9000, Val Acc: 0.7374\n",
      "Epoch 2/50, Train Loss: 0.1350, Train Acc: 0.9498, Val Loss: 1.3064, Val Acc: 0.7428\n",
      "Epoch 3/50, Train Loss: 0.0605, Train Acc: 0.9802, Val Loss: 2.0236, Val Acc: 0.6085\n",
      "Epoch 4/50, Train Loss: 0.0258, Train Acc: 0.9916, Val Loss: 0.4836, Val Acc: 0.8717\n",
      "Epoch 5/50, Train Loss: 0.0167, Train Acc: 0.9949, Val Loss: 0.1508, Val Acc: 0.9508\n",
      "Epoch 6/50, Train Loss: 0.0105, Train Acc: 0.9967, Val Loss: 0.4044, Val Acc: 0.8867\n",
      "Epoch 7/50, Train Loss: 0.0070, Train Acc: 0.9982, Val Loss: 0.0102, Val Acc: 0.9964\n",
      "Epoch 8/50, Train Loss: 0.0033, Train Acc: 0.9988, Val Loss: 0.0853, Val Acc: 0.9694\n",
      "Epoch 9/50, Train Loss: 0.0099, Train Acc: 0.9969, Val Loss: 0.5898, Val Acc: 0.8004\n",
      "Epoch 10/50, Train Loss: 0.0048, Train Acc: 0.9988, Val Loss: 0.0069, Val Acc: 0.9976\n",
      "Epoch 11/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0019, Val Acc: 0.9988\n",
      "Epoch 12/50, Train Loss: 0.0012, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9994\n",
      "Epoch 13/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0025, Val Acc: 0.9994\n",
      "Epoch 14/50, Train Loss: 0.0051, Train Acc: 0.9999, Val Loss: 0.0028, Val Acc: 0.9988\n",
      "Epoch 15/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0018, Val Acc: 0.9994\n",
      "Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994\n",
      "Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988\n",
      "Early stopping!\n",
      "\n",
      "Run 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.4289, Train Acc: 0.8302, Val Loss: 3.5411, Val Acc: 0.5366\n",
      "Epoch 2/50, Train Loss: 0.1194, Train Acc: 0.9573, Val Loss: 0.1552, Val Acc: 0.9376\n",
      "Epoch 3/50, Train Loss: 0.0637, Train Acc: 0.9781, Val Loss: 0.0546, Val Acc: 0.9760\n",
      "Epoch 4/50, Train Loss: 0.0257, Train Acc: 0.9913, Val Loss: 0.1012, Val Acc: 0.9640\n",
      "Epoch 5/50, Train Loss: 0.0349, Train Acc: 0.9894, Val Loss: 0.6453, Val Acc: 0.8771\n",
      "Epoch 6/50, Train Loss: 0.0216, Train Acc: 0.9919, Val Loss: 3.8161, Val Acc: 0.7494\n",
      "Epoch 7/50, Train Loss: 0.0178, Train Acc: 0.9943, Val Loss: 2.2634, Val Acc: 0.6439\n",
      "Epoch 8/50, Train Loss: 0.0121, Train Acc: 0.9972, Val Loss: 0.0194, Val Acc: 0.9946\n",
      "Epoch 9/50, Train Loss: 0.0078, Train Acc: 0.9976, Val Loss: 0.8892, Val Acc: 0.8034\n",
      "Epoch 10/50, Train Loss: 0.0047, Train Acc: 0.9984, Val Loss: 0.5071, Val Acc: 0.8891\n",
      "Epoch 11/50, Train Loss: 0.0033, Train Acc: 0.9991, Val Loss: 0.0038, Val Acc: 0.9988\n",
      "Epoch 12/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0038, Val Acc: 0.9982\n",
      "Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0026, Val Acc: 0.9988\n",
      "Epoch 14/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9976\n",
      "Epoch 15/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9988\n",
      "Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988\n",
      "Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9994\n",
      "Epoch 18/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0026, Val Acc: 0.9988\n",
      "Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0026, Val Acc: 0.9982\n",
      "Epoch 20/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9994\n",
      "Epoch 21/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0031, Val Acc: 0.9988\n",
      "Early stopping!\n",
      "\n",
      "Run 3/3\n",
      "Epoch 1/50, Train Loss: 0.4106, Train Acc: 0.8355, Val Loss: 4.7089, Val Acc: 0.5216\n",
      "Epoch 2/50, Train Loss: 0.0937, Train Acc: 0.9652, Val Loss: 2.0143, Val Acc: 0.5797\n",
      "Epoch 3/50, Train Loss: 0.0399, Train Acc: 0.9870, Val Loss: 0.0288, Val Acc: 0.9916\n",
      "Epoch 4/50, Train Loss: 0.0246, Train Acc: 0.9924, Val Loss: 0.0975, Val Acc: 0.9604\n",
      "Epoch 5/50, Train Loss: 0.0100, Train Acc: 0.9984, Val Loss: 7.2239, Val Acc: 0.6924\n",
      "Epoch 6/50, Train Loss: 0.0053, Train Acc: 0.9990, Val Loss: 0.0011, Val Acc: 0.9994\n",
      "Epoch 7/50, Train Loss: 0.0032, Train Acc: 0.9993, Val Loss: 0.6646, Val Acc: 0.8273\n",
      "Epoch 8/50, Train Loss: 0.0077, Train Acc: 0.9982, Val Loss: 0.0027, Val Acc: 0.9994\n",
      "Epoch 9/50, Train Loss: 0.0068, Train Acc: 0.9978, Val Loss: 5.2496, Val Acc: 0.5372\n",
      "Epoch 10/50, Train Loss: 0.0030, Train Acc: 0.9991, Val Loss: 0.2146, Val Acc: 0.9251\n",
      "Epoch 11/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 19/50, Train Loss: 0.0034, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 21/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 22/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 23/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 24/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 26/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 27/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 28/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 29/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Source performance: 99.92 99.92 99.92 99.92\n",
      "Target performance: 79.12 75.84 78.37 73.49\n",
      "\n",
      "bpsk: 100.00\n",
      "qpsk: 31.48\n",
      "16qam: 91.93\n",
      "8apsk: 90.05\n",
      "DANN\n",
      "Epoch 1/50, Loss: 2.3791, Domain Loss: 1.4089, Class Loss: 0.9703\n",
      "Epoch 2/50, Loss: 1.6830, Domain Loss: 1.2705, Class Loss: 0.4124\n",
      "Epoch 3/50, Loss: 1.6813, Domain Loss: 1.3294, Class Loss: 0.3519\n",
      "Epoch 4/50, Loss: 1.9517, Domain Loss: 1.7023, Class Loss: 0.2494\n",
      "Epoch 5/50, Loss: 7.5092, Domain Loss: 7.2029, Class Loss: 0.3062\n",
      "Epoch 6/50, Loss: 14.8354, Domain Loss: 13.8066, Class Loss: 1.0288\n",
      "Epoch 7/50, Loss: 10.4794, Domain Loss: 9.9129, Class Loss: 0.5664\n",
      "Epoch 8/50, Loss: 26.8167, Domain Loss: 25.8124, Class Loss: 1.0043\n",
      "Epoch 9/50, Loss: 18.5029, Domain Loss: 17.6177, Class Loss: 0.8852\n",
      "Epoch 10/50, Loss: 7.4717, Domain Loss: 6.5416, Class Loss: 0.9301\n",
      "Epoch 11/50, Loss: 3.2571, Domain Loss: 2.2888, Class Loss: 0.9683\n",
      "Epoch 12/50, Loss: 2.8362, Domain Loss: 1.9838, Class Loss: 0.8524\n",
      "Epoch 13/50, Loss: 2.2280, Domain Loss: 1.5683, Class Loss: 0.6597\n",
      "Epoch 14/50, Loss: 2.0675, Domain Loss: 1.5596, Class Loss: 0.5079\n",
      "Epoch 15/50, Loss: 2.0892, Domain Loss: 1.6120, Class Loss: 0.4772\n",
      "Epoch 16/50, Loss: 2.9093, Domain Loss: 2.3575, Class Loss: 0.5518\n",
      "Epoch 17/50, Loss: 2.7628, Domain Loss: 2.3313, Class Loss: 0.4315\n",
      "Epoch 18/50, Loss: 2.3002, Domain Loss: 1.8728, Class Loss: 0.4274\n",
      "Epoch 19/50, Loss: 2.3576, Domain Loss: 1.8479, Class Loss: 0.5096\n",
      "Epoch 20/50, Loss: 2.0712, Domain Loss: 1.6549, Class Loss: 0.4163\n",
      "Epoch 21/50, Loss: 2.0762, Domain Loss: 1.6773, Class Loss: 0.3989\n",
      "Epoch 22/50, Loss: 2.1511, Domain Loss: 1.7740, Class Loss: 0.3771\n",
      "Epoch 23/50, Loss: 2.3657, Domain Loss: 1.8854, Class Loss: 0.4803\n",
      "Epoch 24/50, Loss: 2.6489, Domain Loss: 2.2311, Class Loss: 0.4178\n",
      "Epoch 25/50, Loss: 2.5084, Domain Loss: 2.0593, Class Loss: 0.4491\n",
      "Epoch 26/50, Loss: 2.4433, Domain Loss: 1.9057, Class Loss: 0.5376\n",
      "Epoch 27/50, Loss: 1.9187, Domain Loss: 1.5307, Class Loss: 0.3880\n",
      "Epoch 28/50, Loss: 1.9485, Domain Loss: 1.5579, Class Loss: 0.3906\n",
      "Epoch 29/50, Loss: 1.9671, Domain Loss: 1.5950, Class Loss: 0.3721\n",
      "Epoch 30/50, Loss: 1.9124, Domain Loss: 1.5353, Class Loss: 0.3771\n",
      "Epoch 31/50, Loss: 1.8963, Domain Loss: 1.5361, Class Loss: 0.3602\n",
      "Epoch 32/50, Loss: 1.9411, Domain Loss: 1.5147, Class Loss: 0.4264\n",
      "Epoch 33/50, Loss: 1.8916, Domain Loss: 1.4691, Class Loss: 0.4224\n",
      "Epoch 34/50, Loss: 1.8582, Domain Loss: 1.4513, Class Loss: 0.4069\n",
      "Epoch 35/50, Loss: 1.6996, Domain Loss: 1.3517, Class Loss: 0.3479\n",
      "Epoch 36/50, Loss: 1.6209, Domain Loss: 1.3054, Class Loss: 0.3154\n",
      "Epoch 37/50, Loss: 1.7412, Domain Loss: 1.4170, Class Loss: 0.3242\n",
      "Epoch 38/50, Loss: 1.7478, Domain Loss: 1.4248, Class Loss: 0.3230\n",
      "Epoch 39/50, Loss: 1.7436, Domain Loss: 1.4340, Class Loss: 0.3095\n",
      "Epoch 40/50, Loss: 1.9342, Domain Loss: 1.6516, Class Loss: 0.2826\n",
      "Epoch 41/50, Loss: 1.8146, Domain Loss: 1.5318, Class Loss: 0.2828\n",
      "Epoch 42/50, Loss: 1.6347, Domain Loss: 1.3276, Class Loss: 0.3071\n",
      "Epoch 43/50, Loss: 1.8725, Domain Loss: 1.5573, Class Loss: 0.3152\n",
      "Epoch 44/50, Loss: 2.0130, Domain Loss: 1.7205, Class Loss: 0.2925\n",
      "Epoch 45/50, Loss: 1.6181, Domain Loss: 1.3726, Class Loss: 0.2455\n",
      "Epoch 46/50, Loss: 1.9013, Domain Loss: 1.5694, Class Loss: 0.3320\n",
      "Epoch 47/50, Loss: 1.9644, Domain Loss: 1.6476, Class Loss: 0.3168\n",
      "Epoch 48/50, Loss: 1.8880, Domain Loss: 1.5622, Class Loss: 0.3258\n",
      "Epoch 49/50, Loss: 1.9051, Domain Loss: 1.5455, Class Loss: 0.3596\n",
      "Epoch 50/50, Loss: 2.3708, Domain Loss: 2.0013, Class Loss: 0.3695\n",
      "50.60\n",
      "\n",
      "\n",
      "Epoch 1/50, Loss: 2.3334, Domain Loss: 1.3861, Class Loss: 0.9473\n",
      "Epoch 2/50, Loss: 1.7066, Domain Loss: 1.2964, Class Loss: 0.4102\n",
      "Epoch 3/50, Loss: 1.6288, Domain Loss: 1.2979, Class Loss: 0.3309\n",
      "Epoch 4/50, Loss: 1.7638, Domain Loss: 1.4923, Class Loss: 0.2715\n",
      "Epoch 5/50, Loss: 4.4979, Domain Loss: 4.2691, Class Loss: 0.2288\n",
      "Epoch 6/50, Loss: 10.2412, Domain Loss: 9.8680, Class Loss: 0.3732\n",
      "Epoch 7/50, Loss: 9.3799, Domain Loss: 8.8766, Class Loss: 0.5033\n",
      "Epoch 8/50, Loss: 9.8868, Domain Loss: 9.3762, Class Loss: 0.5105\n",
      "Epoch 9/50, Loss: 12.9643, Domain Loss: 12.4249, Class Loss: 0.5394\n",
      "Epoch 10/50, Loss: 14.4442, Domain Loss: 13.8219, Class Loss: 0.6222\n",
      "Epoch 11/50, Loss: 6.8769, Domain Loss: 6.3538, Class Loss: 0.5231\n",
      "Epoch 12/50, Loss: 4.5439, Domain Loss: 4.1426, Class Loss: 0.4014\n",
      "Epoch 13/50, Loss: 2.8182, Domain Loss: 2.2867, Class Loss: 0.5315\n",
      "Epoch 14/50, Loss: 2.4358, Domain Loss: 1.9948, Class Loss: 0.4410\n",
      "Epoch 15/50, Loss: 2.0635, Domain Loss: 1.7182, Class Loss: 0.3453\n",
      "Epoch 16/50, Loss: 2.1551, Domain Loss: 1.7885, Class Loss: 0.3667\n",
      "Epoch 17/50, Loss: 2.1661, Domain Loss: 1.6861, Class Loss: 0.4800\n",
      "Epoch 18/50, Loss: 2.1870, Domain Loss: 1.6890, Class Loss: 0.4980\n",
      "Epoch 19/50, Loss: 2.1327, Domain Loss: 1.6905, Class Loss: 0.4422\n",
      "Epoch 20/50, Loss: 2.1091, Domain Loss: 1.6672, Class Loss: 0.4419\n",
      "Epoch 21/50, Loss: 1.7948, Domain Loss: 1.4140, Class Loss: 0.3808\n",
      "Epoch 22/50, Loss: 1.6631, Domain Loss: 1.3396, Class Loss: 0.3235\n",
      "Epoch 23/50, Loss: 2.0626, Domain Loss: 1.6423, Class Loss: 0.4203\n",
      "Epoch 24/50, Loss: 3.8005, Domain Loss: 3.2030, Class Loss: 0.5975\n",
      "Epoch 25/50, Loss: 3.0671, Domain Loss: 2.6464, Class Loss: 0.4207\n",
      "Epoch 26/50, Loss: 2.5621, Domain Loss: 2.0193, Class Loss: 0.5428\n",
      "Epoch 27/50, Loss: 2.2786, Domain Loss: 1.8338, Class Loss: 0.4448\n",
      "Epoch 28/50, Loss: 2.1957, Domain Loss: 1.7805, Class Loss: 0.4152\n",
      "Epoch 29/50, Loss: 2.0743, Domain Loss: 1.7246, Class Loss: 0.3498\n",
      "Epoch 30/50, Loss: 2.0029, Domain Loss: 1.5675, Class Loss: 0.4354\n",
      "Epoch 31/50, Loss: 1.9303, Domain Loss: 1.5920, Class Loss: 0.3382\n",
      "Epoch 32/50, Loss: 1.7400, Domain Loss: 1.3697, Class Loss: 0.3703\n",
      "Epoch 33/50, Loss: 2.1938, Domain Loss: 1.7700, Class Loss: 0.4238\n",
      "Epoch 34/50, Loss: 1.8524, Domain Loss: 1.6139, Class Loss: 0.2385\n",
      "Epoch 35/50, Loss: 1.8041, Domain Loss: 1.5553, Class Loss: 0.2488\n",
      "Epoch 36/50, Loss: 1.8889, Domain Loss: 1.5481, Class Loss: 0.3407\n",
      "Epoch 37/50, Loss: 1.7228, Domain Loss: 1.5396, Class Loss: 0.1832\n",
      "Epoch 38/50, Loss: 1.8169, Domain Loss: 1.5289, Class Loss: 0.2880\n",
      "Epoch 39/50, Loss: 2.3804, Domain Loss: 1.7821, Class Loss: 0.5983\n",
      "Epoch 40/50, Loss: 2.2317, Domain Loss: 1.7467, Class Loss: 0.4850\n",
      "Epoch 41/50, Loss: 1.9000, Domain Loss: 1.4969, Class Loss: 0.4031\n",
      "Epoch 42/50, Loss: 2.1517, Domain Loss: 1.7351, Class Loss: 0.4166\n",
      "Epoch 43/50, Loss: 1.9521, Domain Loss: 1.6538, Class Loss: 0.2983\n",
      "Epoch 44/50, Loss: 2.2777, Domain Loss: 1.9118, Class Loss: 0.3659\n",
      "Epoch 45/50, Loss: 2.1834, Domain Loss: 1.8214, Class Loss: 0.3620\n",
      "Epoch 46/50, Loss: 2.0504, Domain Loss: 1.7840, Class Loss: 0.2663\n",
      "Epoch 47/50, Loss: 3.0783, Domain Loss: 2.1168, Class Loss: 0.9615\n",
      "Epoch 48/50, Loss: 3.4630, Domain Loss: 3.0369, Class Loss: 0.4261\n",
      "Epoch 49/50, Loss: 3.2482, Domain Loss: 2.9362, Class Loss: 0.3120\n",
      "Epoch 50/50, Loss: 3.7705, Domain Loss: 3.0401, Class Loss: 0.7304\n",
      "46.70\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 2.2300, Domain Loss: 1.3667, Class Loss: 0.8634\n",
      "Epoch 2/50, Loss: 1.6555, Domain Loss: 1.2364, Class Loss: 0.4191\n",
      "Epoch 3/50, Loss: 1.9736, Domain Loss: 1.6258, Class Loss: 0.3478\n",
      "Epoch 4/50, Loss: 2.1640, Domain Loss: 1.8500, Class Loss: 0.3140\n",
      "Epoch 5/50, Loss: 7.1205, Domain Loss: 6.6877, Class Loss: 0.4328\n",
      "Epoch 6/50, Loss: 7.6948, Domain Loss: 6.6647, Class Loss: 1.0301\n",
      "Epoch 7/50, Loss: 5.7883, Domain Loss: 5.3354, Class Loss: 0.4529\n",
      "Epoch 8/50, Loss: 6.4265, Domain Loss: 6.0454, Class Loss: 0.3811\n",
      "Epoch 9/50, Loss: 2.2409, Domain Loss: 1.8259, Class Loss: 0.4150\n",
      "Epoch 10/50, Loss: 2.0155, Domain Loss: 1.6981, Class Loss: 0.3174\n",
      "Epoch 11/50, Loss: 2.1387, Domain Loss: 1.8213, Class Loss: 0.3175\n",
      "Epoch 12/50, Loss: 3.3505, Domain Loss: 2.7767, Class Loss: 0.5737\n",
      "Epoch 13/50, Loss: 4.9954, Domain Loss: 4.3333, Class Loss: 0.6622\n",
      "Epoch 14/50, Loss: 2.4752, Domain Loss: 2.1135, Class Loss: 0.3617\n",
      "Epoch 15/50, Loss: 2.3101, Domain Loss: 1.9805, Class Loss: 0.3296\n",
      "Epoch 16/50, Loss: 2.0402, Domain Loss: 1.6766, Class Loss: 0.3636\n",
      "Epoch 17/50, Loss: 1.9163, Domain Loss: 1.6087, Class Loss: 0.3076\n",
      "Epoch 18/50, Loss: 1.7469, Domain Loss: 1.5003, Class Loss: 0.2465\n",
      "Epoch 19/50, Loss: 1.7628, Domain Loss: 1.5442, Class Loss: 0.2186\n",
      "Epoch 20/50, Loss: 1.6522, Domain Loss: 1.4414, Class Loss: 0.2109\n",
      "Epoch 21/50, Loss: 1.8329, Domain Loss: 1.5824, Class Loss: 0.2504\n",
      "Epoch 22/50, Loss: 1.6527, Domain Loss: 1.4475, Class Loss: 0.2052\n",
      "Epoch 23/50, Loss: 1.6481, Domain Loss: 1.4609, Class Loss: 0.1872\n",
      "Epoch 24/50, Loss: 1.7055, Domain Loss: 1.4940, Class Loss: 0.2115\n",
      "Epoch 25/50, Loss: 1.8672, Domain Loss: 1.6395, Class Loss: 0.2277\n",
      "Epoch 26/50, Loss: 1.5402, Domain Loss: 1.3349, Class Loss: 0.2052\n",
      "Epoch 27/50, Loss: 2.1349, Domain Loss: 1.8951, Class Loss: 0.2398\n",
      "Epoch 28/50, Loss: 2.2272, Domain Loss: 1.8291, Class Loss: 0.3981\n",
      "Epoch 29/50, Loss: 2.7541, Domain Loss: 2.0705, Class Loss: 0.6836\n",
      "Epoch 30/50, Loss: 2.4608, Domain Loss: 1.8983, Class Loss: 0.5625\n",
      "Epoch 31/50, Loss: 2.5040, Domain Loss: 2.2142, Class Loss: 0.2898\n",
      "Epoch 32/50, Loss: 2.3067, Domain Loss: 1.9538, Class Loss: 0.3529\n",
      "Epoch 33/50, Loss: 2.9717, Domain Loss: 2.3279, Class Loss: 0.6439\n",
      "Epoch 34/50, Loss: 3.9442, Domain Loss: 3.1505, Class Loss: 0.7936\n",
      "Epoch 35/50, Loss: 2.7158, Domain Loss: 2.2369, Class Loss: 0.4789\n",
      "Epoch 36/50, Loss: 2.8438, Domain Loss: 2.3260, Class Loss: 0.5178\n",
      "Epoch 37/50, Loss: 2.6537, Domain Loss: 2.2011, Class Loss: 0.4526\n",
      "Epoch 38/50, Loss: 2.2166, Domain Loss: 1.7926, Class Loss: 0.4240\n",
      "Epoch 39/50, Loss: 2.1411, Domain Loss: 1.8169, Class Loss: 0.3241\n",
      "Epoch 40/50, Loss: 2.0152, Domain Loss: 1.7021, Class Loss: 0.3132\n",
      "Epoch 41/50, Loss: 2.1706, Domain Loss: 1.7894, Class Loss: 0.3812\n",
      "Epoch 42/50, Loss: 2.2382, Domain Loss: 1.8441, Class Loss: 0.3941\n",
      "Epoch 43/50, Loss: 5.5872, Domain Loss: 4.7574, Class Loss: 0.8298\n",
      "Epoch 44/50, Loss: 3.9066, Domain Loss: 3.5087, Class Loss: 0.3979\n",
      "Epoch 45/50, Loss: 5.4711, Domain Loss: 4.9724, Class Loss: 0.4987\n",
      "Epoch 46/50, Loss: 7.9399, Domain Loss: 7.2536, Class Loss: 0.6862\n",
      "Epoch 47/50, Loss: 10.9926, Domain Loss: 9.9810, Class Loss: 1.0116\n",
      "Epoch 48/50, Loss: 14.0803, Domain Loss: 12.6759, Class Loss: 1.4044\n",
      "Epoch 49/50, Loss: 8.3724, Domain Loss: 7.5988, Class Loss: 0.7736\n",
      "Epoch 50/50, Loss: 6.6354, Domain Loss: 5.9947, Class Loss: 0.6407\n",
      "49.94\n",
      "\n",
      "\n",
      "Source performance:\n",
      "65.83 68.60 66.08 61.90 \n",
      "Target performance:\n",
      "49.08 41.28 49.07 43.34 \n",
      "\n",
      "Per-class target performance: 99.92 6.92 62.27 27.17 Deep CORAL\n",
      "Deep CORAL Run 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Source Val Acc = 0.8129, Target Val Acc = 0.4838\n",
      "Epoch 2: Source Val Acc = 0.7740, Target Val Acc = 0.5713\n",
      "Epoch 3: Source Val Acc = 0.7392, Target Val Acc = 0.4826\n",
      "Epoch 4: Source Val Acc = 0.9748, Target Val Acc = 0.6433\n",
      "Epoch 5: Source Val Acc = 0.8831, Target Val Acc = 0.7794\n",
      "Epoch 6: Source Val Acc = 0.5132, Target Val Acc = 0.4940\n",
      "Epoch 7: Source Val Acc = 0.9928, Target Val Acc = 0.8585\n",
      "Epoch 8: Source Val Acc = 0.9844, Target Val Acc = 0.7944\n",
      "Epoch 9: Source Val Acc = 0.9826, Target Val Acc = 0.7986\n",
      "Epoch 10: Source Val Acc = 0.9958, Target Val Acc = 0.9167\n",
      "Epoch 11: Source Val Acc = 0.9928, Target Val Acc = 0.9323\n",
      "Epoch 12: Source Val Acc = 0.9994, Target Val Acc = 0.9293\n",
      "Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 0.8537\n",
      "Epoch 14: Source Val Acc = 0.8046, Target Val Acc = 0.5815\n",
      "Epoch 15: Source Val Acc = 0.9928, Target Val Acc = 0.7680\n",
      "Epoch 16: Source Val Acc = 0.9976, Target Val Acc = 0.9191\n",
      "Epoch 17: Source Val Acc = 0.9874, Target Val Acc = 0.8393\n",
      "Early stopping triggered.\n",
      "Run 1 finished: Best Source Val Acc = 0.9874, Target Val Acc = 0.8393\n",
      "\n",
      "Deep CORAL Run 2/3\n",
      "Epoch 1: Source Val Acc = 0.7002, Target Val Acc = 0.5420\n",
      "Epoch 2: Source Val Acc = 0.7470, Target Val Acc = 0.4814\n",
      "Epoch 3: Source Val Acc = 0.8135, Target Val Acc = 0.7932\n",
      "Epoch 4: Source Val Acc = 0.9880, Target Val Acc = 0.7566\n",
      "Epoch 5: Source Val Acc = 0.6613, Target Val Acc = 0.5845\n",
      "Epoch 6: Source Val Acc = 0.9958, Target Val Acc = 0.9179\n",
      "Epoch 7: Source Val Acc = 0.9784, Target Val Acc = 0.9209\n",
      "Epoch 8: Source Val Acc = 0.9964, Target Val Acc = 0.8597\n",
      "Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.9347\n",
      "Epoch 10: Source Val Acc = 0.9916, Target Val Acc = 0.9526\n",
      "Epoch 11: Source Val Acc = 0.7482, Target Val Acc = 0.5522\n",
      "Epoch 12: Source Val Acc = 0.9970, Target Val Acc = 0.9251\n",
      "Epoch 13: Source Val Acc = 0.9976, Target Val Acc = 0.8891\n",
      "Epoch 14: Source Val Acc = 0.9820, Target Val Acc = 0.8447\n",
      "Early stopping triggered.\n",
      "Run 2 finished: Best Source Val Acc = 0.9820, Target Val Acc = 0.8447\n",
      "\n",
      "Deep CORAL Run 3/3\n",
      "Epoch 1: Source Val Acc = 0.7632, Target Val Acc = 0.5881\n",
      "Epoch 2: Source Val Acc = 0.8819, Target Val Acc = 0.4898\n",
      "Epoch 3: Source Val Acc = 0.8489, Target Val Acc = 0.6187\n",
      "Epoch 4: Source Val Acc = 0.8579, Target Val Acc = 0.6978\n",
      "Epoch 5: Source Val Acc = 0.6847, Target Val Acc = 0.5683\n",
      "Epoch 6: Source Val Acc = 0.9616, Target Val Acc = 0.7002\n",
      "Epoch 7: Source Val Acc = 0.9143, Target Val Acc = 0.7974\n",
      "Epoch 8: Source Val Acc = 0.9532, Target Val Acc = 0.7194\n",
      "Epoch 9: Source Val Acc = 0.9682, Target Val Acc = 0.6217\n",
      "Epoch 10: Source Val Acc = 0.9940, Target Val Acc = 0.7998\n",
      "Epoch 11: Source Val Acc = 0.9712, Target Val Acc = 0.7122\n",
      "Epoch 12: Source Val Acc = 0.9988, Target Val Acc = 0.9490\n",
      "Epoch 13: Source Val Acc = 0.8861, Target Val Acc = 0.7842\n",
      "Epoch 14: Source Val Acc = 0.8867, Target Val Acc = 0.5671\n",
      "Epoch 15: Source Val Acc = 0.9994, Target Val Acc = 0.7500\n",
      "Epoch 16: Source Val Acc = 1.0000, Target Val Acc = 0.7734\n",
      "Epoch 17: Source Val Acc = 1.0000, Target Val Acc = 0.8195\n",
      "Epoch 18: Source Val Acc = 1.0000, Target Val Acc = 0.8537\n",
      "Epoch 19: Source Val Acc = 1.0000, Target Val Acc = 0.8004\n",
      "Epoch 20: Source Val Acc = 1.0000, Target Val Acc = 0.8927\n",
      "Epoch 21: Source Val Acc = 0.9994, Target Val Acc = 0.7368\n",
      "Early stopping triggered.\n",
      "Run 3 finished: Best Source Val Acc = 0.9994, Target Val Acc = 0.7368\n",
      "\n",
      "Deep CORAL: Average Source Val Acc = 0.9896, Average Target Val Acc = 0.8070\n",
      "STAR\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 1.9191, Discrepancy Loss: 0.1202\n",
      "Epoch [2/50], Class Loss: 0.8203, Discrepancy Loss: 0.0822\n",
      "Epoch [3/50], Class Loss: 0.6857, Discrepancy Loss: 0.0958\n",
      "Epoch [4/50], Class Loss: 0.3450, Discrepancy Loss: 0.0767\n",
      "Epoch [5/50], Class Loss: 0.1800, Discrepancy Loss: 0.0496\n",
      "Epoch [6/50], Class Loss: 0.4068, Discrepancy Loss: 0.0616\n",
      "Epoch [7/50], Class Loss: 0.1424, Discrepancy Loss: 0.0396\n",
      "Epoch [8/50], Class Loss: 0.0342, Discrepancy Loss: 0.0159\n",
      "Epoch [9/50], Class Loss: 0.0460, Discrepancy Loss: 0.0148\n",
      "Epoch [10/50], Class Loss: 0.0362, Discrepancy Loss: 0.0152\n",
      "Epoch [11/50], Class Loss: 0.0059, Discrepancy Loss: 0.0104\n",
      "Epoch [12/50], Class Loss: 0.0086, Discrepancy Loss: 0.0112\n",
      "Epoch [13/50], Class Loss: 0.0056, Discrepancy Loss: 0.0081\n",
      "Epoch [14/50], Class Loss: 0.0029, Discrepancy Loss: 0.0076\n",
      "Epoch [15/50], Class Loss: 0.0044, Discrepancy Loss: 0.0064\n",
      "Epoch [16/50], Class Loss: 0.0053, Discrepancy Loss: 0.0074\n",
      "Epoch [17/50], Class Loss: 0.0234, Discrepancy Loss: 0.0070\n",
      "Epoch [18/50], Class Loss: 0.0221, Discrepancy Loss: 0.0095\n",
      "Epoch [19/50], Class Loss: 0.0033, Discrepancy Loss: 0.0097\n",
      "Epoch [20/50], Class Loss: 0.0044, Discrepancy Loss: 0.0087\n",
      "Epoch [21/50], Class Loss: 0.0027, Discrepancy Loss: 0.0059\n",
      "Epoch [22/50], Class Loss: 0.0323, Discrepancy Loss: 0.0062\n",
      "Epoch [23/50], Class Loss: 0.0020, Discrepancy Loss: 0.0064\n",
      "Epoch [24/50], Class Loss: 0.0041, Discrepancy Loss: 0.0068\n",
      "Epoch [25/50], Class Loss: 0.0030, Discrepancy Loss: 0.0065\n",
      "Epoch [26/50], Class Loss: 0.0017, Discrepancy Loss: 0.0075\n",
      "Epoch [27/50], Class Loss: 0.0038, Discrepancy Loss: 0.0076\n",
      "Epoch [28/50], Class Loss: 0.0015, Discrepancy Loss: 0.0055\n",
      "Epoch [29/50], Class Loss: 0.0029, Discrepancy Loss: 0.0063\n",
      "Epoch [30/50], Class Loss: 0.0009, Discrepancy Loss: 0.0072\n",
      "Epoch [31/50], Class Loss: 0.0091, Discrepancy Loss: 0.0064\n",
      "Epoch [32/50], Class Loss: 0.0012, Discrepancy Loss: 0.0051\n",
      "Epoch [33/50], Class Loss: 0.0047, Discrepancy Loss: 0.0050\n",
      "Epoch [34/50], Class Loss: 0.0452, Discrepancy Loss: 0.0066\n",
      "Epoch [35/50], Class Loss: 0.0023, Discrepancy Loss: 0.0069\n",
      "Epoch [36/50], Class Loss: 0.0022, Discrepancy Loss: 0.0081\n",
      "Epoch [37/50], Class Loss: 0.0058, Discrepancy Loss: 0.0061\n",
      "Epoch [38/50], Class Loss: 0.0027, Discrepancy Loss: 0.0062\n",
      "Epoch [39/50], Class Loss: 0.0017, Discrepancy Loss: 0.0063\n",
      "Epoch [40/50], Class Loss: 0.0136, Discrepancy Loss: 0.0047\n",
      "Epoch [41/50], Class Loss: 0.0011, Discrepancy Loss: 0.0068\n",
      "Epoch [42/50], Class Loss: 0.0012, Discrepancy Loss: 0.0052\n",
      "Epoch [43/50], Class Loss: 0.0021, Discrepancy Loss: 0.0054\n",
      "Epoch [44/50], Class Loss: 0.0013, Discrepancy Loss: 0.0046\n",
      "Epoch [45/50], Class Loss: 0.0027, Discrepancy Loss: 0.0074\n",
      "Epoch [46/50], Class Loss: 0.0011, Discrepancy Loss: 0.0085\n",
      "Epoch [47/50], Class Loss: 0.0039, Discrepancy Loss: 0.0053\n",
      "Epoch [48/50], Class Loss: 0.0034, Discrepancy Loss: 0.0050\n",
      "Epoch [49/50], Class Loss: 0.0020, Discrepancy Loss: 0.0052\n",
      "Epoch [50/50], Class Loss: 0.0014, Discrepancy Loss: 0.0072\n",
      "Source Domain Performance - Accuracy: 99.28%, Precision: 99.28%, Recall: 99.27%, F1 Score: 99.27%\n",
      "Target Domain Performance - Accuracy: 95.68%, Precision: 95.67%, Recall: 95.76%, F1 Score: 95.66%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 1.7752, Discrepancy Loss: 0.1602\n",
      "Epoch [2/50], Class Loss: 0.9975, Discrepancy Loss: 0.1086\n",
      "Epoch [3/50], Class Loss: 0.4518, Discrepancy Loss: 0.0904\n",
      "Epoch [4/50], Class Loss: 0.4017, Discrepancy Loss: 0.0805\n",
      "Epoch [5/50], Class Loss: 0.1606, Discrepancy Loss: 0.0575\n",
      "Epoch [6/50], Class Loss: 0.2127, Discrepancy Loss: 0.0458\n",
      "Epoch [7/50], Class Loss: 0.5507, Discrepancy Loss: 0.0721\n",
      "Epoch [8/50], Class Loss: 0.5128, Discrepancy Loss: 0.1156\n",
      "Epoch [9/50], Class Loss: 0.2547, Discrepancy Loss: 0.0448\n",
      "Epoch [10/50], Class Loss: 0.1635, Discrepancy Loss: 0.0561\n",
      "Epoch [11/50], Class Loss: 0.0490, Discrepancy Loss: 0.0404\n",
      "Epoch [12/50], Class Loss: 0.0341, Discrepancy Loss: 0.0388\n",
      "Epoch [13/50], Class Loss: 0.0451, Discrepancy Loss: 0.0349\n",
      "Epoch [14/50], Class Loss: 0.0251, Discrepancy Loss: 0.0353\n",
      "Epoch [15/50], Class Loss: 0.0316, Discrepancy Loss: 0.0291\n",
      "Epoch [16/50], Class Loss: 0.0254, Discrepancy Loss: 0.0228\n",
      "Epoch [17/50], Class Loss: 0.0513, Discrepancy Loss: 0.0288\n",
      "Epoch [18/50], Class Loss: 0.0192, Discrepancy Loss: 0.0267\n",
      "Epoch [19/50], Class Loss: 0.0409, Discrepancy Loss: 0.0262\n",
      "Epoch [20/50], Class Loss: 0.0415, Discrepancy Loss: 0.0278\n",
      "Epoch [21/50], Class Loss: 0.0154, Discrepancy Loss: 0.0293\n",
      "Epoch [22/50], Class Loss: 0.0096, Discrepancy Loss: 0.0233\n",
      "Epoch [23/50], Class Loss: 0.0134, Discrepancy Loss: 0.0235\n",
      "Epoch [24/50], Class Loss: 0.0410, Discrepancy Loss: 0.0243\n",
      "Epoch [25/50], Class Loss: 0.0550, Discrepancy Loss: 0.0238\n",
      "Epoch [26/50], Class Loss: 0.0190, Discrepancy Loss: 0.0245\n",
      "Epoch [27/50], Class Loss: 0.0147, Discrepancy Loss: 0.0226\n",
      "Epoch [28/50], Class Loss: 0.0162, Discrepancy Loss: 0.0202\n",
      "Epoch [29/50], Class Loss: 0.0108, Discrepancy Loss: 0.0352\n",
      "Epoch [30/50], Class Loss: 0.0539, Discrepancy Loss: 0.0263\n",
      "Epoch [31/50], Class Loss: 0.0110, Discrepancy Loss: 0.0235\n",
      "Epoch [32/50], Class Loss: 0.0350, Discrepancy Loss: 0.0214\n",
      "Epoch [33/50], Class Loss: 0.0173, Discrepancy Loss: 0.0241\n",
      "Epoch [34/50], Class Loss: 0.0132, Discrepancy Loss: 0.0226\n",
      "Epoch [35/50], Class Loss: 0.0296, Discrepancy Loss: 0.0246\n",
      "Epoch [36/50], Class Loss: 0.0221, Discrepancy Loss: 0.0256\n",
      "Epoch [37/50], Class Loss: 0.0148, Discrepancy Loss: 0.0235\n",
      "Epoch [38/50], Class Loss: 0.0294, Discrepancy Loss: 0.0233\n",
      "Epoch [39/50], Class Loss: 0.0113, Discrepancy Loss: 0.0210\n",
      "Epoch [40/50], Class Loss: 0.0105, Discrepancy Loss: 0.0201\n",
      "Epoch [41/50], Class Loss: 0.0166, Discrepancy Loss: 0.0201\n",
      "Epoch [42/50], Class Loss: 0.0102, Discrepancy Loss: 0.0219\n",
      "Epoch [43/50], Class Loss: 0.0109, Discrepancy Loss: 0.0220\n",
      "Epoch [44/50], Class Loss: 0.0318, Discrepancy Loss: 0.0232\n",
      "Epoch [45/50], Class Loss: 0.0079, Discrepancy Loss: 0.0284\n",
      "Epoch [46/50], Class Loss: 0.0124, Discrepancy Loss: 0.0212\n",
      "Epoch [47/50], Class Loss: 0.0379, Discrepancy Loss: 0.0204\n",
      "Epoch [48/50], Class Loss: 0.0233, Discrepancy Loss: 0.0221\n",
      "Epoch [49/50], Class Loss: 0.0348, Discrepancy Loss: 0.0231\n",
      "Epoch [50/50], Class Loss: 0.0172, Discrepancy Loss: 0.0241\n",
      "Source Domain Performance - Accuracy: 83.15%, Precision: 89.58%, Recall: 83.52%, F1 Score: 82.93%\n",
      "Target Domain Performance - Accuracy: 93.59%, Precision: 93.57%, Recall: 93.44%, F1 Score: 93.43%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 1.6181, Discrepancy Loss: 0.1161\n",
      "Epoch [2/50], Class Loss: 0.7431, Discrepancy Loss: 0.0928\n",
      "Epoch [3/50], Class Loss: 0.4689, Discrepancy Loss: 0.0897\n",
      "Epoch [4/50], Class Loss: 0.1586, Discrepancy Loss: 0.0341\n",
      "Epoch [5/50], Class Loss: 1.7151, Discrepancy Loss: 0.1162\n",
      "Epoch [6/50], Class Loss: 0.3443, Discrepancy Loss: 0.0631\n",
      "Epoch [7/50], Class Loss: 0.1417, Discrepancy Loss: 0.0308\n",
      "Epoch [8/50], Class Loss: 0.0534, Discrepancy Loss: 0.0208\n",
      "Epoch [9/50], Class Loss: 0.0483, Discrepancy Loss: 0.0164\n",
      "Epoch [10/50], Class Loss: 0.0167, Discrepancy Loss: 0.0130\n",
      "Epoch [11/50], Class Loss: 0.0059, Discrepancy Loss: 0.0079\n",
      "Epoch [12/50], Class Loss: 0.0079, Discrepancy Loss: 0.0066\n",
      "Epoch [13/50], Class Loss: 0.0037, Discrepancy Loss: 0.0093\n",
      "Epoch [14/50], Class Loss: 0.0072, Discrepancy Loss: 0.0066\n",
      "Epoch [15/50], Class Loss: 0.0049, Discrepancy Loss: 0.0085\n",
      "Epoch [16/50], Class Loss: 0.0060, Discrepancy Loss: 0.0083\n",
      "Epoch [17/50], Class Loss: 0.0478, Discrepancy Loss: 0.0060\n",
      "Epoch [18/50], Class Loss: 0.0200, Discrepancy Loss: 0.0112\n",
      "Epoch [19/50], Class Loss: 0.0066, Discrepancy Loss: 0.0116\n",
      "Epoch [20/50], Class Loss: 0.0033, Discrepancy Loss: 0.0080\n",
      "Epoch [21/50], Class Loss: 0.0640, Discrepancy Loss: 0.0061\n",
      "Epoch [22/50], Class Loss: 0.0036, Discrepancy Loss: 0.0093\n",
      "Epoch [23/50], Class Loss: 0.0043, Discrepancy Loss: 0.0086\n",
      "Epoch [24/50], Class Loss: 0.0050, Discrepancy Loss: 0.0090\n",
      "Epoch [25/50], Class Loss: 0.0111, Discrepancy Loss: 0.0125\n",
      "Epoch [26/50], Class Loss: 0.0033, Discrepancy Loss: 0.0088\n",
      "Epoch [27/50], Class Loss: 0.0132, Discrepancy Loss: 0.0082\n",
      "Epoch [28/50], Class Loss: 0.0026, Discrepancy Loss: 0.0068\n",
      "Epoch [29/50], Class Loss: 0.0038, Discrepancy Loss: 0.0074\n",
      "Epoch [30/50], Class Loss: 0.0041, Discrepancy Loss: 0.0090\n",
      "Epoch [31/50], Class Loss: 0.0083, Discrepancy Loss: 0.0068\n",
      "Epoch [32/50], Class Loss: 0.0049, Discrepancy Loss: 0.0085\n",
      "Epoch [33/50], Class Loss: 0.0080, Discrepancy Loss: 0.0082\n",
      "Epoch [34/50], Class Loss: 0.0031, Discrepancy Loss: 0.0084\n",
      "Epoch [35/50], Class Loss: 0.0050, Discrepancy Loss: 0.0088\n",
      "Epoch [36/50], Class Loss: 0.0285, Discrepancy Loss: 0.0080\n",
      "Epoch [37/50], Class Loss: 0.0027, Discrepancy Loss: 0.0054\n",
      "Epoch [38/50], Class Loss: 0.0244, Discrepancy Loss: 0.0078\n",
      "Epoch [39/50], Class Loss: 0.0033, Discrepancy Loss: 0.0068\n",
      "Epoch [40/50], Class Loss: 0.0028, Discrepancy Loss: 0.0081\n",
      "Epoch [41/50], Class Loss: 0.0022, Discrepancy Loss: 0.0074\n",
      "Epoch [42/50], Class Loss: 0.0030, Discrepancy Loss: 0.0087\n",
      "Epoch [43/50], Class Loss: 0.0046, Discrepancy Loss: 0.0068\n",
      "Epoch [44/50], Class Loss: 0.0021, Discrepancy Loss: 0.0092\n",
      "Epoch [45/50], Class Loss: 0.0154, Discrepancy Loss: 0.0072\n",
      "Epoch [46/50], Class Loss: 0.0024, Discrepancy Loss: 0.0082\n",
      "Epoch [47/50], Class Loss: 0.0421, Discrepancy Loss: 0.0094\n",
      "Epoch [48/50], Class Loss: 0.0151, Discrepancy Loss: 0.0075\n",
      "Epoch [49/50], Class Loss: 0.0028, Discrepancy Loss: 0.0069\n",
      "Epoch [50/50], Class Loss: 0.0313, Discrepancy Loss: 0.0073\n",
      "Source Domain Performance - Accuracy: 99.34%, Precision: 99.35%, Recall: 99.34%, F1 Score: 99.34%\n",
      "Target Domain Performance - Accuracy: 95.50%, Precision: 95.53%, Recall: 95.49%, F1 Score: 95.45%\n",
      "\n",
      "Source performance: 93.92% 96.07% 94.05% 93.85%\n",
      "Target performance: 94.92% 94.92% 94.90% 94.84%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 96.29%\n",
      "16qam: 87.53%\n",
      "8apsk: 95.77%\n",
      "MCD\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 0.9209, Discrepancy Loss: 0.0349\n",
      "Validation Loss: 7.2625\n",
      "Epoch [2/50], Class Loss: 0.2429, Discrepancy Loss: 0.0209\n",
      "Validation Loss: 2.9000\n",
      "Epoch [3/50], Class Loss: 0.1577, Discrepancy Loss: 0.0273\n",
      "Validation Loss: 30.9625\n",
      "Epoch [4/50], Class Loss: 0.2233, Discrepancy Loss: 0.0198\n",
      "Validation Loss: 0.2145\n",
      "Epoch [5/50], Class Loss: 0.1127, Discrepancy Loss: 0.0464\n",
      "Validation Loss: 0.5627\n",
      "Epoch [6/50], Class Loss: 0.0690, Discrepancy Loss: 0.0611\n",
      "Validation Loss: 10.8452\n",
      "Epoch [7/50], Class Loss: 0.0720, Discrepancy Loss: 0.0809\n",
      "Validation Loss: 8.9702\n",
      "Epoch [8/50], Class Loss: 0.0324, Discrepancy Loss: 0.1307\n",
      "Validation Loss: 2.5789\n",
      "Epoch [9/50], Class Loss: 0.0608, Discrepancy Loss: 0.0782\n",
      "Validation Loss: 0.2676\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 97.66%, Precision: 97.84%, Recall: 97.74%, F1 Score: 97.69%\n",
      "Target Domain Performance - Accuracy: 64.03%, Precision: 60.38%, Recall: 62.47%, F1 Score: 56.55%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 0.6669, Discrepancy Loss: 0.0467\n",
      "Validation Loss: 3.2970\n",
      "Epoch [2/50], Class Loss: 0.2576, Discrepancy Loss: 0.0448\n",
      "Validation Loss: 0.8956\n",
      "Epoch [3/50], Class Loss: 0.1980, Discrepancy Loss: 0.0534\n",
      "Validation Loss: 15.0497\n",
      "Epoch [4/50], Class Loss: 0.1833, Discrepancy Loss: 0.0971\n",
      "Validation Loss: 7.0993\n",
      "Epoch [5/50], Class Loss: 0.0418, Discrepancy Loss: 0.0461\n",
      "Validation Loss: 0.4915\n",
      "Epoch [6/50], Class Loss: 0.0302, Discrepancy Loss: 0.0564\n",
      "Validation Loss: 17.4106\n",
      "Epoch [7/50], Class Loss: 0.0214, Discrepancy Loss: 0.0876\n",
      "Validation Loss: 1.1315\n",
      "Epoch [8/50], Class Loss: 0.0350, Discrepancy Loss: 0.1645\n",
      "Validation Loss: 0.0197\n",
      "Epoch [9/50], Class Loss: 0.0341, Discrepancy Loss: 0.1492\n",
      "Validation Loss: 2.4575\n",
      "Epoch [10/50], Class Loss: 0.0706, Discrepancy Loss: 0.1632\n",
      "Validation Loss: 42.7194\n",
      "Epoch [11/50], Class Loss: 0.0271, Discrepancy Loss: 0.0605\n",
      "Validation Loss: 0.0004\n",
      "Epoch [12/50], Class Loss: 0.0245, Discrepancy Loss: 0.0618\n",
      "Validation Loss: 0.5010\n",
      "Epoch [13/50], Class Loss: 0.0177, Discrepancy Loss: 0.0638\n",
      "Validation Loss: 0.0044\n",
      "Epoch [14/50], Class Loss: 0.0184, Discrepancy Loss: 0.0683\n",
      "Validation Loss: 0.0038\n",
      "Epoch [15/50], Class Loss: 0.0137, Discrepancy Loss: 0.0399\n",
      "Validation Loss: 0.0062\n",
      "Epoch [16/50], Class Loss: 0.0041, Discrepancy Loss: 0.0289\n",
      "Validation Loss: 0.0021\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
      "Target Domain Performance - Accuracy: 61.45%, Precision: 75.86%, Recall: 60.21%, F1 Score: 55.86%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 0.7132, Discrepancy Loss: 0.0654\n",
      "Validation Loss: 4.9055\n",
      "Epoch [2/50], Class Loss: 0.2590, Discrepancy Loss: 0.0298\n",
      "Validation Loss: 0.3329\n",
      "Epoch [3/50], Class Loss: 0.1329, Discrepancy Loss: 0.0229\n",
      "Validation Loss: 0.1273\n",
      "Epoch [4/50], Class Loss: 0.1366, Discrepancy Loss: 0.0506\n",
      "Validation Loss: 1.3642\n",
      "Epoch [5/50], Class Loss: 0.0500, Discrepancy Loss: 0.0439\n",
      "Validation Loss: 9.9003\n",
      "Epoch [6/50], Class Loss: 0.1485, Discrepancy Loss: 0.0639\n",
      "Validation Loss: 0.3293\n",
      "Epoch [7/50], Class Loss: 0.0358, Discrepancy Loss: 0.0344\n",
      "Validation Loss: 0.3906\n",
      "Epoch [8/50], Class Loss: 0.0231, Discrepancy Loss: 0.0995\n",
      "Validation Loss: 0.2316\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 97.36%, Precision: 97.48%, Recall: 97.37%, F1 Score: 97.32%\n",
      "Target Domain Performance - Accuracy: 58.27%, Precision: 83.47%, Recall: 59.24%, F1 Score: 53.56%\n",
      "\n",
      "Source performance: 98.34% 98.44% 98.37% 98.34%\n",
      "Target performance: 61.25% 73.24% 60.64% 55.33%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 37.47%\n",
      "16qam: 33.09%\n",
      "8apsk: 72.01%\n",
      "JAN\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 0.3887, JMMD Loss: 0.1898\n",
      "Validation Loss: 1.8580\n",
      "Epoch [2/50], Class Loss: 0.1943, JMMD Loss: 0.1448\n",
      "Validation Loss: 0.5086\n",
      "Epoch [3/50], Class Loss: 0.1198, JMMD Loss: 0.1136\n",
      "Validation Loss: 0.0546\n",
      "Epoch [4/50], Class Loss: 0.0442, JMMD Loss: 0.0988\n",
      "Validation Loss: 0.1430\n",
      "Epoch [5/50], Class Loss: 0.0362, JMMD Loss: 0.1052\n",
      "Validation Loss: 2.3663\n",
      "Epoch [6/50], Class Loss: 0.0265, JMMD Loss: 0.0890\n",
      "Validation Loss: 2.1923\n",
      "Epoch [7/50], Class Loss: 0.0063, JMMD Loss: 0.0978\n",
      "Validation Loss: 2.0150\n",
      "Epoch [8/50], Class Loss: 0.0070, JMMD Loss: 0.0835\n",
      "Validation Loss: 6.0301\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 55.64%, Precision: 83.94%, Recall: 55.28%, F1 Score: 47.25%\n",
      "Target Domain Performance - Accuracy: 76.14%, Precision: 86.17%, Recall: 75.66%, F1 Score: 69.84%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 0.3617, JMMD Loss: 0.2058\n",
      "Validation Loss: 0.1546\n",
      "Epoch [2/50], Class Loss: 0.1348, JMMD Loss: 0.1356\n",
      "Validation Loss: 1.5923\n",
      "Epoch [3/50], Class Loss: 0.1369, JMMD Loss: 0.1073\n",
      "Validation Loss: 0.3917\n",
      "Epoch [4/50], Class Loss: 0.1074, JMMD Loss: 0.1269\n",
      "Validation Loss: 1.0791\n",
      "Epoch [5/50], Class Loss: 0.0526, JMMD Loss: 0.0997\n",
      "Validation Loss: 0.0391\n",
      "Epoch [6/50], Class Loss: 0.0216, JMMD Loss: 0.0910\n",
      "Validation Loss: 0.3624\n",
      "Epoch [7/50], Class Loss: 0.0070, JMMD Loss: 0.0958\n",
      "Validation Loss: 0.0262\n",
      "Epoch [8/50], Class Loss: 0.0050, JMMD Loss: 0.0818\n",
      "Validation Loss: 0.0356\n",
      "Epoch [9/50], Class Loss: 0.0078, JMMD Loss: 0.0810\n",
      "Validation Loss: 0.0837\n",
      "Epoch [10/50], Class Loss: 0.0286, JMMD Loss: 0.0978\n",
      "Validation Loss: 0.5731\n",
      "Epoch [11/50], Class Loss: 0.0141, JMMD Loss: 0.0774\n",
      "Validation Loss: 0.0018\n",
      "Epoch [12/50], Class Loss: 0.0008, JMMD Loss: 0.0655\n",
      "Validation Loss: 0.0009\n",
      "Epoch [13/50], Class Loss: 0.0008, JMMD Loss: 0.0657\n",
      "Validation Loss: 0.0008\n",
      "Epoch [14/50], Class Loss: 0.0007, JMMD Loss: 0.0588\n",
      "Validation Loss: 0.0008\n",
      "Epoch [15/50], Class Loss: 0.0008, JMMD Loss: 0.0607\n",
      "Validation Loss: 0.0009\n",
      "Epoch [16/50], Class Loss: 0.0010, JMMD Loss: 0.0625\n",
      "Validation Loss: 0.0021\n",
      "Epoch [17/50], Class Loss: 0.0009, JMMD Loss: 0.0657\n",
      "Validation Loss: 0.0016\n",
      "Epoch [18/50], Class Loss: 0.0007, JMMD Loss: 0.0508\n",
      "Validation Loss: 0.0014\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
      "Target Domain Performance - Accuracy: 97.42%, Precision: 97.41%, Recall: 97.41%, F1 Score: 97.36%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 0.3995, JMMD Loss: 0.1963\n",
      "Validation Loss: 0.1356\n",
      "Epoch [2/50], Class Loss: 0.1769, JMMD Loss: 0.1297\n",
      "Validation Loss: 0.1401\n",
      "Epoch [3/50], Class Loss: 0.0922, JMMD Loss: 0.1012\n",
      "Validation Loss: 0.0909\n",
      "Epoch [4/50], Class Loss: 0.1465, JMMD Loss: 0.1302\n",
      "Validation Loss: 0.2285\n",
      "Epoch [5/50], Class Loss: 0.1343, JMMD Loss: 0.1027\n",
      "Validation Loss: 2.8069\n",
      "Epoch [6/50], Class Loss: 0.1769, JMMD Loss: 0.1181\n",
      "Validation Loss: 0.0958\n",
      "Epoch [7/50], Class Loss: 0.0605, JMMD Loss: 0.1167\n",
      "Validation Loss: 0.4324\n",
      "Epoch [8/50], Class Loss: 0.0533, JMMD Loss: 0.1150\n",
      "Validation Loss: 0.1110\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 96.16%, Precision: 96.64%, Recall: 96.29%, F1 Score: 96.19%\n",
      "Target Domain Performance - Accuracy: 90.95%, Precision: 91.41%, Recall: 90.91%, F1 Score: 90.96%\n",
      "\n",
      "Source performance: 83.93% 93.53% 83.86% 81.15%\n",
      "Target performance: 88.17% 91.66% 87.99% 86.05%\n",
      "\n",
      "Per-Class Accuracy on Target Domain (Mean over runs):\n",
      "  Class 0: 100.00%\n",
      "  Class 1: 96.20%\n",
      "  Class 2: 59.41%\n",
      "  Class 3: 96.36%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"/home/ash/ic3/testbed_da/data\"\n",
    "\n",
    "# Classes in loaded npy files\n",
    "class_subset = [\"bpsk\", \"qpsk\", \"16qam\", \"8apsk\"]\n",
    "\n",
    "# simulated data\n",
    "X_sim = np.load(file_path + \"/sim_X.npy\")\n",
    "Y_sim = np.load(file_path + \"/sim_Y.npy\")\n",
    "\n",
    "# over the air data\n",
    "X_ota = np.load(file_path + \"/ota_X.npy\")\n",
    "Y_ota = np.load(file_path + \"/ota_Y.npy\")\n",
    "\n",
    "z_val = 10\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "n_runs = 3\n",
    "n_snr = 4\n",
    "\n",
    "# (Lists for storing accuracies; you may also want to add lists for deep coral)\n",
    "t_deep_coral_acc = []\n",
    "t_base_acc = []\n",
    "t_dann_acc = []\n",
    "t_mcd_acc = []\n",
    "t_star_acc = []\n",
    "t_jan_acc = []\n",
    "\n",
    "for i in range(n_snr):\n",
    "    print(f'\\nSNR level: {z_val}')\n",
    "    # Filter for SNR level (for simulated data)\n",
    "    source_mask = (Y_sim[:, 1] == z_val)\n",
    "    X_s = X_sim[source_mask]\n",
    "    Y_s = Y_sim[source_mask]\n",
    "    Y_s = Y_s[:, 0]  # first column: class labels\n",
    "\n",
    "    # Filter for SNR level (for over the air data)\n",
    "    source_mask = (Y_ota[:, 1] == z_val+4)\n",
    "    X_t = X_ota[source_mask]\n",
    "    Y_t = Y_ota[source_mask]\n",
    "    Y_t = Y_t[:, 0]\n",
    "    \n",
    "    # Create data loaders\n",
    "    S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, batch_size=128, permute=False)\n",
    "    T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, batch_size=128, permute=False)\n",
    "\n",
    "    print('Base')\n",
    "    _, t_base = base.Base(\n",
    "        model_cls=DeepResNet,\n",
    "        device=device,\n",
    "        S_train_loader=S_train_loader, \n",
    "        S_val_loader=S_val_loader,\n",
    "        T_val_loader=T_val_loader,\n",
    "        class_subset=class_subset, \n",
    "        n_classes=len(class_subset),\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "        n_runs=n_runs\n",
    "    ).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    #s_deep_coral_acc.append(s_deep)\n",
    "    t_base_acc.append(t_base)\n",
    "\n",
    "    print('DANN')\n",
    "    _, t_dann = dann.DAN(\n",
    "        dann.DANN,\n",
    "        FA=DANN_F,\n",
    "        LP=DANN_LP,\n",
    "        DC=DANN_DC,\n",
    "        device=device,\n",
    "        S_train_loader=S_train_loader,\n",
    "        S_val_loader=S_val_loader,\n",
    "        T_train_loader=T_train_loader,\n",
    "        T_val_loader=T_val_loader,\n",
    "        class_subset=class_subset,\n",
    "        n_classes=len(class_subset),\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "        n_runs=n_runs\n",
    "    ).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    #s_deep_coral_acc.append(s_deep)\n",
    "    t_dann_acc.append(t_dann)\n",
    "\n",
    "    print('Deep CORAL')\n",
    "    _, t_deep = DeepCORAL(\n",
    "        G=CORAL_G, \n",
    "        C=CORAL_C, \n",
    "        device=device, \n",
    "        S_train_loader=S_train_loader,\n",
    "        S_val_loader=S_val_loader,\n",
    "        T_train_loader=T_train_loader,\n",
    "        T_val_loader=T_val_loader,\n",
    "        class_subset=class_subset,\n",
    "        n_classes=len(class_subset),\n",
    "        lr=lr, \n",
    "        n_epochs=n_epochs, \n",
    "        n_runs=n_runs,\n",
    "        patience=5,\n",
    "        lambda_coral=0.5,\n",
    "        deep_weights=(1.0, 1.0, 1.0)\n",
    "    ).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    #s_deep_coral_acc.append(s_deep)\n",
    "    t_deep_coral_acc.append(t_deep)\n",
    "\n",
    "    print('STAR')\n",
    "    _, t_star =  star.Star(\n",
    "        G=STAR_G,\n",
    "        C=STAR_C,\n",
    "        device=device,\n",
    "        S_train_loader=S_train_loader,\n",
    "        S_val_loader=S_val_loader,  \n",
    "        T_train_loader=T_train_loader,\n",
    "        T_val_loader=T_val_loader,\n",
    "        class_subset=class_subset,\n",
    "        n_classes=len(class_subset),\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "        n_runs=n_runs,\n",
    "        patience=5\n",
    "    ).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    #s_deep_coral_acc.append(s_deep)\n",
    "    t_star_acc.append(t_star)\n",
    "\n",
    "    print('MCD')\n",
    "    _, t_mcd = mcd.Mcd(\n",
    "        G=MCD_G,\n",
    "        C=MCD_C,\n",
    "        device=device,\n",
    "        S_train_loader=S_train_loader,\n",
    "        S_val_loader=S_val_loader,  \n",
    "        T_train_loader=T_train_loader,\n",
    "        T_val_loader=T_val_loader,\n",
    "        class_subset=class_subset,\n",
    "        n_classes=len(class_subset),\n",
    "        lr=lr,\n",
    "        n_epochs=n_epochs,\n",
    "        n_runs=n_runs,\n",
    "        patience=5\n",
    "    ).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    #s_deep_coral_acc.append(s_deep)\n",
    "    t_mcd_acc.append(t_mcd)\n",
    "\n",
    "    print('JAN')\n",
    "    _, t_jan = jan.Jan(\n",
    "        C=C_JAN,\n",
    "        G=JAN_G,\n",
    "        num_classes=len(class_subset),\n",
    "        device=device,\n",
    "        S_train_loader=S_train_loader,\n",
    "        T_train_loader=T_train_loader,\n",
    "        S_val_loader=S_val_loader,\n",
    "        T_val_loader=T_val_loader,\n",
    "        n_epochs=n_epochs,\n",
    "        lr=lr,\n",
    "        lambda_jmmd=0.1,\n",
    "        n_runs=n_runs\n",
    "    ).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    #s_deep_coral_acc.append(s_deep)\n",
    "    t_jan_acc.append(t_jan)\n",
    "\n",
    "    \n",
    "\n",
    "    z_val += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35388afa-33e5-4846-86dc-331fc02dda64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1, n_snr+1)\n",
    "print(len(t_deep_coral_acc))\n",
    "print(len(x))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')\n",
    "plt.plot(x, t_dann_acc, marker='^', linestyle='--', label='DANN')\n",
    "plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')\n",
    "plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, t_deep_coral_acc, marker='v', linestyle='--', label='DCORAL')\n",
    "plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc02521-9d1d-4884-8afc-a020b324e18b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resnet for DANN.\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"/home/ash/ic3/testbed_da/data\"\n",
    "\n",
    "# Classes in loaded npy files\n",
    "class_subset = [\"bpsk\", \"qpsk\", \"16qam\", \"8apsk\"]\n",
    "\n",
    "# simulated data\n",
    "X_sim = np.load(file_path + \"/sim_X.npy\")\n",
    "Y_sim = np.load(file_path + \"/sim_Y.npy\")\n",
    "\n",
    "# over the air data\n",
    "X_ota = np.load(file_path + \"/ota_X.npy\")\n",
    "Y_ota = np.load(file_path + \"/ota_Y.npy\")\n",
    "\n",
    "z_val = 10\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "n_runs = 5\n",
    "n_snr = 4\n",
    "\n",
    "# (Lists for storing accuracies; you may also want to add lists for deep coral)\n",
    "t_deep_coral_acc = []\n",
    "s_deep_coral_acc = []\n",
    "\n",
    "for i in range(n_snr):\n",
    "    print(f'\\nSNR level: {z_val}')\n",
    "    # Filter for SNR level (for simulated data)\n",
    "    source_mask = (Y_sim[:, 1] == z_val)\n",
    "    X_s = X_sim[source_mask]\n",
    "    Y_s = Y_sim[source_mask]\n",
    "    Y_s = Y_s[:, 0]  # first column: class labels\n",
    "\n",
    "    # Filter for SNR level (for over the air data)\n",
    "    source_mask = (Y_ota[:, 1] == z_val+4)\n",
    "    X_t = X_ota[source_mask]\n",
    "    Y_t = Y_ota[source_mask]\n",
    "    Y_t = Y_t[:, 0]\n",
    "    \n",
    "    # Create data loaders\n",
    "    S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, batch_size=128, permute=False)\n",
    "    T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, batch_size=128, permute=False)\n",
    "\n",
    "    z_val += 4\n",
    "\n",
    "x = np.arange(1, n_snr+1)\n",
    "print(len(t_deep_coral_acc))\n",
    "print(len(x))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')\n",
    "#plt.plot(x, s_dann_acc, marker='v', linestyle='-', label='Source')\n",
    "#plt.plot(x, t_dann_acc, marker='^', linestyle='-', label='Target')\n",
    "#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')\n",
    "#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, t_deep_coral_acc, marker='v', linestyle='-', label='Target')\n",
    "plt.plot(x, s_deep_coral_acc, marker='^', linestyle='-', label='Source')\n",
    "#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1525c72-3a7a-4191-bb1d-9af898afff3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep CORAL + ResNet.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------\n",
    "# Data loading (same as before)\n",
    "# -------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"/home/ash/ic3/testbed_da/data\"\n",
    "\n",
    "# Classes in loaded npy files\n",
    "class_subset = [\"bpsk\", \"qpsk\", \"16qam\", \"8apsk\"]\n",
    "\n",
    "# simulated data\n",
    "X_sim = np.load(file_path + \"/sim_X.npy\")\n",
    "Y_sim = np.load(file_path + \"/sim_Y.npy\")\n",
    "\n",
    "# over the air data\n",
    "X_ota = np.load(file_path + \"/ota_X.npy\")\n",
    "Y_ota = np.load(file_path + \"/ota_Y.npy\")\n",
    "\n",
    "z_val = 10\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "n_runs = 5\n",
    "n_snr = 4\n",
    "\n",
    "# (Lists for storing accuracies; you may also want to add lists for deep coral)\n",
    "t_deep_coral_acc = []\n",
    "s_deep_coral_acc = []\n",
    "\n",
    "for i in range(n_snr):\n",
    "    print(f'\\nSNR level: {z_val}')\n",
    "    # Filter for SNR level (for simulated data)\n",
    "    source_mask = (Y_sim[:, 1] == z_val)\n",
    "    X_s = X_sim[source_mask]\n",
    "    Y_s = Y_sim[source_mask]\n",
    "    Y_s = Y_s[:, 0]  # first column: class labels\n",
    "\n",
    "    # Filter for SNR level (for over the air data)\n",
    "    source_mask = (Y_ota[:, 1] == z_val+4)\n",
    "    X_t = X_ota[source_mask]\n",
    "    Y_t = Y_ota[source_mask]\n",
    "    Y_t = Y_t[:, 0]\n",
    "    \n",
    "    # Create data loaders\n",
    "    S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, batch_size=128, permute=False)\n",
    "    T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, batch_size=128, permute=False)\n",
    "\n",
    "    z_val += 4\n",
    "\n",
    "x = np.arange(1, n_snr+1)\n",
    "print(len(t_deep_coral_acc))\n",
    "print(len(x))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')\n",
    "#plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')\n",
    "#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, t_deep_coral_acc, marker='v', linestyle='-', label='Target')\n",
    "plt.plot(x, s_deep_coral_acc, marker='^', linestyle='-', label='Source')\n",
    "#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f35aa-6165-44a4-8296-2cae8fc363ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STAR + ResNet.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"/home/ash/ic3/testbed_da/data\"\n",
    "\n",
    "# Classes in loaded npy files\n",
    "class_subset = [\"bpsk\", \"qpsk\", \"16qam\", \"8apsk\"]\n",
    "\n",
    "# simulated data\n",
    "X_sim = np.load(file_path + \"/sim_X.npy\")\n",
    "Y_sim = np.load(file_path + \"/sim_Y.npy\")\n",
    "\n",
    "# over the air data\n",
    "X_ota = np.load(file_path + \"/ota_X.npy\")\n",
    "Y_ota = np.load(file_path + \"/ota_Y.npy\")\n",
    "\n",
    "z_val = 10\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "n_runs = 5\n",
    "n_snr = 4\n",
    "\n",
    "# (Lists for storing accuracies; you may also want to add lists for deep coral)\n",
    "t_deep_coral_acc = []\n",
    "s_deep_coral_acc = []\n",
    "\n",
    "for i in range(n_snr):\n",
    "    print(f'\\nSNR level: {z_val}')\n",
    "    # Filter for SNR level (for simulated data)\n",
    "    source_mask = (Y_sim[:, 1] == z_val)\n",
    "    X_s = X_sim[source_mask]\n",
    "    Y_s = Y_sim[source_mask]\n",
    "    Y_s = Y_s[:, 0]  # first column: class labels\n",
    "\n",
    "    # Filter for SNR level (for over the air data)\n",
    "    source_mask = (Y_ota[:, 1] == z_val+4)\n",
    "    X_t = X_ota[source_mask]\n",
    "    Y_t = Y_ota[source_mask]\n",
    "    Y_t = Y_t[:, 0]\n",
    "    \n",
    "    # Create data loaders\n",
    "    S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, batch_size=128, permute=False)\n",
    "    T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, batch_size=128, permute=False)\n",
    "\n",
    "    z_val += 4\n",
    "\n",
    "x = np.arange(1, n_snr+1)\n",
    "print(len(t_deep_coral_acc))\n",
    "print(len(x))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')\n",
    "#plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')\n",
    "#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, t_deep_coral_acc, marker='v', linestyle='-', label='Target')\n",
    "plt.plot(x, s_deep_coral_acc, marker='^', linestyle='-', label='Source')\n",
    "#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5b42e-8a61-4476-9f9c-15314b3fdf21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resnet for MCD\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"/home/ash/ic3/testbed_da/data\"\n",
    "\n",
    "# Classes in loaded npy files\n",
    "class_subset = [\"bpsk\", \"qpsk\", \"16qam\", \"8apsk\"]\n",
    "\n",
    "# simulated data\n",
    "X_sim = np.load(file_path + \"/sim_X.npy\")\n",
    "Y_sim = np.load(file_path + \"/sim_Y.npy\")\n",
    "\n",
    "# over the air data\n",
    "X_ota = np.load(file_path + \"/ota_X.npy\")\n",
    "Y_ota = np.load(file_path + \"/ota_Y.npy\")\n",
    "\n",
    "z_val = 10\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "n_runs = 5\n",
    "n_snr = 4\n",
    "\n",
    "# (Lists for storing accuracies; you may also want to add lists for deep coral)\n",
    "t_deep_coral_acc = []\n",
    "s_deep_coral_acc = []\n",
    "\n",
    "for i in range(n_snr):\n",
    "    print(f'\\nSNR level: {z_val}')\n",
    "    # Filter for SNR level (for simulated data)\n",
    "    source_mask = (Y_sim[:, 1] == z_val)\n",
    "    X_s = X_sim[source_mask]\n",
    "    Y_s = Y_sim[source_mask]\n",
    "    Y_s = Y_s[:, 0]  # first column: class labels\n",
    "\n",
    "    # Filter for SNR level (for over the air data)\n",
    "    source_mask = (Y_ota[:, 1] == z_val+4)\n",
    "    X_t = X_ota[source_mask]\n",
    "    Y_t = Y_ota[source_mask]\n",
    "    Y_t = Y_t[:, 0]\n",
    "    \n",
    "    # Create data loaders\n",
    "    S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, batch_size=128, permute=False)\n",
    "    T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, batch_size=128, permute=False)\n",
    "\n",
    "    z_val += 4\n",
    "\n",
    "x = np.arange(1, n_snr+1)\n",
    "print(len(t_deep_coral_acc))\n",
    "print(len(x))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')\n",
    "#plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')\n",
    "#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, t_deep_coral_acc, marker='v', linestyle='-', label='Target')\n",
    "plt.plot(x, s_deep_coral_acc, marker='^', linestyle='-', label='Source')\n",
    "#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08b866-4797-445a-94cd-4a0941268e77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resnet for JAN\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"/home/ash/ic3/testbed_da/data\"\n",
    "\n",
    "# Classes in loaded npy files\n",
    "class_subset = [\"bpsk\", \"qpsk\", \"16qam\", \"8apsk\"]\n",
    "\n",
    "# simulated data\n",
    "X_sim = np.load(file_path + \"/sim_X.npy\")\n",
    "Y_sim = np.load(file_path + \"/sim_Y.npy\")\n",
    "\n",
    "# over the air data\n",
    "X_ota = np.load(file_path + \"/ota_X.npy\")\n",
    "Y_ota = np.load(file_path + \"/ota_Y.npy\")\n",
    "\n",
    "z_val = 10\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "n_runs = 5\n",
    "n_snr = 4\n",
    "\n",
    "# (Lists for storing accuracies; you may also want to add lists for deep coral)\n",
    "t_deep_coral_acc = []\n",
    "s_deep_coral_acc = []\n",
    "\n",
    "for i in range(n_snr):\n",
    "    print(f'\\nSNR level: {z_val}')\n",
    "    # Filter for SNR level (for simulated data)\n",
    "    source_mask = (Y_sim[:, 1] == z_val)\n",
    "    X_s = X_sim[source_mask]\n",
    "    Y_s = Y_sim[source_mask]\n",
    "    Y_s = Y_s[:, 0]  # first column: class labels\n",
    "\n",
    "    # Filter for SNR level (for over the air data)\n",
    "    source_mask = (Y_ota[:, 1] == z_val+4)\n",
    "    X_t = X_ota[source_mask]\n",
    "    Y_t = Y_ota[source_mask]\n",
    "    Y_t = Y_t[:, 0]\n",
    "    \n",
    "    # Create data loaders\n",
    "    S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, batch_size=128, permute=False)\n",
    "    T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, batch_size=128, permute=False)\n",
    "\n",
    "    z_val += 4\n",
    "\n",
    "x = np.arange(1, n_snr+1)\n",
    "print(len(t_deep_coral_acc))\n",
    "print(len(x))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')\n",
    "#plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')\n",
    "#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, t_deep_coral_acc, marker='v', linestyle='-', label='Target')\n",
    "plt.plot(x, s_deep_coral_acc, marker='^', linestyle='-', label='Source')\n",
    "#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
