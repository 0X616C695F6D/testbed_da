<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>DA_resnet</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,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);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        const { svg } = await mermaid.render(id, raw, el);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=c2036317-47cb-4da7-9caa-b1f64b1857ce">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">SNR-to-SNR DA using ResNet</span>
<span class="sd">"""</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">funcs</span>
<span class="kn">import</span> <span class="nn">jan</span>
<span class="kn">import</span> <span class="nn">coral</span>
<span class="kn">import</span> <span class="nn">star</span>
<span class="kn">import</span> <span class="nn">mcd</span>
<span class="kn">import</span> <span class="nn">dann</span>
<span class="kn">import</span> <span class="nn">base</span>
<span class="kn">import</span> <span class="nn">plots</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span>

<span class="c1">#%% ResNet block</span>
<span class="k">class</span> <span class="nc">ResidualBlock1D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        A basic residual block for 1D convolutions.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">identity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">+=</span> <span class="n">identity</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1">#%% Base</span>
<span class="k">class</span> <span class="nc">DeepResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    A deep ResNet classifier for 1D signals.</span>
<span class="sd">    </span>
<span class="sd">    This network consists of:</span>
<span class="sd">      - An initial convolution + BN + ReLU + maxpool block.</span>
<span class="sd">      - Four residual layers (with increasing feature channels).</span>
<span class="sd">      - Global average pooling to obtain a fixed–length feature vector.</span>
<span class="sd">      - A bottleneck fully connected layer mapping to 512–dimensions.</span>
<span class="sd">      - A small classifier head (MLP) mapping to the desired output classes.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeepResNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to obtain a fixed-length feature vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck fully-connected layer: maps 512-dim to 512-dim features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
        <span class="c1"># Classifier head (MLP)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Creates a sequential layer composed of multiple residual blocks.</span>
<span class="sd">        If the stride is not 1 or the number of channels change,</span>
<span class="sd">        a downsampling layer is used.</span>
<span class="sd">        """</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch_size, 2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 64, L/4)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 64, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 128, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 256, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 512, ?)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># (B, 512)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 512)</span>
        
        <span class="c1"># Classification head</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1">#%% DANN</span>
<span class="k">class</span> <span class="nc">GradReverse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">=</span> <span class="n">lambda_</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">neg</span><span class="p">()</span> <span class="o">*</span> <span class="n">ctx</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">GradReverse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DANN_F</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Deep ResNet feature extractor for DANN.</span>
<span class="sd">    </span>
<span class="sd">    This network accepts a 2–channel 1D signal and outputs a 512–dimensional feature vector.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DANN_F</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to produce a fixed-length vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck fully-connected layer (optional, here keeping feature dim at 512)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Create a sequential layer composed of multiple residual blocks.</span>
<span class="sd">        """</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Input x: (batch_size, channels=2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># -&gt; (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># -&gt; (B, 64, L/4)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># -&gt; (B, 64, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># -&gt; (B, 128, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># -&gt; (B, 256, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># -&gt; (B, 512, ?)</span>
        
        <span class="c1"># Global average pooling</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># -&gt; (B, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># -&gt; (B, 512)</span>
        <span class="c1"># Bottleneck transformation</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 512)</span>
        <span class="k">return</span> <span class="n">features</span>

<span class="k">class</span> <span class="nc">DANN_LP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Label predictor network that maps 512–dim features to the desired output classes.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DANN_LP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">DANN_DC</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Domain classifier network for DANN.</span>
<span class="sd">    </span>
<span class="sd">    This network applies a gradient reversal layer (using ReverseLayerF) to the feature vector</span>
<span class="sd">    before classifying it as either source or target.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DANN_DC</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="c1"># Reverse gradient during the backward pass</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">GradReverse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1">#%% CORAL</span>
<span class="k">def</span> <span class="nf">coral_loss</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Computes the CORAL loss between two matrices.</span>
<span class="sd">    Assumes input tensors are of shape (batch_size, feature_dim).</span>
<span class="sd">    """</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">source</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ns</span> <span class="o">=</span> <span class="n">source</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">nt</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Center the features</span>
    <span class="n">source_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">target_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">source_centered</span> <span class="o">=</span> <span class="n">source</span> <span class="o">-</span> <span class="n">source_mean</span>
    <span class="n">target_centered</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">target_mean</span>
    <span class="c1"># Compute covariance matrices</span>
    <span class="n">cov_source</span> <span class="o">=</span> <span class="p">(</span><span class="n">source_centered</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="o">@</span> <span class="n">source_centered</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">ns</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">cov_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">target_centered</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="o">@</span> <span class="n">target_centered</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">nt</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Frobenius norm between covariance matrices (scaled)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">cov_source</span> <span class="o">-</span> <span class="n">cov_target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">d</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
        
<span class="k">class</span> <span class="nc">CORAL_G</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Deep ResNet generator that extracts features at multiple depths for CORAL.</span>
<span class="sd">    </span>
<span class="sd">    Architecture:</span>
<span class="sd">      - An initial convolution + batchnorm + ReLU + maxpool.</span>
<span class="sd">      - Four layers (residual blocks) built with 1D convolutions.</span>
<span class="sd">      - Intermediate features are extracted after layer1 (early), layer2 (middle),</span>
<span class="sd">        and layer3 (late) via global average pooling.</span>
<span class="sd">      - The final layer (layer4) is pooled and passed through a fully-connected</span>
<span class="sd">        bottleneck to obtain a 512–dim feature for classification.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CORAL_G</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolution layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling (adaptive to any sequence length)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck fully-connected layer mapping 512-&gt;512 for classification.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Creates a sequential layer composed of multiple residual blocks.</span>
<span class="sd">        If the stride is not 1 or the channel dimensions differ,</span>
<span class="sd">        a downsampling layer is used.</span>
<span class="sd">        """</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Input x: (batch_size, channels=2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, 64, L/4)</span>
        
        <span class="c1"># Layer 1: Early features</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>           <span class="c1"># (B, 64, L1)</span>
        <span class="n">early</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>      <span class="c1"># (B, 64, 1)</span>
        <span class="n">early</span> <span class="o">=</span> <span class="n">early</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>     <span class="c1"># (B, 64)</span>
        
        <span class="c1"># Layer 2: Middle features</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>          <span class="c1"># (B, 128, L2)</span>
        <span class="n">middle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>     <span class="c1"># (B, 128, 1)</span>
        <span class="n">middle</span> <span class="o">=</span> <span class="n">middle</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># (B, 128)</span>
        
        <span class="c1"># Layer 3: Late features for CORAL loss</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>          <span class="c1"># (B, 256, L3)</span>
        <span class="n">late</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>       <span class="c1"># (B, 256, 1)</span>
        <span class="n">late</span> <span class="o">=</span> <span class="n">late</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>       <span class="c1"># (B, 256)</span>
        
        <span class="c1"># Layer 4: Final block for classification</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>          <span class="c1"># (B, 512, L4)</span>
        <span class="n">pooled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>     <span class="c1"># (B, 512, 1)</span>
        <span class="n">pooled</span> <span class="o">=</span> <span class="n">pooled</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># (B, 512)</span>
        <span class="n">final</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">pooled</span><span class="p">)</span>  <span class="c1"># (B, 512)</span>
        
        <span class="c1"># Return multi-level features for deep CORAL</span>
        <span class="k">return</span> <span class="n">early</span><span class="p">,</span> <span class="n">middle</span><span class="p">,</span> <span class="n">late</span><span class="p">,</span> <span class="n">final</span>

<span class="k">class</span> <span class="nc">CORAL_C</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Classifier network that maps the 512–dim bottleneck features to the output classes.</span>
<span class="sd">    </span>
<span class="sd">    Architecture:</span>
<span class="sd">      - A fully-connected layer (512 -&gt; 256) with batch normalization, ReLU, and dropout.</span>
<span class="sd">      - A final fully-connected layer mapping 256 -&gt; output_dim.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CORAL_C</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">DeepCORAL</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> 
                 <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span><span class="p">,</span> 
                 <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span><span class="p">,</span>
                 <span class="n">class_subset</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">n_runs</span><span class="p">,</span> <span class="n">patience</span><span class="p">,</span> 
                 <span class="n">lambda_coral</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">deep_weights</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        deep_weights: tuple of weights for the CORAL loss at (early, middle, late) layers.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">G</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">S_train_loader</span> <span class="o">=</span> <span class="n">S_train_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">S_val_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_train_loader</span> <span class="o">=</span> <span class="n">T_train_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">T_val_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_subset</span> <span class="o">=</span> <span class="n">class_subset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_epochs</span> <span class="o">=</span> <span class="n">n_epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_runs</span> <span class="o">=</span> <span class="n">n_runs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_coral</span> <span class="o">=</span> <span class="n">lambda_coral</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deep_weights</span> <span class="o">=</span> <span class="n">deep_weights</span>
        
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="n">loader</span><span class="p">):</span>
        <span class="n">netG</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">netC</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="c1"># Only need the classification branch (late_fc)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">cls_feat</span> <span class="o">=</span> <span class="n">netG</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">netC</span><span class="p">(</span><span class="n">cls_feat</span><span class="p">)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">best_s_acc_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_t_acc_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_runs</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Deep CORAL Run </span><span class="si">{</span><span class="n">run</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_runs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
            <span class="c1"># Instantiate new networks for each run</span>
            <span class="n">netG</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">netC</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">netG</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">netC</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
            <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">best_val_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">patience_counter</span> <span class="o">=</span> <span class="mi">0</span>
            
            <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epochs</span><span class="p">):</span>
                <span class="n">netG</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                <span class="n">netC</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                <span class="c1"># Use zip to iterate over source and target batches in parallel.</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">s_data</span><span class="p">,</span> <span class="n">s_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">t_data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">S_train_loader</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_train_loader</span><span class="p">):</span>
                    <span class="n">s_data</span><span class="p">,</span> <span class="n">s_labels</span> <span class="o">=</span> <span class="n">s_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">s_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">t_data</span> <span class="o">=</span> <span class="n">t_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    
                    <span class="c1"># Forward pass for source: get all features</span>
                    <span class="n">s_early</span><span class="p">,</span> <span class="n">s_middle</span><span class="p">,</span> <span class="n">s_late</span><span class="p">,</span> <span class="n">s_cls</span> <span class="o">=</span> <span class="n">netG</span><span class="p">(</span><span class="n">s_data</span><span class="p">)</span>
                    <span class="c1"># Classification loss on source (using the classification feature)</span>
                    <span class="n">loss_cls</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">netC</span><span class="p">(</span><span class="n">s_cls</span><span class="p">),</span> <span class="n">s_labels</span><span class="p">)</span>
                    
                    <span class="c1"># Forward pass for target: we only need the intermediate features</span>
                    <span class="n">t_early</span><span class="p">,</span> <span class="n">t_middle</span><span class="p">,</span> <span class="n">t_late</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">netG</span><span class="p">(</span><span class="n">t_data</span><span class="p">)</span>
                    
                    <span class="c1"># Compute CORAL loss for each level</span>
                    <span class="n">loss_early</span> <span class="o">=</span> <span class="n">coral_loss</span><span class="p">(</span><span class="n">s_early</span><span class="p">,</span> <span class="n">t_early</span><span class="p">)</span>
                    <span class="n">loss_middle</span> <span class="o">=</span> <span class="n">coral_loss</span><span class="p">(</span><span class="n">s_middle</span><span class="p">,</span> <span class="n">t_middle</span><span class="p">)</span>
                    <span class="n">loss_late</span> <span class="o">=</span> <span class="n">coral_loss</span><span class="p">(</span><span class="n">s_late</span><span class="p">,</span> <span class="n">t_late</span><span class="p">)</span>
                    <span class="n">loss_coral_total</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">deep_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_early</span> <span class="o">+</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">deep_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_middle</span> <span class="o">+</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">deep_weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_late</span><span class="p">)</span>
                    
                    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">loss_cls</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_coral</span> <span class="o">*</span> <span class="n">loss_coral_total</span>
                    
                    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                    <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                    
                <span class="c1"># End-of-epoch evaluation on source validation set</span>
                <span class="n">s_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S_val_loader</span><span class="p">)</span>
                <span class="n">t_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_val_loader</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: Source Val Acc = </span><span class="si">{</span><span class="n">s_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Target Val Acc = </span><span class="si">{</span><span class="n">t_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                
                <span class="c1"># Early stopping on source validation accuracy</span>
                <span class="k">if</span> <span class="n">s_acc</span> <span class="o">&gt;</span> <span class="n">best_val_acc</span><span class="p">:</span>
                    <span class="n">best_val_acc</span> <span class="o">=</span> <span class="n">s_acc</span>
                    <span class="n">best_model_G</span> <span class="o">=</span> <span class="n">netG</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                    <span class="n">best_model_C</span> <span class="o">=</span> <span class="n">netC</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                    <span class="n">patience_counter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">patience_counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">patience_counter</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">"Early stopping triggered."</span><span class="p">)</span>
                    <span class="k">break</span>
                    
            <span class="c1"># Load best model from this run</span>
            <span class="n">netG</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_model_G</span><span class="p">)</span>
            <span class="n">netC</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_model_C</span><span class="p">)</span>
            <span class="n">s_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S_val_loader</span><span class="p">)</span>
            <span class="n">t_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_val_loader</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Run </span><span class="si">{</span><span class="n">run</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> finished: Best Source Val Acc = </span><span class="si">{</span><span class="n">s_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Target Val Acc = </span><span class="si">{</span><span class="n">t_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
            <span class="n">best_s_acc_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s_acc</span><span class="p">)</span>
            <span class="n">best_t_acc_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_acc</span><span class="p">)</span>
            
        <span class="n">avg_s_acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">best_s_acc_list</span><span class="p">)</span>
        <span class="n">avg_t_acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">best_t_acc_list</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Deep CORAL: Average Source Val Acc = </span><span class="si">{</span><span class="n">avg_s_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Average Target Val Acc = </span><span class="si">{</span><span class="n">avg_t_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">avg_s_acc</span><span class="p">,</span> <span class="n">avg_t_acc</span>

<span class="c1">#%% STAR</span>
<span class="k">class</span> <span class="nc">STAR_G</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Deep ResNet feature extractor for STAR.</span>
<span class="sd">    </span>
<span class="sd">    This network accepts a 2–channel 1D signal and extracts a 512–dimensional</span>
<span class="sd">    feature vector via several residual layers.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">STAR_G</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to obtain a fixed-length feature vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Optional bottleneck fully-connected layer (maps 512-&gt;512)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch_size, channels=2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># (B, 64, L/4)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># (B, 64, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># (B, 128, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># (B, 256, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># (B, 512, ?)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># (B, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># (B, 512)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 512)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">STAR_C</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Stochastic classifier network for STAR.</span>
<span class="sd">    </span>
<span class="sd">    This network receives the 512–dim features from DeepResNet_STAR_G,</span>
<span class="sd">    applies a fully connected layer with batch normalization and ReLU,</span>
<span class="sd">    and then uses a learned weight distribution (mu2, sigma2) to sample</span>
<span class="sd">    classifier weights. During training, it samples num_classifiers_train classifiers,</span>
<span class="sd">    while during evaluation it can either use only the mean (only_mu=True) or</span>
<span class="sd">    sample num_classifiers_test classifiers.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">num_classifiers_train</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_classifiers_test</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                 <span class="n">init</span><span class="o">=</span><span class="s1">'kaiming_u'</span><span class="p">,</span> <span class="n">use_init</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">STAR_C</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classifiers_train</span> <span class="o">=</span> <span class="n">num_classifiers_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classifiers_test</span> <span class="o">=</span> <span class="n">num_classifiers_test</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>

        <span class="n">function_init</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">'kaiming_u'</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">,</span>
            <span class="s1">'kaiming_n'</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">,</span>
            <span class="s1">'xavier'</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span>
        <span class="p">}</span>

        <span class="c1"># Change input dimension to 512 (from the DeepResNet feature extractor)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1_fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

        <span class="c1"># Learnable parameters for the classifier weight distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">use_init</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mu2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma2</span><span class="p">]:</span>
                <span class="n">function_init</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">](</span><span class="n">item</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_dim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">only_mu</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn1_fc</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">sigma2_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma2</span><span class="p">)</span>
        <span class="n">fc2_distribution</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2_pos</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">classifiers</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classifiers_train</span><span class="p">):</span>
                <span class="n">fc2_w</span> <span class="o">=</span> <span class="n">fc2_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>
                <span class="n">classifiers</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">fc2_w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">])</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">classifier</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">classifier</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">classifier</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">outputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">only_mu</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">out</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">classifiers</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classifiers_test</span><span class="p">):</span>
                    <span class="n">fc2_w</span> <span class="o">=</span> <span class="n">fc2_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>
                    <span class="n">classifiers</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">fc2_w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">])</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">classifier</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
                    <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">classifier</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">classifier</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">outputs</span>

<span class="c1">#%% MCD</span>
<span class="k">class</span> <span class="nc">MCD_G</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MCD_G</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to get a fixed-length feature vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck fully-connected layer mapping 512 -&gt; 512 (optional)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Input x shape: (batch_size, 2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># -&gt; (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># -&gt; (B, 64, L/4)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># -&gt; (B, 64, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># -&gt; (B, 128, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># -&gt; (B, 256, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># -&gt; (B, 512, ?)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># -&gt; (B, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># -&gt; (B, 512)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># -&gt; (B, 512)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">MCD_C</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MCD_C</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Since our feature extractor now outputs a 512-dim vector,</span>
        <span class="c1"># adjust the input dimension accordingly.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="c1"># Optionally apply gradient reversal</span>
        <span class="k">if</span> <span class="n">reverse</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1">#%% JAN</span>
<span class="k">class</span> <span class="nc">JAN_G</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">JAN_G</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to obtain a fixed-length feature vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck layer mapping 512 -&gt; 512 dimensions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch_size, 2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># shape: (batch, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># shape: (batch, 512)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># shape: (batch, 512)</span>
        
        <span class="c1"># Debug: Ensure x is a valid tensor</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Generator output is None. Check your forward pass."</span><span class="p">)</span>
        <span class="c1"># You can also uncomment the next line to print the shape during debugging.</span>
        <span class="c1"># print("Generator output shape:", x.shape)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">C_JAN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">C_JAN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_intermediate</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">inter</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_intermediate</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">inter</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=0d5022b5-1350-4b3b-8bcc-c653e90518d2">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Load testbed data</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"/home/ash/ic3/testbed_da/data"</span>

<span class="c1"># Classes</span>
<span class="n">class_subset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bpsk"</span><span class="p">,</span> <span class="s2">"qpsk"</span><span class="p">,</span> <span class="s2">"16qam"</span><span class="p">,</span> <span class="s2">"8apsk"</span><span class="p">]</span>

<span class="c1"># Split source, target</span>
<span class="c1"># try selecting some of the mods, not all</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_X.npy"</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_Y.npy"</span><span class="p">)</span>

<span class="n">sou_snr</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">tar_snr</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">t_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_base_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_dann_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_mcd_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_star_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_jan_acc</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_epochs</span><span class="o">=</span> <span class="mi">50</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">n_snr</span><span class="o">=</span> <span class="mi">6</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_snr</span><span class="p">):</span>    
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">sou_snr</span><span class="p">)</span>
    <span class="n">target_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">tar_snr</span><span class="p">)</span>
    
    <span class="n">X_s</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_s</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">X_t</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">target_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">target_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_t</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>

    
    <span class="c1"># Dataloaders</span>
    <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">Y_s</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_t</span><span class="p">,</span> <span class="n">Y_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'Base'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_base</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">Base</span><span class="p">(</span>
        <span class="n">model_cls</span><span class="o">=</span><span class="n">DeepResNet</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span> 
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span> 
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_base_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_base</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'DANN'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_dann</span> <span class="o">=</span> <span class="n">dann</span><span class="o">.</span><span class="n">DAN</span><span class="p">(</span>
        <span class="n">dann</span><span class="o">.</span><span class="n">DANN</span><span class="p">,</span>
        <span class="n">FA</span><span class="o">=</span><span class="n">DANN_F</span><span class="p">,</span>
        <span class="n">LP</span><span class="o">=</span><span class="n">DANN_LP</span><span class="p">,</span>
        <span class="n">DC</span><span class="o">=</span><span class="n">DANN_DC</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_dann_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_dann</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'Deep CORAL'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_deep</span> <span class="o">=</span> <span class="n">DeepCORAL</span><span class="p">(</span>
        <span class="n">G</span><span class="o">=</span><span class="n">CORAL_G</span><span class="p">,</span> 
        <span class="n">C</span><span class="o">=</span><span class="n">CORAL_C</span><span class="p">,</span> 
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> 
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> 
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> 
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">lambda_coral</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">deep_weights</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_deep_coral_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_deep</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'STAR'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_star</span> <span class="o">=</span>  <span class="n">star</span><span class="o">.</span><span class="n">Star</span><span class="p">(</span>
        <span class="n">G</span><span class="o">=</span><span class="n">STAR_G</span><span class="p">,</span>
        <span class="n">C</span><span class="o">=</span><span class="n">STAR_C</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>  
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_star_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_star</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'MCD'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_mcd</span> <span class="o">=</span> <span class="n">mcd</span><span class="o">.</span><span class="n">Mcd</span><span class="p">(</span>
        <span class="n">G</span><span class="o">=</span><span class="n">MCD_G</span><span class="p">,</span>
        <span class="n">C</span><span class="o">=</span><span class="n">MCD_C</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>  
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_mcd_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_mcd</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'JAN'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_jan</span> <span class="o">=</span> <span class="n">jan</span><span class="o">.</span><span class="n">Jan</span><span class="p">(</span>
        <span class="n">C</span><span class="o">=</span><span class="n">C_JAN</span><span class="p">,</span>
        <span class="n">G</span><span class="o">=</span><span class="n">JAN_G</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">lambda_jmmd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_jan_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_jan</span><span class="p">)</span>
    
    <span class="n">tar_snr</span> <span class="o">+=</span> <span class="mi">4</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Base

Run 1/10
Epoch 1/50, Train Loss: 0.3395, Train Acc: 0.8854, Val Loss: 1.8362, Val Acc: 0.5965
Epoch 2/50, Train Loss: 0.0395, Train Acc: 0.9904, Val Loss: 0.0448, Val Acc: 0.9862
Epoch 3/50, Train Loss: 0.0263, Train Acc: 0.9942, Val Loss: 0.1226, Val Acc: 0.9484
Epoch 4/50, Train Loss: 0.0205, Train Acc: 0.9922, Val Loss: 0.0494, Val Acc: 0.9856
Epoch 5/50, Train Loss: 0.0347, Train Acc: 0.9906, Val Loss: 0.0606, Val Acc: 0.9784
Epoch 6/50, Train Loss: 0.0075, Train Acc: 0.9990, Val Loss: 0.2259, Val Acc: 0.9227
Epoch 7/50, Train Loss: 0.0117, Train Acc: 0.9969, Val Loss: 0.0248, Val Acc: 0.9922
Epoch 8/50, Train Loss: 0.0070, Train Acc: 0.9990, Val Loss: 0.0026, Val Acc: 0.9994
Epoch 9/50, Train Loss: 0.0044, Train Acc: 0.9988, Val Loss: 0.0041, Val Acc: 0.9994
Epoch 10/50, Train Loss: 0.0059, Train Acc: 0.9990, Val Loss: 0.0506, Val Acc: 0.9820
Epoch 11/50, Train Loss: 0.0016, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0027, Train Acc: 0.9993, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0023, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0021, Train Acc: 0.9996, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0023, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Early stopping!

Run 2/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3201, Train Acc: 0.8974, Val Loss: 2.3417, Val Acc: 0.3579
Epoch 2/50, Train Loss: 0.0375, Train Acc: 0.9907, Val Loss: 0.1508, Val Acc: 0.9478
Epoch 3/50, Train Loss: 0.0236, Train Acc: 0.9933, Val Loss: 0.3762, Val Acc: 0.8201
Epoch 4/50, Train Loss: 0.0294, Train Acc: 0.9921, Val Loss: 0.4092, Val Acc: 0.8807
Epoch 5/50, Train Loss: 0.0134, Train Acc: 0.9964, Val Loss: 0.0519, Val Acc: 0.9784
Epoch 6/50, Train Loss: 0.0170, Train Acc: 0.9957, Val Loss: 0.0157, Val Acc: 0.9970
Epoch 7/50, Train Loss: 0.0146, Train Acc: 0.9954, Val Loss: 0.0190, Val Acc: 0.9946
Epoch 8/50, Train Loss: 0.0225, Train Acc: 0.9940, Val Loss: 0.1335, Val Acc: 0.9526
Epoch 9/50, Train Loss: 0.0173, Train Acc: 0.9948, Val Loss: 0.0597, Val Acc: 0.9796
Epoch 10/50, Train Loss: 0.0049, Train Acc: 0.9987, Val Loss: 0.0177, Val Acc: 0.9934
Epoch 11/50, Train Loss: 0.0035, Train Acc: 0.9991, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0024, Train Acc: 0.9994, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0015, Train Acc: 0.9994, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0030, Train Acc: 0.9996, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0019, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Early stopping!

Run 3/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3641, Train Acc: 0.8808, Val Loss: 1.6527, Val Acc: 0.6151
Epoch 2/50, Train Loss: 0.0583, Train Acc: 0.9850, Val Loss: 0.0107, Val Acc: 0.9970
Epoch 3/50, Train Loss: 0.0222, Train Acc: 0.9942, Val Loss: 0.0152, Val Acc: 0.9976
Epoch 4/50, Train Loss: 0.0213, Train Acc: 0.9927, Val Loss: 0.0302, Val Acc: 0.9964
Epoch 5/50, Train Loss: 0.0158, Train Acc: 0.9958, Val Loss: 0.2764, Val Acc: 0.8579
Epoch 6/50, Train Loss: 0.0096, Train Acc: 0.9976, Val Loss: 0.0129, Val Acc: 0.9952
Epoch 7/50, Train Loss: 0.0079, Train Acc: 0.9988, Val Loss: 0.2020, Val Acc: 0.9376
Early stopping!

Run 4/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3465, Train Acc: 0.8881, Val Loss: 1.3210, Val Acc: 0.5252
Epoch 2/50, Train Loss: 0.0430, Train Acc: 0.9877, Val Loss: 0.3832, Val Acc: 0.8771
Epoch 3/50, Train Loss: 0.0330, Train Acc: 0.9903, Val Loss: 0.0487, Val Acc: 0.9820
Epoch 4/50, Train Loss: 0.0283, Train Acc: 0.9916, Val Loss: 0.5089, Val Acc: 0.8429
Epoch 5/50, Train Loss: 0.0308, Train Acc: 0.9913, Val Loss: 0.0071, Val Acc: 0.9970
Epoch 6/50, Train Loss: 0.0179, Train Acc: 0.9963, Val Loss: 0.2267, Val Acc: 0.9275
Epoch 7/50, Train Loss: 0.0167, Train Acc: 0.9946, Val Loss: 0.0145, Val Acc: 0.9958
Epoch 8/50, Train Loss: 0.0240, Train Acc: 0.9951, Val Loss: 0.0461, Val Acc: 0.9880
Epoch 9/50, Train Loss: 0.0029, Train Acc: 0.9996, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 10/50, Train Loss: 0.0055, Train Acc: 0.9979, Val Loss: 0.0137, Val Acc: 0.9958
Epoch 11/50, Train Loss: 0.0045, Train Acc: 0.9988, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0017, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0027, Train Acc: 0.9993, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0026, Train Acc: 0.9996, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0036, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0019, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0029, Train Acc: 0.9990, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0025, Train Acc: 0.9996, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0010, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0010, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994
Early stopping!

Run 5/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3724, Train Acc: 0.8757, Val Loss: 0.5505, Val Acc: 0.8064
Epoch 2/50, Train Loss: 0.0533, Train Acc: 0.9862, Val Loss: 0.8340, Val Acc: 0.7860
Epoch 3/50, Train Loss: 0.0385, Train Acc: 0.9895, Val Loss: 0.1587, Val Acc: 0.9466
Epoch 4/50, Train Loss: 0.0320, Train Acc: 0.9913, Val Loss: 1.1393, Val Acc: 0.7590
Epoch 5/50, Train Loss: 0.0216, Train Acc: 0.9943, Val Loss: 0.6166, Val Acc: 0.8471
Epoch 6/50, Train Loss: 0.0173, Train Acc: 0.9946, Val Loss: 0.0693, Val Acc: 0.9772
Epoch 7/50, Train Loss: 0.0063, Train Acc: 0.9984, Val Loss: 0.0125, Val Acc: 0.9946
Epoch 8/50, Train Loss: 0.0059, Train Acc: 0.9981, Val Loss: 0.6356, Val Acc: 0.7878
Epoch 9/50, Train Loss: 0.0202, Train Acc: 0.9937, Val Loss: 1.7151, Val Acc: 0.7464
Epoch 10/50, Train Loss: 0.0109, Train Acc: 0.9972, Val Loss: 0.0177, Val Acc: 0.9970
Epoch 11/50, Train Loss: 0.0044, Train Acc: 0.9994, Val Loss: 0.0047, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0036, Train Acc: 0.9991, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0024, Train Acc: 0.9999, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0019, Train Acc: 0.9996, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0015, Train Acc: 0.9996, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0018, Train Acc: 0.9996, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0024, Train Acc: 0.9996, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0021, Train Acc: 0.9994, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0013, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0016, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0025, Val Acc: 0.9982
Epoch 23/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 25/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0085, Train Acc: 0.9987, Val Loss: 0.0026, Val Acc: 0.9982
Early stopping!

Run 6/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3527, Train Acc: 0.8850, Val Loss: 0.8774, Val Acc: 0.7278
Epoch 2/50, Train Loss: 0.0407, Train Acc: 0.9912, Val Loss: 0.1475, Val Acc: 0.9502
Epoch 3/50, Train Loss: 0.0263, Train Acc: 0.9924, Val Loss: 0.0129, Val Acc: 0.9970
Epoch 4/50, Train Loss: 0.0110, Train Acc: 0.9970, Val Loss: 0.3553, Val Acc: 0.8933
Epoch 5/50, Train Loss: 0.0239, Train Acc: 0.9937, Val Loss: 0.1890, Val Acc: 0.9850
Epoch 6/50, Train Loss: 0.0214, Train Acc: 0.9933, Val Loss: 0.4364, Val Acc: 0.8741
Epoch 7/50, Train Loss: 0.0118, Train Acc: 0.9976, Val Loss: 1.6708, Val Acc: 0.7644
Epoch 8/50, Train Loss: 0.0063, Train Acc: 0.9988, Val Loss: 0.0124, Val Acc: 0.9964
Epoch 9/50, Train Loss: 0.0111, Train Acc: 0.9969, Val Loss: 0.1478, Val Acc: 0.9347
Epoch 10/50, Train Loss: 0.0127, Train Acc: 0.9967, Val Loss: 0.2897, Val Acc: 0.8345
Epoch 11/50, Train Loss: 0.0040, Train Acc: 0.9993, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0025, Train Acc: 0.9994, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0019, Train Acc: 0.9994, Val Loss: 0.0058, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0027, Train Acc: 0.9993, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0011, Train Acc: 0.9996, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0025, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0025, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0018, Train Acc: 0.9996, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0024, Train Acc: 0.9991, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0014, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0036, Train Acc: 0.9991, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0010, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Early stopping!

Run 7/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.2700, Train Acc: 0.9160, Val Loss: 1.5308, Val Acc: 0.5977
Epoch 2/50, Train Loss: 0.0376, Train Acc: 0.9892, Val Loss: 0.0158, Val Acc: 0.9958
Epoch 3/50, Train Loss: 0.0435, Train Acc: 0.9873, Val Loss: 0.2321, Val Acc: 0.9329
Epoch 4/50, Train Loss: 0.0345, Train Acc: 0.9888, Val Loss: 2.1367, Val Acc: 0.7398
Epoch 5/50, Train Loss: 0.0223, Train Acc: 0.9936, Val Loss: 0.0102, Val Acc: 0.9976
Epoch 6/50, Train Loss: 0.0064, Train Acc: 0.9987, Val Loss: 0.0028, Val Acc: 0.9988
Epoch 7/50, Train Loss: 0.0183, Train Acc: 0.9948, Val Loss: 0.2566, Val Acc: 0.9167
Epoch 8/50, Train Loss: 0.0088, Train Acc: 0.9973, Val Loss: 0.0430, Val Acc: 0.9958
Epoch 9/50, Train Loss: 0.0096, Train Acc: 0.9975, Val Loss: 0.0074, Val Acc: 0.9976
Epoch 10/50, Train Loss: 0.0055, Train Acc: 0.9987, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0028, Train Acc: 0.9994, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0034, Train Acc: 0.9991, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0017, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0013, Train Acc: 0.9996, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0018, Train Acc: 0.9994, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0025, Train Acc: 0.9996, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Early stopping!

Run 8/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3217, Train Acc: 0.8959, Val Loss: 0.1079, Val Acc: 0.9724
Epoch 2/50, Train Loss: 0.0409, Train Acc: 0.9877, Val Loss: 0.2260, Val Acc: 0.9239
Epoch 3/50, Train Loss: 0.0173, Train Acc: 0.9954, Val Loss: 0.0310, Val Acc: 0.9934
Epoch 4/50, Train Loss: 0.0240, Train Acc: 0.9939, Val Loss: 0.0857, Val Acc: 0.9694
Epoch 5/50, Train Loss: 0.0276, Train Acc: 0.9915, Val Loss: 0.2008, Val Acc: 0.9299
Epoch 6/50, Train Loss: 0.0150, Train Acc: 0.9952, Val Loss: 1.1862, Val Acc: 0.7626
Epoch 7/50, Train Loss: 0.0057, Train Acc: 0.9985, Val Loss: 0.0021, Val Acc: 1.0000
Epoch 8/50, Train Loss: 0.0078, Train Acc: 0.9978, Val Loss: 2.7111, Val Acc: 0.7422
Epoch 9/50, Train Loss: 0.0079, Train Acc: 0.9975, Val Loss: 3.2816, Val Acc: 0.5138
Epoch 10/50, Train Loss: 0.0075, Train Acc: 0.9985, Val Loss: 0.0168, Val Acc: 0.9940
Epoch 11/50, Train Loss: 0.0039, Train Acc: 0.9991, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0084, Train Acc: 0.9982, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0025, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0012, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0020, Train Acc: 0.9993, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0078, Train Acc: 0.9993, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0014, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0033, Train Acc: 0.9990, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0017, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0016, Train Acc: 0.9996, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0012, Train Acc: 0.9996, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0030, Train Acc: 0.9993, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0012, Train Acc: 0.9996, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0019, Train Acc: 0.9994, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 27/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 9/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3557, Train Acc: 0.8766, Val Loss: 2.5657, Val Acc: 0.4083
Epoch 2/50, Train Loss: 0.0530, Train Acc: 0.9858, Val Loss: 0.2217, Val Acc: 0.9119
Epoch 3/50, Train Loss: 0.0279, Train Acc: 0.9919, Val Loss: 0.4616, Val Acc: 0.8573
Epoch 4/50, Train Loss: 0.0333, Train Acc: 0.9894, Val Loss: 0.0231, Val Acc: 0.9916
Epoch 5/50, Train Loss: 0.0114, Train Acc: 0.9981, Val Loss: 0.0150, Val Acc: 0.9958
Epoch 6/50, Train Loss: 0.0090, Train Acc: 0.9979, Val Loss: 1.6204, Val Acc: 0.7530
Epoch 7/50, Train Loss: 0.0220, Train Acc: 0.9933, Val Loss: 0.3182, Val Acc: 0.8939
Epoch 8/50, Train Loss: 0.0146, Train Acc: 0.9958, Val Loss: 0.0180, Val Acc: 0.9970
Epoch 9/50, Train Loss: 0.0067, Train Acc: 0.9979, Val Loss: 0.6799, Val Acc: 0.8118
Epoch 10/50, Train Loss: 0.0153, Train Acc: 0.9961, Val Loss: 2.1845, Val Acc: 0.6847
Early stopping!

Run 10/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3396, Train Acc: 0.8865, Val Loss: 2.7183, Val Acc: 0.5288
Epoch 2/50, Train Loss: 0.0400, Train Acc: 0.9894, Val Loss: 0.2548, Val Acc: 0.9053
Epoch 3/50, Train Loss: 0.0301, Train Acc: 0.9913, Val Loss: 0.1137, Val Acc: 0.9568
Epoch 4/50, Train Loss: 0.0145, Train Acc: 0.9967, Val Loss: 0.0047, Val Acc: 0.9976
Epoch 5/50, Train Loss: 0.0083, Train Acc: 0.9972, Val Loss: 0.4841, Val Acc: 0.8597
Epoch 6/50, Train Loss: 0.0134, Train Acc: 0.9952, Val Loss: 0.0162, Val Acc: 0.9952
Epoch 7/50, Train Loss: 0.0109, Train Acc: 0.9967, Val Loss: 0.0499, Val Acc: 0.9844
Epoch 8/50, Train Loss: 0.0132, Train Acc: 0.9964, Val Loss: 0.0040, Val Acc: 0.9988
Epoch 9/50, Train Loss: 0.0046, Train Acc: 0.9988, Val Loss: 0.0610, Val Acc: 0.9850
Epoch 10/50, Train Loss: 0.0101, Train Acc: 0.9973, Val Loss: 0.0881, Val Acc: 0.9742
Epoch 11/50, Train Loss: 0.0035, Train Acc: 0.9993, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0030, Train Acc: 0.9994, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0024, Train Acc: 0.9996, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0021, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0019, Train Acc: 0.9997, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0008, Val Acc: 0.9994
Early stopping!

Source performance: 96.19 95.04 96.09 95.38
Target performance: 51.49 46.65 50.91 40.01

bpsk: 99.86
qpsk: 0.00
16qam: 9.13
8apsk: 94.64
DANN
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2974, Domain Loss: 1.3296, Class Loss: 0.9678
Epoch 2/50, Loss: 1.5972, Domain Loss: 1.1383, Class Loss: 0.4590
Epoch 3/50, Loss: 1.6169, Domain Loss: 1.4289, Class Loss: 0.1881
Epoch 4/50, Loss: 6.3190, Domain Loss: 5.9362, Class Loss: 0.3828
Epoch 5/50, Loss: 9.2052, Domain Loss: 8.9078, Class Loss: 0.2974
Epoch 6/50, Loss: 8.4529, Domain Loss: 8.1916, Class Loss: 0.2613
Epoch 7/50, Loss: 5.2344, Domain Loss: 4.9685, Class Loss: 0.2659
Epoch 8/50, Loss: 5.9163, Domain Loss: 5.5460, Class Loss: 0.3703
Epoch 9/50, Loss: 7.7857, Domain Loss: 7.4581, Class Loss: 0.3276
Epoch 10/50, Loss: 4.9765, Domain Loss: 4.5067, Class Loss: 0.4697
Epoch 11/50, Loss: 5.3837, Domain Loss: 4.8990, Class Loss: 0.4847
Epoch 12/50, Loss: 3.6134, Domain Loss: 3.2825, Class Loss: 0.3309
Epoch 13/50, Loss: 3.1579, Domain Loss: 2.6525, Class Loss: 0.5054
Epoch 14/50, Loss: 3.0276, Domain Loss: 2.6118, Class Loss: 0.4158
Epoch 15/50, Loss: 4.0512, Domain Loss: 3.4952, Class Loss: 0.5560
Epoch 16/50, Loss: 3.8663, Domain Loss: 3.3988, Class Loss: 0.4675
Epoch 17/50, Loss: 2.7692, Domain Loss: 2.5600, Class Loss: 0.2092
Epoch 18/50, Loss: 3.4179, Domain Loss: 2.7855, Class Loss: 0.6324
Epoch 19/50, Loss: 4.4770, Domain Loss: 3.4714, Class Loss: 1.0056
Epoch 20/50, Loss: 3.4903, Domain Loss: 2.8644, Class Loss: 0.6259
Epoch 21/50, Loss: 2.6195, Domain Loss: 2.2609, Class Loss: 0.3586
Epoch 22/50, Loss: 2.5662, Domain Loss: 2.1505, Class Loss: 0.4157
Epoch 23/50, Loss: 1.8627, Domain Loss: 1.6174, Class Loss: 0.2453
Epoch 24/50, Loss: 1.9877, Domain Loss: 1.7576, Class Loss: 0.2302
Epoch 25/50, Loss: 2.7530, Domain Loss: 2.4787, Class Loss: 0.2743
Epoch 26/50, Loss: 2.5635, Domain Loss: 2.3565, Class Loss: 0.2070
Epoch 27/50, Loss: 1.6636, Domain Loss: 1.4850, Class Loss: 0.1786
Epoch 28/50, Loss: 2.2747, Domain Loss: 1.9190, Class Loss: 0.3556
Epoch 29/50, Loss: 1.9249, Domain Loss: 1.6500, Class Loss: 0.2748
Epoch 30/50, Loss: 1.9855, Domain Loss: 1.6755, Class Loss: 0.3101
Epoch 31/50, Loss: 2.5864, Domain Loss: 2.1132, Class Loss: 0.4732
Epoch 32/50, Loss: 2.2508, Domain Loss: 1.8964, Class Loss: 0.3544
Epoch 33/50, Loss: 2.2104, Domain Loss: 1.9436, Class Loss: 0.2668
Epoch 34/50, Loss: 1.8735, Domain Loss: 1.6242, Class Loss: 0.2493
Epoch 35/50, Loss: 1.5700, Domain Loss: 1.4141, Class Loss: 0.1559
Epoch 36/50, Loss: 1.5127, Domain Loss: 1.3737, Class Loss: 0.1390
Epoch 37/50, Loss: 1.9356, Domain Loss: 1.7363, Class Loss: 0.1993
Epoch 38/50, Loss: 1.6001, Domain Loss: 1.4971, Class Loss: 0.1029
Epoch 39/50, Loss: 1.5049, Domain Loss: 1.4270, Class Loss: 0.0779
Epoch 40/50, Loss: 1.4675, Domain Loss: 1.4135, Class Loss: 0.0540
Epoch 41/50, Loss: 1.4960, Domain Loss: 1.4382, Class Loss: 0.0577
Epoch 42/50, Loss: 1.5573, Domain Loss: 1.4787, Class Loss: 0.0787
Epoch 43/50, Loss: 1.3783, Domain Loss: 1.3232, Class Loss: 0.0551
Epoch 44/50, Loss: 1.4974, Domain Loss: 1.4217, Class Loss: 0.0757
Epoch 45/50, Loss: 1.8790, Domain Loss: 1.7474, Class Loss: 0.1315
Epoch 46/50, Loss: 1.4629, Domain Loss: 1.3895, Class Loss: 0.0734
Epoch 47/50, Loss: 1.5163, Domain Loss: 1.4304, Class Loss: 0.0859
Epoch 48/50, Loss: 1.4380, Domain Loss: 1.3777, Class Loss: 0.0603
Epoch 49/50, Loss: 1.4533, Domain Loss: 1.3940, Class Loss: 0.0593
Epoch 50/50, Loss: 1.4405, Domain Loss: 1.3998, Class Loss: 0.0407
49.64


Epoch 1/50, Loss: 2.2760, Domain Loss: 1.3158, Class Loss: 0.9602
Epoch 2/50, Loss: 1.4893, Domain Loss: 1.1754, Class Loss: 0.3139
Epoch 3/50, Loss: 1.5586, Domain Loss: 1.3836, Class Loss: 0.1750
Epoch 4/50, Loss: 2.4107, Domain Loss: 2.2703, Class Loss: 0.1404
Epoch 5/50, Loss: 9.9837, Domain Loss: 9.6799, Class Loss: 0.3038
Epoch 6/50, Loss: 6.3262, Domain Loss: 6.0588, Class Loss: 0.2675
Epoch 7/50, Loss: 5.2657, Domain Loss: 5.0809, Class Loss: 0.1848
Epoch 8/50, Loss: 6.0297, Domain Loss: 5.8517, Class Loss: 0.1780
Epoch 9/50, Loss: 9.9884, Domain Loss: 9.6579, Class Loss: 0.3305
Epoch 10/50, Loss: 13.7807, Domain Loss: 13.4292, Class Loss: 0.3515
Epoch 11/50, Loss: 8.1164, Domain Loss: 7.5974, Class Loss: 0.5190
Epoch 12/50, Loss: 5.6621, Domain Loss: 5.2491, Class Loss: 0.4130
Epoch 13/50, Loss: 3.1557, Domain Loss: 2.6202, Class Loss: 0.5355
Epoch 14/50, Loss: 3.4043, Domain Loss: 2.9835, Class Loss: 0.4208
Epoch 15/50, Loss: 3.1221, Domain Loss: 2.6417, Class Loss: 0.4804
Epoch 16/50, Loss: 2.5158, Domain Loss: 2.2902, Class Loss: 0.2255
Epoch 17/50, Loss: 2.0893, Domain Loss: 1.7584, Class Loss: 0.3309
Epoch 18/50, Loss: 2.3984, Domain Loss: 1.9923, Class Loss: 0.4061
Epoch 19/50, Loss: 2.2391, Domain Loss: 1.7928, Class Loss: 0.4463
Epoch 20/50, Loss: 1.5994, Domain Loss: 1.3498, Class Loss: 0.2496
Epoch 21/50, Loss: 1.6715, Domain Loss: 1.4596, Class Loss: 0.2119
Epoch 22/50, Loss: 1.6525, Domain Loss: 1.5031, Class Loss: 0.1494
Epoch 23/50, Loss: 1.8679, Domain Loss: 1.6978, Class Loss: 0.1701
Epoch 24/50, Loss: 1.9538, Domain Loss: 1.7840, Class Loss: 0.1698
Epoch 25/50, Loss: 2.3928, Domain Loss: 2.2204, Class Loss: 0.1724
Epoch 26/50, Loss: 3.3890, Domain Loss: 3.1865, Class Loss: 0.2026
Epoch 27/50, Loss: 4.5165, Domain Loss: 4.0159, Class Loss: 0.5006
Epoch 28/50, Loss: 4.2225, Domain Loss: 3.7487, Class Loss: 0.4738
Epoch 29/50, Loss: 2.8735, Domain Loss: 2.5326, Class Loss: 0.3409
Epoch 30/50, Loss: 2.1188, Domain Loss: 1.7344, Class Loss: 0.3844
Epoch 31/50, Loss: 1.8674, Domain Loss: 1.6534, Class Loss: 0.2140
Epoch 32/50, Loss: 1.6228, Domain Loss: 1.4606, Class Loss: 0.1622
Epoch 33/50, Loss: 1.6005, Domain Loss: 1.4450, Class Loss: 0.1555
Epoch 34/50, Loss: 1.5395, Domain Loss: 1.4405, Class Loss: 0.0990
Epoch 35/50, Loss: 1.7218, Domain Loss: 1.5413, Class Loss: 0.1804
Epoch 36/50, Loss: 1.5305, Domain Loss: 1.4068, Class Loss: 0.1237
Epoch 37/50, Loss: 1.4702, Domain Loss: 1.3830, Class Loss: 0.0872
Epoch 38/50, Loss: 1.4951, Domain Loss: 1.4077, Class Loss: 0.0874
Epoch 39/50, Loss: 1.4428, Domain Loss: 1.3527, Class Loss: 0.0900
Epoch 40/50, Loss: 1.3840, Domain Loss: 1.3267, Class Loss: 0.0573
Epoch 41/50, Loss: 1.3768, Domain Loss: 1.3169, Class Loss: 0.0599
Epoch 42/50, Loss: 1.3625, Domain Loss: 1.3022, Class Loss: 0.0603
Epoch 43/50, Loss: 1.3772, Domain Loss: 1.3194, Class Loss: 0.0578
Epoch 44/50, Loss: 1.3822, Domain Loss: 1.3283, Class Loss: 0.0539
Epoch 45/50, Loss: 1.5014, Domain Loss: 1.4322, Class Loss: 0.0691
Epoch 46/50, Loss: 1.6248, Domain Loss: 1.5013, Class Loss: 0.1235
Epoch 47/50, Loss: 1.5241, Domain Loss: 1.4279, Class Loss: 0.0962
Epoch 48/50, Loss: 1.4582, Domain Loss: 1.3595, Class Loss: 0.0987
Epoch 49/50, Loss: 1.5620, Domain Loss: 1.4333, Class Loss: 0.1286
Epoch 50/50, Loss: 1.5162, Domain Loss: 1.4499, Class Loss: 0.0663
59.47


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2962, Domain Loss: 1.3277, Class Loss: 0.9685
Epoch 2/50, Loss: 1.5394, Domain Loss: 1.2737, Class Loss: 0.2657
Epoch 3/50, Loss: 2.4755, Domain Loss: 2.2582, Class Loss: 0.2173
Epoch 4/50, Loss: 3.7437, Domain Loss: 3.5833, Class Loss: 0.1604
Epoch 5/50, Loss: 6.2104, Domain Loss: 5.9781, Class Loss: 0.2323
Epoch 6/50, Loss: 3.8307, Domain Loss: 3.5565, Class Loss: 0.2742
Epoch 7/50, Loss: 4.2934, Domain Loss: 3.9968, Class Loss: 0.2966
Epoch 8/50, Loss: 3.9219, Domain Loss: 3.6097, Class Loss: 0.3121
Epoch 9/50, Loss: 4.6265, Domain Loss: 4.2917, Class Loss: 0.3348
Epoch 10/50, Loss: 3.0692, Domain Loss: 2.6471, Class Loss: 0.4222
Epoch 11/50, Loss: 3.0218, Domain Loss: 2.5131, Class Loss: 0.5087
Epoch 12/50, Loss: 2.0524, Domain Loss: 1.7988, Class Loss: 0.2535
Epoch 13/50, Loss: 2.2847, Domain Loss: 2.1126, Class Loss: 0.1721
Epoch 14/50, Loss: 3.3949, Domain Loss: 3.2377, Class Loss: 0.1572
Epoch 15/50, Loss: 3.7218, Domain Loss: 3.4287, Class Loss: 0.2930
Epoch 16/50, Loss: 3.0221, Domain Loss: 2.6635, Class Loss: 0.3586
Epoch 17/50, Loss: 4.2714, Domain Loss: 3.7532, Class Loss: 0.5181
Epoch 18/50, Loss: 3.8505, Domain Loss: 3.3629, Class Loss: 0.4876
Epoch 19/50, Loss: 3.2437, Domain Loss: 2.7225, Class Loss: 0.5213
Epoch 20/50, Loss: 2.1527, Domain Loss: 1.8580, Class Loss: 0.2946
Epoch 21/50, Loss: 2.2333, Domain Loss: 1.8506, Class Loss: 0.3828
Epoch 22/50, Loss: 1.7863, Domain Loss: 1.5296, Class Loss: 0.2567
Epoch 23/50, Loss: 1.9307, Domain Loss: 1.7111, Class Loss: 0.2195
Epoch 24/50, Loss: 1.9257, Domain Loss: 1.6591, Class Loss: 0.2666
Epoch 25/50, Loss: 1.7680, Domain Loss: 1.5611, Class Loss: 0.2070
Epoch 26/50, Loss: 2.6945, Domain Loss: 2.3037, Class Loss: 0.3909
Epoch 27/50, Loss: 4.2909, Domain Loss: 3.6894, Class Loss: 0.6015
Epoch 28/50, Loss: 4.6494, Domain Loss: 4.1752, Class Loss: 0.4742
Epoch 29/50, Loss: 3.2606, Domain Loss: 2.9511, Class Loss: 0.3095
Epoch 30/50, Loss: 2.8057, Domain Loss: 2.4991, Class Loss: 0.3066
Epoch 31/50, Loss: 2.5957, Domain Loss: 2.3199, Class Loss: 0.2758
Epoch 32/50, Loss: 3.0865, Domain Loss: 2.7752, Class Loss: 0.3114
Epoch 33/50, Loss: 2.2839, Domain Loss: 2.0374, Class Loss: 0.2465
Epoch 34/50, Loss: 2.3367, Domain Loss: 1.9724, Class Loss: 0.3643
Epoch 35/50, Loss: 2.2678, Domain Loss: 2.0439, Class Loss: 0.2239
Epoch 36/50, Loss: 1.9460, Domain Loss: 1.7459, Class Loss: 0.2001
Epoch 37/50, Loss: 1.8745, Domain Loss: 1.6118, Class Loss: 0.2627
Epoch 38/50, Loss: 1.9531, Domain Loss: 1.6913, Class Loss: 0.2618
Epoch 39/50, Loss: 3.2932, Domain Loss: 2.7513, Class Loss: 0.5419
Epoch 40/50, Loss: 2.5143, Domain Loss: 2.0520, Class Loss: 0.4623
Epoch 41/50, Loss: 2.3073, Domain Loss: 1.8911, Class Loss: 0.4162
Epoch 42/50, Loss: 1.8501, Domain Loss: 1.6572, Class Loss: 0.1928
Epoch 43/50, Loss: 1.5368, Domain Loss: 1.3076, Class Loss: 0.2292
Epoch 44/50, Loss: 2.0978, Domain Loss: 1.8141, Class Loss: 0.2837
Epoch 45/50, Loss: 1.9970, Domain Loss: 1.6201, Class Loss: 0.3769
Epoch 46/50, Loss: 1.9165, Domain Loss: 1.6648, Class Loss: 0.2517
Epoch 47/50, Loss: 1.4537, Domain Loss: 1.2593, Class Loss: 0.1944
Epoch 48/50, Loss: 1.4365, Domain Loss: 1.2609, Class Loss: 0.1756
Epoch 49/50, Loss: 1.7108, Domain Loss: 1.5519, Class Loss: 0.1590
Epoch 50/50, Loss: 1.7650, Domain Loss: 1.5680, Class Loss: 0.1970
74.64


Epoch 1/50, Loss: 2.3274, Domain Loss: 1.3530, Class Loss: 0.9744
Epoch 2/50, Loss: 1.6307, Domain Loss: 1.2463, Class Loss: 0.3844
Epoch 3/50, Loss: 1.4574, Domain Loss: 1.2961, Class Loss: 0.1613
Epoch 4/50, Loss: 3.2166, Domain Loss: 3.0547, Class Loss: 0.1619
Epoch 5/50, Loss: 4.8414, Domain Loss: 4.6717, Class Loss: 0.1697
Epoch 6/50, Loss: 3.3372, Domain Loss: 3.2232, Class Loss: 0.1141
Epoch 7/50, Loss: 2.3224, Domain Loss: 2.1957, Class Loss: 0.1267
Epoch 8/50, Loss: 4.0743, Domain Loss: 3.8421, Class Loss: 0.2322
Epoch 9/50, Loss: 5.6951, Domain Loss: 5.5104, Class Loss: 0.1847
Epoch 10/50, Loss: 5.6490, Domain Loss: 5.3606, Class Loss: 0.2884
Epoch 11/50, Loss: 2.7835, Domain Loss: 2.5481, Class Loss: 0.2354
Epoch 12/50, Loss: 2.3214, Domain Loss: 2.1876, Class Loss: 0.1338
Epoch 13/50, Loss: 2.6688, Domain Loss: 2.3916, Class Loss: 0.2772
Epoch 14/50, Loss: 2.4337, Domain Loss: 2.3183, Class Loss: 0.1154
Epoch 15/50, Loss: 2.3866, Domain Loss: 2.2048, Class Loss: 0.1818
Epoch 16/50, Loss: 2.9708, Domain Loss: 2.7236, Class Loss: 0.2472
Epoch 17/50, Loss: 2.9540, Domain Loss: 2.7324, Class Loss: 0.2217
Epoch 18/50, Loss: 2.2604, Domain Loss: 1.9997, Class Loss: 0.2607
Epoch 19/50, Loss: 3.6851, Domain Loss: 3.4978, Class Loss: 0.1872
Epoch 20/50, Loss: 4.5781, Domain Loss: 4.3813, Class Loss: 0.1968
Epoch 21/50, Loss: 6.5820, Domain Loss: 6.2434, Class Loss: 0.3386
Epoch 22/50, Loss: 5.4571, Domain Loss: 5.0098, Class Loss: 0.4473
Epoch 23/50, Loss: 7.6438, Domain Loss: 7.1816, Class Loss: 0.4622
Epoch 24/50, Loss: 5.7015, Domain Loss: 5.1920, Class Loss: 0.5095
Epoch 25/50, Loss: 4.5390, Domain Loss: 3.9917, Class Loss: 0.5473
Epoch 26/50, Loss: 4.0751, Domain Loss: 3.6351, Class Loss: 0.4400
Epoch 27/50, Loss: 6.7198, Domain Loss: 4.0742, Class Loss: 2.6456
Epoch 28/50, Loss: 4.3453, Domain Loss: 3.7222, Class Loss: 0.6230
Epoch 29/50, Loss: 2.9282, Domain Loss: 2.4306, Class Loss: 0.4976
Epoch 30/50, Loss: 2.8492, Domain Loss: 2.4141, Class Loss: 0.4350
Epoch 31/50, Loss: 3.2114, Domain Loss: 2.8302, Class Loss: 0.3812
Epoch 32/50, Loss: 2.3635, Domain Loss: 1.9506, Class Loss: 0.4129
Epoch 33/50, Loss: 2.3229, Domain Loss: 1.7926, Class Loss: 0.5303
Epoch 34/50, Loss: 2.8715, Domain Loss: 2.1913, Class Loss: 0.6802
Epoch 35/50, Loss: 2.3827, Domain Loss: 1.8784, Class Loss: 0.5043
Epoch 36/50, Loss: 2.1154, Domain Loss: 1.6317, Class Loss: 0.4837
Epoch 37/50, Loss: 2.2768, Domain Loss: 1.8816, Class Loss: 0.3951
Epoch 38/50, Loss: 2.3418, Domain Loss: 1.8449, Class Loss: 0.4969
Epoch 39/50, Loss: 1.9342, Domain Loss: 1.5571, Class Loss: 0.3771
Epoch 40/50, Loss: 2.3159, Domain Loss: 1.8075, Class Loss: 0.5084
Epoch 41/50, Loss: 2.0047, Domain Loss: 1.6727, Class Loss: 0.3320
Epoch 42/50, Loss: 2.0698, Domain Loss: 1.8022, Class Loss: 0.2676
Epoch 43/50, Loss: 1.6032, Domain Loss: 1.3424, Class Loss: 0.2608
Epoch 44/50, Loss: 1.5854, Domain Loss: 1.3093, Class Loss: 0.2761
Epoch 45/50, Loss: 1.5276, Domain Loss: 1.2734, Class Loss: 0.2542
Epoch 46/50, Loss: 1.5275, Domain Loss: 1.3501, Class Loss: 0.1774
Epoch 47/50, Loss: 1.6355, Domain Loss: 1.3869, Class Loss: 0.2486
Epoch 48/50, Loss: 1.6883, Domain Loss: 1.4635, Class Loss: 0.2248
Epoch 49/50, Loss: 1.6689, Domain Loss: 1.4898, Class Loss: 0.1791
Epoch 50/50, Loss: 1.6317, Domain Loss: 1.4755, Class Loss: 0.1562
61.93


Epoch 1/50, Loss: 2.2876, Domain Loss: 1.3252, Class Loss: 0.9624
Epoch 2/50, Loss: 1.4530, Domain Loss: 1.1510, Class Loss: 0.3019
Epoch 3/50, Loss: 1.8106, Domain Loss: 1.6402, Class Loss: 0.1705
Epoch 4/50, Loss: 2.4301, Domain Loss: 2.2533, Class Loss: 0.1768
Epoch 5/50, Loss: 9.8047, Domain Loss: 9.3582, Class Loss: 0.4464
Epoch 6/50, Loss: 8.3488, Domain Loss: 7.8384, Class Loss: 0.5104
Epoch 7/50, Loss: 3.6907, Domain Loss: 3.3662, Class Loss: 0.3244
Epoch 8/50, Loss: 2.3784, Domain Loss: 2.1833, Class Loss: 0.1952
Epoch 9/50, Loss: 6.4559, Domain Loss: 6.2610, Class Loss: 0.1949
Epoch 10/50, Loss: 11.6719, Domain Loss: 11.2545, Class Loss: 0.4173
Epoch 11/50, Loss: 8.1784, Domain Loss: 7.7171, Class Loss: 0.4613
Epoch 12/50, Loss: 3.3573, Domain Loss: 3.0708, Class Loss: 0.2865
Epoch 13/50, Loss: 2.1871, Domain Loss: 1.9600, Class Loss: 0.2271
Epoch 14/50, Loss: 1.6455, Domain Loss: 1.5216, Class Loss: 0.1239
Epoch 15/50, Loss: 1.5495, Domain Loss: 1.4499, Class Loss: 0.0996
Epoch 16/50, Loss: 1.4765, Domain Loss: 1.4270, Class Loss: 0.0496
Epoch 17/50, Loss: 1.6839, Domain Loss: 1.5804, Class Loss: 0.1034
Epoch 18/50, Loss: 1.6531, Domain Loss: 1.5424, Class Loss: 0.1107
Epoch 19/50, Loss: 1.7475, Domain Loss: 1.6931, Class Loss: 0.0544
Epoch 20/50, Loss: 2.4102, Domain Loss: 2.3425, Class Loss: 0.0677
Epoch 21/50, Loss: 3.6065, Domain Loss: 3.4937, Class Loss: 0.1128
Epoch 22/50, Loss: 3.2529, Domain Loss: 3.1415, Class Loss: 0.1114
Epoch 23/50, Loss: 2.4191, Domain Loss: 2.1343, Class Loss: 0.2847
Epoch 24/50, Loss: 2.4187, Domain Loss: 2.2981, Class Loss: 0.1207
Epoch 25/50, Loss: 2.5086, Domain Loss: 2.3836, Class Loss: 0.1251
Epoch 26/50, Loss: 1.8573, Domain Loss: 1.7083, Class Loss: 0.1490
Epoch 27/50, Loss: 1.8878, Domain Loss: 1.7033, Class Loss: 0.1846
Epoch 28/50, Loss: 1.5837, Domain Loss: 1.4806, Class Loss: 0.1031
Epoch 29/50, Loss: 1.4188, Domain Loss: 1.3662, Class Loss: 0.0526
Epoch 30/50, Loss: 1.5271, Domain Loss: 1.4560, Class Loss: 0.0711
Epoch 31/50, Loss: 1.5275, Domain Loss: 1.4381, Class Loss: 0.0894
Epoch 32/50, Loss: 1.5975, Domain Loss: 1.5268, Class Loss: 0.0707
Epoch 33/50, Loss: 1.4666, Domain Loss: 1.4127, Class Loss: 0.0538
Epoch 34/50, Loss: 1.5246, Domain Loss: 1.4759, Class Loss: 0.0486
Epoch 35/50, Loss: 1.6072, Domain Loss: 1.5253, Class Loss: 0.0818
Epoch 36/50, Loss: 1.5500, Domain Loss: 1.4822, Class Loss: 0.0678
Epoch 37/50, Loss: 1.4274, Domain Loss: 1.3916, Class Loss: 0.0358
Epoch 38/50, Loss: 1.5996, Domain Loss: 1.5196, Class Loss: 0.0800
Epoch 39/50, Loss: 1.6463, Domain Loss: 1.5814, Class Loss: 0.0649
Epoch 40/50, Loss: 1.6902, Domain Loss: 1.6323, Class Loss: 0.0579
Epoch 41/50, Loss: 1.7764, Domain Loss: 1.7229, Class Loss: 0.0535
Epoch 42/50, Loss: 1.7388, Domain Loss: 1.6190, Class Loss: 0.1198
Epoch 43/50, Loss: 2.1620, Domain Loss: 1.9293, Class Loss: 0.2327
Epoch 44/50, Loss: 2.6041, Domain Loss: 2.4452, Class Loss: 0.1589
Epoch 45/50, Loss: 3.2341, Domain Loss: 3.0629, Class Loss: 0.1712
Epoch 46/50, Loss: 3.3646, Domain Loss: 3.0140, Class Loss: 0.3506
Epoch 47/50, Loss: 2.9202, Domain Loss: 2.7435, Class Loss: 0.1767
Epoch 48/50, Loss: 2.5558, Domain Loss: 2.2886, Class Loss: 0.2672
Epoch 49/50, Loss: 2.6545, Domain Loss: 2.3472, Class Loss: 0.3072
Epoch 50/50, Loss: 3.2385, Domain Loss: 2.7880, Class Loss: 0.4505
65.71


Epoch 1/50, Loss: 2.2375, Domain Loss: 1.3145, Class Loss: 0.9229
Epoch 2/50, Loss: 1.6329, Domain Loss: 1.2843, Class Loss: 0.3486
Epoch 3/50, Loss: 1.6899, Domain Loss: 1.4212, Class Loss: 0.2687
Epoch 4/50, Loss: 1.6249, Domain Loss: 1.4387, Class Loss: 0.1862
Epoch 5/50, Loss: 5.2687, Domain Loss: 5.0165, Class Loss: 0.2522
Epoch 6/50, Loss: 24.1884, Domain Loss: 23.6282, Class Loss: 0.5602
Epoch 7/50, Loss: 18.9027, Domain Loss: 18.2992, Class Loss: 0.6035
Epoch 8/50, Loss: 9.8996, Domain Loss: 9.6179, Class Loss: 0.2818
Epoch 9/50, Loss: 4.7258, Domain Loss: 3.8804, Class Loss: 0.8453
Epoch 10/50, Loss: 7.2891, Domain Loss: 5.2009, Class Loss: 2.0882
Epoch 11/50, Loss: 3.6891, Domain Loss: 3.1826, Class Loss: 0.5065
Epoch 12/50, Loss: 2.3587, Domain Loss: 2.0832, Class Loss: 0.2755
Epoch 13/50, Loss: 1.7540, Domain Loss: 1.5711, Class Loss: 0.1829
Epoch 14/50, Loss: 2.1067, Domain Loss: 1.9099, Class Loss: 0.1969
Epoch 15/50, Loss: 3.3462, Domain Loss: 2.9401, Class Loss: 0.4061
Epoch 16/50, Loss: 3.1250, Domain Loss: 2.8098, Class Loss: 0.3152
Epoch 17/50, Loss: 7.3044, Domain Loss: 5.0524, Class Loss: 2.2520
Epoch 18/50, Loss: 8.0027, Domain Loss: 6.5475, Class Loss: 1.4553
Epoch 19/50, Loss: 6.0512, Domain Loss: 4.6660, Class Loss: 1.3851
Epoch 20/50, Loss: 5.8343, Domain Loss: 4.6780, Class Loss: 1.1562
Epoch 21/50, Loss: 3.8916, Domain Loss: 3.1089, Class Loss: 0.7827
Epoch 22/50, Loss: 2.7606, Domain Loss: 2.3169, Class Loss: 0.4437
Epoch 23/50, Loss: 3.1517, Domain Loss: 2.5988, Class Loss: 0.5530
Epoch 24/50, Loss: 2.2540, Domain Loss: 1.7954, Class Loss: 0.4586
Epoch 25/50, Loss: 1.9457, Domain Loss: 1.6047, Class Loss: 0.3410
Epoch 26/50, Loss: 1.9423, Domain Loss: 1.6526, Class Loss: 0.2897
Epoch 27/50, Loss: 1.7932, Domain Loss: 1.5682, Class Loss: 0.2249
Epoch 28/50, Loss: 1.7962, Domain Loss: 1.5657, Class Loss: 0.2306
Epoch 29/50, Loss: 1.7870, Domain Loss: 1.5387, Class Loss: 0.2484
Epoch 30/50, Loss: 1.7277, Domain Loss: 1.5126, Class Loss: 0.2152
Epoch 31/50, Loss: 1.6208, Domain Loss: 1.4668, Class Loss: 0.1541
Epoch 32/50, Loss: 1.6490, Domain Loss: 1.5062, Class Loss: 0.1428
Epoch 33/50, Loss: 1.6729, Domain Loss: 1.5401, Class Loss: 0.1328
Epoch 34/50, Loss: 1.6794, Domain Loss: 1.5509, Class Loss: 0.1286
Epoch 35/50, Loss: 1.6937, Domain Loss: 1.5502, Class Loss: 0.1435
Epoch 36/50, Loss: 1.8574, Domain Loss: 1.6805, Class Loss: 0.1769
Epoch 37/50, Loss: 1.8032, Domain Loss: 1.6426, Class Loss: 0.1605
Epoch 38/50, Loss: 1.9565, Domain Loss: 1.8130, Class Loss: 0.1435
Epoch 39/50, Loss: 2.3762, Domain Loss: 2.2202, Class Loss: 0.1560
Epoch 40/50, Loss: 2.2298, Domain Loss: 2.0210, Class Loss: 0.2088
Epoch 41/50, Loss: 1.9286, Domain Loss: 1.7753, Class Loss: 0.1533
Epoch 42/50, Loss: 1.9792, Domain Loss: 1.8327, Class Loss: 0.1465
Epoch 43/50, Loss: 2.2209, Domain Loss: 2.0176, Class Loss: 0.2033
Epoch 44/50, Loss: 2.3622, Domain Loss: 2.1212, Class Loss: 0.2409
Epoch 45/50, Loss: 2.9980, Domain Loss: 2.7997, Class Loss: 0.1983
Epoch 46/50, Loss: 1.8417, Domain Loss: 1.7294, Class Loss: 0.1123
Epoch 47/50, Loss: 1.6078, Domain Loss: 1.4800, Class Loss: 0.1279
Epoch 48/50, Loss: 1.4552, Domain Loss: 1.3504, Class Loss: 0.1049
Epoch 49/50, Loss: 1.5055, Domain Loss: 1.4362, Class Loss: 0.0693
Epoch 50/50, Loss: 1.7082, Domain Loss: 1.5965, Class Loss: 0.1117
58.75


Epoch 1/50, Loss: 2.3382, Domain Loss: 1.3295, Class Loss: 1.0087
Epoch 2/50, Loss: 1.5218, Domain Loss: 1.1625, Class Loss: 0.3593
Epoch 3/50, Loss: 1.3904, Domain Loss: 1.2298, Class Loss: 0.1607
Epoch 4/50, Loss: 1.9654, Domain Loss: 1.8001, Class Loss: 0.1652
Epoch 5/50, Loss: 5.1647, Domain Loss: 4.9581, Class Loss: 0.2066
Epoch 6/50, Loss: 25.7399, Domain Loss: 25.4062, Class Loss: 0.3338
Epoch 7/50, Loss: 15.9565, Domain Loss: 15.6902, Class Loss: 0.2663
Epoch 8/50, Loss: 13.8013, Domain Loss: 13.5264, Class Loss: 0.2750
Epoch 9/50, Loss: 12.5256, Domain Loss: 12.1828, Class Loss: 0.3428
Epoch 10/50, Loss: 4.5315, Domain Loss: 4.2967, Class Loss: 0.2349
Epoch 11/50, Loss: 2.7335, Domain Loss: 2.3928, Class Loss: 0.3408
Epoch 12/50, Loss: 2.9828, Domain Loss: 2.5521, Class Loss: 0.4308
Epoch 13/50, Loss: 4.0093, Domain Loss: 3.7097, Class Loss: 0.2996
Epoch 14/50, Loss: 3.3158, Domain Loss: 3.1304, Class Loss: 0.1854
Epoch 15/50, Loss: 3.1402, Domain Loss: 2.8499, Class Loss: 0.2903
Epoch 16/50, Loss: 3.2817, Domain Loss: 2.9376, Class Loss: 0.3440
Epoch 17/50, Loss: 1.9023, Domain Loss: 1.7518, Class Loss: 0.1505
Epoch 18/50, Loss: 2.0381, Domain Loss: 1.8840, Class Loss: 0.1540
Epoch 19/50, Loss: 5.0204, Domain Loss: 4.4063, Class Loss: 0.6141
Epoch 20/50, Loss: 5.3645, Domain Loss: 4.8399, Class Loss: 0.5246
Epoch 21/50, Loss: 3.0426, Domain Loss: 2.6598, Class Loss: 0.3828
Epoch 22/50, Loss: 2.8337, Domain Loss: 2.5079, Class Loss: 0.3257
Epoch 23/50, Loss: 1.9659, Domain Loss: 1.7318, Class Loss: 0.2341
Epoch 24/50, Loss: 2.6821, Domain Loss: 2.3241, Class Loss: 0.3580
Epoch 25/50, Loss: 2.1210, Domain Loss: 1.8062, Class Loss: 0.3148
Epoch 26/50, Loss: 1.6793, Domain Loss: 1.5579, Class Loss: 0.1214
Epoch 27/50, Loss: 1.5883, Domain Loss: 1.3876, Class Loss: 0.2007
Epoch 28/50, Loss: 1.7214, Domain Loss: 1.4852, Class Loss: 0.2363
Epoch 29/50, Loss: 1.5729, Domain Loss: 1.4400, Class Loss: 0.1329
Epoch 30/50, Loss: 1.8464, Domain Loss: 1.7232, Class Loss: 0.1231
Epoch 31/50, Loss: 1.6018, Domain Loss: 1.5273, Class Loss: 0.0745
Epoch 32/50, Loss: 1.9526, Domain Loss: 1.7122, Class Loss: 0.2404
Epoch 33/50, Loss: 1.5362, Domain Loss: 1.4349, Class Loss: 0.1013
Epoch 34/50, Loss: 1.7064, Domain Loss: 1.5708, Class Loss: 0.1356
Epoch 35/50, Loss: 1.5166, Domain Loss: 1.4448, Class Loss: 0.0718
Epoch 36/50, Loss: 1.5026, Domain Loss: 1.4025, Class Loss: 0.1000
Epoch 37/50, Loss: 1.3497, Domain Loss: 1.2930, Class Loss: 0.0567
Epoch 38/50, Loss: 1.3778, Domain Loss: 1.3423, Class Loss: 0.0355
Epoch 39/50, Loss: 1.5180, Domain Loss: 1.4529, Class Loss: 0.0651
Epoch 40/50, Loss: 1.5546, Domain Loss: 1.5048, Class Loss: 0.0499
Epoch 41/50, Loss: 1.5819, Domain Loss: 1.5420, Class Loss: 0.0399
Epoch 42/50, Loss: 1.5312, Domain Loss: 1.4838, Class Loss: 0.0475
Epoch 43/50, Loss: 1.5098, Domain Loss: 1.4744, Class Loss: 0.0354
Epoch 44/50, Loss: 1.7052, Domain Loss: 1.6332, Class Loss: 0.0720
Epoch 45/50, Loss: 1.6440, Domain Loss: 1.5906, Class Loss: 0.0533
Epoch 46/50, Loss: 1.6205, Domain Loss: 1.5668, Class Loss: 0.0536
Epoch 47/50, Loss: 1.6828, Domain Loss: 1.6337, Class Loss: 0.0491
Epoch 48/50, Loss: 1.6271, Domain Loss: 1.5902, Class Loss: 0.0369
Epoch 49/50, Loss: 1.9035, Domain Loss: 1.8404, Class Loss: 0.0631
Epoch 50/50, Loss: 2.0070, Domain Loss: 1.9399, Class Loss: 0.0671
59.05


Epoch 1/50, Loss: 2.3210, Domain Loss: 1.3221, Class Loss: 0.9989
Epoch 2/50, Loss: 1.5434, Domain Loss: 1.2472, Class Loss: 0.2961
Epoch 3/50, Loss: 1.4753, Domain Loss: 1.3065, Class Loss: 0.1688
Epoch 4/50, Loss: 2.9276, Domain Loss: 2.7986, Class Loss: 0.1289
Epoch 5/50, Loss: 5.7102, Domain Loss: 5.4495, Class Loss: 0.2608
Epoch 6/50, Loss: 5.4267, Domain Loss: 5.2333, Class Loss: 0.1934
Epoch 7/50, Loss: 6.9702, Domain Loss: 5.5103, Class Loss: 1.4598
Epoch 8/50, Loss: 10.3158, Domain Loss: 9.5270, Class Loss: 0.7888
Epoch 9/50, Loss: 10.9424, Domain Loss: 10.2689, Class Loss: 0.6734
Epoch 10/50, Loss: 25.8672, Domain Loss: 24.9181, Class Loss: 0.9490
Epoch 11/50, Loss: 18.2935, Domain Loss: 17.1861, Class Loss: 1.1074
Epoch 12/50, Loss: 14.7887, Domain Loss: 13.7279, Class Loss: 1.0609
Epoch 13/50, Loss: 11.3521, Domain Loss: 10.1103, Class Loss: 1.2418
Epoch 14/50, Loss: 5.5644, Domain Loss: 4.6276, Class Loss: 0.9368
Epoch 15/50, Loss: 4.1944, Domain Loss: 3.2997, Class Loss: 0.8947
Epoch 16/50, Loss: 2.4309, Domain Loss: 1.6846, Class Loss: 0.7463
Epoch 17/50, Loss: 2.3705, Domain Loss: 1.7819, Class Loss: 0.5886
Epoch 18/50, Loss: 2.9871, Domain Loss: 2.3483, Class Loss: 0.6388
Epoch 19/50, Loss: 2.2554, Domain Loss: 1.7940, Class Loss: 0.4614
Epoch 20/50, Loss: 2.0886, Domain Loss: 1.6991, Class Loss: 0.3894
Epoch 21/50, Loss: 1.8936, Domain Loss: 1.5650, Class Loss: 0.3287
Epoch 22/50, Loss: 1.8720, Domain Loss: 1.6030, Class Loss: 0.2691
Epoch 23/50, Loss: 1.7795, Domain Loss: 1.5418, Class Loss: 0.2376
Epoch 24/50, Loss: 1.7303, Domain Loss: 1.5261, Class Loss: 0.2043
Epoch 25/50, Loss: 1.7312, Domain Loss: 1.5526, Class Loss: 0.1786
Epoch 26/50, Loss: 1.8039, Domain Loss: 1.6430, Class Loss: 0.1608
Epoch 27/50, Loss: 1.8964, Domain Loss: 1.7483, Class Loss: 0.1481
Epoch 28/50, Loss: 2.0258, Domain Loss: 1.8686, Class Loss: 0.1572
Epoch 29/50, Loss: 2.0362, Domain Loss: 1.9052, Class Loss: 0.1310
Epoch 30/50, Loss: 1.9476, Domain Loss: 1.8119, Class Loss: 0.1357
Epoch 31/50, Loss: 1.7028, Domain Loss: 1.5534, Class Loss: 0.1494
Epoch 32/50, Loss: 1.4494, Domain Loss: 1.3581, Class Loss: 0.0913
Epoch 33/50, Loss: 1.4729, Domain Loss: 1.3869, Class Loss: 0.0860
Epoch 34/50, Loss: 1.6000, Domain Loss: 1.5137, Class Loss: 0.0863
Epoch 35/50, Loss: 1.9774, Domain Loss: 1.8716, Class Loss: 0.1058
Epoch 36/50, Loss: 2.2847, Domain Loss: 2.1596, Class Loss: 0.1251
Epoch 37/50, Loss: 2.0734, Domain Loss: 1.8254, Class Loss: 0.2480
Epoch 38/50, Loss: 1.8889, Domain Loss: 1.6728, Class Loss: 0.2161
Epoch 39/50, Loss: 2.2204, Domain Loss: 1.9467, Class Loss: 0.2736
Epoch 40/50, Loss: 2.0055, Domain Loss: 1.6496, Class Loss: 0.3559
Epoch 41/50, Loss: 1.7046, Domain Loss: 1.4802, Class Loss: 0.2245
Epoch 42/50, Loss: 1.6100, Domain Loss: 1.4364, Class Loss: 0.1736
Epoch 43/50, Loss: 1.5887, Domain Loss: 1.4640, Class Loss: 0.1247
Epoch 44/50, Loss: 1.4988, Domain Loss: 1.4285, Class Loss: 0.0703
Epoch 45/50, Loss: 1.4744, Domain Loss: 1.4209, Class Loss: 0.0535
Epoch 46/50, Loss: 1.4995, Domain Loss: 1.4566, Class Loss: 0.0429
Epoch 47/50, Loss: 1.5319, Domain Loss: 1.4931, Class Loss: 0.0387
Epoch 48/50, Loss: 1.4938, Domain Loss: 1.4489, Class Loss: 0.0449
Epoch 49/50, Loss: 1.4986, Domain Loss: 1.4543, Class Loss: 0.0444
Epoch 50/50, Loss: 1.5340, Domain Loss: 1.4932, Class Loss: 0.0408
50.36


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.3093, Domain Loss: 1.3395, Class Loss: 0.9698
Epoch 2/50, Loss: 1.6312, Domain Loss: 1.2102, Class Loss: 0.4210
Epoch 3/50, Loss: 2.3609, Domain Loss: 2.1094, Class Loss: 0.2515
Epoch 4/50, Loss: 6.3950, Domain Loss: 6.1570, Class Loss: 0.2380
Epoch 5/50, Loss: 12.0731, Domain Loss: 11.8650, Class Loss: 0.2081
Epoch 6/50, Loss: 10.7351, Domain Loss: 10.4927, Class Loss: 0.2425
Epoch 7/50, Loss: 7.7889, Domain Loss: 7.5526, Class Loss: 0.2363
Epoch 8/50, Loss: 3.3754, Domain Loss: 3.2000, Class Loss: 0.1755
Epoch 9/50, Loss: 2.1420, Domain Loss: 2.0006, Class Loss: 0.1414
Epoch 10/50, Loss: 1.6464, Domain Loss: 1.5364, Class Loss: 0.1100
Epoch 11/50, Loss: 2.2151, Domain Loss: 1.9817, Class Loss: 0.2334
Epoch 12/50, Loss: 2.1797, Domain Loss: 2.0383, Class Loss: 0.1414
Epoch 13/50, Loss: 2.1194, Domain Loss: 1.8781, Class Loss: 0.2413
Epoch 14/50, Loss: 2.2320, Domain Loss: 1.8204, Class Loss: 0.4116
Epoch 15/50, Loss: 1.6904, Domain Loss: 1.4524, Class Loss: 0.2381
Epoch 16/50, Loss: 1.5716, Domain Loss: 1.4442, Class Loss: 0.1273
Epoch 17/50, Loss: 1.3930, Domain Loss: 1.2944, Class Loss: 0.0985
Epoch 18/50, Loss: 1.8177, Domain Loss: 1.6878, Class Loss: 0.1300
Epoch 19/50, Loss: 2.2202, Domain Loss: 2.0517, Class Loss: 0.1685
Epoch 20/50, Loss: 1.7656, Domain Loss: 1.6623, Class Loss: 0.1032
Epoch 21/50, Loss: 1.6173, Domain Loss: 1.5175, Class Loss: 0.0999
Epoch 22/50, Loss: 1.5189, Domain Loss: 1.4447, Class Loss: 0.0742
Epoch 23/50, Loss: 1.6670, Domain Loss: 1.4866, Class Loss: 0.1804
Epoch 24/50, Loss: 1.7059, Domain Loss: 1.6043, Class Loss: 0.1016
Epoch 25/50, Loss: 2.2188, Domain Loss: 1.8973, Class Loss: 0.3215
Epoch 26/50, Loss: 1.6040, Domain Loss: 1.4505, Class Loss: 0.1535
Epoch 27/50, Loss: 1.8340, Domain Loss: 1.7102, Class Loss: 0.1238
Epoch 28/50, Loss: 2.2097, Domain Loss: 2.0446, Class Loss: 0.1651
Epoch 29/50, Loss: 2.7651, Domain Loss: 2.4858, Class Loss: 0.2793
Epoch 30/50, Loss: 4.5816, Domain Loss: 3.3113, Class Loss: 1.2704
Epoch 31/50, Loss: 4.9553, Domain Loss: 4.1790, Class Loss: 0.7763
Epoch 32/50, Loss: 4.4555, Domain Loss: 3.8433, Class Loss: 0.6121
Epoch 33/50, Loss: 3.1386, Domain Loss: 2.6350, Class Loss: 0.5036
Epoch 34/50, Loss: 2.7898, Domain Loss: 2.3447, Class Loss: 0.4450
Epoch 35/50, Loss: 1.8346, Domain Loss: 1.5732, Class Loss: 0.2614
Epoch 36/50, Loss: 2.6168, Domain Loss: 2.1743, Class Loss: 0.4425
Epoch 37/50, Loss: 2.1825, Domain Loss: 1.8014, Class Loss: 0.3811
Epoch 38/50, Loss: 2.4083, Domain Loss: 2.0677, Class Loss: 0.3406
Epoch 39/50, Loss: 2.0646, Domain Loss: 1.7916, Class Loss: 0.2730
Epoch 40/50, Loss: 1.5344, Domain Loss: 1.3123, Class Loss: 0.2221
Epoch 41/50, Loss: 2.0443, Domain Loss: 1.8660, Class Loss: 0.1783
Epoch 42/50, Loss: 3.5231, Domain Loss: 3.0005, Class Loss: 0.5225
Epoch 43/50, Loss: 2.8965, Domain Loss: 2.5822, Class Loss: 0.3143
Epoch 44/50, Loss: 3.8351, Domain Loss: 3.5974, Class Loss: 0.2378
Epoch 45/50, Loss: 3.2695, Domain Loss: 3.0084, Class Loss: 0.2611
Epoch 46/50, Loss: 2.8939, Domain Loss: 2.7309, Class Loss: 0.1631
Epoch 47/50, Loss: 4.6381, Domain Loss: 3.9983, Class Loss: 0.6398
Epoch 48/50, Loss: 4.1893, Domain Loss: 3.7810, Class Loss: 0.4083
Epoch 49/50, Loss: 3.6120, Domain Loss: 3.3310, Class Loss: 0.2810
Epoch 50/50, Loss: 2.1777, Domain Loss: 1.8991, Class Loss: 0.2786
54.68


Epoch 1/50, Loss: 2.3385, Domain Loss: 1.3305, Class Loss: 1.0080
Epoch 2/50, Loss: 1.7834, Domain Loss: 1.2257, Class Loss: 0.5577
Epoch 3/50, Loss: 1.5590, Domain Loss: 1.3158, Class Loss: 0.2432
Epoch 4/50, Loss: 4.7758, Domain Loss: 4.4973, Class Loss: 0.2785
Epoch 5/50, Loss: 7.5127, Domain Loss: 7.2437, Class Loss: 0.2690
Epoch 6/50, Loss: 8.4368, Domain Loss: 8.0774, Class Loss: 0.3594
Epoch 7/50, Loss: 5.6413, Domain Loss: 4.9186, Class Loss: 0.7227
Epoch 8/50, Loss: 5.4039, Domain Loss: 4.9852, Class Loss: 0.4188
Epoch 9/50, Loss: 4.1651, Domain Loss: 3.7596, Class Loss: 0.4054
Epoch 10/50, Loss: 2.9819, Domain Loss: 2.4867, Class Loss: 0.4952
Epoch 11/50, Loss: 2.4728, Domain Loss: 1.9701, Class Loss: 0.5027
Epoch 12/50, Loss: 1.9110, Domain Loss: 1.4787, Class Loss: 0.4323
Epoch 13/50, Loss: 2.2143, Domain Loss: 1.7586, Class Loss: 0.4557
Epoch 14/50, Loss: 2.8674, Domain Loss: 2.3093, Class Loss: 0.5582
Epoch 15/50, Loss: 3.3843, Domain Loss: 2.9650, Class Loss: 0.4193
Epoch 16/50, Loss: 4.4293, Domain Loss: 3.9802, Class Loss: 0.4491
Epoch 17/50, Loss: 5.8216, Domain Loss: 5.3622, Class Loss: 0.4594
Epoch 18/50, Loss: 4.1588, Domain Loss: 3.8522, Class Loss: 0.3066
Epoch 19/50, Loss: 2.1592, Domain Loss: 1.7821, Class Loss: 0.3771
Epoch 20/50, Loss: 1.6488, Domain Loss: 1.3041, Class Loss: 0.3447
Epoch 21/50, Loss: 2.0945, Domain Loss: 1.6360, Class Loss: 0.4586
Epoch 22/50, Loss: 2.3581, Domain Loss: 1.8576, Class Loss: 0.5005
Epoch 23/50, Loss: 2.5323, Domain Loss: 2.2588, Class Loss: 0.2735
Epoch 24/50, Loss: 2.6442, Domain Loss: 2.2997, Class Loss: 0.3445
Epoch 25/50, Loss: 2.0184, Domain Loss: 1.7214, Class Loss: 0.2970
Epoch 26/50, Loss: 2.3251, Domain Loss: 1.9595, Class Loss: 0.3656
Epoch 27/50, Loss: 2.2813, Domain Loss: 1.7811, Class Loss: 0.5002
Epoch 28/50, Loss: 1.7654, Domain Loss: 1.5227, Class Loss: 0.2427
Epoch 29/50, Loss: 2.0979, Domain Loss: 1.7388, Class Loss: 0.3591
Epoch 30/50, Loss: 1.7147, Domain Loss: 1.4697, Class Loss: 0.2450
Epoch 31/50, Loss: 1.7027, Domain Loss: 1.4605, Class Loss: 0.2421
Epoch 32/50, Loss: 1.7093, Domain Loss: 1.4933, Class Loss: 0.2160
Epoch 33/50, Loss: 1.6795, Domain Loss: 1.5167, Class Loss: 0.1628
Epoch 34/50, Loss: 1.6128, Domain Loss: 1.4562, Class Loss: 0.1566
Epoch 35/50, Loss: 1.6976, Domain Loss: 1.5558, Class Loss: 0.1418
Epoch 36/50, Loss: 1.5452, Domain Loss: 1.4287, Class Loss: 0.1165
Epoch 37/50, Loss: 1.4409, Domain Loss: 1.3680, Class Loss: 0.0729
Epoch 38/50, Loss: 1.5957, Domain Loss: 1.4973, Class Loss: 0.0984
Epoch 39/50, Loss: 1.6698, Domain Loss: 1.5512, Class Loss: 0.1186
Epoch 40/50, Loss: 1.5690, Domain Loss: 1.4507, Class Loss: 0.1183
Epoch 41/50, Loss: 1.5528, Domain Loss: 1.4347, Class Loss: 0.1181
Epoch 42/50, Loss: 1.5539, Domain Loss: 1.4621, Class Loss: 0.0918
Epoch 43/50, Loss: 1.4457, Domain Loss: 1.3737, Class Loss: 0.0720
Epoch 44/50, Loss: 1.6752, Domain Loss: 1.5587, Class Loss: 0.1165
Epoch 45/50, Loss: 1.5288, Domain Loss: 1.4551, Class Loss: 0.0737
Epoch 46/50, Loss: 1.5657, Domain Loss: 1.4932, Class Loss: 0.0726
Epoch 47/50, Loss: 1.5155, Domain Loss: 1.4366, Class Loss: 0.0789
Epoch 48/50, Loss: 1.5645, Domain Loss: 1.4695, Class Loss: 0.0949
Epoch 49/50, Loss: 1.5646, Domain Loss: 1.4362, Class Loss: 0.1284
Epoch 50/50, Loss: 1.6753, Domain Loss: 1.5625, Class Loss: 0.1128
51.74


Source performance:
66.24 67.13 67.44 61.50 
Target performance:
58.60 60.51 58.35 52.96 

Per-class target performance: 95.15 40.25 45.76 52.21 Deep CORAL
Deep CORAL Run 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1: Source Val Acc = 0.5024, Target Val Acc = 0.5150
Epoch 2: Source Val Acc = 0.4952, Target Val Acc = 0.4850
Epoch 3: Source Val Acc = 0.5096, Target Val Acc = 0.4982
Epoch 4: Source Val Acc = 0.4934, Target Val Acc = 0.5234
Epoch 5: Source Val Acc = 0.5024, Target Val Acc = 0.5641
Epoch 6: Source Val Acc = 0.9574, Target Val Acc = 0.5456
Epoch 7: Source Val Acc = 0.5498, Target Val Acc = 0.6289
Epoch 8: Source Val Acc = 0.5084, Target Val Acc = 0.6091
Epoch 9: Source Val Acc = 0.6325, Target Val Acc = 0.5348
Epoch 10: Source Val Acc = 0.5030, Target Val Acc = 0.5558
Epoch 11: Source Val Acc = 0.8417, Target Val Acc = 0.5486
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.8417, Target Val Acc = 0.5486

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.5222, Target Val Acc = 0.5618
Epoch 2: Source Val Acc = 0.2614, Target Val Acc = 0.2536
Epoch 3: Source Val Acc = 0.5096, Target Val Acc = 0.4880
Epoch 4: Source Val Acc = 0.5306, Target Val Acc = 0.4904
Epoch 5: Source Val Acc = 0.4502, Target Val Acc = 0.4934
Epoch 6: Source Val Acc = 0.2680, Target Val Acc = 0.4826
Epoch 7: Source Val Acc = 0.4940, Target Val Acc = 0.5300
Epoch 8: Source Val Acc = 0.5180, Target Val Acc = 0.4682
Epoch 9: Source Val Acc = 0.4922, Target Val Acc = 0.4802
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.4922, Target Val Acc = 0.4802

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.4946, Target Val Acc = 0.4922
Epoch 2: Source Val Acc = 0.4934, Target Val Acc = 0.3195
Epoch 3: Source Val Acc = 0.4718, Target Val Acc = 0.4478
Epoch 4: Source Val Acc = 0.4958, Target Val Acc = 0.5210
Epoch 5: Source Val Acc = 0.9622, Target Val Acc = 0.5282
Epoch 6: Source Val Acc = 0.5306, Target Val Acc = 0.5761
Epoch 7: Source Val Acc = 0.5102, Target Val Acc = 0.5881
Epoch 8: Source Val Acc = 0.3717, Target Val Acc = 0.5468
Epoch 9: Source Val Acc = 0.2836, Target Val Acc = 0.3285
Epoch 10: Source Val Acc = 0.7350, Target Val Acc = 0.5192
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.7350, Target Val Acc = 0.5192

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.5186, Target Val Acc = 0.3094
Epoch 2: Source Val Acc = 0.4952, Target Val Acc = 0.4484
Epoch 3: Source Val Acc = 0.4934, Target Val Acc = 0.4712
Epoch 4: Source Val Acc = 0.4928, Target Val Acc = 0.3327
Epoch 5: Source Val Acc = 0.8801, Target Val Acc = 0.5486
Epoch 6: Source Val Acc = 0.8735, Target Val Acc = 0.5204
Epoch 7: Source Val Acc = 0.2650, Target Val Acc = 0.2494
Epoch 8: Source Val Acc = 0.5138, Target Val Acc = 0.5833
Epoch 9: Source Val Acc = 0.6589, Target Val Acc = 0.5719
Epoch 10: Source Val Acc = 0.5180, Target Val Acc = 0.5186
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.5180, Target Val Acc = 0.5186

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.4940, Target Val Acc = 0.4484
Epoch 2: Source Val Acc = 0.4934, Target Val Acc = 0.4323
Epoch 3: Source Val Acc = 0.7416, Target Val Acc = 0.3693
Epoch 4: Source Val Acc = 0.5857, Target Val Acc = 0.5821
Epoch 5: Source Val Acc = 0.5635, Target Val Acc = 0.5773
Epoch 6: Source Val Acc = 0.2644, Target Val Acc = 0.3831
Epoch 7: Source Val Acc = 0.7158, Target Val Acc = 0.5294
Epoch 8: Source Val Acc = 0.4952, Target Val Acc = 0.4640
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.4952, Target Val Acc = 0.4640

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.4958, Target Val Acc = 0.5905
Epoch 2: Source Val Acc = 0.5066, Target Val Acc = 0.6259
Epoch 3: Source Val Acc = 0.4778, Target Val Acc = 0.3561
Epoch 4: Source Val Acc = 0.5432, Target Val Acc = 0.5264
Epoch 5: Source Val Acc = 0.5851, Target Val Acc = 0.5258
Epoch 6: Source Val Acc = 0.7866, Target Val Acc = 0.5564
Epoch 7: Source Val Acc = 0.4976, Target Val Acc = 0.5012
Epoch 8: Source Val Acc = 0.9149, Target Val Acc = 0.5456
Epoch 9: Source Val Acc = 0.8016, Target Val Acc = 0.5108
Epoch 10: Source Val Acc = 0.5468, Target Val Acc = 0.5384
Epoch 11: Source Val Acc = 0.5552, Target Val Acc = 0.5432
Epoch 12: Source Val Acc = 0.2812, Target Val Acc = 0.3351
Epoch 13: Source Val Acc = 0.7404, Target Val Acc = 0.5414
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 0.7404, Target Val Acc = 0.5414

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.5222, Target Val Acc = 0.4796
Epoch 2: Source Val Acc = 0.6031, Target Val Acc = 0.5941
Epoch 3: Source Val Acc = 0.5300, Target Val Acc = 0.5743
Epoch 4: Source Val Acc = 0.4958, Target Val Acc = 0.5719
Epoch 5: Source Val Acc = 0.5402, Target Val Acc = 0.4412
Epoch 6: Source Val Acc = 0.5180, Target Val Acc = 0.5378
Epoch 7: Source Val Acc = 0.5444, Target Val Acc = 0.4400
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.5444, Target Val Acc = 0.4400

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.4934, Target Val Acc = 0.5378
Epoch 2: Source Val Acc = 0.7320, Target Val Acc = 0.5731
Epoch 3: Source Val Acc = 0.3591, Target Val Acc = 0.3573
Epoch 4: Source Val Acc = 0.4910, Target Val Acc = 0.4293
Epoch 5: Source Val Acc = 0.6841, Target Val Acc = 0.5204
Epoch 6: Source Val Acc = 0.6865, Target Val Acc = 0.5318
Epoch 7: Source Val Acc = 0.4281, Target Val Acc = 0.5342
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.4281, Target Val Acc = 0.5342

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.7464, Target Val Acc = 0.5036
Epoch 2: Source Val Acc = 0.4598, Target Val Acc = 0.5324
Epoch 3: Source Val Acc = 0.4934, Target Val Acc = 0.5072
Epoch 4: Source Val Acc = 0.4532, Target Val Acc = 0.3891
Epoch 5: Source Val Acc = 0.4934, Target Val Acc = 0.4766
Epoch 6: Source Val Acc = 0.7176, Target Val Acc = 0.4928
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 0.7176, Target Val Acc = 0.4928

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.7302, Target Val Acc = 0.5372
Epoch 2: Source Val Acc = 0.8831, Target Val Acc = 0.6079
Epoch 3: Source Val Acc = 0.3999, Target Val Acc = 0.5983
Epoch 4: Source Val Acc = 0.4994, Target Val Acc = 0.5588
Epoch 5: Source Val Acc = 0.5006, Target Val Acc = 0.5977
Epoch 6: Source Val Acc = 0.7626, Target Val Acc = 0.5336
Epoch 7: Source Val Acc = 0.5743, Target Val Acc = 0.5522
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.5743, Target Val Acc = 0.5522

Deep CORAL: Average Source Val Acc = 0.6087, Average Target Val Acc = 0.5091
STAR

Run 1/10
Epoch [1/50], Class Loss: 1.5874, Discrepancy Loss: 0.1123
Epoch [2/50], Class Loss: 0.2179, Discrepancy Loss: 0.0857
Epoch [3/50], Class Loss: 0.3954, Discrepancy Loss: 0.0922
Epoch [4/50], Class Loss: 0.1869, Discrepancy Loss: 0.0400
Epoch [5/50], Class Loss: 0.1659, Discrepancy Loss: 0.0871
Epoch [6/50], Class Loss: 0.0899, Discrepancy Loss: 0.0754
Epoch [7/50], Class Loss: 0.0611, Discrepancy Loss: 0.0558
Epoch [8/50], Class Loss: 0.1099, Discrepancy Loss: 0.0405
Epoch [9/50], Class Loss: 0.0450, Discrepancy Loss: 0.0353
Epoch [10/50], Class Loss: 0.0473, Discrepancy Loss: 0.0394
Epoch [11/50], Class Loss: 0.0381, Discrepancy Loss: 0.0323
Epoch [12/50], Class Loss: 0.0326, Discrepancy Loss: 0.0279
Epoch [13/50], Class Loss: 0.0199, Discrepancy Loss: 0.0329
Epoch [14/50], Class Loss: 0.1304, Discrepancy Loss: 0.0374
Epoch [15/50], Class Loss: 0.0367, Discrepancy Loss: 0.0355
Epoch [16/50], Class Loss: 0.0214, Discrepancy Loss: 0.0341
Epoch [17/50], Class Loss: 0.0211, Discrepancy Loss: 0.0338
Epoch [18/50], Class Loss: 0.0381, Discrepancy Loss: 0.0356
Epoch [19/50], Class Loss: 0.0206, Discrepancy Loss: 0.0316
Epoch [20/50], Class Loss: 0.0251, Discrepancy Loss: 0.0297
Epoch [21/50], Class Loss: 0.0139, Discrepancy Loss: 0.0313
Epoch [22/50], Class Loss: 0.0178, Discrepancy Loss: 0.0307
Epoch [23/50], Class Loss: 0.0124, Discrepancy Loss: 0.0294
Epoch [24/50], Class Loss: 0.0121, Discrepancy Loss: 0.0304
Epoch [25/50], Class Loss: 0.0123, Discrepancy Loss: 0.0273
Epoch [26/50], Class Loss: 0.0093, Discrepancy Loss: 0.0290
Epoch [27/50], Class Loss: 0.0163, Discrepancy Loss: 0.0312
Epoch [28/50], Class Loss: 0.0091, Discrepancy Loss: 0.0332
Epoch [29/50], Class Loss: 0.0824, Discrepancy Loss: 0.0284
Epoch [30/50], Class Loss: 0.0137, Discrepancy Loss: 0.0295
Epoch [31/50], Class Loss: 0.0067, Discrepancy Loss: 0.0315
Epoch [32/50], Class Loss: 0.0105, Discrepancy Loss: 0.0301
Epoch [33/50], Class Loss: 0.0085, Discrepancy Loss: 0.0319
Epoch [34/50], Class Loss: 0.1570, Discrepancy Loss: 0.0290
Epoch [35/50], Class Loss: 0.0085, Discrepancy Loss: 0.0325
Epoch [36/50], Class Loss: 0.0070, Discrepancy Loss: 0.0297
Epoch [37/50], Class Loss: 0.0106, Discrepancy Loss: 0.0326
Epoch [38/50], Class Loss: 0.0123, Discrepancy Loss: 0.0296
Epoch [39/50], Class Loss: 0.1208, Discrepancy Loss: 0.0341
Epoch [40/50], Class Loss: 0.0238, Discrepancy Loss: 0.0312
Epoch [41/50], Class Loss: 0.1300, Discrepancy Loss: 0.0284
Epoch [42/50], Class Loss: 0.1274, Discrepancy Loss: 0.0300
Epoch [43/50], Class Loss: 0.0065, Discrepancy Loss: 0.0271
Epoch [44/50], Class Loss: 0.0091, Discrepancy Loss: 0.0298
Epoch [45/50], Class Loss: 0.0082, Discrepancy Loss: 0.0297
Epoch [46/50], Class Loss: 0.0295, Discrepancy Loss: 0.0291
Epoch [47/50], Class Loss: 0.0060, Discrepancy Loss: 0.0271
Epoch [48/50], Class Loss: 0.0130, Discrepancy Loss: 0.0332
Epoch [49/50], Class Loss: 0.0082, Discrepancy Loss: 0.0301
Epoch [50/50], Class Loss: 0.1620, Discrepancy Loss: 0.0329
Source Domain Performance - Accuracy: 82.61%, Precision: 89.66%, Recall: 83.85%, F1 Score: 81.76%
Target Domain Performance - Accuracy: 68.53%, Precision: 75.75%, Recall: 67.61%, F1 Score: 63.50%

Run 2/10
Epoch [1/50], Class Loss: 2.3262, Discrepancy Loss: 0.1487
Epoch [2/50], Class Loss: 0.5547, Discrepancy Loss: 0.1126
Epoch [3/50], Class Loss: 0.1962, Discrepancy Loss: 0.0692
Epoch [4/50], Class Loss: 0.1285, Discrepancy Loss: 0.0572
Epoch [5/50], Class Loss: 0.2136, Discrepancy Loss: 0.0400
Epoch [6/50], Class Loss: 0.1157, Discrepancy Loss: 0.0516
Epoch [7/50], Class Loss: 0.0728, Discrepancy Loss: 0.0499
Epoch [8/50], Class Loss: 0.1045, Discrepancy Loss: 0.0348
Epoch [9/50], Class Loss: 0.0293, Discrepancy Loss: 0.0294
Epoch [10/50], Class Loss: 0.0710, Discrepancy Loss: 0.0490
Epoch [11/50], Class Loss: 0.0340, Discrepancy Loss: 0.0394
Epoch [12/50], Class Loss: 0.0270, Discrepancy Loss: 0.0341
Epoch [13/50], Class Loss: 0.0163, Discrepancy Loss: 0.0286
Epoch [14/50], Class Loss: 0.0279, Discrepancy Loss: 0.0378
Epoch [15/50], Class Loss: 0.0357, Discrepancy Loss: 0.0341
Epoch [16/50], Class Loss: 0.0266, Discrepancy Loss: 0.0286
Epoch [17/50], Class Loss: 0.0164, Discrepancy Loss: 0.0240
Epoch [18/50], Class Loss: 0.0199, Discrepancy Loss: 0.0232
Epoch [19/50], Class Loss: 0.0132, Discrepancy Loss: 0.0204
Epoch [20/50], Class Loss: 0.0290, Discrepancy Loss: 0.0244
Epoch [21/50], Class Loss: 0.1041, Discrepancy Loss: 0.0241
Epoch [22/50], Class Loss: 0.0169, Discrepancy Loss: 0.0246
Epoch [23/50], Class Loss: 0.0080, Discrepancy Loss: 0.0221
Epoch [24/50], Class Loss: 0.0135, Discrepancy Loss: 0.0223
Epoch [25/50], Class Loss: 0.1682, Discrepancy Loss: 0.0210
Epoch [26/50], Class Loss: 0.0165, Discrepancy Loss: 0.0258
Epoch [27/50], Class Loss: 0.0122, Discrepancy Loss: 0.0229
Epoch [28/50], Class Loss: 0.0094, Discrepancy Loss: 0.0233
Epoch [29/50], Class Loss: 0.0166, Discrepancy Loss: 0.0271
Epoch [30/50], Class Loss: 0.0099, Discrepancy Loss: 0.0211
Epoch [31/50], Class Loss: 0.0063, Discrepancy Loss: 0.0180
Epoch [32/50], Class Loss: 0.0367, Discrepancy Loss: 0.0189
Epoch [33/50], Class Loss: 0.0106, Discrepancy Loss: 0.0226
Epoch [34/50], Class Loss: 0.0088, Discrepancy Loss: 0.0220
Epoch [35/50], Class Loss: 0.0131, Discrepancy Loss: 0.0208
Epoch [36/50], Class Loss: 0.0099, Discrepancy Loss: 0.0189
Epoch [37/50], Class Loss: 0.0086, Discrepancy Loss: 0.0221
Epoch [38/50], Class Loss: 0.0077, Discrepancy Loss: 0.0209
Epoch [39/50], Class Loss: 0.0059, Discrepancy Loss: 0.0226
Epoch [40/50], Class Loss: 0.0098, Discrepancy Loss: 0.0215
Epoch [41/50], Class Loss: 0.0079, Discrepancy Loss: 0.0213
Epoch [42/50], Class Loss: 0.0075, Discrepancy Loss: 0.0191
Epoch [43/50], Class Loss: 0.0146, Discrepancy Loss: 0.0209
Epoch [44/50], Class Loss: 0.0076, Discrepancy Loss: 0.0236
Epoch [45/50], Class Loss: 0.0123, Discrepancy Loss: 0.0197
Epoch [46/50], Class Loss: 0.0116, Discrepancy Loss: 0.0189
Epoch [47/50], Class Loss: 0.0095, Discrepancy Loss: 0.0226
Epoch [48/50], Class Loss: 0.0066, Discrepancy Loss: 0.0182
Epoch [49/50], Class Loss: 0.0350, Discrepancy Loss: 0.0212
Epoch [50/50], Class Loss: 0.0090, Discrepancy Loss: 0.0193
Source Domain Performance - Accuracy: 46.88%, Precision: 41.77%, Recall: 47.37%, F1 Score: 36.31%
Target Domain Performance - Accuracy: 54.38%, Precision: 44.26%, Recall: 53.64%, F1 Score: 47.66%

Run 3/10
Epoch [1/50], Class Loss: 1.5574, Discrepancy Loss: 0.1070
Epoch [2/50], Class Loss: 0.3912, Discrepancy Loss: 0.0687
Epoch [3/50], Class Loss: 0.1549, Discrepancy Loss: 0.0527
Epoch [4/50], Class Loss: 1.5390, Discrepancy Loss: 0.1023
Epoch [5/50], Class Loss: 0.1103, Discrepancy Loss: 0.0667
Epoch [6/50], Class Loss: 0.0853, Discrepancy Loss: 0.0407
Epoch [7/50], Class Loss: 0.0599, Discrepancy Loss: 0.0402
Epoch [8/50], Class Loss: 0.0819, Discrepancy Loss: 0.0480
Epoch [9/50], Class Loss: 0.0311, Discrepancy Loss: 0.0372
Epoch [10/50], Class Loss: 0.0361, Discrepancy Loss: 0.0321
Epoch [11/50], Class Loss: 0.0262, Discrepancy Loss: 0.0272
Epoch [12/50], Class Loss: 0.0105, Discrepancy Loss: 0.0258
Epoch [13/50], Class Loss: 0.0194, Discrepancy Loss: 0.0217
Epoch [14/50], Class Loss: 0.0243, Discrepancy Loss: 0.0263
Epoch [15/50], Class Loss: 0.0160, Discrepancy Loss: 0.0247
Epoch [16/50], Class Loss: 0.0105, Discrepancy Loss: 0.0246
Epoch [17/50], Class Loss: 0.0186, Discrepancy Loss: 0.0281
Epoch [18/50], Class Loss: 0.0182, Discrepancy Loss: 0.0247
Epoch [19/50], Class Loss: 0.0110, Discrepancy Loss: 0.0319
Epoch [20/50], Class Loss: 0.0127, Discrepancy Loss: 0.0250
Epoch [21/50], Class Loss: 0.0125, Discrepancy Loss: 0.0267
Epoch [22/50], Class Loss: 0.0084, Discrepancy Loss: 0.0278
Epoch [23/50], Class Loss: 0.0188, Discrepancy Loss: 0.0291
Epoch [24/50], Class Loss: 0.0114, Discrepancy Loss: 0.0261
Epoch [25/50], Class Loss: 0.0264, Discrepancy Loss: 0.0303
Epoch [26/50], Class Loss: 0.0118, Discrepancy Loss: 0.0221
Epoch [27/50], Class Loss: 0.0103, Discrepancy Loss: 0.0263
Epoch [28/50], Class Loss: 0.0099, Discrepancy Loss: 0.0262
Epoch [29/50], Class Loss: 0.1737, Discrepancy Loss: 0.0229
Epoch [30/50], Class Loss: 0.0253, Discrepancy Loss: 0.0294
Epoch [31/50], Class Loss: 0.0116, Discrepancy Loss: 0.0295
Epoch [32/50], Class Loss: 0.0290, Discrepancy Loss: 0.0299
Epoch [33/50], Class Loss: 0.0352, Discrepancy Loss: 0.0338
Epoch [34/50], Class Loss: 0.0088, Discrepancy Loss: 0.0258
Epoch [35/50], Class Loss: 0.0272, Discrepancy Loss: 0.0296
Epoch [36/50], Class Loss: 0.0167, Discrepancy Loss: 0.0313
Epoch [37/50], Class Loss: 0.0293, Discrepancy Loss: 0.0258
Epoch [38/50], Class Loss: 0.0110, Discrepancy Loss: 0.0275
Epoch [39/50], Class Loss: 0.0248, Discrepancy Loss: 0.0288
Epoch [40/50], Class Loss: 0.0096, Discrepancy Loss: 0.0302
Epoch [41/50], Class Loss: 0.0078, Discrepancy Loss: 0.0298
Epoch [42/50], Class Loss: 0.0126, Discrepancy Loss: 0.0306
Epoch [43/50], Class Loss: 0.0153, Discrepancy Loss: 0.0273
Epoch [44/50], Class Loss: 0.0120, Discrepancy Loss: 0.0282
Epoch [45/50], Class Loss: 0.0080, Discrepancy Loss: 0.0257
Epoch [46/50], Class Loss: 0.0081, Discrepancy Loss: 0.0267
Epoch [47/50], Class Loss: 0.0109, Discrepancy Loss: 0.0251
Epoch [48/50], Class Loss: 0.0174, Discrepancy Loss: 0.0283
Epoch [49/50], Class Loss: 0.0093, Discrepancy Loss: 0.0255
Epoch [50/50], Class Loss: 0.0080, Discrepancy Loss: 0.0225
Source Domain Performance - Accuracy: 84.59%, Precision: 87.01%, Recall: 85.57%, F1 Score: 83.24%
Target Domain Performance - Accuracy: 73.44%, Precision: 84.60%, Recall: 72.54%, F1 Score: 68.90%

Run 4/10
Epoch [1/50], Class Loss: 1.8075, Discrepancy Loss: 0.1330
Epoch [2/50], Class Loss: 0.6974, Discrepancy Loss: 0.0801
Epoch [3/50], Class Loss: 0.3458, Discrepancy Loss: 0.0826
Epoch [4/50], Class Loss: 0.1042, Discrepancy Loss: 0.0714
Epoch [5/50], Class Loss: 0.1965, Discrepancy Loss: 0.0782
Epoch [6/50], Class Loss: 0.1582, Discrepancy Loss: 0.0514
Epoch [7/50], Class Loss: 0.2023, Discrepancy Loss: 0.0862
Epoch [8/50], Class Loss: 0.0311, Discrepancy Loss: 0.0326
Epoch [9/50], Class Loss: 0.0447, Discrepancy Loss: 0.0282
Epoch [10/50], Class Loss: 0.0223, Discrepancy Loss: 0.0310
Epoch [11/50], Class Loss: 0.0193, Discrepancy Loss: 0.0311
Epoch [12/50], Class Loss: 0.0631, Discrepancy Loss: 0.0250
Epoch [13/50], Class Loss: 0.0341, Discrepancy Loss: 0.0294
Epoch [14/50], Class Loss: 0.0167, Discrepancy Loss: 0.0325
Epoch [15/50], Class Loss: 0.0240, Discrepancy Loss: 0.0306
Epoch [16/50], Class Loss: 0.0150, Discrepancy Loss: 0.0272
Epoch [17/50], Class Loss: 0.0230, Discrepancy Loss: 0.0271
Epoch [18/50], Class Loss: 0.0112, Discrepancy Loss: 0.0341
Epoch [19/50], Class Loss: 0.0119, Discrepancy Loss: 0.0272
Epoch [20/50], Class Loss: 0.0090, Discrepancy Loss: 0.0269
Epoch [21/50], Class Loss: 0.0061, Discrepancy Loss: 0.0279
Epoch [22/50], Class Loss: 0.0977, Discrepancy Loss: 0.0246
Epoch [23/50], Class Loss: 0.0192, Discrepancy Loss: 0.0236
Epoch [24/50], Class Loss: 0.0114, Discrepancy Loss: 0.0247
Epoch [25/50], Class Loss: 0.0054, Discrepancy Loss: 0.0229
Epoch [26/50], Class Loss: 0.0744, Discrepancy Loss: 0.0238
Epoch [27/50], Class Loss: 0.0074, Discrepancy Loss: 0.0291
Epoch [28/50], Class Loss: 0.0240, Discrepancy Loss: 0.0270
Epoch [29/50], Class Loss: 0.0149, Discrepancy Loss: 0.0234
Epoch [30/50], Class Loss: 0.0167, Discrepancy Loss: 0.0273
Epoch [31/50], Class Loss: 0.0248, Discrepancy Loss: 0.0240
Epoch [32/50], Class Loss: 0.0082, Discrepancy Loss: 0.0254
Epoch [33/50], Class Loss: 0.0246, Discrepancy Loss: 0.0231
Epoch [34/50], Class Loss: 0.0127, Discrepancy Loss: 0.0233
Epoch [35/50], Class Loss: 0.0060, Discrepancy Loss: 0.0258
Epoch [36/50], Class Loss: 0.0125, Discrepancy Loss: 0.0251
Epoch [37/50], Class Loss: 0.0092, Discrepancy Loss: 0.0245
Epoch [38/50], Class Loss: 0.0058, Discrepancy Loss: 0.0262
Epoch [39/50], Class Loss: 0.0061, Discrepancy Loss: 0.0229
Epoch [40/50], Class Loss: 0.0047, Discrepancy Loss: 0.0235
Epoch [41/50], Class Loss: 0.0162, Discrepancy Loss: 0.0246
Epoch [42/50], Class Loss: 0.0103, Discrepancy Loss: 0.0231
Epoch [43/50], Class Loss: 0.0129, Discrepancy Loss: 0.0299
Epoch [44/50], Class Loss: 0.0508, Discrepancy Loss: 0.0263
Epoch [45/50], Class Loss: 0.0253, Discrepancy Loss: 0.0261
Epoch [46/50], Class Loss: 0.0133, Discrepancy Loss: 0.0257
Epoch [47/50], Class Loss: 0.0376, Discrepancy Loss: 0.0272
Epoch [48/50], Class Loss: 0.0131, Discrepancy Loss: 0.0270
Epoch [49/50], Class Loss: 0.0111, Discrepancy Loss: 0.0239
Epoch [50/50], Class Loss: 0.0066, Discrepancy Loss: 0.0237
Source Domain Performance - Accuracy: 79.38%, Precision: 88.65%, Recall: 80.66%, F1 Score: 78.18%
Target Domain Performance - Accuracy: 73.32%, Precision: 82.49%, Recall: 72.49%, F1 Score: 70.04%

Run 5/10
Epoch [1/50], Class Loss: 2.2835, Discrepancy Loss: 0.1539
Epoch [2/50], Class Loss: 1.1139, Discrepancy Loss: 0.1314
Epoch [3/50], Class Loss: 0.4672, Discrepancy Loss: 0.0750
Epoch [4/50], Class Loss: 0.1882, Discrepancy Loss: 0.0702
Epoch [5/50], Class Loss: 0.2081, Discrepancy Loss: 0.0562
Epoch [6/50], Class Loss: 0.1280, Discrepancy Loss: 0.0599
Epoch [7/50], Class Loss: 0.0834, Discrepancy Loss: 0.0460
Epoch [8/50], Class Loss: 0.0825, Discrepancy Loss: 0.0437
Epoch [9/50], Class Loss: 0.2060, Discrepancy Loss: 0.0742
Epoch [10/50], Class Loss: 0.2964, Discrepancy Loss: 0.0396
Epoch [11/50], Class Loss: 0.0420, Discrepancy Loss: 0.0422
Epoch [12/50], Class Loss: 0.0425, Discrepancy Loss: 0.0394
Epoch [13/50], Class Loss: 0.0272, Discrepancy Loss: 0.0365
Epoch [14/50], Class Loss: 0.0273, Discrepancy Loss: 0.0429
Epoch [15/50], Class Loss: 0.0200, Discrepancy Loss: 0.0379
Epoch [16/50], Class Loss: 0.0244, Discrepancy Loss: 0.0410
Epoch [17/50], Class Loss: 0.0749, Discrepancy Loss: 0.0380
Epoch [18/50], Class Loss: 0.0268, Discrepancy Loss: 0.0345
Epoch [19/50], Class Loss: 0.0147, Discrepancy Loss: 0.0424
Epoch [20/50], Class Loss: 0.0134, Discrepancy Loss: 0.0372
Epoch [21/50], Class Loss: 0.0076, Discrepancy Loss: 0.0337
Epoch [22/50], Class Loss: 0.0139, Discrepancy Loss: 0.0334
Epoch [23/50], Class Loss: 0.0100, Discrepancy Loss: 0.0345
Epoch [24/50], Class Loss: 0.0126, Discrepancy Loss: 0.0402
Epoch [25/50], Class Loss: 0.0182, Discrepancy Loss: 0.0346
Epoch [26/50], Class Loss: 0.0111, Discrepancy Loss: 0.0359
Epoch [27/50], Class Loss: 0.0115, Discrepancy Loss: 0.0327
Epoch [28/50], Class Loss: 0.0157, Discrepancy Loss: 0.0353
Epoch [29/50], Class Loss: 0.0200, Discrepancy Loss: 0.0364
Epoch [30/50], Class Loss: 0.0133, Discrepancy Loss: 0.0356
Epoch [31/50], Class Loss: 0.0140, Discrepancy Loss: 0.0309
Epoch [32/50], Class Loss: 0.0092, Discrepancy Loss: 0.0352
Epoch [33/50], Class Loss: 0.0138, Discrepancy Loss: 0.0327
Epoch [34/50], Class Loss: 0.0110, Discrepancy Loss: 0.0319
Epoch [35/50], Class Loss: 0.0125, Discrepancy Loss: 0.0349
Epoch [36/50], Class Loss: 0.0157, Discrepancy Loss: 0.0334
Epoch [37/50], Class Loss: 0.0312, Discrepancy Loss: 0.0313
Epoch [38/50], Class Loss: 0.0080, Discrepancy Loss: 0.0310
Epoch [39/50], Class Loss: 0.0106, Discrepancy Loss: 0.0301
Epoch [40/50], Class Loss: 0.0167, Discrepancy Loss: 0.0375
Epoch [41/50], Class Loss: 0.0263, Discrepancy Loss: 0.0319
Epoch [42/50], Class Loss: 0.0058, Discrepancy Loss: 0.0361
Epoch [43/50], Class Loss: 0.0138, Discrepancy Loss: 0.0279
Epoch [44/50], Class Loss: 0.0218, Discrepancy Loss: 0.0347
Epoch [45/50], Class Loss: 0.0116, Discrepancy Loss: 0.0318
Epoch [46/50], Class Loss: 0.0319, Discrepancy Loss: 0.0316
Epoch [47/50], Class Loss: 0.0711, Discrepancy Loss: 0.0313
Epoch [48/50], Class Loss: 0.0159, Discrepancy Loss: 0.0343
Epoch [49/50], Class Loss: 0.0627, Discrepancy Loss: 0.0364
Epoch [50/50], Class Loss: 0.0118, Discrepancy Loss: 0.0359
Source Domain Performance - Accuracy: 77.76%, Precision: 83.46%, Recall: 78.99%, F1 Score: 76.68%
Target Domain Performance - Accuracy: 69.12%, Precision: 81.36%, Recall: 68.16%, F1 Score: 62.94%

Run 6/10
Epoch [1/50], Class Loss: 1.5939, Discrepancy Loss: 0.1282
Epoch [2/50], Class Loss: 0.3400, Discrepancy Loss: 0.0716
Epoch [3/50], Class Loss: 0.1742, Discrepancy Loss: 0.0543
Epoch [4/50], Class Loss: 0.1658, Discrepancy Loss: 0.0474
Epoch [5/50], Class Loss: 0.0852, Discrepancy Loss: 0.0442
Epoch [6/50], Class Loss: 0.0507, Discrepancy Loss: 0.0350
Epoch [7/50], Class Loss: 0.0641, Discrepancy Loss: 0.0395
Epoch [8/50], Class Loss: 0.0478, Discrepancy Loss: 0.0333
Epoch [9/50], Class Loss: 0.0447, Discrepancy Loss: 0.0432
Epoch [10/50], Class Loss: 0.0866, Discrepancy Loss: 0.0489
Epoch [11/50], Class Loss: 0.0427, Discrepancy Loss: 0.0457
Epoch [12/50], Class Loss: 0.0381, Discrepancy Loss: 0.0383
Epoch [13/50], Class Loss: 0.0414, Discrepancy Loss: 0.0343
Epoch [14/50], Class Loss: 0.0515, Discrepancy Loss: 0.0417
Epoch [15/50], Class Loss: 0.0257, Discrepancy Loss: 0.0372
Epoch [16/50], Class Loss: 0.0299, Discrepancy Loss: 0.0397
Epoch [17/50], Class Loss: 0.0358, Discrepancy Loss: 0.0343
Epoch [18/50], Class Loss: 0.0269, Discrepancy Loss: 0.0333
Epoch [19/50], Class Loss: 0.0166, Discrepancy Loss: 0.0280
Epoch [20/50], Class Loss: 0.0185, Discrepancy Loss: 0.0256
Epoch [21/50], Class Loss: 0.0313, Discrepancy Loss: 0.0281
Epoch [22/50], Class Loss: 0.0215, Discrepancy Loss: 0.0219
Epoch [23/50], Class Loss: 0.0098, Discrepancy Loss: 0.0319
Epoch [24/50], Class Loss: 0.0119, Discrepancy Loss: 0.0310
Epoch [25/50], Class Loss: 0.0397, Discrepancy Loss: 0.0314
Epoch [26/50], Class Loss: 0.0129, Discrepancy Loss: 0.0270
Epoch [27/50], Class Loss: 0.0156, Discrepancy Loss: 0.0279
Epoch [28/50], Class Loss: 0.0133, Discrepancy Loss: 0.0264
Epoch [29/50], Class Loss: 0.0117, Discrepancy Loss: 0.0298
Epoch [30/50], Class Loss: 0.0205, Discrepancy Loss: 0.0261
Epoch [31/50], Class Loss: 0.1174, Discrepancy Loss: 0.0274
Epoch [32/50], Class Loss: 0.0144, Discrepancy Loss: 0.0218
Epoch [33/50], Class Loss: 0.0104, Discrepancy Loss: 0.0310
Epoch [34/50], Class Loss: 0.0126, Discrepancy Loss: 0.0213
Epoch [35/50], Class Loss: 0.0296, Discrepancy Loss: 0.0278
Epoch [36/50], Class Loss: 0.0254, Discrepancy Loss: 0.0285
Epoch [37/50], Class Loss: 0.0124, Discrepancy Loss: 0.0262
Epoch [38/50], Class Loss: 0.0096, Discrepancy Loss: 0.0289
Epoch [39/50], Class Loss: 0.0254, Discrepancy Loss: 0.0261
Epoch [40/50], Class Loss: 0.0141, Discrepancy Loss: 0.0303
Epoch [41/50], Class Loss: 0.0107, Discrepancy Loss: 0.0215
Epoch [42/50], Class Loss: 0.0218, Discrepancy Loss: 0.0281
Epoch [43/50], Class Loss: 0.0225, Discrepancy Loss: 0.0229
Epoch [44/50], Class Loss: 0.0259, Discrepancy Loss: 0.0270
Epoch [45/50], Class Loss: 0.0807, Discrepancy Loss: 0.0330
Epoch [46/50], Class Loss: 0.0096, Discrepancy Loss: 0.0248
Epoch [47/50], Class Loss: 0.0234, Discrepancy Loss: 0.0330
Epoch [48/50], Class Loss: 0.0511, Discrepancy Loss: 0.0283
Epoch [49/50], Class Loss: 0.0973, Discrepancy Loss: 0.0258
Epoch [50/50], Class Loss: 0.0151, Discrepancy Loss: 0.0239
Source Domain Performance - Accuracy: 77.04%, Precision: 87.89%, Recall: 78.56%, F1 Score: 74.29%
Target Domain Performance - Accuracy: 70.86%, Precision: 82.64%, Recall: 69.93%, F1 Score: 66.67%

Run 7/10
Epoch [1/50], Class Loss: 2.0214, Discrepancy Loss: 0.1544
Epoch [2/50], Class Loss: 0.9508, Discrepancy Loss: 0.1339
Epoch [3/50], Class Loss: 0.3948, Discrepancy Loss: 0.0919
Epoch [4/50], Class Loss: 0.1367, Discrepancy Loss: 0.0783
Epoch [5/50], Class Loss: 0.1315, Discrepancy Loss: 0.0655
Epoch [6/50], Class Loss: 0.0640, Discrepancy Loss: 0.0473
Epoch [7/50], Class Loss: 0.0587, Discrepancy Loss: 0.0494
Epoch [8/50], Class Loss: 0.0618, Discrepancy Loss: 0.0407
Epoch [9/50], Class Loss: 0.0734, Discrepancy Loss: 0.0423
Epoch [10/50], Class Loss: 0.0578, Discrepancy Loss: 0.0307
Epoch [11/50], Class Loss: 0.0719, Discrepancy Loss: 0.0288
Epoch [12/50], Class Loss: 0.0199, Discrepancy Loss: 0.0288
Epoch [13/50], Class Loss: 0.0142, Discrepancy Loss: 0.0295
Epoch [14/50], Class Loss: 0.1540, Discrepancy Loss: 0.0273
Epoch [15/50], Class Loss: 0.0213, Discrepancy Loss: 0.0304
Epoch [16/50], Class Loss: 0.0187, Discrepancy Loss: 0.0333
Epoch [17/50], Class Loss: 0.0220, Discrepancy Loss: 0.0285
Epoch [18/50], Class Loss: 0.0081, Discrepancy Loss: 0.0332
Epoch [19/50], Class Loss: 0.0159, Discrepancy Loss: 0.0287
Epoch [20/50], Class Loss: 0.0186, Discrepancy Loss: 0.0313
Epoch [21/50], Class Loss: 0.0090, Discrepancy Loss: 0.0244
Epoch [22/50], Class Loss: 0.0108, Discrepancy Loss: 0.0282
Epoch [23/50], Class Loss: 0.0435, Discrepancy Loss: 0.0270
Epoch [24/50], Class Loss: 0.0064, Discrepancy Loss: 0.0257
Epoch [25/50], Class Loss: 0.0064, Discrepancy Loss: 0.0274
Epoch [26/50], Class Loss: 0.0076, Discrepancy Loss: 0.0273
Epoch [27/50], Class Loss: 0.0085, Discrepancy Loss: 0.0296
Epoch [28/50], Class Loss: 0.0062, Discrepancy Loss: 0.0283
Epoch [29/50], Class Loss: 0.0097, Discrepancy Loss: 0.0243
Epoch [30/50], Class Loss: 0.0075, Discrepancy Loss: 0.0290
Epoch [31/50], Class Loss: 0.0078, Discrepancy Loss: 0.0268
Epoch [32/50], Class Loss: 0.0047, Discrepancy Loss: 0.0288
Epoch [33/50], Class Loss: 0.0107, Discrepancy Loss: 0.0261
Epoch [34/50], Class Loss: 0.1473, Discrepancy Loss: 0.0255
Epoch [35/50], Class Loss: 0.0080, Discrepancy Loss: 0.0230
Epoch [36/50], Class Loss: 0.0661, Discrepancy Loss: 0.0268
Epoch [37/50], Class Loss: 0.0057, Discrepancy Loss: 0.0300
Epoch [38/50], Class Loss: 0.0085, Discrepancy Loss: 0.0251
Epoch [39/50], Class Loss: 0.1317, Discrepancy Loss: 0.0285
Epoch [40/50], Class Loss: 0.0105, Discrepancy Loss: 0.0277
Epoch [41/50], Class Loss: 0.0141, Discrepancy Loss: 0.0277
Epoch [42/50], Class Loss: 0.0092, Discrepancy Loss: 0.0256
Epoch [43/50], Class Loss: 0.0102, Discrepancy Loss: 0.0278
Epoch [44/50], Class Loss: 0.0108, Discrepancy Loss: 0.0306
Epoch [45/50], Class Loss: 0.0078, Discrepancy Loss: 0.0267
Epoch [46/50], Class Loss: 0.0072, Discrepancy Loss: 0.0281
Epoch [47/50], Class Loss: 0.0076, Discrepancy Loss: 0.0284
Epoch [48/50], Class Loss: 0.0068, Discrepancy Loss: 0.0261
Epoch [49/50], Class Loss: 0.0041, Discrepancy Loss: 0.0314
Epoch [50/50], Class Loss: 0.0103, Discrepancy Loss: 0.0284
Source Domain Performance - Accuracy: 70.92%, Precision: 72.61%, Recall: 72.46%, F1 Score: 67.42%
Target Domain Performance - Accuracy: 66.37%, Precision: 70.93%, Recall: 65.52%, F1 Score: 61.99%

Run 8/10
Epoch [1/50], Class Loss: 1.8190, Discrepancy Loss: 0.1200
Epoch [2/50], Class Loss: 0.7523, Discrepancy Loss: 0.0852
Epoch [3/50], Class Loss: 0.2176, Discrepancy Loss: 0.0632
Epoch [4/50], Class Loss: 0.1004, Discrepancy Loss: 0.0498
Epoch [5/50], Class Loss: 0.0860, Discrepancy Loss: 0.0404
Epoch [6/50], Class Loss: 0.0920, Discrepancy Loss: 0.0363
Epoch [7/50], Class Loss: 0.0643, Discrepancy Loss: 0.0380
Epoch [8/50], Class Loss: 0.2796, Discrepancy Loss: 0.0837
Epoch [9/50], Class Loss: 0.0725, Discrepancy Loss: 0.0362
Epoch [10/50], Class Loss: 0.0623, Discrepancy Loss: 0.0321
Epoch [11/50], Class Loss: 0.0201, Discrepancy Loss: 0.0308
Epoch [12/50], Class Loss: 0.0969, Discrepancy Loss: 0.0289
Epoch [13/50], Class Loss: 0.0204, Discrepancy Loss: 0.0285
Epoch [14/50], Class Loss: 0.0349, Discrepancy Loss: 0.0259
Epoch [15/50], Class Loss: 0.0598, Discrepancy Loss: 0.0269
Epoch [16/50], Class Loss: 0.0442, Discrepancy Loss: 0.0261
Epoch [17/50], Class Loss: 0.0250, Discrepancy Loss: 0.0276
Epoch [18/50], Class Loss: 0.0176, Discrepancy Loss: 0.0294
Epoch [19/50], Class Loss: 0.0194, Discrepancy Loss: 0.0258
Epoch [20/50], Class Loss: 0.0225, Discrepancy Loss: 0.0277
Epoch [21/50], Class Loss: 0.0218, Discrepancy Loss: 0.0290
Epoch [22/50], Class Loss: 0.0228, Discrepancy Loss: 0.0289
Epoch [23/50], Class Loss: 0.0325, Discrepancy Loss: 0.0263
Epoch [24/50], Class Loss: 0.0214, Discrepancy Loss: 0.0230
Epoch [25/50], Class Loss: 0.0191, Discrepancy Loss: 0.0266
Epoch [26/50], Class Loss: 0.0626, Discrepancy Loss: 0.0264
Epoch [27/50], Class Loss: 0.0350, Discrepancy Loss: 0.0218
Epoch [28/50], Class Loss: 0.0216, Discrepancy Loss: 0.0311
Epoch [29/50], Class Loss: 0.0146, Discrepancy Loss: 0.0249
Epoch [30/50], Class Loss: 0.0181, Discrepancy Loss: 0.0248
Epoch [31/50], Class Loss: 0.0261, Discrepancy Loss: 0.0210
Epoch [32/50], Class Loss: 0.0236, Discrepancy Loss: 0.0265
Epoch [33/50], Class Loss: 0.0141, Discrepancy Loss: 0.0223
Epoch [34/50], Class Loss: 0.0198, Discrepancy Loss: 0.0210
Epoch [35/50], Class Loss: 0.0543, Discrepancy Loss: 0.0223
Epoch [36/50], Class Loss: 0.0179, Discrepancy Loss: 0.0250
Epoch [37/50], Class Loss: 0.0161, Discrepancy Loss: 0.0248
Epoch [38/50], Class Loss: 0.0167, Discrepancy Loss: 0.0235
Epoch [39/50], Class Loss: 0.0226, Discrepancy Loss: 0.0224
Epoch [40/50], Class Loss: 0.0249, Discrepancy Loss: 0.0260
Epoch [41/50], Class Loss: 0.0155, Discrepancy Loss: 0.0203
Epoch [42/50], Class Loss: 0.0167, Discrepancy Loss: 0.0246
Epoch [43/50], Class Loss: 0.0155, Discrepancy Loss: 0.0231
Epoch [44/50], Class Loss: 0.0532, Discrepancy Loss: 0.0236
Epoch [45/50], Class Loss: 0.0288, Discrepancy Loss: 0.0262
Epoch [46/50], Class Loss: 0.0216, Discrepancy Loss: 0.0207
Epoch [47/50], Class Loss: 0.1301, Discrepancy Loss: 0.0311
Epoch [48/50], Class Loss: 0.0405, Discrepancy Loss: 0.0222
Epoch [49/50], Class Loss: 0.0187, Discrepancy Loss: 0.0200
Epoch [50/50], Class Loss: 0.0163, Discrepancy Loss: 0.0252
Source Domain Performance - Accuracy: 76.38%, Precision: 85.14%, Recall: 78.05%, F1 Score: 71.77%
Target Domain Performance - Accuracy: 76.14%, Precision: 83.64%, Recall: 75.41%, F1 Score: 74.38%

Run 9/10
Epoch [1/50], Class Loss: 1.3064, Discrepancy Loss: 0.1096
Epoch [2/50], Class Loss: 0.1687, Discrepancy Loss: 0.0525
Epoch [3/50], Class Loss: 0.1471, Discrepancy Loss: 0.0572
Epoch [4/50], Class Loss: 0.0846, Discrepancy Loss: 0.0458
Epoch [5/50], Class Loss: 0.0814, Discrepancy Loss: 0.0387
Epoch [6/50], Class Loss: 0.0347, Discrepancy Loss: 0.0335
Epoch [7/50], Class Loss: 0.0426, Discrepancy Loss: 0.0261
Epoch [8/50], Class Loss: 0.0636, Discrepancy Loss: 0.0386
Epoch [9/50], Class Loss: 0.0614, Discrepancy Loss: 0.0316
Epoch [10/50], Class Loss: 0.1065, Discrepancy Loss: 0.0354
Epoch [11/50], Class Loss: 0.0992, Discrepancy Loss: 0.0355
Epoch [12/50], Class Loss: 0.0239, Discrepancy Loss: 0.0340
Epoch [13/50], Class Loss: 0.0143, Discrepancy Loss: 0.0328
Epoch [14/50], Class Loss: 0.0131, Discrepancy Loss: 0.0316
Epoch [15/50], Class Loss: 0.0157, Discrepancy Loss: 0.0219
Epoch [16/50], Class Loss: 0.0118, Discrepancy Loss: 0.0243
Epoch [17/50], Class Loss: 0.0122, Discrepancy Loss: 0.0225
Epoch [18/50], Class Loss: 0.0129, Discrepancy Loss: 0.0208
Epoch [19/50], Class Loss: 0.0149, Discrepancy Loss: 0.0248
Epoch [20/50], Class Loss: 0.0176, Discrepancy Loss: 0.0198
Epoch [21/50], Class Loss: 0.0133, Discrepancy Loss: 0.0212
Epoch [22/50], Class Loss: 0.0127, Discrepancy Loss: 0.0234
Epoch [23/50], Class Loss: 0.0103, Discrepancy Loss: 0.0187
Epoch [24/50], Class Loss: 0.0098, Discrepancy Loss: 0.0180
Epoch [25/50], Class Loss: 0.0074, Discrepancy Loss: 0.0155
Epoch [26/50], Class Loss: 0.0130, Discrepancy Loss: 0.0162
Epoch [27/50], Class Loss: 0.0070, Discrepancy Loss: 0.0147
Epoch [28/50], Class Loss: 0.0093, Discrepancy Loss: 0.0141
Epoch [29/50], Class Loss: 0.0353, Discrepancy Loss: 0.0186
Epoch [30/50], Class Loss: 0.0154, Discrepancy Loss: 0.0180
Epoch [31/50], Class Loss: 0.0070, Discrepancy Loss: 0.0164
Epoch [32/50], Class Loss: 0.0067, Discrepancy Loss: 0.0183
Epoch [33/50], Class Loss: 0.0117, Discrepancy Loss: 0.0173
Epoch [34/50], Class Loss: 0.0364, Discrepancy Loss: 0.0164
Epoch [35/50], Class Loss: 0.0445, Discrepancy Loss: 0.0159
Epoch [36/50], Class Loss: 0.0732, Discrepancy Loss: 0.0181
Epoch [37/50], Class Loss: 0.0073, Discrepancy Loss: 0.0160
Epoch [38/50], Class Loss: 0.0054, Discrepancy Loss: 0.0185
Epoch [39/50], Class Loss: 0.0079, Discrepancy Loss: 0.0146
Epoch [40/50], Class Loss: 0.0049, Discrepancy Loss: 0.0164
Epoch [41/50], Class Loss: 0.0118, Discrepancy Loss: 0.0161
Epoch [42/50], Class Loss: 0.0044, Discrepancy Loss: 0.0164
Epoch [43/50], Class Loss: 0.0077, Discrepancy Loss: 0.0160
Epoch [44/50], Class Loss: 0.0051, Discrepancy Loss: 0.0158
Epoch [45/50], Class Loss: 0.0036, Discrepancy Loss: 0.0184
Epoch [46/50], Class Loss: 0.0045, Discrepancy Loss: 0.0199
Epoch [47/50], Class Loss: 0.0060, Discrepancy Loss: 0.0173
Epoch [48/50], Class Loss: 0.0162, Discrepancy Loss: 0.0179
Epoch [49/50], Class Loss: 0.0069, Discrepancy Loss: 0.0144
Epoch [50/50], Class Loss: 0.0074, Discrepancy Loss: 0.0162
Source Domain Performance - Accuracy: 48.20%, Precision: 33.46%, Recall: 48.79%, F1 Score: 37.02%
Target Domain Performance - Accuracy: 49.10%, Precision: 38.78%, Recall: 48.56%, F1 Score: 38.43%

Run 10/10
Epoch [1/50], Class Loss: 1.9202, Discrepancy Loss: 0.1479
Epoch [2/50], Class Loss: 0.4565, Discrepancy Loss: 0.0937
Epoch [3/50], Class Loss: 0.2350, Discrepancy Loss: 0.0675
Epoch [4/50], Class Loss: 0.1097, Discrepancy Loss: 0.0448
Epoch [5/50], Class Loss: 0.0818, Discrepancy Loss: 0.0362
Epoch [6/50], Class Loss: 0.2111, Discrepancy Loss: 0.0496
Epoch [7/50], Class Loss: 0.1523, Discrepancy Loss: 0.0523
Epoch [8/50], Class Loss: 0.1200, Discrepancy Loss: 0.0449
Epoch [9/50], Class Loss: 0.0440, Discrepancy Loss: 0.0408
Epoch [10/50], Class Loss: 0.0508, Discrepancy Loss: 0.0534
Epoch [11/50], Class Loss: 0.0277, Discrepancy Loss: 0.0344
Epoch [12/50], Class Loss: 0.0151, Discrepancy Loss: 0.0366
Epoch [13/50], Class Loss: 0.0207, Discrepancy Loss: 0.0306
Epoch [14/50], Class Loss: 0.0308, Discrepancy Loss: 0.0313
Epoch [15/50], Class Loss: 0.0099, Discrepancy Loss: 0.0293
Epoch [16/50], Class Loss: 0.0250, Discrepancy Loss: 0.0312
Epoch [17/50], Class Loss: 0.0200, Discrepancy Loss: 0.0254
Epoch [18/50], Class Loss: 0.0076, Discrepancy Loss: 0.0265
Epoch [19/50], Class Loss: 0.0237, Discrepancy Loss: 0.0249
Epoch [20/50], Class Loss: 0.0862, Discrepancy Loss: 0.0301
Epoch [21/50], Class Loss: 0.0223, Discrepancy Loss: 0.0349
Epoch [22/50], Class Loss: 0.0112, Discrepancy Loss: 0.0346
Epoch [23/50], Class Loss: 0.0172, Discrepancy Loss: 0.0340
Epoch [24/50], Class Loss: 0.0156, Discrepancy Loss: 0.0310
Epoch [25/50], Class Loss: 0.0258, Discrepancy Loss: 0.0336
Epoch [26/50], Class Loss: 0.0210, Discrepancy Loss: 0.0314
Epoch [27/50], Class Loss: 0.0111, Discrepancy Loss: 0.0314
Epoch [28/50], Class Loss: 0.0097, Discrepancy Loss: 0.0313
Epoch [29/50], Class Loss: 0.0145, Discrepancy Loss: 0.0330
Epoch [30/50], Class Loss: 0.0089, Discrepancy Loss: 0.0314
Epoch [31/50], Class Loss: 0.0081, Discrepancy Loss: 0.0312
Epoch [32/50], Class Loss: 0.0320, Discrepancy Loss: 0.0327
Epoch [33/50], Class Loss: 0.0983, Discrepancy Loss: 0.0310
Epoch [34/50], Class Loss: 0.0203, Discrepancy Loss: 0.0303
Epoch [35/50], Class Loss: 0.0169, Discrepancy Loss: 0.0305
Epoch [36/50], Class Loss: 0.0077, Discrepancy Loss: 0.0290
Epoch [37/50], Class Loss: 0.0071, Discrepancy Loss: 0.0285
Epoch [38/50], Class Loss: 0.0092, Discrepancy Loss: 0.0334
Epoch [39/50], Class Loss: 0.0123, Discrepancy Loss: 0.0341
Epoch [40/50], Class Loss: 0.0087, Discrepancy Loss: 0.0295
Epoch [41/50], Class Loss: 0.1061, Discrepancy Loss: 0.0251
Epoch [42/50], Class Loss: 0.0073, Discrepancy Loss: 0.0254
Epoch [43/50], Class Loss: 0.0097, Discrepancy Loss: 0.0289
Epoch [44/50], Class Loss: 0.0096, Discrepancy Loss: 0.0276
Epoch [45/50], Class Loss: 0.0121, Discrepancy Loss: 0.0297
Epoch [46/50], Class Loss: 0.0165, Discrepancy Loss: 0.0284
Epoch [47/50], Class Loss: 0.0291, Discrepancy Loss: 0.0289
Epoch [48/50], Class Loss: 0.1377, Discrepancy Loss: 0.0289
Epoch [49/50], Class Loss: 0.0506, Discrepancy Loss: 0.0312
Epoch [50/50], Class Loss: 0.0214, Discrepancy Loss: 0.0295
Source Domain Performance - Accuracy: 68.29%, Precision: 64.93%, Recall: 69.87%, F1 Score: 64.27%
Target Domain Performance - Accuracy: 65.95%, Precision: 70.76%, Recall: 65.09%, F1 Score: 61.76%

Source performance: 71.21% 73.46% 72.42% 67.09%
Target performance: 66.72% 71.52% 65.89% 61.63%

Per-Class Accuracy on Target Domain:
bpsk: 99.98%
qpsk: 10.73%
16qam: 74.73%
8apsk: 78.14%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.5387, Discrepancy Loss: 0.0740
Validation Loss: 1.4331
Epoch [2/50], Class Loss: 0.1275, Discrepancy Loss: 0.1067
Validation Loss: 0.8516
Epoch [3/50], Class Loss: 0.1872, Discrepancy Loss: 0.1896
Validation Loss: 10.3524
Epoch [4/50], Class Loss: 0.0977, Discrepancy Loss: 0.2188
Validation Loss: 0.4094
Epoch [5/50], Class Loss: 0.1238, Discrepancy Loss: 0.1076
Validation Loss: 0.4999
Epoch [6/50], Class Loss: 0.1578, Discrepancy Loss: 0.3087
Validation Loss: 1.5474
Epoch [7/50], Class Loss: 0.0591, Discrepancy Loss: 0.1562
Validation Loss: 0.7034
Epoch [8/50], Class Loss: 0.1646, Discrepancy Loss: 0.0330
Validation Loss: 0.2601
Epoch [9/50], Class Loss: 0.1255, Discrepancy Loss: 0.0707
Validation Loss: 0.0345
Epoch [10/50], Class Loss: 0.0946, Discrepancy Loss: 0.1204
Validation Loss: 2.5310
Epoch [11/50], Class Loss: 0.0420, Discrepancy Loss: 0.0692
Validation Loss: 0.0340
Epoch [12/50], Class Loss: 0.0253, Discrepancy Loss: 0.0231
Validation Loss: 0.0163
Epoch [13/50], Class Loss: 0.1904, Discrepancy Loss: 0.0562
Validation Loss: 0.0181
Epoch [14/50], Class Loss: 0.0850, Discrepancy Loss: 0.0706
Validation Loss: 0.0505
Epoch [15/50], Class Loss: 0.0228, Discrepancy Loss: 0.0861
Validation Loss: 0.0312
Epoch [16/50], Class Loss: 0.0213, Discrepancy Loss: 0.0857
Validation Loss: 0.0172
Epoch [17/50], Class Loss: 0.0202, Discrepancy Loss: 0.0771
Validation Loss: 0.0356
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 18.05%, Precision: 14.11%, Recall: 17.68%, F1 Score: 15.65%

Run 2/10
Epoch [1/50], Class Loss: 0.5870, Discrepancy Loss: 0.0481
Validation Loss: 6.3088
Epoch [2/50], Class Loss: 0.0707, Discrepancy Loss: 0.0290
Validation Loss: 5.2138
Epoch [3/50], Class Loss: 0.0657, Discrepancy Loss: 0.0698
Validation Loss: 0.1815
Epoch [4/50], Class Loss: 0.1530, Discrepancy Loss: 0.2014
Validation Loss: 0.1295
Epoch [5/50], Class Loss: 0.1635, Discrepancy Loss: 0.1026
Validation Loss: 0.8526
Epoch [6/50], Class Loss: 0.1165, Discrepancy Loss: 0.0545
Validation Loss: 2.3112
Epoch [7/50], Class Loss: 0.2726, Discrepancy Loss: 0.0824
Validation Loss: 0.2681
Epoch [8/50], Class Loss: 0.1563, Discrepancy Loss: 0.2058
Validation Loss: 5.4665
Epoch [9/50], Class Loss: 0.0364, Discrepancy Loss: 0.1895
Validation Loss: 3.6098
Early stopping!
Source Domain Performance - Accuracy: 76.56%, Precision: 60.93%, Recall: 75.00%, F1 Score: 66.43%
Target Domain Performance - Accuracy: 24.04%, Precision: 10.40%, Recall: 23.84%, F1 Score: 12.00%

Run 3/10
Epoch [1/50], Class Loss: 0.5453, Discrepancy Loss: 0.0597
Validation Loss: 3.8150
Epoch [2/50], Class Loss: 0.0720, Discrepancy Loss: 0.1002
Validation Loss: 4.8856
Epoch [3/50], Class Loss: 0.0976, Discrepancy Loss: 0.1249
Validation Loss: 0.4900
Epoch [4/50], Class Loss: 0.0772, Discrepancy Loss: 0.0538
Validation Loss: 5.8776
Epoch [5/50], Class Loss: 0.0995, Discrepancy Loss: 0.1147
Validation Loss: 0.3049
Epoch [6/50], Class Loss: 0.0848, Discrepancy Loss: 0.0418
Validation Loss: 1.4425
Epoch [7/50], Class Loss: 0.1109, Discrepancy Loss: 0.0558
Validation Loss: 0.0349
Epoch [8/50], Class Loss: 0.2495, Discrepancy Loss: 0.1712
Validation Loss: 5.0501
Epoch [9/50], Class Loss: 0.0490, Discrepancy Loss: 0.0870
Validation Loss: 0.0232
Epoch [10/50], Class Loss: 0.0713, Discrepancy Loss: 0.0520
Validation Loss: 12.6039
Epoch [11/50], Class Loss: 0.0600, Discrepancy Loss: 0.0195
Validation Loss: 0.0182
Epoch [12/50], Class Loss: 0.0073, Discrepancy Loss: 0.0095
Validation Loss: 0.0009
Epoch [13/50], Class Loss: 0.0182, Discrepancy Loss: 0.0080
Validation Loss: 0.0038
Epoch [14/50], Class Loss: 0.0230, Discrepancy Loss: 0.0088
Validation Loss: 0.0129
Epoch [15/50], Class Loss: 0.0096, Discrepancy Loss: 0.0090
Validation Loss: 0.0027
Epoch [16/50], Class Loss: 0.0106, Discrepancy Loss: 0.0106
Validation Loss: 0.0007
Epoch [17/50], Class Loss: 0.0053, Discrepancy Loss: 0.0119
Validation Loss: 0.0026
Epoch [18/50], Class Loss: 0.0050, Discrepancy Loss: 0.0096
Validation Loss: 0.0007
Epoch [19/50], Class Loss: 0.0054, Discrepancy Loss: 0.0084
Validation Loss: 0.0111
Epoch [20/50], Class Loss: 0.0032, Discrepancy Loss: 0.0088
Validation Loss: 0.0005
Epoch [21/50], Class Loss: 0.0026, Discrepancy Loss: 0.0079
Validation Loss: 0.0004
Epoch [22/50], Class Loss: 0.0399, Discrepancy Loss: 0.0074
Validation Loss: 0.0004
Epoch [23/50], Class Loss: 0.0034, Discrepancy Loss: 0.0074
Validation Loss: 0.0004
Epoch [24/50], Class Loss: 0.0040, Discrepancy Loss: 0.0076
Validation Loss: 0.0003
Epoch [25/50], Class Loss: 0.0021, Discrepancy Loss: 0.0075
Validation Loss: 0.0007
Epoch [26/50], Class Loss: 0.0036, Discrepancy Loss: 0.0077
Validation Loss: 0.0004
Epoch [27/50], Class Loss: 0.0091, Discrepancy Loss: 0.0078
Validation Loss: 0.0004
Epoch [28/50], Class Loss: 0.0036, Discrepancy Loss: 0.0083
Validation Loss: 0.0008
Epoch [29/50], Class Loss: 0.0042, Discrepancy Loss: 0.0079
Validation Loss: 0.0003
Epoch [30/50], Class Loss: 0.0028, Discrepancy Loss: 0.0079
Validation Loss: 0.0004
Epoch [31/50], Class Loss: 0.0010, Discrepancy Loss: 0.0079
Validation Loss: 0.0003
Epoch [32/50], Class Loss: 0.0028, Discrepancy Loss: 0.0079
Validation Loss: 0.0005
Epoch [33/50], Class Loss: 0.0022, Discrepancy Loss: 0.0078
Validation Loss: 0.0004
Epoch [34/50], Class Loss: 0.0008, Discrepancy Loss: 0.0080
Validation Loss: 0.0003
Epoch [35/50], Class Loss: 0.0282, Discrepancy Loss: 0.0080
Validation Loss: 0.0003
Epoch [36/50], Class Loss: 0.0005, Discrepancy Loss: 0.0081
Validation Loss: 0.0006
Epoch [37/50], Class Loss: 0.0033, Discrepancy Loss: 0.0081
Validation Loss: 0.0003
Epoch [38/50], Class Loss: 0.0189, Discrepancy Loss: 0.0082
Validation Loss: 0.0003
Epoch [39/50], Class Loss: 0.0025, Discrepancy Loss: 0.0079
Validation Loss: 0.0003
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 51.92%, Precision: 47.73%, Recall: 51.12%, F1 Score: 44.09%

Run 4/10
Epoch [1/50], Class Loss: 0.5435, Discrepancy Loss: 0.0831
Validation Loss: 0.4015
Epoch [2/50], Class Loss: 0.1680, Discrepancy Loss: 0.1370
Validation Loss: 0.1905
Epoch [3/50], Class Loss: 0.1215, Discrepancy Loss: 0.1330
Validation Loss: 1.3701
Epoch [4/50], Class Loss: 0.2830, Discrepancy Loss: 0.1700
Validation Loss: 0.1071
Epoch [5/50], Class Loss: 0.0677, Discrepancy Loss: 0.1624
Validation Loss: 6.4489
Epoch [6/50], Class Loss: 0.2347, Discrepancy Loss: 0.1692
Validation Loss: 0.0346
Epoch [7/50], Class Loss: 0.0984, Discrepancy Loss: 0.1581
Validation Loss: 1.8664
Epoch [8/50], Class Loss: 0.0570, Discrepancy Loss: 0.1835
Validation Loss: 0.0109
Epoch [9/50], Class Loss: 0.0225, Discrepancy Loss: 0.1879
Validation Loss: 1.8259
Epoch [10/50], Class Loss: 0.1668, Discrepancy Loss: 0.1060
Validation Loss: 0.4462
Epoch [11/50], Class Loss: 0.0284, Discrepancy Loss: 0.1271
Validation Loss: 0.0332
Epoch [12/50], Class Loss: 0.0198, Discrepancy Loss: 0.1850
Validation Loss: 0.0177
Epoch [13/50], Class Loss: 0.0217, Discrepancy Loss: 0.1566
Validation Loss: 0.0446
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 11.99%, Precision: 29.14%, Recall: 11.82%, F1 Score: 8.42%

Run 5/10
Epoch [1/50], Class Loss: 0.5866, Discrepancy Loss: 0.0231
Validation Loss: 0.8345
Epoch [2/50], Class Loss: 0.1152, Discrepancy Loss: 0.0309
Validation Loss: 0.5604
Epoch [3/50], Class Loss: 0.1783, Discrepancy Loss: 0.0797
Validation Loss: 2.1033
Epoch [4/50], Class Loss: 0.1209, Discrepancy Loss: 0.1082
Validation Loss: 1.4272
Epoch [5/50], Class Loss: 0.0918, Discrepancy Loss: 0.1850
Validation Loss: 1.4080
Epoch [6/50], Class Loss: 0.0916, Discrepancy Loss: 0.1939
Validation Loss: 0.0280
Epoch [7/50], Class Loss: 0.1319, Discrepancy Loss: 0.1668
Validation Loss: 0.5269
Epoch [8/50], Class Loss: 0.1116, Discrepancy Loss: 0.0766
Validation Loss: 12.3808
Epoch [9/50], Class Loss: 0.0489, Discrepancy Loss: 0.0281
Validation Loss: 0.6796
Epoch [10/50], Class Loss: 0.0508, Discrepancy Loss: 0.0553
Validation Loss: 4.5325
Epoch [11/50], Class Loss: 0.0476, Discrepancy Loss: 0.0032
Validation Loss: 0.0097
Epoch [12/50], Class Loss: 0.0077, Discrepancy Loss: 0.0030
Validation Loss: 0.0016
Epoch [13/50], Class Loss: 0.0484, Discrepancy Loss: 0.0017
Validation Loss: 0.0024
Epoch [14/50], Class Loss: 0.0221, Discrepancy Loss: 0.0016
Validation Loss: 0.0016
Epoch [15/50], Class Loss: 0.0056, Discrepancy Loss: 0.0033
Validation Loss: 0.0010
Epoch [16/50], Class Loss: 0.0075, Discrepancy Loss: 0.0025
Validation Loss: 0.0055
Epoch [17/50], Class Loss: 0.0020, Discrepancy Loss: 0.0046
Validation Loss: 0.0040
Epoch [18/50], Class Loss: 0.0156, Discrepancy Loss: 0.1100
Validation Loss: 0.3269
Epoch [19/50], Class Loss: 0.0090, Discrepancy Loss: 0.0893
Validation Loss: 0.1315
Epoch [20/50], Class Loss: 0.0064, Discrepancy Loss: 0.1182
Validation Loss: 0.0047
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 34.05%, Precision: 35.14%, Recall: 33.61%, F1 Score: 25.68%

Run 6/10
Epoch [1/50], Class Loss: 0.6063, Discrepancy Loss: 0.0822
Validation Loss: 2.1136
Epoch [2/50], Class Loss: 0.1857, Discrepancy Loss: 0.0539
Validation Loss: 0.3947
Epoch [3/50], Class Loss: 0.0943, Discrepancy Loss: 0.1777
Validation Loss: 7.3706
Epoch [4/50], Class Loss: 0.1079, Discrepancy Loss: 0.2049
Validation Loss: 0.6657
Epoch [5/50], Class Loss: 0.0933, Discrepancy Loss: 0.2125
Validation Loss: 1.3116
Epoch [6/50], Class Loss: 0.0651, Discrepancy Loss: 0.1845
Validation Loss: 5.2412
Epoch [7/50], Class Loss: 0.1017, Discrepancy Loss: 0.1922
Validation Loss: 1.8247
Early stopping!
Source Domain Performance - Accuracy: 75.06%, Precision: 82.64%, Recall: 74.27%, F1 Score: 73.63%
Target Domain Performance - Accuracy: 25.54%, Precision: 7.47%, Recall: 24.94%, F1 Score: 10.61%

Run 7/10
Epoch [1/50], Class Loss: 0.5435, Discrepancy Loss: 0.0669
Validation Loss: 2.0967
Epoch [2/50], Class Loss: 0.1462, Discrepancy Loss: 0.0872
Validation Loss: 12.0493
Epoch [3/50], Class Loss: 0.3528, Discrepancy Loss: 0.0549
Validation Loss: 0.4426
Epoch [4/50], Class Loss: 0.0502, Discrepancy Loss: 0.0785
Validation Loss: 0.0334
Epoch [5/50], Class Loss: 0.1139, Discrepancy Loss: 0.3041
Validation Loss: 1.5074
Epoch [6/50], Class Loss: 0.1220, Discrepancy Loss: 0.1850
Validation Loss: 2.7655
Epoch [7/50], Class Loss: 0.2407, Discrepancy Loss: 0.0904
Validation Loss: 0.3721
Epoch [8/50], Class Loss: 0.1453, Discrepancy Loss: 0.0878
Validation Loss: 0.0179
Epoch [9/50], Class Loss: 0.0515, Discrepancy Loss: 0.0111
Validation Loss: 2.5908
Epoch [10/50], Class Loss: 0.0363, Discrepancy Loss: 0.0129
Validation Loss: 0.4407
Epoch [11/50], Class Loss: 0.0128, Discrepancy Loss: 0.0087
Validation Loss: 0.0010
Epoch [12/50], Class Loss: 0.0085, Discrepancy Loss: 0.0053
Validation Loss: 0.0003
Epoch [13/50], Class Loss: 0.0064, Discrepancy Loss: 0.0055
Validation Loss: 0.0002
Epoch [14/50], Class Loss: 0.0325, Discrepancy Loss: 0.0058
Validation Loss: 0.0002
Epoch [15/50], Class Loss: 0.0863, Discrepancy Loss: 0.0066
Validation Loss: 0.0006
Epoch [16/50], Class Loss: 0.0056, Discrepancy Loss: 0.0080
Validation Loss: 0.0013
Epoch [17/50], Class Loss: 0.0373, Discrepancy Loss: 0.0089
Validation Loss: 0.0005
Epoch [18/50], Class Loss: 0.0023, Discrepancy Loss: 0.0099
Validation Loss: 0.0048
Epoch [19/50], Class Loss: 0.0481, Discrepancy Loss: 0.0133
Validation Loss: 0.0060
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.87%, Recall: 99.89%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 43.94%, Precision: 41.67%, Recall: 43.45%, F1 Score: 33.62%

Run 8/10
Epoch [1/50], Class Loss: 0.5456, Discrepancy Loss: 0.0468
Validation Loss: 12.6387
Epoch [2/50], Class Loss: 0.0827, Discrepancy Loss: 0.0657
Validation Loss: 0.0584
Epoch [3/50], Class Loss: 0.0976, Discrepancy Loss: 0.1655
Validation Loss: 10.4393
Epoch [4/50], Class Loss: 0.1492, Discrepancy Loss: 0.1716
Validation Loss: 0.1511
Epoch [5/50], Class Loss: 0.1081, Discrepancy Loss: 0.0788
Validation Loss: 1.3426
Epoch [6/50], Class Loss: 0.0404, Discrepancy Loss: 0.1905
Validation Loss: 0.0235
Epoch [7/50], Class Loss: 0.0580, Discrepancy Loss: 0.2944
Validation Loss: 0.3884
Epoch [8/50], Class Loss: 0.0724, Discrepancy Loss: 0.0452
Validation Loss: 0.1272
Epoch [9/50], Class Loss: 0.0619, Discrepancy Loss: 0.0644
Validation Loss: 1.2232
Epoch [10/50], Class Loss: 0.0460, Discrepancy Loss: 0.2523
Validation Loss: 1.5405
Epoch [11/50], Class Loss: 0.0172, Discrepancy Loss: 0.1096
Validation Loss: 0.0053
Epoch [12/50], Class Loss: 0.0181, Discrepancy Loss: 0.2033
Validation Loss: 0.3786
Epoch [13/50], Class Loss: 0.0199, Discrepancy Loss: 0.3699
Validation Loss: 0.4345
Epoch [14/50], Class Loss: 0.0116, Discrepancy Loss: 0.3352
Validation Loss: 0.0014
Epoch [15/50], Class Loss: 0.0163, Discrepancy Loss: 0.2428
Validation Loss: 0.0564
Epoch [16/50], Class Loss: 0.0310, Discrepancy Loss: 0.0993
Validation Loss: 0.0163
Epoch [17/50], Class Loss: 0.0218, Discrepancy Loss: 0.1717
Validation Loss: 0.0266
Epoch [18/50], Class Loss: 0.0138, Discrepancy Loss: 0.2175
Validation Loss: 0.0072
Epoch [19/50], Class Loss: 0.0182, Discrepancy Loss: 0.1237
Validation Loss: 0.0118
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.89%, Recall: 99.87%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 30.64%, Precision: 16.73%, Recall: 30.23%, F1 Score: 20.09%

Run 9/10
Epoch [1/50], Class Loss: 0.5896, Discrepancy Loss: 0.0408
Validation Loss: 0.6916
Epoch [2/50], Class Loss: 0.1274, Discrepancy Loss: 0.0341
Validation Loss: 0.0995
Epoch [3/50], Class Loss: 0.2723, Discrepancy Loss: 0.0497
Validation Loss: 0.6027
Epoch [4/50], Class Loss: 0.1098, Discrepancy Loss: 0.0386
Validation Loss: 6.1250
Epoch [5/50], Class Loss: 0.0475, Discrepancy Loss: 0.0127
Validation Loss: 0.0234
Epoch [6/50], Class Loss: 0.0657, Discrepancy Loss: 0.1044
Validation Loss: 7.7015
Epoch [7/50], Class Loss: 0.1017, Discrepancy Loss: 0.1752
Validation Loss: 5.0641
Epoch [8/50], Class Loss: 0.0619, Discrepancy Loss: 0.1655
Validation Loss: 5.8339
Epoch [9/50], Class Loss: 0.1017, Discrepancy Loss: 0.0308
Validation Loss: 0.0494
Epoch [10/50], Class Loss: 0.0588, Discrepancy Loss: 0.1294
Validation Loss: 0.1570
Early stopping!
Source Domain Performance - Accuracy: 98.08%, Precision: 98.13%, Recall: 98.18%, F1 Score: 98.13%
Target Domain Performance - Accuracy: 54.56%, Precision: 59.01%, Recall: 53.58%, F1 Score: 45.09%

Run 10/10
Epoch [1/50], Class Loss: 0.5427, Discrepancy Loss: 0.0318
Validation Loss: 5.1975
Epoch [2/50], Class Loss: 0.1383, Discrepancy Loss: 0.0252
Validation Loss: 0.1400
Epoch [3/50], Class Loss: 0.2496, Discrepancy Loss: 0.0961
Validation Loss: 2.6842
Epoch [4/50], Class Loss: 0.1192, Discrepancy Loss: 0.1044
Validation Loss: 9.6636
Epoch [5/50], Class Loss: 0.1505, Discrepancy Loss: 0.2080
Validation Loss: 1.3364
Epoch [6/50], Class Loss: 0.0833, Discrepancy Loss: 0.2252
Validation Loss: 0.0599
Epoch [7/50], Class Loss: 0.0514, Discrepancy Loss: 0.1530
Validation Loss: 0.2102
Epoch [8/50], Class Loss: 0.1576, Discrepancy Loss: 0.0214
Validation Loss: 2.0724
Epoch [9/50], Class Loss: 0.2141, Discrepancy Loss: 0.0652
Validation Loss: 0.0894
Epoch [10/50], Class Loss: 0.0833, Discrepancy Loss: 0.0919
Validation Loss: 5.3950
Epoch [11/50], Class Loss: 0.0146, Discrepancy Loss: 0.1128
Validation Loss: 0.0033
Epoch [12/50], Class Loss: 0.0056, Discrepancy Loss: 0.0457
Validation Loss: 0.0027
Epoch [13/50], Class Loss: 0.0044, Discrepancy Loss: 0.0654
Validation Loss: 0.0061
Epoch [14/50], Class Loss: 0.0131, Discrepancy Loss: 0.0904
Validation Loss: 0.0183
Epoch [15/50], Class Loss: 0.0151, Discrepancy Loss: 0.0177
Validation Loss: 0.0086
Epoch [16/50], Class Loss: 0.0228, Discrepancy Loss: 0.0095
Validation Loss: 0.0090
Epoch [17/50], Class Loss: 0.0094, Discrepancy Loss: 0.0036
Validation Loss: 0.0062
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 52.52%, Precision: 57.86%, Recall: 51.55%, F1 Score: 40.97%

Source performance: 94.92% 94.12% 94.70% 93.77%
Target performance: 34.72% 31.93% 34.18% 25.62%

Per-Class Accuracy on Target Domain:
bpsk: 65.34%
qpsk: 0.00%
16qam: 42.69%
8apsk: 28.69%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.3108, JMMD Loss: 0.3608
Validation Loss: 13.0857
Epoch [2/50], Class Loss: 0.1452, JMMD Loss: 0.3960
Validation Loss: 0.0308
Epoch [3/50], Class Loss: 0.0283, JMMD Loss: 0.1867
Validation Loss: 5.6064
Epoch [4/50], Class Loss: 0.0907, JMMD Loss: 0.2953
Validation Loss: 0.0848
Epoch [5/50], Class Loss: 0.0295, JMMD Loss: 0.1774
Validation Loss: 0.0654
Epoch [6/50], Class Loss: 0.0182, JMMD Loss: 0.1111
Validation Loss: 0.7159
Epoch [7/50], Class Loss: 0.0239, JMMD Loss: 0.1156
Validation Loss: 0.2048
Early stopping!
Source Domain Performance - Accuracy: 92.45%, Precision: 94.05%, Recall: 92.92%, F1 Score: 92.74%
Target Domain Performance - Accuracy: 62.47%, Precision: 83.54%, Recall: 61.80%, F1 Score: 58.04%

Run 2/10
Epoch [1/50], Class Loss: 0.3613, JMMD Loss: 0.2378
Validation Loss: 6.9513
Epoch [2/50], Class Loss: 0.0745, JMMD Loss: 0.2102
Validation Loss: 14.6337
Epoch [3/50], Class Loss: 0.0534, JMMD Loss: 0.1530
Validation Loss: 59.7019
Epoch [4/50], Class Loss: 0.1676, JMMD Loss: 0.2799
Validation Loss: 7.9155
Epoch [5/50], Class Loss: 0.0393, JMMD Loss: 0.1758
Validation Loss: 11.3117
Epoch [6/50], Class Loss: 0.0261, JMMD Loss: 0.1013
Validation Loss: 10.8750
Early stopping!
Source Domain Performance - Accuracy: 48.92%, Precision: 37.79%, Recall: 49.44%, F1 Score: 38.79%
Target Domain Performance - Accuracy: 52.04%, Precision: 45.39%, Recall: 51.45%, F1 Score: 41.71%

Run 3/10
Epoch [1/50], Class Loss: 0.3472, JMMD Loss: 0.3524
Validation Loss: 0.3564
Epoch [2/50], Class Loss: 0.0946, JMMD Loss: 0.3907
Validation Loss: 26.5344
Epoch [3/50], Class Loss: 0.0458, JMMD Loss: 0.2164
Validation Loss: 0.7575
Epoch [4/50], Class Loss: 0.0341, JMMD Loss: 0.1675
Validation Loss: 7.2577
Epoch [5/50], Class Loss: 0.0379, JMMD Loss: 0.0941
Validation Loss: 1.4746
Epoch [6/50], Class Loss: 0.1493, JMMD Loss: 0.3894
Validation Loss: 1.5822
Early stopping!
Source Domain Performance - Accuracy: 59.11%, Precision: 84.92%, Recall: 58.65%, F1 Score: 53.62%
Target Domain Performance - Accuracy: 50.54%, Precision: 49.48%, Recall: 49.97%, F1 Score: 40.13%

Run 4/10
Epoch [1/50], Class Loss: 0.4099, JMMD Loss: 0.2777
Validation Loss: 2.1639
Epoch [2/50], Class Loss: 0.1028, JMMD Loss: 0.2782
Validation Loss: 0.0496
Epoch [3/50], Class Loss: 0.0309, JMMD Loss: 0.1514
Validation Loss: 0.0383
Epoch [4/50], Class Loss: 0.0617, JMMD Loss: 0.1263
Validation Loss: 0.0406
Epoch [5/50], Class Loss: 0.0245, JMMD Loss: 0.0817
Validation Loss: 1.9563
Epoch [6/50], Class Loss: 0.0399, JMMD Loss: 0.1141
Validation Loss: 4.8476
Epoch [7/50], Class Loss: 0.0788, JMMD Loss: 0.1847
Validation Loss: 0.2237
Epoch [8/50], Class Loss: 0.0566, JMMD Loss: 0.1563
Validation Loss: 0.3349
Early stopping!
Source Domain Performance - Accuracy: 88.73%, Precision: 91.15%, Recall: 89.35%, F1 Score: 89.03%
Target Domain Performance - Accuracy: 69.48%, Precision: 75.62%, Recall: 69.48%, F1 Score: 69.85%

Run 5/10
Epoch [1/50], Class Loss: 0.3971, JMMD Loss: 0.2872
Validation Loss: 22.8044
Epoch [2/50], Class Loss: 0.1523, JMMD Loss: 0.2445
Validation Loss: 10.6117
Epoch [3/50], Class Loss: 0.0462, JMMD Loss: 0.1544
Validation Loss: 5.7531
Epoch [4/50], Class Loss: 0.0383, JMMD Loss: 0.1153
Validation Loss: 9.9695
Epoch [5/50], Class Loss: 0.0280, JMMD Loss: 0.0751
Validation Loss: 6.5362
Epoch [6/50], Class Loss: 0.0431, JMMD Loss: 0.1747
Validation Loss: 7.2349
Epoch [7/50], Class Loss: 0.0113, JMMD Loss: 0.0697
Validation Loss: 9.6866
Epoch [8/50], Class Loss: 0.0379, JMMD Loss: 0.0965
Validation Loss: 3.4765
Epoch [9/50], Class Loss: 0.1323, JMMD Loss: 0.1905
Validation Loss: 5.0646
Epoch [10/50], Class Loss: 0.0235, JMMD Loss: 0.1027
Validation Loss: 4.9773
Epoch [11/50], Class Loss: 0.0377, JMMD Loss: 0.0977
Validation Loss: 6.9108
Epoch [12/50], Class Loss: 0.0311, JMMD Loss: 0.1783
Validation Loss: 5.7434
Epoch [13/50], Class Loss: 0.0150, JMMD Loss: 0.1546
Validation Loss: 5.2030
Early stopping!
Source Domain Performance - Accuracy: 50.42%, Precision: 63.36%, Recall: 51.01%, F1 Score: 42.61%
Target Domain Performance - Accuracy: 52.34%, Precision: 48.72%, Recall: 51.75%, F1 Score: 41.73%

Run 6/10
Epoch [1/50], Class Loss: 0.3636, JMMD Loss: 0.2655
Validation Loss: 19.9594
Epoch [2/50], Class Loss: 0.0772, JMMD Loss: 0.2071
Validation Loss: 4.4979
Epoch [3/50], Class Loss: 0.0899, JMMD Loss: 0.2045
Validation Loss: 5.1370
Epoch [4/50], Class Loss: 0.0191, JMMD Loss: 0.1035
Validation Loss: 10.1186
Epoch [5/50], Class Loss: 0.0369, JMMD Loss: 0.1037
Validation Loss: 8.1568
Epoch [6/50], Class Loss: 0.0346, JMMD Loss: 0.0935
Validation Loss: 11.0171
Epoch [7/50], Class Loss: 0.0524, JMMD Loss: 0.1414
Validation Loss: 15.7429
Early stopping!
Source Domain Performance - Accuracy: 49.94%, Precision: 42.90%, Recall: 50.56%, F1 Score: 38.89%
Target Domain Performance - Accuracy: 50.48%, Precision: 40.97%, Recall: 49.84%, F1 Score: 43.76%

Run 7/10
Epoch [1/50], Class Loss: 0.4031, JMMD Loss: 0.2793
Validation Loss: 7.2927
Epoch [2/50], Class Loss: 0.0895, JMMD Loss: 0.2673
Validation Loss: 0.1384
Epoch [3/50], Class Loss: 0.0943, JMMD Loss: 0.2256
Validation Loss: 8.3395
Epoch [4/50], Class Loss: 0.0609, JMMD Loss: 0.1511
Validation Loss: 0.3363
Epoch [5/50], Class Loss: 0.0804, JMMD Loss: 0.1767
Validation Loss: 6.6662
Epoch [6/50], Class Loss: 0.0267, JMMD Loss: 0.1087
Validation Loss: 15.3142
Epoch [7/50], Class Loss: 0.0388, JMMD Loss: 0.1015
Validation Loss: 8.0015
Early stopping!
Source Domain Performance - Accuracy: 49.88%, Precision: 37.78%, Recall: 50.50%, F1 Score: 41.96%
Target Domain Performance - Accuracy: 50.96%, Precision: 43.32%, Recall: 50.39%, F1 Score: 38.15%

Run 8/10
Epoch [1/50], Class Loss: 0.3199, JMMD Loss: 0.3753
Validation Loss: 5.3751
Epoch [2/50], Class Loss: 0.0670, JMMD Loss: 0.2746
Validation Loss: 7.7967
Epoch [3/50], Class Loss: 0.0802, JMMD Loss: 0.2256
Validation Loss: 3.5982
Epoch [4/50], Class Loss: 0.0249, JMMD Loss: 0.1132
Validation Loss: 13.2894
Epoch [5/50], Class Loss: 0.0238, JMMD Loss: 0.0917
Validation Loss: 11.4189
Epoch [6/50], Class Loss: 0.0138, JMMD Loss: 0.1028
Validation Loss: 4.6037
Epoch [7/50], Class Loss: 0.0323, JMMD Loss: 0.0780
Validation Loss: 6.3740
Epoch [8/50], Class Loss: 0.0601, JMMD Loss: 0.1268
Validation Loss: 5.5913
Early stopping!
Source Domain Performance - Accuracy: 50.42%, Precision: 54.70%, Recall: 51.06%, F1 Score: 43.15%
Target Domain Performance - Accuracy: 51.08%, Precision: 46.10%, Recall: 50.53%, F1 Score: 38.59%

Run 9/10
Epoch [1/50], Class Loss: 0.4222, JMMD Loss: 0.2338
Validation Loss: 0.1608
Epoch [2/50], Class Loss: 0.1062, JMMD Loss: 0.2151
Validation Loss: 0.0299
Epoch [3/50], Class Loss: 0.0720, JMMD Loss: 0.1029
Validation Loss: 1.4585
Epoch [4/50], Class Loss: 0.1253, JMMD Loss: 0.4785
Validation Loss: 6.7078
Epoch [5/50], Class Loss: 0.0380, JMMD Loss: 0.3004
Validation Loss: 0.0340
Epoch [6/50], Class Loss: 0.0540, JMMD Loss: 0.1750
Validation Loss: 2.9330
Epoch [7/50], Class Loss: 0.1038, JMMD Loss: 0.3887
Validation Loss: 0.0275
Epoch [8/50], Class Loss: 0.0277, JMMD Loss: 0.1942
Validation Loss: 0.0646
Epoch [9/50], Class Loss: 0.0166, JMMD Loss: 0.1372
Validation Loss: 0.2668
Epoch [10/50], Class Loss: 0.0550, JMMD Loss: 0.1690
Validation Loss: 0.4148
Epoch [11/50], Class Loss: 0.0318, JMMD Loss: 0.1268
Validation Loss: 0.0384
Epoch [12/50], Class Loss: 0.0264, JMMD Loss: 0.1176
Validation Loss: 0.0613
Early stopping!
Source Domain Performance - Accuracy: 97.66%, Precision: 97.89%, Recall: 97.82%, F1 Score: 97.77%
Target Domain Performance - Accuracy: 67.93%, Precision: 84.21%, Recall: 66.94%, F1 Score: 62.04%

Run 10/10
Epoch [1/50], Class Loss: 0.4175, JMMD Loss: 0.2799
Validation Loss: 0.2284
Epoch [2/50], Class Loss: 0.1127, JMMD Loss: 0.4050
Validation Loss: 7.4927
Epoch [3/50], Class Loss: 0.1651, JMMD Loss: 0.3772
Validation Loss: 7.0743
Epoch [4/50], Class Loss: 0.0874, JMMD Loss: 0.3038
Validation Loss: 3.7378
Epoch [5/50], Class Loss: 0.0590, JMMD Loss: 0.1841
Validation Loss: 2.5391
Epoch [6/50], Class Loss: 0.0824, JMMD Loss: 0.1953
Validation Loss: 0.9295
Early stopping!
Source Domain Performance - Accuracy: 71.52%, Precision: 77.94%, Recall: 71.64%, F1 Score: 71.97%
Target Domain Performance - Accuracy: 41.85%, Precision: 82.16%, Recall: 41.30%, F1 Score: 35.73%

Source performance: 65.91% 68.25% 66.29% 61.05%
Target performance: 54.92% 59.95% 54.34% 46.97%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 89.65%
  Class 1: 11.78%
  Class 2: 41.85%
  Class 3: 74.10%
Base

Run 1/10
Epoch 1/50, Train Loss: 0.3786, Train Acc: 0.8671, Val Loss: 2.0451, Val Acc: 0.5222
Epoch 2/50, Train Loss: 0.0531, Train Acc: 0.9856, Val Loss: 0.0939, Val Acc: 0.9754
Epoch 3/50, Train Loss: 0.0345, Train Acc: 0.9901, Val Loss: 0.0134, Val Acc: 0.9976
Epoch 4/50, Train Loss: 0.0173, Train Acc: 0.9961, Val Loss: 0.0540, Val Acc: 0.9814
Epoch 5/50, Train Loss: 0.0111, Train Acc: 0.9978, Val Loss: 0.2753, Val Acc: 0.9029
Epoch 6/50, Train Loss: 0.0090, Train Acc: 0.9976, Val Loss: 0.6105, Val Acc: 0.8399
Epoch 7/50, Train Loss: 0.0233, Train Acc: 0.9945, Val Loss: 0.8819, Val Acc: 0.7290
Epoch 8/50, Train Loss: 0.0088, Train Acc: 0.9976, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 9/50, Train Loss: 0.0058, Train Acc: 0.9988, Val Loss: 0.0084, Val Acc: 0.9988
Epoch 10/50, Train Loss: 0.0168, Train Acc: 0.9948, Val Loss: 0.1205, Val Acc: 0.9574
Epoch 11/50, Train Loss: 0.0079, Train Acc: 0.9982, Val Loss: 0.0063, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0050, Train Acc: 0.9993, Val Loss: 0.0021, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0034, Train Acc: 0.9993, Val Loss: 0.0008, Val Acc: 1.0000
Early stopping!

Run 2/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3367, Train Acc: 0.8899, Val Loss: 1.2048, Val Acc: 0.7104
Epoch 2/50, Train Loss: 0.0400, Train Acc: 0.9886, Val Loss: 0.0798, Val Acc: 0.9742
Epoch 3/50, Train Loss: 0.0276, Train Acc: 0.9937, Val Loss: 0.0686, Val Acc: 0.9754
Epoch 4/50, Train Loss: 0.0220, Train Acc: 0.9940, Val Loss: 0.0799, Val Acc: 0.9832
Epoch 5/50, Train Loss: 0.0151, Train Acc: 0.9952, Val Loss: 1.3206, Val Acc: 0.7878
Epoch 6/50, Train Loss: 0.0519, Train Acc: 0.9889, Val Loss: 1.2291, Val Acc: 0.7530
Epoch 7/50, Train Loss: 0.0091, Train Acc: 0.9972, Val Loss: 0.0705, Val Acc: 0.9736
Epoch 8/50, Train Loss: 0.0040, Train Acc: 0.9985, Val Loss: 0.0974, Val Acc: 0.9640
Early stopping!

Run 3/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3265, Train Acc: 0.8895, Val Loss: 0.4096, Val Acc: 0.8489
Epoch 2/50, Train Loss: 0.0452, Train Acc: 0.9877, Val Loss: 5.4222, Val Acc: 0.5096
Epoch 3/50, Train Loss: 0.0470, Train Acc: 0.9877, Val Loss: 0.0135, Val Acc: 0.9994
Epoch 4/50, Train Loss: 0.0147, Train Acc: 0.9951, Val Loss: 0.1383, Val Acc: 0.9532
Epoch 5/50, Train Loss: 0.0220, Train Acc: 0.9939, Val Loss: 0.1033, Val Acc: 0.9706
Epoch 6/50, Train Loss: 0.0215, Train Acc: 0.9933, Val Loss: 0.0285, Val Acc: 0.9910
Epoch 7/50, Train Loss: 0.0098, Train Acc: 0.9972, Val Loss: 0.7116, Val Acc: 0.8249
Epoch 8/50, Train Loss: 0.0166, Train Acc: 0.9946, Val Loss: 0.0744, Val Acc: 0.9712
Early stopping!

Run 4/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3166, Train Acc: 0.8941, Val Loss: 0.8660, Val Acc: 0.7488
Epoch 2/50, Train Loss: 0.0388, Train Acc: 0.9877, Val Loss: 0.0270, Val Acc: 0.9934
Epoch 3/50, Train Loss: 0.0236, Train Acc: 0.9933, Val Loss: 0.0129, Val Acc: 0.9970
Epoch 4/50, Train Loss: 0.0204, Train Acc: 0.9939, Val Loss: 2.3354, Val Acc: 0.5653
Epoch 5/50, Train Loss: 0.0297, Train Acc: 0.9916, Val Loss: 0.0281, Val Acc: 0.9898
Epoch 6/50, Train Loss: 0.0127, Train Acc: 0.9961, Val Loss: 0.1480, Val Acc: 0.9454
Epoch 7/50, Train Loss: 0.0114, Train Acc: 0.9972, Val Loss: 0.0584, Val Acc: 0.9778
Epoch 8/50, Train Loss: 0.0155, Train Acc: 0.9957, Val Loss: 1.8771, Val Acc: 0.7518
Early stopping!

Run 5/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3442, Train Acc: 0.8832, Val Loss: 1.0625, Val Acc: 0.6469
Epoch 2/50, Train Loss: 0.0448, Train Acc: 0.9882, Val Loss: 0.0174, Val Acc: 0.9970
Epoch 3/50, Train Loss: 0.0250, Train Acc: 0.9937, Val Loss: 0.0391, Val Acc: 0.9910
Epoch 4/50, Train Loss: 0.0244, Train Acc: 0.9931, Val Loss: 2.4025, Val Acc: 0.5288
Epoch 5/50, Train Loss: 0.0356, Train Acc: 0.9895, Val Loss: 1.9240, Val Acc: 0.7422
Epoch 6/50, Train Loss: 0.0144, Train Acc: 0.9964, Val Loss: 0.2597, Val Acc: 0.8885
Epoch 7/50, Train Loss: 0.0278, Train Acc: 0.9927, Val Loss: 0.0286, Val Acc: 0.9904
Early stopping!

Run 6/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3485, Train Acc: 0.8811, Val Loss: 1.7267, Val Acc: 0.6019
Epoch 2/50, Train Loss: 0.0363, Train Acc: 0.9909, Val Loss: 0.1642, Val Acc: 0.9311
Epoch 3/50, Train Loss: 0.0238, Train Acc: 0.9931, Val Loss: 0.2352, Val Acc: 0.9035
Epoch 4/50, Train Loss: 0.0224, Train Acc: 0.9940, Val Loss: 0.1138, Val Acc: 0.9598
Epoch 5/50, Train Loss: 0.0179, Train Acc: 0.9949, Val Loss: 2.9791, Val Acc: 0.5653
Epoch 6/50, Train Loss: 0.0235, Train Acc: 0.9930, Val Loss: 0.0832, Val Acc: 0.9736
Epoch 7/50, Train Loss: 0.0134, Train Acc: 0.9964, Val Loss: 0.0142, Val Acc: 0.9964
Epoch 8/50, Train Loss: 0.0098, Train Acc: 0.9975, Val Loss: 0.0023, Val Acc: 0.9988
Epoch 9/50, Train Loss: 0.0091, Train Acc: 0.9973, Val Loss: 0.3910, Val Acc: 0.8231
Epoch 10/50, Train Loss: 0.0150, Train Acc: 0.9966, Val Loss: 0.3924, Val Acc: 0.8315
Epoch 11/50, Train Loss: 0.0032, Train Acc: 0.9993, Val Loss: 0.0043, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0018, Train Acc: 0.9997, Val Loss: 0.0017, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0061, Val Acc: 0.9982
Epoch 14/50, Train Loss: 0.0014, Train Acc: 0.9997, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0022, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0031, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0073, Train Acc: 0.9991, Val Loss: 0.0022, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0030, Train Acc: 0.9994, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0076, Train Acc: 0.9990, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0021, Train Acc: 0.9994, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0072, Train Acc: 0.9991, Val Loss: 0.0015, Val Acc: 0.9988
Epoch 24/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0013, Val Acc: 0.9994
Early stopping!

Run 7/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3502, Train Acc: 0.8808, Val Loss: 0.4632, Val Acc: 0.8561
Epoch 2/50, Train Loss: 0.0467, Train Acc: 0.9868, Val Loss: 2.7236, Val Acc: 0.3651
Epoch 3/50, Train Loss: 0.0360, Train Acc: 0.9895, Val Loss: 0.7358, Val Acc: 0.7542
Epoch 4/50, Train Loss: 0.0172, Train Acc: 0.9952, Val Loss: 0.9543, Val Acc: 0.8129
Epoch 5/50, Train Loss: 0.0164, Train Acc: 0.9963, Val Loss: 0.4606, Val Acc: 0.8219
Epoch 6/50, Train Loss: 0.0143, Train Acc: 0.9964, Val Loss: 0.0263, Val Acc: 0.9916
Epoch 7/50, Train Loss: 0.0111, Train Acc: 0.9970, Val Loss: 0.0167, Val Acc: 0.9940
Epoch 8/50, Train Loss: 0.0145, Train Acc: 0.9963, Val Loss: 0.0305, Val Acc: 0.9934
Epoch 9/50, Train Loss: 0.0117, Train Acc: 0.9973, Val Loss: 0.0076, Val Acc: 1.0000
Epoch 10/50, Train Loss: 0.0075, Train Acc: 0.9975, Val Loss: 0.0136, Val Acc: 0.9952
Epoch 11/50, Train Loss: 0.0037, Train Acc: 0.9993, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0054, Train Acc: 0.9985, Val Loss: 0.0018, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0018, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0017, Train Acc: 0.9993, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0018, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0021, Train Acc: 0.9996, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0020, Train Acc: 0.9996, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0019, Train Acc: 0.9996, Val Loss: 0.0009, Val Acc: 1.0000
Early stopping!

Run 8/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3210, Train Acc: 0.8943, Val Loss: 0.5409, Val Acc: 0.8034
Epoch 2/50, Train Loss: 0.0402, Train Acc: 0.9888, Val Loss: 0.5103, Val Acc: 0.8483
Epoch 3/50, Train Loss: 0.0193, Train Acc: 0.9942, Val Loss: 0.0130, Val Acc: 0.9964
Epoch 4/50, Train Loss: 0.0348, Train Acc: 0.9904, Val Loss: 0.3578, Val Acc: 0.8687
Epoch 5/50, Train Loss: 0.0172, Train Acc: 0.9951, Val Loss: 1.3662, Val Acc: 0.7560
Epoch 6/50, Train Loss: 0.0128, Train Acc: 0.9966, Val Loss: 0.1887, Val Acc: 0.9359
Epoch 7/50, Train Loss: 0.0149, Train Acc: 0.9964, Val Loss: 0.3197, Val Acc: 0.9005
Epoch 8/50, Train Loss: 0.0078, Train Acc: 0.9985, Val Loss: 0.1165, Val Acc: 0.9652
Early stopping!

Run 9/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3105, Train Acc: 0.9045, Val Loss: 0.9668, Val Acc: 0.7782
Epoch 2/50, Train Loss: 0.0563, Train Acc: 0.9841, Val Loss: 0.0385, Val Acc: 0.9892
Epoch 3/50, Train Loss: 0.0208, Train Acc: 0.9939, Val Loss: 1.4088, Val Acc: 0.6259
Epoch 4/50, Train Loss: 0.0302, Train Acc: 0.9921, Val Loss: 0.0098, Val Acc: 0.9976
Epoch 5/50, Train Loss: 0.0139, Train Acc: 0.9955, Val Loss: 0.4747, Val Acc: 0.8507
Epoch 6/50, Train Loss: 0.0177, Train Acc: 0.9957, Val Loss: 0.1791, Val Acc: 0.9215
Epoch 7/50, Train Loss: 0.0276, Train Acc: 0.9930, Val Loss: 2.5139, Val Acc: 0.5953
Epoch 8/50, Train Loss: 0.0108, Train Acc: 0.9964, Val Loss: 0.0083, Val Acc: 0.9982
Epoch 9/50, Train Loss: 0.0063, Train Acc: 0.9981, Val Loss: 0.0138, Val Acc: 0.9958
Epoch 10/50, Train Loss: 0.0074, Train Acc: 0.9978, Val Loss: 1.0361, Val Acc: 0.7668
Epoch 11/50, Train Loss: 0.0037, Train Acc: 0.9990, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0039, Train Acc: 0.9991, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0019, Train Acc: 0.9994, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0036, Train Acc: 0.9996, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0015, Train Acc: 0.9994, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Early stopping!

Run 10/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3658, Train Acc: 0.8767, Val Loss: 0.5808, Val Acc: 0.8585
Epoch 2/50, Train Loss: 0.0431, Train Acc: 0.9874, Val Loss: 0.4451, Val Acc: 0.8525
Epoch 3/50, Train Loss: 0.0314, Train Acc: 0.9907, Val Loss: 1.0433, Val Acc: 0.8147
Epoch 4/50, Train Loss: 0.0225, Train Acc: 0.9942, Val Loss: 0.0116, Val Acc: 0.9976
Epoch 5/50, Train Loss: 0.0113, Train Acc: 0.9967, Val Loss: 0.0060, Val Acc: 0.9988
Epoch 6/50, Train Loss: 0.0103, Train Acc: 0.9966, Val Loss: 0.0031, Val Acc: 0.9994
Epoch 7/50, Train Loss: 0.0106, Train Acc: 0.9976, Val Loss: 0.3969, Val Acc: 0.9035
Epoch 8/50, Train Loss: 0.0119, Train Acc: 0.9969, Val Loss: 0.0370, Val Acc: 0.9892
Epoch 9/50, Train Loss: 0.0131, Train Acc: 0.9969, Val Loss: 0.0264, Val Acc: 0.9916
Epoch 10/50, Train Loss: 0.0067, Train Acc: 0.9990, Val Loss: 0.2101, Val Acc: 0.9436
Epoch 11/50, Train Loss: 0.0025, Train Acc: 0.9994, Val Loss: 0.0023, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0027, Train Acc: 0.9993, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0033, Train Acc: 0.9990, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0014, Train Acc: 0.9997, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 0.9994
Early stopping!

Source performance: 96.41 95.18 96.34 95.51
Target performance: 48.09 34.51 48.54 36.91

bpsk: 89.76
qpsk: 0.00
16qam: 4.92
8apsk: 99.48
DANN
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.3598, Domain Loss: 1.3632, Class Loss: 0.9966
Epoch 2/50, Loss: 1.8634, Domain Loss: 1.3220, Class Loss: 0.5414
Epoch 3/50, Loss: 1.6740, Domain Loss: 1.3687, Class Loss: 0.3053
Epoch 4/50, Loss: 1.6056, Domain Loss: 1.4024, Class Loss: 0.2032
Epoch 5/50, Loss: 2.9506, Domain Loss: 2.7430, Class Loss: 0.2076
Epoch 6/50, Loss: 11.2440, Domain Loss: 10.9425, Class Loss: 0.3015
Epoch 7/50, Loss: 7.8798, Domain Loss: 6.9689, Class Loss: 0.9109
Epoch 8/50, Loss: 7.4221, Domain Loss: 6.5250, Class Loss: 0.8971
Epoch 9/50, Loss: 5.5863, Domain Loss: 4.9573, Class Loss: 0.6290
Epoch 10/50, Loss: 8.2276, Domain Loss: 7.3425, Class Loss: 0.8851
Epoch 11/50, Loss: 10.5029, Domain Loss: 9.3319, Class Loss: 1.1710
Epoch 12/50, Loss: 5.1600, Domain Loss: 4.1587, Class Loss: 1.0014
Epoch 13/50, Loss: 3.0576, Domain Loss: 2.2994, Class Loss: 0.7582
Epoch 14/50, Loss: 2.8107, Domain Loss: 2.0287, Class Loss: 0.7820
Epoch 15/50, Loss: 2.4638, Domain Loss: 1.7771, Class Loss: 0.6867
Epoch 16/50, Loss: 1.9314, Domain Loss: 1.4079, Class Loss: 0.5234
Epoch 17/50, Loss: 2.0423, Domain Loss: 1.4638, Class Loss: 0.5785
Epoch 18/50, Loss: 2.2641, Domain Loss: 1.6170, Class Loss: 0.6470
Epoch 19/50, Loss: 1.9901, Domain Loss: 1.4769, Class Loss: 0.5131
Epoch 20/50, Loss: 2.0574, Domain Loss: 1.5260, Class Loss: 0.5313
Epoch 21/50, Loss: 1.8289, Domain Loss: 1.4074, Class Loss: 0.4215
Epoch 22/50, Loss: 2.5848, Domain Loss: 1.9399, Class Loss: 0.6449
Epoch 23/50, Loss: 2.0780, Domain Loss: 1.5984, Class Loss: 0.4796
Epoch 24/50, Loss: 1.8985, Domain Loss: 1.4825, Class Loss: 0.4160
Epoch 25/50, Loss: 2.1623, Domain Loss: 1.7252, Class Loss: 0.4371
Epoch 26/50, Loss: 2.1737, Domain Loss: 1.6994, Class Loss: 0.4743
Epoch 27/50, Loss: 2.1378, Domain Loss: 1.8118, Class Loss: 0.3260
Epoch 28/50, Loss: 1.9933, Domain Loss: 1.6961, Class Loss: 0.2971
Epoch 29/50, Loss: 2.2659, Domain Loss: 1.8068, Class Loss: 0.4591
Epoch 30/50, Loss: 2.0211, Domain Loss: 1.6971, Class Loss: 0.3240
Epoch 31/50, Loss: 1.8178, Domain Loss: 1.5480, Class Loss: 0.2697
Epoch 32/50, Loss: 1.6105, Domain Loss: 1.4484, Class Loss: 0.1621
Epoch 33/50, Loss: 1.5592, Domain Loss: 1.4293, Class Loss: 0.1300
Epoch 34/50, Loss: 1.5273, Domain Loss: 1.4235, Class Loss: 0.1038
Epoch 35/50, Loss: 1.5194, Domain Loss: 1.4190, Class Loss: 0.1004
Epoch 36/50, Loss: 1.4751, Domain Loss: 1.4023, Class Loss: 0.0729
Epoch 37/50, Loss: 1.4499, Domain Loss: 1.3929, Class Loss: 0.0570
Epoch 38/50, Loss: 1.4329, Domain Loss: 1.3771, Class Loss: 0.0557
Epoch 39/50, Loss: 1.4706, Domain Loss: 1.4038, Class Loss: 0.0668
Epoch 40/50, Loss: 1.4824, Domain Loss: 1.4157, Class Loss: 0.0667
Epoch 41/50, Loss: 1.4915, Domain Loss: 1.4276, Class Loss: 0.0639
Epoch 42/50, Loss: 1.4678, Domain Loss: 1.4112, Class Loss: 0.0567
Epoch 43/50, Loss: 1.4836, Domain Loss: 1.4493, Class Loss: 0.0343
Epoch 44/50, Loss: 1.4969, Domain Loss: 1.4597, Class Loss: 0.0373
Epoch 45/50, Loss: 1.5102, Domain Loss: 1.4674, Class Loss: 0.0428
Epoch 46/50, Loss: 1.5226, Domain Loss: 1.4984, Class Loss: 0.0242
Epoch 47/50, Loss: 1.5759, Domain Loss: 1.5270, Class Loss: 0.0489
Epoch 48/50, Loss: 1.5860, Domain Loss: 1.5508, Class Loss: 0.0352
Epoch 49/50, Loss: 1.4699, Domain Loss: 1.4418, Class Loss: 0.0281
Epoch 50/50, Loss: 1.7067, Domain Loss: 1.6005, Class Loss: 0.1062
42.15


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2846, Domain Loss: 1.3879, Class Loss: 0.8966
Epoch 2/50, Loss: 1.4902, Domain Loss: 1.2781, Class Loss: 0.2121
Epoch 3/50, Loss: 1.4179, Domain Loss: 1.2360, Class Loss: 0.1819
Epoch 4/50, Loss: 2.2411, Domain Loss: 2.0945, Class Loss: 0.1466
Epoch 5/50, Loss: 7.1123, Domain Loss: 6.9538, Class Loss: 0.1585
Epoch 6/50, Loss: 4.3970, Domain Loss: 4.2069, Class Loss: 0.1901
Epoch 7/50, Loss: 4.7549, Domain Loss: 4.3958, Class Loss: 0.3591
Epoch 8/50, Loss: 4.3474, Domain Loss: 4.0976, Class Loss: 0.2497
Epoch 9/50, Loss: 2.4069, Domain Loss: 2.1453, Class Loss: 0.2616
Epoch 10/50, Loss: 4.4225, Domain Loss: 4.0002, Class Loss: 0.4223
Epoch 11/50, Loss: 10.8491, Domain Loss: 10.3169, Class Loss: 0.5322
Epoch 12/50, Loss: 5.8572, Domain Loss: 5.3882, Class Loss: 0.4689
Epoch 13/50, Loss: 5.6270, Domain Loss: 4.9640, Class Loss: 0.6630
Epoch 14/50, Loss: 3.7631, Domain Loss: 3.0842, Class Loss: 0.6790
Epoch 15/50, Loss: 2.4697, Domain Loss: 2.1163, Class Loss: 0.3534
Epoch 16/50, Loss: 2.1094, Domain Loss: 1.8433, Class Loss: 0.2662
Epoch 17/50, Loss: 1.9068, Domain Loss: 1.6944, Class Loss: 0.2124
Epoch 18/50, Loss: 2.0055, Domain Loss: 1.7877, Class Loss: 0.2178
Epoch 19/50, Loss: 1.8953, Domain Loss: 1.6319, Class Loss: 0.2634
Epoch 20/50, Loss: 2.0828, Domain Loss: 1.8757, Class Loss: 0.2071
Epoch 21/50, Loss: 1.8778, Domain Loss: 1.6563, Class Loss: 0.2215
Epoch 22/50, Loss: 1.5212, Domain Loss: 1.3981, Class Loss: 0.1231
Epoch 23/50, Loss: 1.4878, Domain Loss: 1.3746, Class Loss: 0.1133
Epoch 24/50, Loss: 1.5171, Domain Loss: 1.4069, Class Loss: 0.1102
Epoch 25/50, Loss: 1.4779, Domain Loss: 1.4104, Class Loss: 0.0675
Epoch 26/50, Loss: 1.5251, Domain Loss: 1.4461, Class Loss: 0.0790
Epoch 27/50, Loss: 1.5430, Domain Loss: 1.4732, Class Loss: 0.0699
Epoch 28/50, Loss: 1.6651, Domain Loss: 1.5922, Class Loss: 0.0728
Epoch 29/50, Loss: 1.7172, Domain Loss: 1.6474, Class Loss: 0.0697
Epoch 30/50, Loss: 1.8322, Domain Loss: 1.7546, Class Loss: 0.0776
Epoch 31/50, Loss: 1.7339, Domain Loss: 1.6609, Class Loss: 0.0730
Epoch 32/50, Loss: 1.7835, Domain Loss: 1.7074, Class Loss: 0.0760
Epoch 33/50, Loss: 2.1920, Domain Loss: 2.0369, Class Loss: 0.1550
Epoch 34/50, Loss: 2.1361, Domain Loss: 1.9997, Class Loss: 0.1365
Epoch 35/50, Loss: 2.9455, Domain Loss: 2.3298, Class Loss: 0.6157
Epoch 36/50, Loss: 1.6792, Domain Loss: 1.5251, Class Loss: 0.1541
Epoch 37/50, Loss: 1.6118, Domain Loss: 1.4970, Class Loss: 0.1148
Epoch 38/50, Loss: 1.4768, Domain Loss: 1.4158, Class Loss: 0.0610
Epoch 39/50, Loss: 1.4135, Domain Loss: 1.3584, Class Loss: 0.0552
Epoch 40/50, Loss: 1.4008, Domain Loss: 1.3431, Class Loss: 0.0577
Epoch 41/50, Loss: 1.4189, Domain Loss: 1.3570, Class Loss: 0.0619
Epoch 42/50, Loss: 1.4440, Domain Loss: 1.3901, Class Loss: 0.0539
Epoch 43/50, Loss: 1.4591, Domain Loss: 1.4011, Class Loss: 0.0580
Epoch 44/50, Loss: 1.4192, Domain Loss: 1.3816, Class Loss: 0.0376
Epoch 45/50, Loss: 1.4413, Domain Loss: 1.3928, Class Loss: 0.0485
Epoch 46/50, Loss: 1.4287, Domain Loss: 1.3994, Class Loss: 0.0293
Epoch 47/50, Loss: 1.4899, Domain Loss: 1.4604, Class Loss: 0.0295
Epoch 48/50, Loss: 1.4635, Domain Loss: 1.4243, Class Loss: 0.0391
Epoch 49/50, Loss: 1.5013, Domain Loss: 1.4527, Class Loss: 0.0485
Epoch 50/50, Loss: 1.7245, Domain Loss: 1.6804, Class Loss: 0.0442
74.34


Epoch 1/50, Loss: 2.4103, Domain Loss: 1.3821, Class Loss: 1.0282
Epoch 2/50, Loss: 1.9069, Domain Loss: 1.2817, Class Loss: 0.6252
Epoch 3/50, Loss: 1.4530, Domain Loss: 1.1750, Class Loss: 0.2780
Epoch 4/50, Loss: 2.1960, Domain Loss: 1.9738, Class Loss: 0.2222
Epoch 5/50, Loss: 3.6200, Domain Loss: 3.3887, Class Loss: 0.2313
Epoch 6/50, Loss: 5.3018, Domain Loss: 4.6572, Class Loss: 0.6446
Epoch 7/50, Loss: 7.9789, Domain Loss: 7.5434, Class Loss: 0.4355
Epoch 8/50, Loss: 6.2866, Domain Loss: 5.8790, Class Loss: 0.4075
Epoch 9/50, Loss: 4.5047, Domain Loss: 3.8599, Class Loss: 0.6447
Epoch 10/50, Loss: 7.8009, Domain Loss: 6.8321, Class Loss: 0.9688
Epoch 11/50, Loss: 10.7478, Domain Loss: 9.5513, Class Loss: 1.1966
Epoch 12/50, Loss: 5.1532, Domain Loss: 4.6379, Class Loss: 0.5153
Epoch 13/50, Loss: 2.9619, Domain Loss: 2.3906, Class Loss: 0.5713
Epoch 14/50, Loss: 3.8893, Domain Loss: 3.1870, Class Loss: 0.7023
Epoch 15/50, Loss: 3.2909, Domain Loss: 2.5263, Class Loss: 0.7647
Epoch 16/50, Loss: 2.5212, Domain Loss: 1.8782, Class Loss: 0.6430
Epoch 17/50, Loss: 2.1266, Domain Loss: 1.6360, Class Loss: 0.4906
Epoch 18/50, Loss: 1.9533, Domain Loss: 1.6790, Class Loss: 0.2743
Epoch 19/50, Loss: 2.0235, Domain Loss: 1.6921, Class Loss: 0.3314
Epoch 20/50, Loss: 1.8116, Domain Loss: 1.5691, Class Loss: 0.2425
Epoch 21/50, Loss: 1.7274, Domain Loss: 1.5571, Class Loss: 0.1703
Epoch 22/50, Loss: 1.7891, Domain Loss: 1.4818, Class Loss: 0.3073
Epoch 23/50, Loss: 1.5802, Domain Loss: 1.4043, Class Loss: 0.1759
Epoch 24/50, Loss: 1.6172, Domain Loss: 1.4499, Class Loss: 0.1673
Epoch 25/50, Loss: 1.7332, Domain Loss: 1.5152, Class Loss: 0.2181
Epoch 26/50, Loss: 1.7436, Domain Loss: 1.5948, Class Loss: 0.1488
Epoch 27/50, Loss: 1.8419, Domain Loss: 1.6960, Class Loss: 0.1459
Epoch 28/50, Loss: 2.0377, Domain Loss: 1.8896, Class Loss: 0.1481
Epoch 29/50, Loss: 2.4455, Domain Loss: 2.2681, Class Loss: 0.1774
Epoch 30/50, Loss: 2.1711, Domain Loss: 1.9801, Class Loss: 0.1911
Epoch 31/50, Loss: 1.9131, Domain Loss: 1.6481, Class Loss: 0.2649
Epoch 32/50, Loss: 1.6753, Domain Loss: 1.5339, Class Loss: 0.1413
Epoch 33/50, Loss: 1.5084, Domain Loss: 1.4021, Class Loss: 0.1063
Epoch 34/50, Loss: 1.5388, Domain Loss: 1.4183, Class Loss: 0.1205
Epoch 35/50, Loss: 1.5593, Domain Loss: 1.4539, Class Loss: 0.1053
Epoch 36/50, Loss: 1.6845, Domain Loss: 1.5711, Class Loss: 0.1134
Epoch 37/50, Loss: 1.8189, Domain Loss: 1.7179, Class Loss: 0.1010
Epoch 38/50, Loss: 1.9633, Domain Loss: 1.8497, Class Loss: 0.1136
Epoch 39/50, Loss: 1.6280, Domain Loss: 1.5652, Class Loss: 0.0628
Epoch 40/50, Loss: 1.5001, Domain Loss: 1.4264, Class Loss: 0.0738
Epoch 41/50, Loss: 1.5717, Domain Loss: 1.4849, Class Loss: 0.0868
Epoch 42/50, Loss: 1.6594, Domain Loss: 1.5610, Class Loss: 0.0984
Epoch 43/50, Loss: 1.4614, Domain Loss: 1.3959, Class Loss: 0.0655
Epoch 44/50, Loss: 1.4717, Domain Loss: 1.4088, Class Loss: 0.0629
Epoch 45/50, Loss: 1.4657, Domain Loss: 1.4255, Class Loss: 0.0402
Epoch 46/50, Loss: 1.4663, Domain Loss: 1.4257, Class Loss: 0.0406
Epoch 47/50, Loss: 1.4583, Domain Loss: 1.3935, Class Loss: 0.0649
Epoch 48/50, Loss: 1.4004, Domain Loss: 1.3580, Class Loss: 0.0424
Epoch 49/50, Loss: 1.4064, Domain Loss: 1.3659, Class Loss: 0.0405
Epoch 50/50, Loss: 1.4070, Domain Loss: 1.3731, Class Loss: 0.0339
81.47


Epoch 1/50, Loss: 2.3572, Domain Loss: 1.3705, Class Loss: 0.9867
Epoch 2/50, Loss: 1.4815, Domain Loss: 1.1436, Class Loss: 0.3379
Epoch 3/50, Loss: 1.3078, Domain Loss: 1.1457, Class Loss: 0.1620
Epoch 4/50, Loss: 5.8251, Domain Loss: 5.5710, Class Loss: 0.2541
Epoch 5/50, Loss: 16.2800, Domain Loss: 16.0561, Class Loss: 0.2239
Epoch 6/50, Loss: 25.0029, Domain Loss: 24.7301, Class Loss: 0.2728
Epoch 7/50, Loss: 8.1908, Domain Loss: 7.9775, Class Loss: 0.2133
Epoch 8/50, Loss: 4.8116, Domain Loss: 4.6652, Class Loss: 0.1464
Epoch 9/50, Loss: 3.3027, Domain Loss: 3.1183, Class Loss: 0.1845
Epoch 10/50, Loss: 2.3815, Domain Loss: 2.2543, Class Loss: 0.1272
Epoch 11/50, Loss: 4.5467, Domain Loss: 4.1941, Class Loss: 0.3526
Epoch 12/50, Loss: 20.9004, Domain Loss: 20.1605, Class Loss: 0.7399
Epoch 13/50, Loss: 23.8625, Domain Loss: 23.1476, Class Loss: 0.7149
Epoch 14/50, Loss: 9.9680, Domain Loss: 9.5741, Class Loss: 0.3939
Epoch 15/50, Loss: 6.8895, Domain Loss: 6.5479, Class Loss: 0.3416
Epoch 16/50, Loss: 3.9615, Domain Loss: 3.5237, Class Loss: 0.4377
Epoch 17/50, Loss: 2.5150, Domain Loss: 2.1377, Class Loss: 0.3774
Epoch 18/50, Loss: 2.0535, Domain Loss: 1.6999, Class Loss: 0.3537
Epoch 19/50, Loss: 1.7435, Domain Loss: 1.4557, Class Loss: 0.2878
Epoch 20/50, Loss: 1.7707, Domain Loss: 1.5203, Class Loss: 0.2504
Epoch 21/50, Loss: 1.6935, Domain Loss: 1.5212, Class Loss: 0.1723
Epoch 22/50, Loss: 1.8237, Domain Loss: 1.6078, Class Loss: 0.2159
Epoch 23/50, Loss: 2.6126, Domain Loss: 2.0620, Class Loss: 0.5507
Epoch 24/50, Loss: 2.2582, Domain Loss: 2.0064, Class Loss: 0.2517
Epoch 25/50, Loss: 2.3729, Domain Loss: 1.9299, Class Loss: 0.4429
Epoch 26/50, Loss: 2.4112, Domain Loss: 2.0669, Class Loss: 0.3443
Epoch 27/50, Loss: 2.3634, Domain Loss: 2.0194, Class Loss: 0.3439
Epoch 28/50, Loss: 2.8222, Domain Loss: 2.3996, Class Loss: 0.4226
Epoch 29/50, Loss: 2.1378, Domain Loss: 1.7701, Class Loss: 0.3677
Epoch 30/50, Loss: 1.9545, Domain Loss: 1.6842, Class Loss: 0.2703
Epoch 31/50, Loss: 2.1817, Domain Loss: 1.9295, Class Loss: 0.2522
Epoch 32/50, Loss: 1.9018, Domain Loss: 1.6163, Class Loss: 0.2855
Epoch 33/50, Loss: 1.7247, Domain Loss: 1.5527, Class Loss: 0.1720
Epoch 34/50, Loss: 1.8624, Domain Loss: 1.7346, Class Loss: 0.1278
Epoch 35/50, Loss: 1.5420, Domain Loss: 1.4625, Class Loss: 0.0795
Epoch 36/50, Loss: 1.6977, Domain Loss: 1.5592, Class Loss: 0.1385
Epoch 37/50, Loss: 1.8270, Domain Loss: 1.7234, Class Loss: 0.1036
Epoch 38/50, Loss: 1.6696, Domain Loss: 1.5105, Class Loss: 0.1591
Epoch 39/50, Loss: 2.0603, Domain Loss: 1.9104, Class Loss: 0.1499
Epoch 40/50, Loss: 1.7833, Domain Loss: 1.5970, Class Loss: 0.1864
Epoch 41/50, Loss: 1.6408, Domain Loss: 1.5022, Class Loss: 0.1386
Epoch 42/50, Loss: 1.8830, Domain Loss: 1.7300, Class Loss: 0.1530
Epoch 43/50, Loss: 1.5992, Domain Loss: 1.5227, Class Loss: 0.0765
Epoch 44/50, Loss: 1.6521, Domain Loss: 1.5520, Class Loss: 0.1001
Epoch 45/50, Loss: 1.5354, Domain Loss: 1.4822, Class Loss: 0.0532
Epoch 46/50, Loss: 1.6521, Domain Loss: 1.6090, Class Loss: 0.0431
Epoch 47/50, Loss: 1.8168, Domain Loss: 1.7690, Class Loss: 0.0479
Epoch 48/50, Loss: 2.1146, Domain Loss: 2.0207, Class Loss: 0.0939
Epoch 49/50, Loss: 2.0059, Domain Loss: 1.9056, Class Loss: 0.1003
Epoch 50/50, Loss: 2.3850, Domain Loss: 2.1537, Class Loss: 0.2312
12.11


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.3892, Domain Loss: 1.3910, Class Loss: 0.9983
Epoch 2/50, Loss: 1.5509, Domain Loss: 1.2089, Class Loss: 0.3419
Epoch 3/50, Loss: 1.4987, Domain Loss: 1.3273, Class Loss: 0.1714
Epoch 4/50, Loss: 2.4644, Domain Loss: 2.2446, Class Loss: 0.2198
Epoch 5/50, Loss: 5.5090, Domain Loss: 5.2133, Class Loss: 0.2956
Epoch 6/50, Loss: 12.3296, Domain Loss: 11.9149, Class Loss: 0.4147
Epoch 7/50, Loss: 6.1170, Domain Loss: 4.7758, Class Loss: 1.3413
Epoch 8/50, Loss: 4.1472, Domain Loss: 3.5691, Class Loss: 0.5781
Epoch 9/50, Loss: 2.6509, Domain Loss: 2.2650, Class Loss: 0.3859
Epoch 10/50, Loss: 2.3703, Domain Loss: 2.0819, Class Loss: 0.2884
Epoch 11/50, Loss: 3.2454, Domain Loss: 2.9820, Class Loss: 0.2634
Epoch 12/50, Loss: 4.5442, Domain Loss: 4.1681, Class Loss: 0.3761
Epoch 13/50, Loss: 2.8180, Domain Loss: 2.5728, Class Loss: 0.2452
Epoch 14/50, Loss: 2.4260, Domain Loss: 2.1526, Class Loss: 0.2735
Epoch 15/50, Loss: 2.9062, Domain Loss: 2.5292, Class Loss: 0.3770
Epoch 16/50, Loss: 3.4399, Domain Loss: 3.1829, Class Loss: 0.2570
Epoch 17/50, Loss: 3.0425, Domain Loss: 2.7746, Class Loss: 0.2679
Epoch 18/50, Loss: 3.2921, Domain Loss: 2.9978, Class Loss: 0.2944
Epoch 19/50, Loss: 5.4390, Domain Loss: 5.0304, Class Loss: 0.4086
Epoch 20/50, Loss: 5.8324, Domain Loss: 4.9269, Class Loss: 0.9055
Epoch 21/50, Loss: 2.9410, Domain Loss: 2.5815, Class Loss: 0.3595
Epoch 22/50, Loss: 2.6520, Domain Loss: 2.3193, Class Loss: 0.3327
Epoch 23/50, Loss: 2.0996, Domain Loss: 1.7371, Class Loss: 0.3625
Epoch 24/50, Loss: 2.0062, Domain Loss: 1.7319, Class Loss: 0.2743
Epoch 25/50, Loss: 1.8846, Domain Loss: 1.6995, Class Loss: 0.1851
Epoch 26/50, Loss: 2.4780, Domain Loss: 2.0879, Class Loss: 0.3900
Epoch 27/50, Loss: 7.1059, Domain Loss: 5.4466, Class Loss: 1.6593
Epoch 28/50, Loss: 2.8980, Domain Loss: 2.6173, Class Loss: 0.2807
Epoch 29/50, Loss: 2.0045, Domain Loss: 1.7787, Class Loss: 0.2258
Epoch 30/50, Loss: 1.6568, Domain Loss: 1.4769, Class Loss: 0.1798
Epoch 31/50, Loss: 1.5769, Domain Loss: 1.4269, Class Loss: 0.1500
Epoch 32/50, Loss: 1.5148, Domain Loss: 1.3695, Class Loss: 0.1453
Epoch 33/50, Loss: 1.4568, Domain Loss: 1.3369, Class Loss: 0.1199
Epoch 34/50, Loss: 1.4912, Domain Loss: 1.3668, Class Loss: 0.1244
Epoch 35/50, Loss: 1.4666, Domain Loss: 1.3563, Class Loss: 0.1104
Epoch 36/50, Loss: 1.4519, Domain Loss: 1.3360, Class Loss: 0.1159
Epoch 37/50, Loss: 1.4747, Domain Loss: 1.3763, Class Loss: 0.0984
Epoch 38/50, Loss: 1.5061, Domain Loss: 1.4056, Class Loss: 0.1005
Epoch 39/50, Loss: 1.4303, Domain Loss: 1.3640, Class Loss: 0.0663
Epoch 40/50, Loss: 1.4690, Domain Loss: 1.4014, Class Loss: 0.0677
Epoch 41/50, Loss: 1.5308, Domain Loss: 1.4558, Class Loss: 0.0750
Epoch 42/50, Loss: 1.4957, Domain Loss: 1.4038, Class Loss: 0.0919
Epoch 43/50, Loss: 1.4859, Domain Loss: 1.4156, Class Loss: 0.0703
Epoch 44/50, Loss: 1.5226, Domain Loss: 1.4324, Class Loss: 0.0901
Epoch 45/50, Loss: 1.4989, Domain Loss: 1.4277, Class Loss: 0.0712
Epoch 46/50, Loss: 1.5246, Domain Loss: 1.4476, Class Loss: 0.0770
Epoch 47/50, Loss: 1.5464, Domain Loss: 1.5035, Class Loss: 0.0430
Epoch 48/50, Loss: 1.6069, Domain Loss: 1.5562, Class Loss: 0.0507
Epoch 49/50, Loss: 1.7118, Domain Loss: 1.6450, Class Loss: 0.0668
Epoch 50/50, Loss: 1.8853, Domain Loss: 1.8205, Class Loss: 0.0648
86.63


Epoch 1/50, Loss: 2.4808, Domain Loss: 1.3830, Class Loss: 1.0978
Epoch 2/50, Loss: 1.5443, Domain Loss: 1.2309, Class Loss: 0.3135
Epoch 3/50, Loss: 1.2996, Domain Loss: 1.1770, Class Loss: 0.1226
Epoch 4/50, Loss: 2.7143, Domain Loss: 2.5674, Class Loss: 0.1469
Epoch 5/50, Loss: 8.8663, Domain Loss: 8.5042, Class Loss: 0.3621
Epoch 6/50, Loss: 11.3198, Domain Loss: 10.9981, Class Loss: 0.3217
Epoch 7/50, Loss: 12.2328, Domain Loss: 10.5232, Class Loss: 1.7096
Epoch 8/50, Loss: 11.0876, Domain Loss: 10.0383, Class Loss: 1.0493
Epoch 9/50, Loss: 11.3344, Domain Loss: 10.3456, Class Loss: 0.9888
Epoch 10/50, Loss: 8.4691, Domain Loss: 7.8515, Class Loss: 0.6175
Epoch 11/50, Loss: 4.1374, Domain Loss: 3.5808, Class Loss: 0.5566
Epoch 12/50, Loss: 5.9835, Domain Loss: 5.4463, Class Loss: 0.5372
Epoch 13/50, Loss: 8.3991, Domain Loss: 7.6666, Class Loss: 0.7325
Epoch 14/50, Loss: 6.0111, Domain Loss: 5.2671, Class Loss: 0.7440
Epoch 15/50, Loss: 3.1005, Domain Loss: 2.5814, Class Loss: 0.5191
Epoch 16/50, Loss: 1.9402, Domain Loss: 1.5301, Class Loss: 0.4101
Epoch 17/50, Loss: 1.5788, Domain Loss: 1.2762, Class Loss: 0.3026
Epoch 18/50, Loss: 1.6089, Domain Loss: 1.3617, Class Loss: 0.2471
Epoch 19/50, Loss: 1.7095, Domain Loss: 1.4667, Class Loss: 0.2428
Epoch 20/50, Loss: 1.6668, Domain Loss: 1.4459, Class Loss: 0.2209
Epoch 21/50, Loss: 1.5699, Domain Loss: 1.3895, Class Loss: 0.1804
Epoch 22/50, Loss: 1.4897, Domain Loss: 1.3745, Class Loss: 0.1152
Epoch 23/50, Loss: 1.5202, Domain Loss: 1.4096, Class Loss: 0.1106
Epoch 24/50, Loss: 1.5514, Domain Loss: 1.4503, Class Loss: 0.1011
Epoch 25/50, Loss: 1.5445, Domain Loss: 1.4230, Class Loss: 0.1215
Epoch 26/50, Loss: 1.4722, Domain Loss: 1.4020, Class Loss: 0.0702
Epoch 27/50, Loss: 1.4570, Domain Loss: 1.3886, Class Loss: 0.0683
Epoch 28/50, Loss: 1.4495, Domain Loss: 1.3889, Class Loss: 0.0606
Epoch 29/50, Loss: 1.4904, Domain Loss: 1.4275, Class Loss: 0.0629
Epoch 30/50, Loss: 1.5045, Domain Loss: 1.4323, Class Loss: 0.0722
Epoch 31/50, Loss: 1.4997, Domain Loss: 1.4358, Class Loss: 0.0638
Epoch 32/50, Loss: 1.4722, Domain Loss: 1.4078, Class Loss: 0.0644
Epoch 33/50, Loss: 1.4721, Domain Loss: 1.4185, Class Loss: 0.0536
Epoch 34/50, Loss: 1.5033, Domain Loss: 1.4702, Class Loss: 0.0331
Epoch 35/50, Loss: 1.4683, Domain Loss: 1.4222, Class Loss: 0.0461
Epoch 36/50, Loss: 1.6094, Domain Loss: 1.5348, Class Loss: 0.0746
Epoch 37/50, Loss: 1.7020, Domain Loss: 1.6080, Class Loss: 0.0941
Epoch 38/50, Loss: 1.5387, Domain Loss: 1.4797, Class Loss: 0.0590
Epoch 39/50, Loss: 1.8777, Domain Loss: 1.7581, Class Loss: 0.1196
Epoch 40/50, Loss: 1.6915, Domain Loss: 1.6149, Class Loss: 0.0766
Epoch 41/50, Loss: 1.5069, Domain Loss: 1.4727, Class Loss: 0.0342
Epoch 42/50, Loss: 1.5246, Domain Loss: 1.4425, Class Loss: 0.0821
Epoch 43/50, Loss: 1.5875, Domain Loss: 1.4852, Class Loss: 0.1023
Epoch 44/50, Loss: 1.4926, Domain Loss: 1.4226, Class Loss: 0.0700
Epoch 45/50, Loss: 1.4587, Domain Loss: 1.4109, Class Loss: 0.0477
Epoch 46/50, Loss: 1.4666, Domain Loss: 1.4236, Class Loss: 0.0430
Epoch 47/50, Loss: 1.5325, Domain Loss: 1.4902, Class Loss: 0.0423
Epoch 48/50, Loss: 1.5112, Domain Loss: 1.4798, Class Loss: 0.0314
Epoch 49/50, Loss: 1.6640, Domain Loss: 1.5908, Class Loss: 0.0732
Epoch 50/50, Loss: 1.6723, Domain Loss: 1.5224, Class Loss: 0.1499
79.74


Epoch 1/50, Loss: 2.3447, Domain Loss: 1.3843, Class Loss: 0.9604
Epoch 2/50, Loss: 1.4856, Domain Loss: 1.1957, Class Loss: 0.2899
Epoch 3/50, Loss: 1.5109, Domain Loss: 1.3240, Class Loss: 0.1869
Epoch 4/50, Loss: 6.1449, Domain Loss: 5.7953, Class Loss: 0.3496
Epoch 5/50, Loss: 17.8786, Domain Loss: 17.4831, Class Loss: 0.3954
Epoch 6/50, Loss: 10.5431, Domain Loss: 10.1890, Class Loss: 0.3541
Epoch 7/50, Loss: 22.1094, Domain Loss: 21.6279, Class Loss: 0.4815
Epoch 8/50, Loss: 15.4017, Domain Loss: 15.1153, Class Loss: 0.2865
Epoch 9/50, Loss: 4.6517, Domain Loss: 4.4428, Class Loss: 0.2089
Epoch 10/50, Loss: 2.8179, Domain Loss: 2.5710, Class Loss: 0.2469
Epoch 11/50, Loss: 4.0428, Domain Loss: 3.1783, Class Loss: 0.8645
Epoch 12/50, Loss: 7.3445, Domain Loss: 6.5871, Class Loss: 0.7574
Epoch 13/50, Loss: 12.1688, Domain Loss: 10.8144, Class Loss: 1.3543
Epoch 14/50, Loss: 25.9230, Domain Loss: 24.6988, Class Loss: 1.2242
Epoch 15/50, Loss: 6.7172, Domain Loss: 6.1276, Class Loss: 0.5896
Epoch 16/50, Loss: 3.1837, Domain Loss: 2.6376, Class Loss: 0.5461
Epoch 17/50, Loss: 2.1422, Domain Loss: 1.7939, Class Loss: 0.3483
Epoch 18/50, Loss: 1.9020, Domain Loss: 1.6766, Class Loss: 0.2254
Epoch 19/50, Loss: 1.6864, Domain Loss: 1.4906, Class Loss: 0.1958
Epoch 20/50, Loss: 1.5854, Domain Loss: 1.4481, Class Loss: 0.1373
Epoch 21/50, Loss: 1.5990, Domain Loss: 1.4848, Class Loss: 0.1142
Epoch 22/50, Loss: 1.6156, Domain Loss: 1.5205, Class Loss: 0.0951
Epoch 23/50, Loss: 1.7286, Domain Loss: 1.6533, Class Loss: 0.0753
Epoch 24/50, Loss: 1.6809, Domain Loss: 1.6194, Class Loss: 0.0615
Epoch 25/50, Loss: 1.5975, Domain Loss: 1.5225, Class Loss: 0.0749
Epoch 26/50, Loss: 1.7349, Domain Loss: 1.6381, Class Loss: 0.0968
Epoch 27/50, Loss: 1.7540, Domain Loss: 1.6436, Class Loss: 0.1104
Epoch 28/50, Loss: 1.8621, Domain Loss: 1.7884, Class Loss: 0.0737
Epoch 29/50, Loss: 1.9004, Domain Loss: 1.8179, Class Loss: 0.0825
Epoch 30/50, Loss: 1.8406, Domain Loss: 1.7595, Class Loss: 0.0811
Epoch 31/50, Loss: 1.5349, Domain Loss: 1.4584, Class Loss: 0.0766
Epoch 32/50, Loss: 1.7345, Domain Loss: 1.6668, Class Loss: 0.0678
Epoch 33/50, Loss: 1.7006, Domain Loss: 1.6443, Class Loss: 0.0563
Epoch 34/50, Loss: 1.6197, Domain Loss: 1.5411, Class Loss: 0.0786
Epoch 35/50, Loss: 1.6643, Domain Loss: 1.5855, Class Loss: 0.0788
Epoch 36/50, Loss: 1.4159, Domain Loss: 1.3490, Class Loss: 0.0669
Epoch 37/50, Loss: 1.4666, Domain Loss: 1.3993, Class Loss: 0.0673
Epoch 38/50, Loss: 1.4685, Domain Loss: 1.4188, Class Loss: 0.0497
Epoch 39/50, Loss: 1.5066, Domain Loss: 1.4557, Class Loss: 0.0509
Epoch 40/50, Loss: 1.5868, Domain Loss: 1.5210, Class Loss: 0.0657
Epoch 41/50, Loss: 1.5791, Domain Loss: 1.5545, Class Loss: 0.0246
Epoch 42/50, Loss: 1.5280, Domain Loss: 1.4822, Class Loss: 0.0458
Epoch 43/50, Loss: 1.4161, Domain Loss: 1.3761, Class Loss: 0.0399
Epoch 44/50, Loss: 1.3605, Domain Loss: 1.3070, Class Loss: 0.0535
Epoch 45/50, Loss: 1.4938, Domain Loss: 1.4398, Class Loss: 0.0540
Epoch 46/50, Loss: 1.6163, Domain Loss: 1.5502, Class Loss: 0.0661
Epoch 47/50, Loss: 1.4640, Domain Loss: 1.4232, Class Loss: 0.0408
Epoch 48/50, Loss: 1.4236, Domain Loss: 1.3723, Class Loss: 0.0513
Epoch 49/50, Loss: 1.4218, Domain Loss: 1.3743, Class Loss: 0.0474
Epoch 50/50, Loss: 1.4341, Domain Loss: 1.4023, Class Loss: 0.0319
73.26


Epoch 1/50, Loss: 2.2886, Domain Loss: 1.3557, Class Loss: 0.9329
Epoch 2/50, Loss: 1.5861, Domain Loss: 1.1828, Class Loss: 0.4033
Epoch 3/50, Loss: 1.4268, Domain Loss: 1.2327, Class Loss: 0.1941
Epoch 4/50, Loss: 2.8177, Domain Loss: 2.6179, Class Loss: 0.1999
Epoch 5/50, Loss: 9.1221, Domain Loss: 8.6897, Class Loss: 0.4324
Epoch 6/50, Loss: 4.7999, Domain Loss: 4.4683, Class Loss: 0.3316
Epoch 7/50, Loss: 8.8762, Domain Loss: 8.4888, Class Loss: 0.3875
Epoch 8/50, Loss: 12.8022, Domain Loss: 12.2962, Class Loss: 0.5060
Epoch 9/50, Loss: 4.3072, Domain Loss: 4.0216, Class Loss: 0.2856
Epoch 10/50, Loss: 2.1365, Domain Loss: 2.0060, Class Loss: 0.1305
Epoch 11/50, Loss: 1.7358, Domain Loss: 1.6275, Class Loss: 0.1083
Epoch 12/50, Loss: 1.6648, Domain Loss: 1.5544, Class Loss: 0.1104
Epoch 13/50, Loss: 1.6760, Domain Loss: 1.5929, Class Loss: 0.0831
Epoch 14/50, Loss: 1.7806, Domain Loss: 1.6652, Class Loss: 0.1154
Epoch 15/50, Loss: 2.8626, Domain Loss: 2.6450, Class Loss: 0.2176
Epoch 16/50, Loss: 7.4077, Domain Loss: 6.4364, Class Loss: 0.9713
Epoch 17/50, Loss: 3.4157, Domain Loss: 3.1724, Class Loss: 0.2433
Epoch 18/50, Loss: 2.0170, Domain Loss: 1.8743, Class Loss: 0.1427
Epoch 19/50, Loss: 1.6021, Domain Loss: 1.5270, Class Loss: 0.0751
Epoch 20/50, Loss: 1.5037, Domain Loss: 1.4513, Class Loss: 0.0523
Epoch 21/50, Loss: 1.4833, Domain Loss: 1.4278, Class Loss: 0.0556
Epoch 22/50, Loss: 1.4880, Domain Loss: 1.4549, Class Loss: 0.0331
Epoch 23/50, Loss: 1.5491, Domain Loss: 1.4810, Class Loss: 0.0681
Epoch 24/50, Loss: 1.5059, Domain Loss: 1.4509, Class Loss: 0.0550
Epoch 25/50, Loss: 1.6609, Domain Loss: 1.6024, Class Loss: 0.0585
Epoch 26/50, Loss: 2.1963, Domain Loss: 2.1231, Class Loss: 0.0732
Epoch 27/50, Loss: 2.2242, Domain Loss: 2.0895, Class Loss: 0.1346
Epoch 28/50, Loss: 2.0419, Domain Loss: 1.9603, Class Loss: 0.0815
Epoch 29/50, Loss: 1.5654, Domain Loss: 1.5092, Class Loss: 0.0561
Epoch 30/50, Loss: 1.4823, Domain Loss: 1.4409, Class Loss: 0.0414
Epoch 31/50, Loss: 1.4533, Domain Loss: 1.4224, Class Loss: 0.0309
Epoch 32/50, Loss: 1.4407, Domain Loss: 1.4123, Class Loss: 0.0284
Epoch 33/50, Loss: 1.4430, Domain Loss: 1.4070, Class Loss: 0.0361
Epoch 34/50, Loss: 1.4337, Domain Loss: 1.4074, Class Loss: 0.0263
Epoch 35/50, Loss: 1.4484, Domain Loss: 1.4067, Class Loss: 0.0417
Epoch 36/50, Loss: 1.4284, Domain Loss: 1.3896, Class Loss: 0.0388
Epoch 37/50, Loss: 1.4099, Domain Loss: 1.3940, Class Loss: 0.0159
Epoch 38/50, Loss: 1.4451, Domain Loss: 1.4219, Class Loss: 0.0232
Epoch 39/50, Loss: 1.4532, Domain Loss: 1.4286, Class Loss: 0.0246
Epoch 40/50, Loss: 1.4764, Domain Loss: 1.4405, Class Loss: 0.0358
Epoch 41/50, Loss: 1.4577, Domain Loss: 1.4337, Class Loss: 0.0240
Epoch 42/50, Loss: 1.4665, Domain Loss: 1.4447, Class Loss: 0.0218
Epoch 43/50, Loss: 1.4499, Domain Loss: 1.4180, Class Loss: 0.0320
Epoch 44/50, Loss: 1.4401, Domain Loss: 1.4166, Class Loss: 0.0236
Epoch 45/50, Loss: 1.4368, Domain Loss: 1.4260, Class Loss: 0.0108
Epoch 46/50, Loss: 1.4641, Domain Loss: 1.4242, Class Loss: 0.0399
Epoch 47/50, Loss: 1.4784, Domain Loss: 1.4495, Class Loss: 0.0289
Epoch 48/50, Loss: 1.4580, Domain Loss: 1.4249, Class Loss: 0.0331
Epoch 49/50, Loss: 1.5125, Domain Loss: 1.4771, Class Loss: 0.0354
Epoch 50/50, Loss: 1.4488, Domain Loss: 1.4160, Class Loss: 0.0328
67.69


Epoch 1/50, Loss: 2.3904, Domain Loss: 1.3647, Class Loss: 1.0257
Epoch 2/50, Loss: 1.7606, Domain Loss: 1.2288, Class Loss: 0.5318
Epoch 3/50, Loss: 1.5538, Domain Loss: 1.2214, Class Loss: 0.3323
Epoch 4/50, Loss: 7.6615, Domain Loss: 7.2311, Class Loss: 0.4304
Epoch 5/50, Loss: 11.1142, Domain Loss: 10.7184, Class Loss: 0.3958
Epoch 6/50, Loss: 14.9989, Domain Loss: 14.3401, Class Loss: 0.6588
Epoch 7/50, Loss: 20.7944, Domain Loss: 20.0633, Class Loss: 0.7312
Epoch 8/50, Loss: 30.2544, Domain Loss: 29.4869, Class Loss: 0.7675
Epoch 9/50, Loss: 19.9495, Domain Loss: 18.5021, Class Loss: 1.4474
Epoch 10/50, Loss: 10.7638, Domain Loss: 9.6240, Class Loss: 1.1398
Epoch 11/50, Loss: 7.6378, Domain Loss: 6.8080, Class Loss: 0.8298
Epoch 12/50, Loss: 4.1343, Domain Loss: 3.3493, Class Loss: 0.7850
Epoch 13/50, Loss: 2.8651, Domain Loss: 2.1455, Class Loss: 0.7196
Epoch 14/50, Loss: 2.9334, Domain Loss: 2.2450, Class Loss: 0.6884
Epoch 15/50, Loss: 2.8494, Domain Loss: 2.2397, Class Loss: 0.6096
Epoch 16/50, Loss: 2.5425, Domain Loss: 1.9994, Class Loss: 0.5431
Epoch 17/50, Loss: 2.3637, Domain Loss: 1.8870, Class Loss: 0.4767
Epoch 18/50, Loss: 2.1504, Domain Loss: 1.7621, Class Loss: 0.3883
Epoch 19/50, Loss: 1.6450, Domain Loss: 1.4298, Class Loss: 0.2153
Epoch 20/50, Loss: 2.0261, Domain Loss: 1.7845, Class Loss: 0.2415
Epoch 21/50, Loss: 1.7212, Domain Loss: 1.5078, Class Loss: 0.2134
Epoch 22/50, Loss: 1.6670, Domain Loss: 1.4979, Class Loss: 0.1691
Epoch 23/50, Loss: 1.8247, Domain Loss: 1.6769, Class Loss: 0.1479
Epoch 24/50, Loss: 1.9979, Domain Loss: 1.7144, Class Loss: 0.2834
Epoch 25/50, Loss: 2.3902, Domain Loss: 2.0010, Class Loss: 0.3892
Epoch 26/50, Loss: 1.7971, Domain Loss: 1.5842, Class Loss: 0.2128
Epoch 27/50, Loss: 2.0363, Domain Loss: 1.7619, Class Loss: 0.2744
Epoch 28/50, Loss: 1.8427, Domain Loss: 1.6501, Class Loss: 0.1926
Epoch 29/50, Loss: 1.6568, Domain Loss: 1.5318, Class Loss: 0.1250
Epoch 30/50, Loss: 1.5806, Domain Loss: 1.4888, Class Loss: 0.0918
Epoch 31/50, Loss: 1.5813, Domain Loss: 1.4730, Class Loss: 0.1082
Epoch 32/50, Loss: 1.6466, Domain Loss: 1.5262, Class Loss: 0.1203
Epoch 33/50, Loss: 1.6137, Domain Loss: 1.4706, Class Loss: 0.1431
Epoch 34/50, Loss: 1.5210, Domain Loss: 1.4210, Class Loss: 0.1000
Epoch 35/50, Loss: 1.4994, Domain Loss: 1.4253, Class Loss: 0.0741
Epoch 36/50, Loss: 1.5827, Domain Loss: 1.4376, Class Loss: 0.1451
Epoch 37/50, Loss: 1.5167, Domain Loss: 1.4328, Class Loss: 0.0839
Epoch 38/50, Loss: 1.5047, Domain Loss: 1.4324, Class Loss: 0.0723
Epoch 39/50, Loss: 1.6194, Domain Loss: 1.5233, Class Loss: 0.0961
Epoch 40/50, Loss: 1.5216, Domain Loss: 1.4518, Class Loss: 0.0698
Epoch 41/50, Loss: 1.4813, Domain Loss: 1.4219, Class Loss: 0.0595
Epoch 42/50, Loss: 1.4656, Domain Loss: 1.4109, Class Loss: 0.0547
Epoch 43/50, Loss: 1.5157, Domain Loss: 1.4543, Class Loss: 0.0615
Epoch 44/50, Loss: 1.5146, Domain Loss: 1.4561, Class Loss: 0.0584
Epoch 45/50, Loss: 1.4621, Domain Loss: 1.4327, Class Loss: 0.0294
Epoch 46/50, Loss: 1.5526, Domain Loss: 1.4699, Class Loss: 0.0827
Epoch 47/50, Loss: 1.5173, Domain Loss: 1.4506, Class Loss: 0.0666
Epoch 48/50, Loss: 1.5075, Domain Loss: 1.4555, Class Loss: 0.0520
Epoch 49/50, Loss: 1.5209, Domain Loss: 1.4670, Class Loss: 0.0539
Epoch 50/50, Loss: 1.5050, Domain Loss: 1.4618, Class Loss: 0.0432
91.97


Epoch 1/50, Loss: 2.4214, Domain Loss: 1.3869, Class Loss: 1.0345
Epoch 2/50, Loss: 1.7257, Domain Loss: 1.2525, Class Loss: 0.4733
Epoch 3/50, Loss: 1.4023, Domain Loss: 1.1747, Class Loss: 0.2276
Epoch 4/50, Loss: 3.1947, Domain Loss: 2.9775, Class Loss: 0.2172
Epoch 5/50, Loss: 7.9312, Domain Loss: 7.6482, Class Loss: 0.2830
Epoch 6/50, Loss: 10.8173, Domain Loss: 9.8868, Class Loss: 0.9306
Epoch 7/50, Loss: 7.2857, Domain Loss: 6.1106, Class Loss: 1.1751
Epoch 8/50, Loss: 4.1179, Domain Loss: 3.3076, Class Loss: 0.8103
Epoch 9/50, Loss: 2.7738, Domain Loss: 2.3489, Class Loss: 0.4249
Epoch 10/50, Loss: 2.4667, Domain Loss: 2.1931, Class Loss: 0.2737
Epoch 11/50, Loss: 2.2378, Domain Loss: 1.9610, Class Loss: 0.2769
Epoch 12/50, Loss: 1.9177, Domain Loss: 1.6847, Class Loss: 0.2330
Epoch 13/50, Loss: 2.0423, Domain Loss: 1.8592, Class Loss: 0.1832
Epoch 14/50, Loss: 2.7831, Domain Loss: 2.5357, Class Loss: 0.2474
Epoch 15/50, Loss: 1.9278, Domain Loss: 1.7580, Class Loss: 0.1698
Epoch 16/50, Loss: 1.7538, Domain Loss: 1.6287, Class Loss: 0.1250
Epoch 17/50, Loss: 1.5608, Domain Loss: 1.4543, Class Loss: 0.1066
Epoch 18/50, Loss: 1.5546, Domain Loss: 1.4299, Class Loss: 0.1247
Epoch 19/50, Loss: 1.6142, Domain Loss: 1.4817, Class Loss: 0.1325
Epoch 20/50, Loss: 1.5948, Domain Loss: 1.5065, Class Loss: 0.0883
Epoch 21/50, Loss: 1.7316, Domain Loss: 1.6432, Class Loss: 0.0884
Epoch 22/50, Loss: 2.0904, Domain Loss: 1.9626, Class Loss: 0.1278
Epoch 23/50, Loss: 1.7320, Domain Loss: 1.6593, Class Loss: 0.0727
Epoch 24/50, Loss: 1.5456, Domain Loss: 1.4763, Class Loss: 0.0693
Epoch 25/50, Loss: 1.8722, Domain Loss: 1.7592, Class Loss: 0.1130
Epoch 26/50, Loss: 1.9857, Domain Loss: 1.8199, Class Loss: 0.1658
Epoch 27/50, Loss: 2.0835, Domain Loss: 1.9585, Class Loss: 0.1250
Epoch 28/50, Loss: 2.5319, Domain Loss: 2.3560, Class Loss: 0.1759
Epoch 29/50, Loss: 2.2888, Domain Loss: 2.0226, Class Loss: 0.2662
Epoch 30/50, Loss: 2.9528, Domain Loss: 2.6289, Class Loss: 0.3239
Epoch 31/50, Loss: 2.6126, Domain Loss: 2.4576, Class Loss: 0.1549
Epoch 32/50, Loss: 2.6907, Domain Loss: 2.5558, Class Loss: 0.1349
Epoch 33/50, Loss: 2.7671, Domain Loss: 2.4305, Class Loss: 0.3366
Epoch 34/50, Loss: 2.7692, Domain Loss: 2.1670, Class Loss: 0.6022
Epoch 35/50, Loss: 2.0121, Domain Loss: 1.7308, Class Loss: 0.2812
Epoch 36/50, Loss: 1.9930, Domain Loss: 1.7899, Class Loss: 0.2031
Epoch 37/50, Loss: 1.7879, Domain Loss: 1.6388, Class Loss: 0.1491
Epoch 38/50, Loss: 1.7543, Domain Loss: 1.6220, Class Loss: 0.1323
Epoch 39/50, Loss: 2.2038, Domain Loss: 2.0672, Class Loss: 0.1366
Epoch 40/50, Loss: 2.5645, Domain Loss: 2.4251, Class Loss: 0.1394
Epoch 41/50, Loss: 2.0524, Domain Loss: 1.9862, Class Loss: 0.0661
Epoch 42/50, Loss: 2.5788, Domain Loss: 2.4928, Class Loss: 0.0860
Epoch 43/50, Loss: 2.5119, Domain Loss: 2.3831, Class Loss: 0.1288
Epoch 44/50, Loss: 1.6757, Domain Loss: 1.5331, Class Loss: 0.1426
Epoch 45/50, Loss: 1.4612, Domain Loss: 1.3863, Class Loss: 0.0749
Epoch 46/50, Loss: 1.4827, Domain Loss: 1.4199, Class Loss: 0.0628
Epoch 47/50, Loss: 1.5328, Domain Loss: 1.4772, Class Loss: 0.0556
Epoch 48/50, Loss: 1.5645, Domain Loss: 1.4987, Class Loss: 0.0658
Epoch 49/50, Loss: 1.4807, Domain Loss: 1.4242, Class Loss: 0.0565
Epoch 50/50, Loss: 1.4272, Domain Loss: 1.3932, Class Loss: 0.0341
80.10


Source performance:
82.84 84.65 82.86 80.74 
Target performance:
68.94 72.09 69.33 67.02 

Per-class target performance: 86.31 49.16 65.23 76.62 Deep CORAL
Deep CORAL Run 1/10
Epoch 1: Source Val Acc = 0.4868, Target Val Acc = 0.2668
Epoch 2: Source Val Acc = 0.9640, Target Val Acc = 0.5402
Epoch 3: Source Val Acc = 0.6229, Target Val Acc = 0.5324
Epoch 4: Source Val Acc = 0.8219, Target Val Acc = 0.5084
Epoch 5: Source Val Acc = 0.9856, Target Val Acc = 0.5192
Epoch 6: Source Val Acc = 0.7518, Target Val Acc = 0.2776
Epoch 7: Source Val Acc = 0.8453, Target Val Acc = 0.5084
Epoch 8: Source Val Acc = 0.8537, Target Val Acc = 0.5174
Epoch 9: Source Val Acc = 0.5420, Target Val Acc = 0.5294
Epoch 10: Source Val Acc = 0.8022, Target Val Acc = 0.5156
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.8022, Target Val Acc = 0.5156

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.5618, Target Val Acc = 0.5060
Epoch 2: Source Val Acc = 0.2590, Target Val Acc = 0.2500
Epoch 3: Source Val Acc = 0.5300, Target Val Acc = 0.4598
Epoch 4: Source Val Acc = 0.5108, Target Val Acc = 0.4874
Epoch 5: Source Val Acc = 0.5030, Target Val Acc = 0.5180
Epoch 6: Source Val Acc = 0.7494, Target Val Acc = 0.5018
Epoch 7: Source Val Acc = 0.5618, Target Val Acc = 0.3153
Epoch 8: Source Val Acc = 0.5534, Target Val Acc = 0.6007
Epoch 9: Source Val Acc = 0.8735, Target Val Acc = 0.5312
Epoch 10: Source Val Acc = 0.5204, Target Val Acc = 0.5372
Epoch 11: Source Val Acc = 0.9568, Target Val Acc = 0.6103
Epoch 12: Source Val Acc = 0.9209, Target Val Acc = 0.5516
Epoch 13: Source Val Acc = 0.7182, Target Val Acc = 0.6211
Epoch 14: Source Val Acc = 0.5282, Target Val Acc = 0.5210
Epoch 15: Source Val Acc = 0.8759, Target Val Acc = 0.5600
Epoch 16: Source Val Acc = 0.5042, Target Val Acc = 0.5707
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.5042, Target Val Acc = 0.5707

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.5012, Target Val Acc = 0.4011
Epoch 2: Source Val Acc = 0.5677, Target Val Acc = 0.6097
Epoch 3: Source Val Acc = 0.5354, Target Val Acc = 0.6277
Epoch 4: Source Val Acc = 0.3381, Target Val Acc = 0.2764
Epoch 5: Source Val Acc = 0.7950, Target Val Acc = 0.5438
Epoch 6: Source Val Acc = 0.9640, Target Val Acc = 0.5695
Epoch 7: Source Val Acc = 0.8789, Target Val Acc = 0.5084
Epoch 8: Source Val Acc = 0.8771, Target Val Acc = 0.5647
Epoch 9: Source Val Acc = 0.6954, Target Val Acc = 0.5845
Epoch 10: Source Val Acc = 0.6619, Target Val Acc = 0.3735
Epoch 11: Source Val Acc = 0.9532, Target Val Acc = 0.5881
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.9532, Target Val Acc = 0.5881

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.8195, Target Val Acc = 0.5162
Epoch 2: Source Val Acc = 0.5186, Target Val Acc = 0.3303
Epoch 3: Source Val Acc = 0.5024, Target Val Acc = 0.5989
Epoch 4: Source Val Acc = 0.6709, Target Val Acc = 0.5492
Epoch 5: Source Val Acc = 0.9898, Target Val Acc = 0.5414
Epoch 6: Source Val Acc = 0.6733, Target Val Acc = 0.3609
Epoch 7: Source Val Acc = 0.8921, Target Val Acc = 0.5168
Epoch 8: Source Val Acc = 0.5516, Target Val Acc = 0.5366
Epoch 9: Source Val Acc = 0.9502, Target Val Acc = 0.5546
Epoch 10: Source Val Acc = 0.9323, Target Val Acc = 0.5282
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.9323, Target Val Acc = 0.5282

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.7464, Target Val Acc = 0.4802
Epoch 2: Source Val Acc = 0.3273, Target Val Acc = 0.2884
Epoch 3: Source Val Acc = 0.5018, Target Val Acc = 0.4736
Epoch 4: Source Val Acc = 0.5228, Target Val Acc = 0.5713
Epoch 5: Source Val Acc = 0.5486, Target Val Acc = 0.5420
Epoch 6: Source Val Acc = 0.6163, Target Val Acc = 0.6109
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.6163, Target Val Acc = 0.6109

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.8213, Target Val Acc = 0.5270
Epoch 2: Source Val Acc = 0.5126, Target Val Acc = 0.5671
Epoch 3: Source Val Acc = 0.8933, Target Val Acc = 0.5522
Epoch 4: Source Val Acc = 0.8567, Target Val Acc = 0.5863
Epoch 5: Source Val Acc = 0.7890, Target Val Acc = 0.4928
Epoch 6: Source Val Acc = 0.9365, Target Val Acc = 0.6139
Epoch 7: Source Val Acc = 0.8399, Target Val Acc = 0.6283
Epoch 8: Source Val Acc = 0.8339, Target Val Acc = 0.5048
Epoch 9: Source Val Acc = 0.8273, Target Val Acc = 0.6367
Epoch 10: Source Val Acc = 0.8477, Target Val Acc = 0.5126
Epoch 11: Source Val Acc = 0.5941, Target Val Acc = 0.5492
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 0.5941, Target Val Acc = 0.5492

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.7194, Target Val Acc = 0.6475
Epoch 2: Source Val Acc = 0.9670, Target Val Acc = 0.5090
Epoch 3: Source Val Acc = 0.6517, Target Val Acc = 0.2656
Epoch 4: Source Val Acc = 0.6187, Target Val Acc = 0.3195
Epoch 5: Source Val Acc = 0.5216, Target Val Acc = 0.6229
Epoch 6: Source Val Acc = 0.8939, Target Val Acc = 0.5953
Epoch 7: Source Val Acc = 0.5851, Target Val Acc = 0.6439
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.5851, Target Val Acc = 0.6439

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.7800, Target Val Acc = 0.5132
Epoch 2: Source Val Acc = 0.2800, Target Val Acc = 0.2428
Epoch 3: Source Val Acc = 0.3177, Target Val Acc = 0.2866
Epoch 4: Source Val Acc = 0.6954, Target Val Acc = 0.4880
Epoch 5: Source Val Acc = 0.4017, Target Val Acc = 0.5306
Epoch 6: Source Val Acc = 0.5701, Target Val Acc = 0.5108
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.5701, Target Val Acc = 0.5108

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.7110, Target Val Acc = 0.3183
Epoch 2: Source Val Acc = 0.2662, Target Val Acc = 0.2644
Epoch 3: Source Val Acc = 0.5641, Target Val Acc = 0.6037
Epoch 4: Source Val Acc = 0.4227, Target Val Acc = 0.5629
Epoch 5: Source Val Acc = 0.4766, Target Val Acc = 0.5492
Epoch 6: Source Val Acc = 0.7374, Target Val Acc = 0.5893
Epoch 7: Source Val Acc = 0.5150, Target Val Acc = 0.5378
Epoch 8: Source Val Acc = 0.8285, Target Val Acc = 0.3255
Epoch 9: Source Val Acc = 0.5210, Target Val Acc = 0.5144
Epoch 10: Source Val Acc = 0.9041, Target Val Acc = 0.5282
Epoch 11: Source Val Acc = 0.6193, Target Val Acc = 0.5588
Epoch 12: Source Val Acc = 0.9568, Target Val Acc = 0.5192
Epoch 13: Source Val Acc = 0.8183, Target Val Acc = 0.5324
Epoch 14: Source Val Acc = 0.9263, Target Val Acc = 0.5647
Epoch 15: Source Val Acc = 0.5695, Target Val Acc = 0.5414
Epoch 16: Source Val Acc = 0.7578, Target Val Acc = 0.5384
Epoch 17: Source Val Acc = 0.8267, Target Val Acc = 0.5138
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 0.8267, Target Val Acc = 0.5138

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.5258, Target Val Acc = 0.5600
Epoch 2: Source Val Acc = 0.3555, Target Val Acc = 0.4263
Epoch 3: Source Val Acc = 0.9652, Target Val Acc = 0.5120
Epoch 4: Source Val Acc = 0.5246, Target Val Acc = 0.5198
Epoch 5: Source Val Acc = 0.9041, Target Val Acc = 0.5444
Epoch 6: Source Val Acc = 0.7866, Target Val Acc = 0.5192
Epoch 7: Source Val Acc = 0.4119, Target Val Acc = 0.4850
Epoch 8: Source Val Acc = 0.8076, Target Val Acc = 0.5522
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.8076, Target Val Acc = 0.5522

Deep CORAL: Average Source Val Acc = 0.7192, Average Target Val Acc = 0.5583
STAR

Run 1/10
Epoch [1/50], Class Loss: 1.6903, Discrepancy Loss: 0.1127
Epoch [2/50], Class Loss: 0.2752, Discrepancy Loss: 0.0562
Epoch [3/50], Class Loss: 0.1574, Discrepancy Loss: 0.0418
Epoch [4/50], Class Loss: 0.0543, Discrepancy Loss: 0.0293
Epoch [5/50], Class Loss: 0.0332, Discrepancy Loss: 0.0280
Epoch [6/50], Class Loss: 0.0856, Discrepancy Loss: 0.0275
Epoch [7/50], Class Loss: 0.0610, Discrepancy Loss: 0.0187
Epoch [8/50], Class Loss: 0.1758, Discrepancy Loss: 0.0252
Epoch [9/50], Class Loss: 0.0359, Discrepancy Loss: 0.0180
Epoch [10/50], Class Loss: 0.0926, Discrepancy Loss: 0.0322
Epoch [11/50], Class Loss: 0.0464, Discrepancy Loss: 0.0317
Epoch [12/50], Class Loss: 0.0615, Discrepancy Loss: 0.0319
Epoch [13/50], Class Loss: 0.0286, Discrepancy Loss: 0.0296
Epoch [14/50], Class Loss: 0.0305, Discrepancy Loss: 0.0228
Epoch [15/50], Class Loss: 0.0220, Discrepancy Loss: 0.0276
Epoch [16/50], Class Loss: 0.0432, Discrepancy Loss: 0.0259
Epoch [17/50], Class Loss: 0.0235, Discrepancy Loss: 0.0201
Epoch [18/50], Class Loss: 0.0158, Discrepancy Loss: 0.0238
Epoch [19/50], Class Loss: 0.0228, Discrepancy Loss: 0.0239
Epoch [20/50], Class Loss: 0.0145, Discrepancy Loss: 0.0200
Epoch [21/50], Class Loss: 0.0166, Discrepancy Loss: 0.0202
Epoch [22/50], Class Loss: 0.0186, Discrepancy Loss: 0.0183
Epoch [23/50], Class Loss: 0.0375, Discrepancy Loss: 0.0166
Epoch [24/50], Class Loss: 0.0141, Discrepancy Loss: 0.0230
Epoch [25/50], Class Loss: 0.0244, Discrepancy Loss: 0.0255
Epoch [26/50], Class Loss: 0.0124, Discrepancy Loss: 0.0235
Epoch [27/50], Class Loss: 0.0179, Discrepancy Loss: 0.0195
Epoch [28/50], Class Loss: 0.0153, Discrepancy Loss: 0.0230
Epoch [29/50], Class Loss: 0.0802, Discrepancy Loss: 0.0202
Epoch [30/50], Class Loss: 0.0406, Discrepancy Loss: 0.0243
Epoch [31/50], Class Loss: 0.0187, Discrepancy Loss: 0.0232
Epoch [32/50], Class Loss: 0.0098, Discrepancy Loss: 0.0273
Epoch [33/50], Class Loss: 0.0316, Discrepancy Loss: 0.0227
Epoch [34/50], Class Loss: 0.0117, Discrepancy Loss: 0.0268
Epoch [35/50], Class Loss: 0.0302, Discrepancy Loss: 0.0228
Epoch [36/50], Class Loss: 0.0118, Discrepancy Loss: 0.0223
Epoch [37/50], Class Loss: 0.0147, Discrepancy Loss: 0.0214
Epoch [38/50], Class Loss: 0.1644, Discrepancy Loss: 0.0216
Epoch [39/50], Class Loss: 0.0783, Discrepancy Loss: 0.0232
Epoch [40/50], Class Loss: 0.0155, Discrepancy Loss: 0.0188
Epoch [41/50], Class Loss: 0.0841, Discrepancy Loss: 0.0243
Epoch [42/50], Class Loss: 0.0101, Discrepancy Loss: 0.0285
Epoch [43/50], Class Loss: 0.0120, Discrepancy Loss: 0.0219
Epoch [44/50], Class Loss: 0.0182, Discrepancy Loss: 0.0208
Epoch [45/50], Class Loss: 0.0142, Discrepancy Loss: 0.0211
Epoch [46/50], Class Loss: 0.0175, Discrepancy Loss: 0.0259
Epoch [47/50], Class Loss: 0.0118, Discrepancy Loss: 0.0231
Epoch [48/50], Class Loss: 0.0166, Discrepancy Loss: 0.0215
Epoch [49/50], Class Loss: 0.0184, Discrepancy Loss: 0.0274
Epoch [50/50], Class Loss: 0.0129, Discrepancy Loss: 0.0300
Source Domain Performance - Accuracy: 94.00%, Precision: 95.21%, Recall: 94.08%, F1 Score: 94.02%
Target Domain Performance - Accuracy: 88.49%, Precision: 91.07%, Recall: 88.76%, F1 Score: 88.48%

Run 2/10
Epoch [1/50], Class Loss: 1.8118, Discrepancy Loss: 0.1407
Epoch [2/50], Class Loss: 0.2766, Discrepancy Loss: 0.0589
Epoch [3/50], Class Loss: 0.0644, Discrepancy Loss: 0.0372
Epoch [4/50], Class Loss: 0.0703, Discrepancy Loss: 0.0472
Epoch [5/50], Class Loss: 0.0644, Discrepancy Loss: 0.0308
Epoch [6/50], Class Loss: 0.0929, Discrepancy Loss: 0.0248
Epoch [7/50], Class Loss: 0.1155, Discrepancy Loss: 0.0377
Epoch [8/50], Class Loss: 0.0574, Discrepancy Loss: 0.0309
Epoch [9/50], Class Loss: 0.0458, Discrepancy Loss: 0.0318
Epoch [10/50], Class Loss: 0.0354, Discrepancy Loss: 0.0254
Epoch [11/50], Class Loss: 0.0164, Discrepancy Loss: 0.0247
Epoch [12/50], Class Loss: 0.0102, Discrepancy Loss: 0.0216
Epoch [13/50], Class Loss: 0.0409, Discrepancy Loss: 0.0226
Epoch [14/50], Class Loss: 0.0184, Discrepancy Loss: 0.0188
Epoch [15/50], Class Loss: 0.0129, Discrepancy Loss: 0.0178
Epoch [16/50], Class Loss: 0.0101, Discrepancy Loss: 0.0172
Epoch [17/50], Class Loss: 0.0270, Discrepancy Loss: 0.0202
Epoch [18/50], Class Loss: 0.0222, Discrepancy Loss: 0.0275
Epoch [19/50], Class Loss: 0.0120, Discrepancy Loss: 0.0176
Epoch [20/50], Class Loss: 0.0158, Discrepancy Loss: 0.0171
Epoch [21/50], Class Loss: 0.0173, Discrepancy Loss: 0.0215
Epoch [22/50], Class Loss: 0.0103, Discrepancy Loss: 0.0191
Epoch [23/50], Class Loss: 0.0316, Discrepancy Loss: 0.0232
Epoch [24/50], Class Loss: 0.0104, Discrepancy Loss: 0.0181
Epoch [25/50], Class Loss: 0.1634, Discrepancy Loss: 0.0171
Epoch [26/50], Class Loss: 0.0101, Discrepancy Loss: 0.0184
Epoch [27/50], Class Loss: 0.0159, Discrepancy Loss: 0.0196
Epoch [28/50], Class Loss: 0.0117, Discrepancy Loss: 0.0184
Epoch [29/50], Class Loss: 0.0842, Discrepancy Loss: 0.0193
Epoch [30/50], Class Loss: 0.0104, Discrepancy Loss: 0.0159
Epoch [31/50], Class Loss: 0.0190, Discrepancy Loss: 0.0182
Epoch [32/50], Class Loss: 0.0147, Discrepancy Loss: 0.0170
Epoch [33/50], Class Loss: 0.0103, Discrepancy Loss: 0.0174
Epoch [34/50], Class Loss: 0.0124, Discrepancy Loss: 0.0162
Epoch [35/50], Class Loss: 0.0117, Discrepancy Loss: 0.0154
Epoch [36/50], Class Loss: 0.0082, Discrepancy Loss: 0.0161
Epoch [37/50], Class Loss: 0.0067, Discrepancy Loss: 0.0165
Epoch [38/50], Class Loss: 0.0294, Discrepancy Loss: 0.0147
Epoch [39/50], Class Loss: 0.0071, Discrepancy Loss: 0.0145
Epoch [40/50], Class Loss: 0.0782, Discrepancy Loss: 0.0195
Epoch [41/50], Class Loss: 0.0091, Discrepancy Loss: 0.0165
Epoch [42/50], Class Loss: 0.0062, Discrepancy Loss: 0.0177
Epoch [43/50], Class Loss: 0.0613, Discrepancy Loss: 0.0159
Epoch [44/50], Class Loss: 0.0134, Discrepancy Loss: 0.0144
Epoch [45/50], Class Loss: 0.0753, Discrepancy Loss: 0.0181
Epoch [46/50], Class Loss: 0.0052, Discrepancy Loss: 0.0147
Epoch [47/50], Class Loss: 0.0078, Discrepancy Loss: 0.0164
Epoch [48/50], Class Loss: 0.0079, Discrepancy Loss: 0.0151
Epoch [49/50], Class Loss: 0.0187, Discrepancy Loss: 0.0166
Epoch [50/50], Class Loss: 0.0090, Discrepancy Loss: 0.0188
Source Domain Performance - Accuracy: 93.65%, Precision: 94.96%, Recall: 93.66%, F1 Score: 93.78%
Target Domain Performance - Accuracy: 82.55%, Precision: 86.42%, Recall: 82.87%, F1 Score: 82.25%

Run 3/10
Epoch [1/50], Class Loss: 2.0409, Discrepancy Loss: 0.1232
Epoch [2/50], Class Loss: 0.4145, Discrepancy Loss: 0.0660
Epoch [3/50], Class Loss: 0.0710, Discrepancy Loss: 0.0299
Epoch [4/50], Class Loss: 0.0749, Discrepancy Loss: 0.0316
Epoch [5/50], Class Loss: 0.0794, Discrepancy Loss: 0.0310
Epoch [6/50], Class Loss: 0.0371, Discrepancy Loss: 0.0201
Epoch [7/50], Class Loss: 0.0640, Discrepancy Loss: 0.0272
Epoch [8/50], Class Loss: 0.0686, Discrepancy Loss: 0.0229
Epoch [9/50], Class Loss: 0.0876, Discrepancy Loss: 0.0256
Epoch [10/50], Class Loss: 0.0509, Discrepancy Loss: 0.0198
Epoch [11/50], Class Loss: 0.0265, Discrepancy Loss: 0.0205
Epoch [12/50], Class Loss: 0.0527, Discrepancy Loss: 0.0154
Epoch [13/50], Class Loss: 0.0173, Discrepancy Loss: 0.0166
Epoch [14/50], Class Loss: 0.0210, Discrepancy Loss: 0.0145
Epoch [15/50], Class Loss: 0.0493, Discrepancy Loss: 0.0140
Epoch [16/50], Class Loss: 0.0393, Discrepancy Loss: 0.0183
Epoch [17/50], Class Loss: 0.0549, Discrepancy Loss: 0.0191
Epoch [18/50], Class Loss: 0.0211, Discrepancy Loss: 0.0175
Epoch [19/50], Class Loss: 0.0285, Discrepancy Loss: 0.0171
Epoch [20/50], Class Loss: 0.0182, Discrepancy Loss: 0.0217
Epoch [21/50], Class Loss: 0.0075, Discrepancy Loss: 0.0182
Epoch [22/50], Class Loss: 0.1492, Discrepancy Loss: 0.0169
Epoch [23/50], Class Loss: 0.0125, Discrepancy Loss: 0.0159
Epoch [24/50], Class Loss: 0.0538, Discrepancy Loss: 0.0139
Epoch [25/50], Class Loss: 0.0061, Discrepancy Loss: 0.0145
Epoch [26/50], Class Loss: 0.1182, Discrepancy Loss: 0.0150
Epoch [27/50], Class Loss: 0.0180, Discrepancy Loss: 0.0149
Epoch [28/50], Class Loss: 0.0098, Discrepancy Loss: 0.0121
Epoch [29/50], Class Loss: 0.0083, Discrepancy Loss: 0.0141
Epoch [30/50], Class Loss: 0.0072, Discrepancy Loss: 0.0127
Epoch [31/50], Class Loss: 0.0064, Discrepancy Loss: 0.0131
Epoch [32/50], Class Loss: 0.0083, Discrepancy Loss: 0.0111
Epoch [33/50], Class Loss: 0.0062, Discrepancy Loss: 0.0133
Epoch [34/50], Class Loss: 0.0099, Discrepancy Loss: 0.0114
Epoch [35/50], Class Loss: 0.0134, Discrepancy Loss: 0.0107
Epoch [36/50], Class Loss: 0.0063, Discrepancy Loss: 0.0154
Epoch [37/50], Class Loss: 0.0048, Discrepancy Loss: 0.0128
Epoch [38/50], Class Loss: 0.0091, Discrepancy Loss: 0.0117
Epoch [39/50], Class Loss: 0.0355, Discrepancy Loss: 0.0129
Epoch [40/50], Class Loss: 0.0073, Discrepancy Loss: 0.0140
Epoch [41/50], Class Loss: 0.0926, Discrepancy Loss: 0.0123
Epoch [42/50], Class Loss: 0.0056, Discrepancy Loss: 0.0122
Epoch [43/50], Class Loss: 0.0251, Discrepancy Loss: 0.0130
Epoch [44/50], Class Loss: 0.1509, Discrepancy Loss: 0.0111
Epoch [45/50], Class Loss: 0.0105, Discrepancy Loss: 0.0138
Epoch [46/50], Class Loss: 0.0082, Discrepancy Loss: 0.0127
Epoch [47/50], Class Loss: 0.0113, Discrepancy Loss: 0.0127
Epoch [48/50], Class Loss: 0.0215, Discrepancy Loss: 0.0130
Epoch [49/50], Class Loss: 0.0082, Discrepancy Loss: 0.0164
Epoch [50/50], Class Loss: 0.0655, Discrepancy Loss: 0.0126
Source Domain Performance - Accuracy: 94.00%, Precision: 94.81%, Recall: 94.07%, F1 Score: 93.90%
Target Domain Performance - Accuracy: 85.55%, Precision: 88.99%, Recall: 85.85%, F1 Score: 85.27%

Run 4/10
Epoch [1/50], Class Loss: 1.4720, Discrepancy Loss: 0.1334
Epoch [2/50], Class Loss: 0.3942, Discrepancy Loss: 0.0730
Epoch [3/50], Class Loss: 0.1931, Discrepancy Loss: 0.0489
Epoch [4/50], Class Loss: 0.6997, Discrepancy Loss: 0.0595
Epoch [5/50], Class Loss: 0.0837, Discrepancy Loss: 0.0387
Epoch [6/50], Class Loss: 0.1139, Discrepancy Loss: 0.0298
Epoch [7/50], Class Loss: 0.2475, Discrepancy Loss: 0.0305
Epoch [8/50], Class Loss: 0.1183, Discrepancy Loss: 0.0392
Epoch [9/50], Class Loss: 0.0318, Discrepancy Loss: 0.0326
Epoch [10/50], Class Loss: 0.0421, Discrepancy Loss: 0.0208
Epoch [11/50], Class Loss: 0.0253, Discrepancy Loss: 0.0298
Epoch [12/50], Class Loss: 0.0753, Discrepancy Loss: 0.0288
Epoch [13/50], Class Loss: 0.0175, Discrepancy Loss: 0.0338
Epoch [14/50], Class Loss: 0.0896, Discrepancy Loss: 0.0268
Epoch [15/50], Class Loss: 0.0373, Discrepancy Loss: 0.0278
Epoch [16/50], Class Loss: 0.0161, Discrepancy Loss: 0.0254
Epoch [17/50], Class Loss: 0.0233, Discrepancy Loss: 0.0311
Epoch [18/50], Class Loss: 0.0310, Discrepancy Loss: 0.0252
Epoch [19/50], Class Loss: 0.0242, Discrepancy Loss: 0.0292
Epoch [20/50], Class Loss: 0.0139, Discrepancy Loss: 0.0265
Epoch [21/50], Class Loss: 0.0157, Discrepancy Loss: 0.0273
Epoch [22/50], Class Loss: 0.0187, Discrepancy Loss: 0.0260
Epoch [23/50], Class Loss: 0.0136, Discrepancy Loss: 0.0225
Epoch [24/50], Class Loss: 0.0139, Discrepancy Loss: 0.0290
Epoch [25/50], Class Loss: 0.0114, Discrepancy Loss: 0.0262
Epoch [26/50], Class Loss: 0.0258, Discrepancy Loss: 0.0232
Epoch [27/50], Class Loss: 0.0240, Discrepancy Loss: 0.0212
Epoch [28/50], Class Loss: 0.0136, Discrepancy Loss: 0.0234
Epoch [29/50], Class Loss: 0.0130, Discrepancy Loss: 0.0217
Epoch [30/50], Class Loss: 0.0137, Discrepancy Loss: 0.0210
Epoch [31/50], Class Loss: 0.0528, Discrepancy Loss: 0.0226
Epoch [32/50], Class Loss: 0.0664, Discrepancy Loss: 0.0262
Epoch [33/50], Class Loss: 0.0148, Discrepancy Loss: 0.0273
Epoch [34/50], Class Loss: 0.0591, Discrepancy Loss: 0.0268
Epoch [35/50], Class Loss: 0.0150, Discrepancy Loss: 0.0243
Epoch [36/50], Class Loss: 0.0266, Discrepancy Loss: 0.0248
Epoch [37/50], Class Loss: 0.0162, Discrepancy Loss: 0.0264
Epoch [38/50], Class Loss: 0.1141, Discrepancy Loss: 0.0245
Epoch [39/50], Class Loss: 0.0150, Discrepancy Loss: 0.0210
Epoch [40/50], Class Loss: 0.0133, Discrepancy Loss: 0.0268
Epoch [41/50], Class Loss: 0.0162, Discrepancy Loss: 0.0265
Epoch [42/50], Class Loss: 0.0445, Discrepancy Loss: 0.0217
Epoch [43/50], Class Loss: 0.0117, Discrepancy Loss: 0.0225
Epoch [44/50], Class Loss: 0.0188, Discrepancy Loss: 0.0254
Epoch [45/50], Class Loss: 0.0655, Discrepancy Loss: 0.0224
Epoch [46/50], Class Loss: 0.0175, Discrepancy Loss: 0.0258
Epoch [47/50], Class Loss: 0.0122, Discrepancy Loss: 0.0233
Epoch [48/50], Class Loss: 0.0130, Discrepancy Loss: 0.0258
Epoch [49/50], Class Loss: 0.0197, Discrepancy Loss: 0.0227
Epoch [50/50], Class Loss: 0.0180, Discrepancy Loss: 0.0256
Source Domain Performance - Accuracy: 82.31%, Precision: 89.25%, Recall: 82.51%, F1 Score: 80.32%
Target Domain Performance - Accuracy: 82.97%, Precision: 85.77%, Recall: 83.23%, F1 Score: 82.96%

Run 5/10
Epoch [1/50], Class Loss: 2.1004, Discrepancy Loss: 0.1460
Epoch [2/50], Class Loss: 0.5988, Discrepancy Loss: 0.0950
Epoch [3/50], Class Loss: 0.1646, Discrepancy Loss: 0.0698
Epoch [4/50], Class Loss: 0.2630, Discrepancy Loss: 0.0515
Epoch [5/50], Class Loss: 0.3848, Discrepancy Loss: 0.0509
Epoch [6/50], Class Loss: 0.1990, Discrepancy Loss: 0.0475
Epoch [7/50], Class Loss: 0.1295, Discrepancy Loss: 0.0382
Epoch [8/50], Class Loss: 0.0450, Discrepancy Loss: 0.0388
Epoch [9/50], Class Loss: 0.0540, Discrepancy Loss: 0.0332
Epoch [10/50], Class Loss: 0.1397, Discrepancy Loss: 0.0317
Epoch [11/50], Class Loss: 0.0328, Discrepancy Loss: 0.0233
Epoch [12/50], Class Loss: 0.1439, Discrepancy Loss: 0.0253
Epoch [13/50], Class Loss: 0.0299, Discrepancy Loss: 0.0306
Epoch [14/50], Class Loss: 0.1389, Discrepancy Loss: 0.0283
Epoch [15/50], Class Loss: 0.0238, Discrepancy Loss: 0.0245
Epoch [16/50], Class Loss: 0.0158, Discrepancy Loss: 0.0244
Epoch [17/50], Class Loss: 0.0156, Discrepancy Loss: 0.0272
Epoch [18/50], Class Loss: 0.0154, Discrepancy Loss: 0.0276
Epoch [19/50], Class Loss: 0.0114, Discrepancy Loss: 0.0208
Epoch [20/50], Class Loss: 0.0121, Discrepancy Loss: 0.0257
Epoch [21/50], Class Loss: 0.0147, Discrepancy Loss: 0.0247
Epoch [22/50], Class Loss: 0.0143, Discrepancy Loss: 0.0244
Epoch [23/50], Class Loss: 0.0119, Discrepancy Loss: 0.0218
Epoch [24/50], Class Loss: 0.0123, Discrepancy Loss: 0.0254
Epoch [25/50], Class Loss: 0.0153, Discrepancy Loss: 0.0244
Epoch [26/50], Class Loss: 0.0385, Discrepancy Loss: 0.0228
Epoch [27/50], Class Loss: 0.0162, Discrepancy Loss: 0.0206
Epoch [28/50], Class Loss: 0.0106, Discrepancy Loss: 0.0286
Epoch [29/50], Class Loss: 0.0132, Discrepancy Loss: 0.0264
Epoch [30/50], Class Loss: 0.0107, Discrepancy Loss: 0.0225
Epoch [31/50], Class Loss: 0.0162, Discrepancy Loss: 0.0219
Epoch [32/50], Class Loss: 0.0114, Discrepancy Loss: 0.0234
Epoch [33/50], Class Loss: 0.0107, Discrepancy Loss: 0.0216
Epoch [34/50], Class Loss: 0.0335, Discrepancy Loss: 0.0241
Epoch [35/50], Class Loss: 0.0111, Discrepancy Loss: 0.0225
Epoch [36/50], Class Loss: 0.0290, Discrepancy Loss: 0.0224
Epoch [37/50], Class Loss: 0.0173, Discrepancy Loss: 0.0263
Epoch [38/50], Class Loss: 0.0252, Discrepancy Loss: 0.0253
Epoch [39/50], Class Loss: 0.0091, Discrepancy Loss: 0.0271
Epoch [40/50], Class Loss: 0.0117, Discrepancy Loss: 0.0218
Epoch [41/50], Class Loss: 0.0113, Discrepancy Loss: 0.0283
Epoch [42/50], Class Loss: 0.0483, Discrepancy Loss: 0.0213
Epoch [43/50], Class Loss: 0.0376, Discrepancy Loss: 0.0227
Epoch [44/50], Class Loss: 0.0704, Discrepancy Loss: 0.0231
Epoch [45/50], Class Loss: 0.0129, Discrepancy Loss: 0.0213
Epoch [46/50], Class Loss: 0.0092, Discrepancy Loss: 0.0268
Epoch [47/50], Class Loss: 0.0386, Discrepancy Loss: 0.0229
Epoch [48/50], Class Loss: 0.0117, Discrepancy Loss: 0.0223
Epoch [49/50], Class Loss: 0.0132, Discrepancy Loss: 0.0224
Epoch [50/50], Class Loss: 0.0116, Discrepancy Loss: 0.0238
Source Domain Performance - Accuracy: 66.97%, Precision: 67.94%, Recall: 67.01%, F1 Score: 63.98%
Target Domain Performance - Accuracy: 85.07%, Precision: 87.66%, Recall: 85.34%, F1 Score: 85.21%

Run 6/10
Epoch [1/50], Class Loss: 1.9009, Discrepancy Loss: 0.1159
Epoch [2/50], Class Loss: 0.6020, Discrepancy Loss: 0.1006
Epoch [3/50], Class Loss: 0.1788, Discrepancy Loss: 0.0952
Epoch [4/50], Class Loss: 0.0605, Discrepancy Loss: 0.0562
Epoch [5/50], Class Loss: 0.0643, Discrepancy Loss: 0.0503
Epoch [6/50], Class Loss: 0.1246, Discrepancy Loss: 0.0389
Epoch [7/50], Class Loss: 0.4137, Discrepancy Loss: 0.0727
Epoch [8/50], Class Loss: 0.1216, Discrepancy Loss: 0.0731
Epoch [9/50], Class Loss: 0.1306, Discrepancy Loss: 0.0531
Epoch [10/50], Class Loss: 0.2838, Discrepancy Loss: 0.0631
Epoch [11/50], Class Loss: 0.1454, Discrepancy Loss: 0.0476
Epoch [12/50], Class Loss: 0.0716, Discrepancy Loss: 0.0407
Epoch [13/50], Class Loss: 0.0361, Discrepancy Loss: 0.0374
Epoch [14/50], Class Loss: 0.0405, Discrepancy Loss: 0.0357
Epoch [15/50], Class Loss: 0.0417, Discrepancy Loss: 0.0415
Epoch [16/50], Class Loss: 0.0304, Discrepancy Loss: 0.0384
Epoch [17/50], Class Loss: 0.0416, Discrepancy Loss: 0.0366
Epoch [18/50], Class Loss: 0.0382, Discrepancy Loss: 0.0403
Epoch [19/50], Class Loss: 0.0988, Discrepancy Loss: 0.0373
Epoch [20/50], Class Loss: 0.0520, Discrepancy Loss: 0.0441
Epoch [21/50], Class Loss: 0.0303, Discrepancy Loss: 0.0349
Epoch [22/50], Class Loss: 0.0289, Discrepancy Loss: 0.0361
Epoch [23/50], Class Loss: 0.0540, Discrepancy Loss: 0.0328
Epoch [24/50], Class Loss: 0.0271, Discrepancy Loss: 0.0372
Epoch [25/50], Class Loss: 0.0251, Discrepancy Loss: 0.0352
Epoch [26/50], Class Loss: 0.0257, Discrepancy Loss: 0.0316
Epoch [27/50], Class Loss: 0.0252, Discrepancy Loss: 0.0318
Epoch [28/50], Class Loss: 0.0459, Discrepancy Loss: 0.0287
Epoch [29/50], Class Loss: 0.0236, Discrepancy Loss: 0.0340
Epoch [30/50], Class Loss: 0.0333, Discrepancy Loss: 0.0310
Epoch [31/50], Class Loss: 0.0277, Discrepancy Loss: 0.0300
Epoch [32/50], Class Loss: 0.0366, Discrepancy Loss: 0.0421
Epoch [33/50], Class Loss: 0.0213, Discrepancy Loss: 0.0325
Epoch [34/50], Class Loss: 0.0279, Discrepancy Loss: 0.0336
Epoch [35/50], Class Loss: 0.0199, Discrepancy Loss: 0.0336
Epoch [36/50], Class Loss: 0.0211, Discrepancy Loss: 0.0301
Epoch [37/50], Class Loss: 0.0293, Discrepancy Loss: 0.0324
Epoch [38/50], Class Loss: 0.0412, Discrepancy Loss: 0.0374
Epoch [39/50], Class Loss: 0.0261, Discrepancy Loss: 0.0286
Epoch [40/50], Class Loss: 0.0957, Discrepancy Loss: 0.0301
Epoch [41/50], Class Loss: 0.0219, Discrepancy Loss: 0.0306
Epoch [42/50], Class Loss: 0.0290, Discrepancy Loss: 0.0331
Epoch [43/50], Class Loss: 0.1519, Discrepancy Loss: 0.0278
Epoch [44/50], Class Loss: 0.0226, Discrepancy Loss: 0.0355
Epoch [45/50], Class Loss: 0.1271, Discrepancy Loss: 0.0342
Epoch [46/50], Class Loss: 0.0276, Discrepancy Loss: 0.0288
Epoch [47/50], Class Loss: 0.0236, Discrepancy Loss: 0.0304
Epoch [48/50], Class Loss: 0.0288, Discrepancy Loss: 0.0324
Epoch [49/50], Class Loss: 0.0379, Discrepancy Loss: 0.0295
Epoch [50/50], Class Loss: 0.0355, Discrepancy Loss: 0.0292
Source Domain Performance - Accuracy: 86.57%, Precision: 91.33%, Recall: 86.72%, F1 Score: 85.94%
Target Domain Performance - Accuracy: 74.88%, Precision: 80.38%, Recall: 75.26%, F1 Score: 74.33%

Run 7/10
Epoch [1/50], Class Loss: 1.6082, Discrepancy Loss: 0.1127
Epoch [2/50], Class Loss: 0.6650, Discrepancy Loss: 0.0640
Epoch [3/50], Class Loss: 0.1879, Discrepancy Loss: 0.0535
Epoch [4/50], Class Loss: 0.0926, Discrepancy Loss: 0.0436
Epoch [5/50], Class Loss: 0.3636, Discrepancy Loss: 0.0481
Epoch [6/50], Class Loss: 0.1426, Discrepancy Loss: 0.0629
Epoch [7/50], Class Loss: 0.0526, Discrepancy Loss: 0.0470
Epoch [8/50], Class Loss: 0.0566, Discrepancy Loss: 0.0301
Epoch [9/50], Class Loss: 0.1561, Discrepancy Loss: 0.0343
Epoch [10/50], Class Loss: 0.1356, Discrepancy Loss: 0.0678
Epoch [11/50], Class Loss: 0.0457, Discrepancy Loss: 0.0339
Epoch [12/50], Class Loss: 0.0297, Discrepancy Loss: 0.0335
Epoch [13/50], Class Loss: 0.0199, Discrepancy Loss: 0.0304
Epoch [14/50], Class Loss: 0.0403, Discrepancy Loss: 0.0253
Epoch [15/50], Class Loss: 0.0285, Discrepancy Loss: 0.0308
Epoch [16/50], Class Loss: 0.0426, Discrepancy Loss: 0.0359
Epoch [17/50], Class Loss: 0.0194, Discrepancy Loss: 0.0262
Epoch [18/50], Class Loss: 0.0193, Discrepancy Loss: 0.0211
Epoch [19/50], Class Loss: 0.0180, Discrepancy Loss: 0.0246
Epoch [20/50], Class Loss: 0.0226, Discrepancy Loss: 0.0215
Epoch [21/50], Class Loss: 0.0114, Discrepancy Loss: 0.0241
Epoch [22/50], Class Loss: 0.0153, Discrepancy Loss: 0.0220
Epoch [23/50], Class Loss: 0.0108, Discrepancy Loss: 0.0226
Epoch [24/50], Class Loss: 0.0667, Discrepancy Loss: 0.0257
Epoch [25/50], Class Loss: 0.0152, Discrepancy Loss: 0.0244
Epoch [26/50], Class Loss: 0.0092, Discrepancy Loss: 0.0224
Epoch [27/50], Class Loss: 0.0462, Discrepancy Loss: 0.0243
Epoch [28/50], Class Loss: 0.0095, Discrepancy Loss: 0.0223
Epoch [29/50], Class Loss: 0.0083, Discrepancy Loss: 0.0201
Epoch [30/50], Class Loss: 0.0094, Discrepancy Loss: 0.0184
Epoch [31/50], Class Loss: 0.0084, Discrepancy Loss: 0.0202
Epoch [32/50], Class Loss: 0.0146, Discrepancy Loss: 0.0223
Epoch [33/50], Class Loss: 0.0137, Discrepancy Loss: 0.0223
Epoch [34/50], Class Loss: 0.0101, Discrepancy Loss: 0.0193
Epoch [35/50], Class Loss: 0.0113, Discrepancy Loss: 0.0205
Epoch [36/50], Class Loss: 0.0158, Discrepancy Loss: 0.0241
Epoch [37/50], Class Loss: 0.0167, Discrepancy Loss: 0.0197
Epoch [38/50], Class Loss: 0.0097, Discrepancy Loss: 0.0212
Epoch [39/50], Class Loss: 0.0139, Discrepancy Loss: 0.0213
Epoch [40/50], Class Loss: 0.0112, Discrepancy Loss: 0.0213
Epoch [41/50], Class Loss: 0.0117, Discrepancy Loss: 0.0213
Epoch [42/50], Class Loss: 0.0074, Discrepancy Loss: 0.0191
Epoch [43/50], Class Loss: 0.0297, Discrepancy Loss: 0.0198
Epoch [44/50], Class Loss: 0.0098, Discrepancy Loss: 0.0216
Epoch [45/50], Class Loss: 0.0112, Discrepancy Loss: 0.0197
Epoch [46/50], Class Loss: 0.1460, Discrepancy Loss: 0.0221
Epoch [47/50], Class Loss: 0.0109, Discrepancy Loss: 0.0230
Epoch [48/50], Class Loss: 0.0095, Discrepancy Loss: 0.0239
Epoch [49/50], Class Loss: 0.0101, Discrepancy Loss: 0.0239
Epoch [50/50], Class Loss: 0.0086, Discrepancy Loss: 0.0223
Source Domain Performance - Accuracy: 77.64%, Precision: 83.11%, Recall: 77.81%, F1 Score: 74.94%
Target Domain Performance - Accuracy: 80.10%, Precision: 85.26%, Recall: 80.49%, F1 Score: 79.39%

Run 8/10
Epoch [1/50], Class Loss: 1.9487, Discrepancy Loss: 0.1417
Epoch [2/50], Class Loss: 0.6343, Discrepancy Loss: 0.0875
Epoch [3/50], Class Loss: 0.1251, Discrepancy Loss: 0.0501
Epoch [4/50], Class Loss: 0.0955, Discrepancy Loss: 0.0399
Epoch [5/50], Class Loss: 0.0931, Discrepancy Loss: 0.0291
Epoch [6/50], Class Loss: 0.0720, Discrepancy Loss: 0.0253
Epoch [7/50], Class Loss: 0.0387, Discrepancy Loss: 0.0211
Epoch [8/50], Class Loss: 0.0795, Discrepancy Loss: 0.0280
Epoch [9/50], Class Loss: 0.0374, Discrepancy Loss: 0.0200
Epoch [10/50], Class Loss: 0.0559, Discrepancy Loss: 0.0227
Epoch [11/50], Class Loss: 0.0240, Discrepancy Loss: 0.0216
Epoch [12/50], Class Loss: 0.0150, Discrepancy Loss: 0.0189
Epoch [13/50], Class Loss: 0.0209, Discrepancy Loss: 0.0152
Epoch [14/50], Class Loss: 0.0108, Discrepancy Loss: 0.0129
Epoch [15/50], Class Loss: 0.0115, Discrepancy Loss: 0.0157
Epoch [16/50], Class Loss: 0.0086, Discrepancy Loss: 0.0142
Epoch [17/50], Class Loss: 0.0094, Discrepancy Loss: 0.0130
Epoch [18/50], Class Loss: 0.0102, Discrepancy Loss: 0.0135
Epoch [19/50], Class Loss: 0.0070, Discrepancy Loss: 0.0139
Epoch [20/50], Class Loss: 0.0063, Discrepancy Loss: 0.0146
Epoch [21/50], Class Loss: 0.1263, Discrepancy Loss: 0.0119
Epoch [22/50], Class Loss: 0.0283, Discrepancy Loss: 0.0138
Epoch [23/50], Class Loss: 0.0135, Discrepancy Loss: 0.0142
Epoch [24/50], Class Loss: 0.0099, Discrepancy Loss: 0.0131
Epoch [25/50], Class Loss: 0.0120, Discrepancy Loss: 0.0151
Epoch [26/50], Class Loss: 0.0074, Discrepancy Loss: 0.0118
Epoch [27/50], Class Loss: 0.0069, Discrepancy Loss: 0.0109
Epoch [28/50], Class Loss: 0.0054, Discrepancy Loss: 0.0133
Epoch [29/50], Class Loss: 0.1877, Discrepancy Loss: 0.0129
Epoch [30/50], Class Loss: 0.0434, Discrepancy Loss: 0.0148
Epoch [31/50], Class Loss: 0.1425, Discrepancy Loss: 0.0145
Epoch [32/50], Class Loss: 0.0076, Discrepancy Loss: 0.0134
Epoch [33/50], Class Loss: 0.0073, Discrepancy Loss: 0.0127
Epoch [34/50], Class Loss: 0.0155, Discrepancy Loss: 0.0110
Epoch [35/50], Class Loss: 0.0055, Discrepancy Loss: 0.0123
Epoch [36/50], Class Loss: 0.0075, Discrepancy Loss: 0.0130
Epoch [37/50], Class Loss: 0.0133, Discrepancy Loss: 0.0120
Epoch [38/50], Class Loss: 0.0069, Discrepancy Loss: 0.0111
Epoch [39/50], Class Loss: 0.0384, Discrepancy Loss: 0.0154
Epoch [40/50], Class Loss: 0.0097, Discrepancy Loss: 0.0129
Epoch [41/50], Class Loss: 0.1903, Discrepancy Loss: 0.0148
Epoch [42/50], Class Loss: 0.0061, Discrepancy Loss: 0.0150
Epoch [43/50], Class Loss: 0.0058, Discrepancy Loss: 0.0134
Epoch [44/50], Class Loss: 0.0120, Discrepancy Loss: 0.0131
Epoch [45/50], Class Loss: 0.0054, Discrepancy Loss: 0.0129
Epoch [46/50], Class Loss: 0.0062, Discrepancy Loss: 0.0122
Epoch [47/50], Class Loss: 0.0064, Discrepancy Loss: 0.0114
Epoch [48/50], Class Loss: 0.0267, Discrepancy Loss: 0.0138
Epoch [49/50], Class Loss: 0.0213, Discrepancy Loss: 0.0145
Epoch [50/50], Class Loss: 0.0100, Discrepancy Loss: 0.0126
Source Domain Performance - Accuracy: 87.05%, Precision: 91.59%, Recall: 87.17%, F1 Score: 86.67%
Target Domain Performance - Accuracy: 91.79%, Precision: 92.84%, Recall: 91.94%, F1 Score: 91.87%

Run 9/10
Epoch [1/50], Class Loss: 1.8868, Discrepancy Loss: 0.1400
Epoch [2/50], Class Loss: 0.5204, Discrepancy Loss: 0.1016
Epoch [3/50], Class Loss: 0.1411, Discrepancy Loss: 0.0724
Epoch [4/50], Class Loss: 0.0530, Discrepancy Loss: 0.0606
Epoch [5/50], Class Loss: 0.1476, Discrepancy Loss: 0.0426
Epoch [6/50], Class Loss: 0.4610, Discrepancy Loss: 0.0647
Epoch [7/50], Class Loss: 0.0955, Discrepancy Loss: 0.0650
Epoch [8/50], Class Loss: 0.0896, Discrepancy Loss: 0.0357
Epoch [9/50], Class Loss: 0.1427, Discrepancy Loss: 0.0523
Epoch [10/50], Class Loss: 0.1066, Discrepancy Loss: 0.0531
Epoch [11/50], Class Loss: 0.0593, Discrepancy Loss: 0.0314
Epoch [12/50], Class Loss: 0.0646, Discrepancy Loss: 0.0346
Epoch [13/50], Class Loss: 0.0338, Discrepancy Loss: 0.0270
Epoch [14/50], Class Loss: 0.0147, Discrepancy Loss: 0.0326
Epoch [15/50], Class Loss: 0.0259, Discrepancy Loss: 0.0320
Epoch [16/50], Class Loss: 0.0096, Discrepancy Loss: 0.0217
Epoch [17/50], Class Loss: 0.0164, Discrepancy Loss: 0.0271
Epoch [18/50], Class Loss: 0.0107, Discrepancy Loss: 0.0236
Epoch [19/50], Class Loss: 0.0150, Discrepancy Loss: 0.0255
Epoch [20/50], Class Loss: 0.0142, Discrepancy Loss: 0.0181
Epoch [21/50], Class Loss: 0.0130, Discrepancy Loss: 0.0244
Epoch [22/50], Class Loss: 0.0149, Discrepancy Loss: 0.0211
Epoch [23/50], Class Loss: 0.0136, Discrepancy Loss: 0.0236
Epoch [24/50], Class Loss: 0.0102, Discrepancy Loss: 0.0213
Epoch [25/50], Class Loss: 0.0204, Discrepancy Loss: 0.0210
Epoch [26/50], Class Loss: 0.0252, Discrepancy Loss: 0.0240
Epoch [27/50], Class Loss: 0.0136, Discrepancy Loss: 0.0220
Epoch [28/50], Class Loss: 0.0108, Discrepancy Loss: 0.0188
Epoch [29/50], Class Loss: 0.0092, Discrepancy Loss: 0.0205
Epoch [30/50], Class Loss: 0.0129, Discrepancy Loss: 0.0240
Epoch [31/50], Class Loss: 0.0091, Discrepancy Loss: 0.0209
Epoch [32/50], Class Loss: 0.0106, Discrepancy Loss: 0.0184
Epoch [33/50], Class Loss: 0.0105, Discrepancy Loss: 0.0198
Epoch [34/50], Class Loss: 0.0092, Discrepancy Loss: 0.0189
Epoch [35/50], Class Loss: 0.0148, Discrepancy Loss: 0.0176
Epoch [36/50], Class Loss: 0.0078, Discrepancy Loss: 0.0179
Epoch [37/50], Class Loss: 0.0107, Discrepancy Loss: 0.0230
Epoch [38/50], Class Loss: 0.0100, Discrepancy Loss: 0.0193
Epoch [39/50], Class Loss: 0.0152, Discrepancy Loss: 0.0169
Epoch [40/50], Class Loss: 0.0134, Discrepancy Loss: 0.0226
Epoch [41/50], Class Loss: 0.0085, Discrepancy Loss: 0.0194
Epoch [42/50], Class Loss: 0.0097, Discrepancy Loss: 0.0165
Epoch [43/50], Class Loss: 0.0099, Discrepancy Loss: 0.0191
Epoch [44/50], Class Loss: 0.0125, Discrepancy Loss: 0.0194
Epoch [45/50], Class Loss: 0.0125, Discrepancy Loss: 0.0212
Epoch [46/50], Class Loss: 0.0091, Discrepancy Loss: 0.0228
Epoch [47/50], Class Loss: 0.0098, Discrepancy Loss: 0.0205
Epoch [48/50], Class Loss: 0.0089, Discrepancy Loss: 0.0182
Epoch [49/50], Class Loss: 0.0184, Discrepancy Loss: 0.0184
Epoch [50/50], Class Loss: 0.0189, Discrepancy Loss: 0.0240
Source Domain Performance - Accuracy: 82.91%, Precision: 89.64%, Recall: 83.10%, F1 Score: 81.21%
Target Domain Performance - Accuracy: 88.19%, Precision: 90.67%, Recall: 88.44%, F1 Score: 88.16%

Run 10/10
Epoch [1/50], Class Loss: 1.6735, Discrepancy Loss: 0.1283
Epoch [2/50], Class Loss: 0.4230, Discrepancy Loss: 0.0751
Epoch [3/50], Class Loss: 0.0872, Discrepancy Loss: 0.0523
Epoch [4/50], Class Loss: 0.0690, Discrepancy Loss: 0.0348
Epoch [5/50], Class Loss: 0.0676, Discrepancy Loss: 0.0258
Epoch [6/50], Class Loss: 0.0910, Discrepancy Loss: 0.0405
Epoch [7/50], Class Loss: 0.0234, Discrepancy Loss: 0.0225
Epoch [8/50], Class Loss: 0.0334, Discrepancy Loss: 0.0254
Epoch [9/50], Class Loss: 0.0648, Discrepancy Loss: 0.0247
Epoch [10/50], Class Loss: 0.1584, Discrepancy Loss: 0.0476
Epoch [11/50], Class Loss: 0.0586, Discrepancy Loss: 0.0245
Epoch [12/50], Class Loss: 0.0238, Discrepancy Loss: 0.0227
Epoch [13/50], Class Loss: 0.0191, Discrepancy Loss: 0.0213
Epoch [14/50], Class Loss: 0.0159, Discrepancy Loss: 0.0242
Epoch [15/50], Class Loss: 0.0223, Discrepancy Loss: 0.0203
Epoch [16/50], Class Loss: 0.0242, Discrepancy Loss: 0.0205
Epoch [17/50], Class Loss: 0.0282, Discrepancy Loss: 0.0194
Epoch [18/50], Class Loss: 0.0189, Discrepancy Loss: 0.0237
Epoch [19/50], Class Loss: 0.0107, Discrepancy Loss: 0.0170
Epoch [20/50], Class Loss: 0.0134, Discrepancy Loss: 0.0179
Epoch [21/50], Class Loss: 0.0146, Discrepancy Loss: 0.0158
Epoch [22/50], Class Loss: 0.0115, Discrepancy Loss: 0.0196
Epoch [23/50], Class Loss: 0.0081, Discrepancy Loss: 0.0185
Epoch [24/50], Class Loss: 0.0111, Discrepancy Loss: 0.0153
Epoch [25/50], Class Loss: 0.0183, Discrepancy Loss: 0.0203
Epoch [26/50], Class Loss: 0.0089, Discrepancy Loss: 0.0185
Epoch [27/50], Class Loss: 0.0187, Discrepancy Loss: 0.0173
Epoch [28/50], Class Loss: 0.1391, Discrepancy Loss: 0.0161
Epoch [29/50], Class Loss: 0.0269, Discrepancy Loss: 0.0157
Epoch [30/50], Class Loss: 0.0414, Discrepancy Loss: 0.0165
Epoch [31/50], Class Loss: 0.0212, Discrepancy Loss: 0.0165
Epoch [32/50], Class Loss: 0.0088, Discrepancy Loss: 0.0165
Epoch [33/50], Class Loss: 0.0252, Discrepancy Loss: 0.0164
Epoch [34/50], Class Loss: 0.0123, Discrepancy Loss: 0.0164
Epoch [35/50], Class Loss: 0.0189, Discrepancy Loss: 0.0175
Epoch [36/50], Class Loss: 0.0092, Discrepancy Loss: 0.0184
Epoch [37/50], Class Loss: 0.0623, Discrepancy Loss: 0.0171
Epoch [38/50], Class Loss: 0.0075, Discrepancy Loss: 0.0179
Epoch [39/50], Class Loss: 0.0099, Discrepancy Loss: 0.0187
Epoch [40/50], Class Loss: 0.0107, Discrepancy Loss: 0.0145
Epoch [41/50], Class Loss: 0.0129, Discrepancy Loss: 0.0182
Epoch [42/50], Class Loss: 0.0074, Discrepancy Loss: 0.0184
Epoch [43/50], Class Loss: 0.0178, Discrepancy Loss: 0.0171
Epoch [44/50], Class Loss: 0.0221, Discrepancy Loss: 0.0158
Epoch [45/50], Class Loss: 0.0161, Discrepancy Loss: 0.0153
Epoch [46/50], Class Loss: 0.0064, Discrepancy Loss: 0.0161
Epoch [47/50], Class Loss: 0.0107, Discrepancy Loss: 0.0170
Epoch [48/50], Class Loss: 0.0102, Discrepancy Loss: 0.0164
Epoch [49/50], Class Loss: 0.1198, Discrepancy Loss: 0.0172
Epoch [50/50], Class Loss: 0.0057, Discrepancy Loss: 0.0187
Source Domain Performance - Accuracy: 84.65%, Precision: 90.58%, Recall: 84.83%, F1 Score: 83.45%
Target Domain Performance - Accuracy: 84.35%, Precision: 88.61%, Recall: 84.70%, F1 Score: 83.87%

Source performance: 84.98% 88.84% 85.10% 83.82%
Target performance: 84.39% 87.77% 84.69% 84.18%

Per-Class Accuracy on Target Domain:
bpsk: 99.97%
qpsk: 56.87%
16qam: 82.73%
8apsk: 99.18%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.5610, Discrepancy Loss: 0.0366
Validation Loss: 2.0998
Epoch [2/50], Class Loss: 0.1448, Discrepancy Loss: 0.0218
Validation Loss: 0.0171
Epoch [3/50], Class Loss: 0.1054, Discrepancy Loss: 0.0642
Validation Loss: 3.0699
Epoch [4/50], Class Loss: 0.1386, Discrepancy Loss: 0.0507
Validation Loss: 14.9634
Epoch [5/50], Class Loss: 0.1075, Discrepancy Loss: 0.0580
Validation Loss: 0.0365
Epoch [6/50], Class Loss: 0.1150, Discrepancy Loss: 0.0995
Validation Loss: 2.6838
Epoch [7/50], Class Loss: 0.1379, Discrepancy Loss: 0.1031
Validation Loss: 0.1628
Early stopping!
Source Domain Performance - Accuracy: 98.38%, Precision: 98.44%, Recall: 98.40%, F1 Score: 98.37%
Target Domain Performance - Accuracy: 52.22%, Precision: 40.52%, Recall: 52.92%, F1 Score: 42.94%

Run 2/10
Epoch [1/50], Class Loss: 0.6311, Discrepancy Loss: 0.0370
Validation Loss: 6.9672
Epoch [2/50], Class Loss: 0.0796, Discrepancy Loss: 0.0129
Validation Loss: 0.1216
Epoch [3/50], Class Loss: 0.1293, Discrepancy Loss: 0.0146
Validation Loss: 0.1078
Epoch [4/50], Class Loss: 0.0415, Discrepancy Loss: 0.0338
Validation Loss: 5.8309
Epoch [5/50], Class Loss: 0.0870, Discrepancy Loss: 0.0414
Validation Loss: 4.2675
Epoch [6/50], Class Loss: 0.0672, Discrepancy Loss: 0.1087
Validation Loss: 3.9694
Epoch [7/50], Class Loss: 0.1264, Discrepancy Loss: 0.1732
Validation Loss: 1.7034
Epoch [8/50], Class Loss: 0.0599, Discrepancy Loss: 0.1052
Validation Loss: 0.7207
Early stopping!
Source Domain Performance - Accuracy: 89.09%, Precision: 91.18%, Recall: 89.17%, F1 Score: 88.98%
Target Domain Performance - Accuracy: 79.86%, Precision: 88.78%, Recall: 80.47%, F1 Score: 80.12%

Run 3/10
Epoch [1/50], Class Loss: 0.5221, Discrepancy Loss: 0.0295
Validation Loss: 1.2206
Epoch [2/50], Class Loss: 0.1214, Discrepancy Loss: 0.0188
Validation Loss: 1.5749
Epoch [3/50], Class Loss: 0.0485, Discrepancy Loss: 0.0143
Validation Loss: 0.4177
Epoch [4/50], Class Loss: 0.0967, Discrepancy Loss: 0.0501
Validation Loss: 4.3967
Epoch [5/50], Class Loss: 0.1703, Discrepancy Loss: 0.1190
Validation Loss: 2.0649
Epoch [6/50], Class Loss: 0.1554, Discrepancy Loss: 0.1187
Validation Loss: 2.0447
Epoch [7/50], Class Loss: 0.0558, Discrepancy Loss: 0.0480
Validation Loss: 0.0385
Epoch [8/50], Class Loss: 0.0360, Discrepancy Loss: 0.0207
Validation Loss: 0.1049
Epoch [9/50], Class Loss: 0.0659, Discrepancy Loss: 0.0287
Validation Loss: 0.1206
Epoch [10/50], Class Loss: 0.0574, Discrepancy Loss: 0.0358
Validation Loss: 0.9896
Epoch [11/50], Class Loss: 0.0100, Discrepancy Loss: 0.0136
Validation Loss: 0.0055
Epoch [12/50], Class Loss: 0.0061, Discrepancy Loss: 0.0154
Validation Loss: 0.0076
Epoch [13/50], Class Loss: 0.0095, Discrepancy Loss: 0.0362
Validation Loss: 0.0132
Epoch [14/50], Class Loss: 0.0248, Discrepancy Loss: 0.0957
Validation Loss: 0.0265
Epoch [15/50], Class Loss: 0.0173, Discrepancy Loss: 0.0879
Validation Loss: 0.0062
Epoch [16/50], Class Loss: 0.0062, Discrepancy Loss: 0.0179
Validation Loss: 0.0109
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 55.04%, Precision: 38.06%, Recall: 55.70%, F1 Score: 44.45%

Run 4/10
Epoch [1/50], Class Loss: 0.4506, Discrepancy Loss: 0.0453
Validation Loss: 7.6579
Epoch [2/50], Class Loss: 0.2235, Discrepancy Loss: 0.0403
Validation Loss: 5.4317
Epoch [3/50], Class Loss: 0.0965, Discrepancy Loss: 0.0268
Validation Loss: 0.4629
Epoch [4/50], Class Loss: 0.1036, Discrepancy Loss: 0.0466
Validation Loss: 0.1165
Epoch [5/50], Class Loss: 0.0463, Discrepancy Loss: 0.0366
Validation Loss: 2.2545
Epoch [6/50], Class Loss: 0.1034, Discrepancy Loss: 0.0220
Validation Loss: 1.8078
Epoch [7/50], Class Loss: 0.0715, Discrepancy Loss: 0.0213
Validation Loss: 0.5824
Epoch [8/50], Class Loss: 0.0254, Discrepancy Loss: 0.0198
Validation Loss: 3.3532
Epoch [9/50], Class Loss: 0.0227, Discrepancy Loss: 0.0231
Validation Loss: 4.6294
Early stopping!
Source Domain Performance - Accuracy: 75.66%, Precision: 62.74%, Recall: 75.00%, F1 Score: 66.88%
Target Domain Performance - Accuracy: 55.88%, Precision: 56.47%, Recall: 54.26%, F1 Score: 47.37%

Run 5/10
Epoch [1/50], Class Loss: 0.4664, Discrepancy Loss: 0.0246
Validation Loss: 1.0798
Epoch [2/50], Class Loss: 0.1194, Discrepancy Loss: 0.0286
Validation Loss: 0.5848
Epoch [3/50], Class Loss: 0.1216, Discrepancy Loss: 0.0306
Validation Loss: 3.8323
Epoch [4/50], Class Loss: 0.0445, Discrepancy Loss: 0.0312
Validation Loss: 0.5576
Epoch [5/50], Class Loss: 0.0499, Discrepancy Loss: 0.0098
Validation Loss: 11.6431
Epoch [6/50], Class Loss: 0.0273, Discrepancy Loss: 0.0197
Validation Loss: 4.8185
Epoch [7/50], Class Loss: 0.2267, Discrepancy Loss: 0.0529
Validation Loss: 0.2024
Epoch [8/50], Class Loss: 0.0592, Discrepancy Loss: 0.0399
Validation Loss: 4.3615
Epoch [9/50], Class Loss: 0.0380, Discrepancy Loss: 0.0453
Validation Loss: 0.0908
Epoch [10/50], Class Loss: 0.0827, Discrepancy Loss: 0.0966
Validation Loss: 1.0761
Epoch [11/50], Class Loss: 0.0277, Discrepancy Loss: 0.1241
Validation Loss: 0.0182
Epoch [12/50], Class Loss: 0.0303, Discrepancy Loss: 0.1670
Validation Loss: 0.0389
Epoch [13/50], Class Loss: 0.0371, Discrepancy Loss: 0.2739
Validation Loss: 0.0227
Epoch [14/50], Class Loss: 0.0362, Discrepancy Loss: 0.1802
Validation Loss: 0.0200
Epoch [15/50], Class Loss: 0.0213, Discrepancy Loss: 0.1041
Validation Loss: 0.0358
Epoch [16/50], Class Loss: 0.0272, Discrepancy Loss: 0.1971
Validation Loss: 0.0398
Early stopping!
Source Domain Performance - Accuracy: 99.70%, Precision: 99.70%, Recall: 99.70%, F1 Score: 99.70%
Target Domain Performance - Accuracy: 48.92%, Precision: 41.08%, Recall: 49.88%, F1 Score: 43.07%

Run 6/10
Epoch [1/50], Class Loss: 0.5066, Discrepancy Loss: 0.0341
Validation Loss: 11.2695
Epoch [2/50], Class Loss: 0.1385, Discrepancy Loss: 0.0101
Validation Loss: 0.2013
Epoch [3/50], Class Loss: 0.0379, Discrepancy Loss: 0.0116
Validation Loss: 0.0235
Epoch [4/50], Class Loss: 0.0414, Discrepancy Loss: 0.0063
Validation Loss: 0.0212
Epoch [5/50], Class Loss: 0.0145, Discrepancy Loss: 0.0073
Validation Loss: 0.4155
Epoch [6/50], Class Loss: 0.0474, Discrepancy Loss: 0.0295
Validation Loss: 1.5585
Epoch [7/50], Class Loss: 0.1516, Discrepancy Loss: 0.0851
Validation Loss: 0.7557
Epoch [8/50], Class Loss: 0.1123, Discrepancy Loss: 0.0427
Validation Loss: 2.9342
Epoch [9/50], Class Loss: 0.0864, Discrepancy Loss: 0.0638
Validation Loss: 0.6187
Early stopping!
Source Domain Performance - Accuracy: 88.37%, Precision: 92.13%, Recall: 88.06%, F1 Score: 87.81%
Target Domain Performance - Accuracy: 52.70%, Precision: 45.51%, Recall: 52.94%, F1 Score: 47.29%

Run 7/10
Epoch [1/50], Class Loss: 0.5102, Discrepancy Loss: 0.0312
Validation Loss: 0.4649
Epoch [2/50], Class Loss: 0.0957, Discrepancy Loss: 0.0132
Validation Loss: 0.2840
Epoch [3/50], Class Loss: 0.0762, Discrepancy Loss: 0.0266
Validation Loss: 1.7292
Epoch [4/50], Class Loss: 0.0779, Discrepancy Loss: 0.0222
Validation Loss: 14.2951
Epoch [5/50], Class Loss: 0.0677, Discrepancy Loss: 0.0223
Validation Loss: 2.9609
Epoch [6/50], Class Loss: 0.0429, Discrepancy Loss: 0.0318
Validation Loss: 0.4826
Epoch [7/50], Class Loss: 0.0894, Discrepancy Loss: 0.0605
Validation Loss: 1.9990
Early stopping!
Source Domain Performance - Accuracy: 71.94%, Precision: 59.57%, Recall: 71.29%, F1 Score: 63.69%
Target Domain Performance - Accuracy: 55.10%, Precision: 58.47%, Recall: 53.73%, F1 Score: 48.78%

Run 8/10
Epoch [1/50], Class Loss: 0.4919, Discrepancy Loss: 0.0241
Validation Loss: 4.9770
Epoch [2/50], Class Loss: 0.0543, Discrepancy Loss: 0.0107
Validation Loss: 2.7507
Epoch [3/50], Class Loss: 0.0684, Discrepancy Loss: 0.0095
Validation Loss: 9.8212
Epoch [4/50], Class Loss: 0.0605, Discrepancy Loss: 0.0086
Validation Loss: 0.0165
Epoch [5/50], Class Loss: 0.0373, Discrepancy Loss: 0.0197
Validation Loss: 0.1081
Epoch [6/50], Class Loss: 0.0490, Discrepancy Loss: 0.0219
Validation Loss: 1.2593
Epoch [7/50], Class Loss: 0.0282, Discrepancy Loss: 0.0246
Validation Loss: 2.0380
Epoch [8/50], Class Loss: 0.0819, Discrepancy Loss: 0.0914
Validation Loss: 1.8714
Epoch [9/50], Class Loss: 0.0750, Discrepancy Loss: 0.1331
Validation Loss: 2.6886
Early stopping!
Source Domain Performance - Accuracy: 75.54%, Precision: 85.84%, Recall: 75.07%, F1 Score: 69.34%
Target Domain Performance - Accuracy: 48.98%, Precision: 58.15%, Recall: 51.02%, F1 Score: 39.39%

Run 9/10
Epoch [1/50], Class Loss: 0.5398, Discrepancy Loss: 0.0294
Validation Loss: 9.6414
Epoch [2/50], Class Loss: 0.0850, Discrepancy Loss: 0.0127
Validation Loss: 0.2085
Epoch [3/50], Class Loss: 0.1008, Discrepancy Loss: 0.0108
Validation Loss: 3.5971
Epoch [4/50], Class Loss: 0.0750, Discrepancy Loss: 0.0365
Validation Loss: 0.6624
Epoch [5/50], Class Loss: 0.0464, Discrepancy Loss: 0.0354
Validation Loss: 0.0477
Epoch [6/50], Class Loss: 0.1509, Discrepancy Loss: 0.0613
Validation Loss: 15.8339
Epoch [7/50], Class Loss: 0.0725, Discrepancy Loss: 0.0326
Validation Loss: 0.2803
Epoch [8/50], Class Loss: 0.0880, Discrepancy Loss: 0.0938
Validation Loss: 10.9227
Epoch [9/50], Class Loss: 0.0518, Discrepancy Loss: 0.1143
Validation Loss: 3.5386
Epoch [10/50], Class Loss: 0.0692, Discrepancy Loss: 0.0640
Validation Loss: 1.4299
Early stopping!
Source Domain Performance - Accuracy: 78.06%, Precision: 87.65%, Recall: 77.48%, F1 Score: 72.04%
Target Domain Performance - Accuracy: 43.94%, Precision: 37.15%, Recall: 43.56%, F1 Score: 39.26%

Run 10/10
Epoch [1/50], Class Loss: 0.6088, Discrepancy Loss: 0.0247
Validation Loss: 10.1345
Epoch [2/50], Class Loss: 0.1107, Discrepancy Loss: 0.0159
Validation Loss: 0.6273
Epoch [3/50], Class Loss: 0.0596, Discrepancy Loss: 0.0107
Validation Loss: 1.6671
Epoch [4/50], Class Loss: 0.0620, Discrepancy Loss: 0.0351
Validation Loss: 4.7204
Epoch [5/50], Class Loss: 0.1481, Discrepancy Loss: 0.1270
Validation Loss: 0.9964
Epoch [6/50], Class Loss: 0.0796, Discrepancy Loss: 0.1549
Validation Loss: 2.0282
Epoch [7/50], Class Loss: 0.2162, Discrepancy Loss: 0.1320
Validation Loss: 0.8767
Early stopping!
Source Domain Performance - Accuracy: 88.85%, Precision: 90.45%, Recall: 88.72%, F1 Score: 88.67%
Target Domain Performance - Accuracy: 54.74%, Precision: 38.37%, Recall: 55.45%, F1 Score: 44.88%

Source performance: 86.55% 86.76% 86.28% 83.54%
Target performance: 54.74% 50.26% 54.99% 47.76%

Per-Class Accuracy on Target Domain:
bpsk: 72.15%
qpsk: 11.26%
16qam: 54.36%
8apsk: 82.20%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.4261, JMMD Loss: 0.2423
Validation Loss: 8.2587
Epoch [2/50], Class Loss: 0.0776, JMMD Loss: 0.2485
Validation Loss: 0.0437
Epoch [3/50], Class Loss: 0.0653, JMMD Loss: 0.1214
Validation Loss: 0.8483
Epoch [4/50], Class Loss: 0.0855, JMMD Loss: 0.1971
Validation Loss: 0.0336
Epoch [5/50], Class Loss: 0.0377, JMMD Loss: 0.1189
Validation Loss: 1.1486
Epoch [6/50], Class Loss: 0.0679, JMMD Loss: 0.1250
Validation Loss: 0.0105
Epoch [7/50], Class Loss: 0.0339, JMMD Loss: 0.1141
Validation Loss: 0.6666
Epoch [8/50], Class Loss: 0.0226, JMMD Loss: 0.1036
Validation Loss: 0.0057
Epoch [9/50], Class Loss: 0.0346, JMMD Loss: 0.0868
Validation Loss: 0.0216
Epoch [10/50], Class Loss: 0.0288, JMMD Loss: 0.1063
Validation Loss: 0.0794
Epoch [11/50], Class Loss: 0.0149, JMMD Loss: 0.0829
Validation Loss: 0.0063
Epoch [12/50], Class Loss: 0.0161, JMMD Loss: 0.0820
Validation Loss: 0.0557
Epoch [13/50], Class Loss: 0.0114, JMMD Loss: 0.0833
Validation Loss: 0.0661
Early stopping!
Source Domain Performance - Accuracy: 97.48%, Precision: 97.78%, Recall: 97.50%, F1 Score: 97.54%
Target Domain Performance - Accuracy: 90.77%, Precision: 92.59%, Recall: 90.98%, F1 Score: 90.78%

Run 2/10
Epoch [1/50], Class Loss: 0.3415, JMMD Loss: 0.3083
Validation Loss: 0.9172
Epoch [2/50], Class Loss: 0.1175, JMMD Loss: 0.2140
Validation Loss: 0.0874
Epoch [3/50], Class Loss: 0.0541, JMMD Loss: 0.1448
Validation Loss: 1.9556
Epoch [4/50], Class Loss: 0.0809, JMMD Loss: 0.1201
Validation Loss: 0.6047
Epoch [5/50], Class Loss: 0.1074, JMMD Loss: 0.1517
Validation Loss: 4.4521
Epoch [6/50], Class Loss: 0.1298, JMMD Loss: 0.1867
Validation Loss: 0.0446
Epoch [7/50], Class Loss: 0.0282, JMMD Loss: 0.1442
Validation Loss: 0.7541
Epoch [8/50], Class Loss: 0.0451, JMMD Loss: 0.1115
Validation Loss: 0.0545
Epoch [9/50], Class Loss: 0.0169, JMMD Loss: 0.1024
Validation Loss: 0.0807
Epoch [10/50], Class Loss: 0.0287, JMMD Loss: 0.0929
Validation Loss: 0.1263
Epoch [11/50], Class Loss: 0.0095, JMMD Loss: 0.0820
Validation Loss: 0.0246
Epoch [12/50], Class Loss: 0.0613, JMMD Loss: 0.0817
Validation Loss: 0.0233
Epoch [13/50], Class Loss: 0.0117, JMMD Loss: 0.0744
Validation Loss: 0.0144
Epoch [14/50], Class Loss: 0.0088, JMMD Loss: 0.0667
Validation Loss: 0.0130
Epoch [15/50], Class Loss: 0.0075, JMMD Loss: 0.0642
Validation Loss: 0.0247
Epoch [16/50], Class Loss: 0.0047, JMMD Loss: 0.0641
Validation Loss: 0.0335
Epoch [17/50], Class Loss: 0.0071, JMMD Loss: 0.0702
Validation Loss: 0.0126
Epoch [18/50], Class Loss: 0.0076, JMMD Loss: 0.0604
Validation Loss: 0.0195
Epoch [19/50], Class Loss: 0.0064, JMMD Loss: 0.0647
Validation Loss: 0.0138
Epoch [20/50], Class Loss: 0.0047, JMMD Loss: 0.0592
Validation Loss: 0.0148
Epoch [21/50], Class Loss: 0.0058, JMMD Loss: 0.0563
Validation Loss: 0.0236
Epoch [22/50], Class Loss: 0.0039, JMMD Loss: 0.0590
Validation Loss: 0.0143
Early stopping!
Source Domain Performance - Accuracy: 99.40%, Precision: 99.43%, Recall: 99.40%, F1 Score: 99.41%
Target Domain Performance - Accuracy: 94.78%, Precision: 95.53%, Recall: 94.91%, F1 Score: 94.85%

Run 3/10
Epoch [1/50], Class Loss: 0.3153, JMMD Loss: 0.3036
Validation Loss: 5.0130
Epoch [2/50], Class Loss: 0.0574, JMMD Loss: 0.1893
Validation Loss: 0.1464
Epoch [3/50], Class Loss: 0.0767, JMMD Loss: 0.1058
Validation Loss: 6.0619
Epoch [4/50], Class Loss: 0.1210, JMMD Loss: 0.2006
Validation Loss: 0.2556
Epoch [5/50], Class Loss: 0.0416, JMMD Loss: 0.1500
Validation Loss: 0.0204
Epoch [6/50], Class Loss: 0.0177, JMMD Loss: 0.1060
Validation Loss: 0.0249
Epoch [7/50], Class Loss: 0.0107, JMMD Loss: 0.0903
Validation Loss: 0.0732
Epoch [8/50], Class Loss: 0.0143, JMMD Loss: 0.0803
Validation Loss: 0.1358
Epoch [9/50], Class Loss: 0.0190, JMMD Loss: 0.0835
Validation Loss: 0.0775
Epoch [10/50], Class Loss: 0.0238, JMMD Loss: 0.0927
Validation Loss: 1.5156
Early stopping!
Source Domain Performance - Accuracy: 80.34%, Precision: 88.39%, Recall: 80.57%, F1 Score: 77.04%
Target Domain Performance - Accuracy: 56.18%, Precision: 69.00%, Recall: 56.80%, F1 Score: 50.56%

Run 4/10
Epoch [1/50], Class Loss: 0.2984, JMMD Loss: 0.3256
Validation Loss: 8.6821
Epoch [2/50], Class Loss: 0.0794, JMMD Loss: 0.2233
Validation Loss: 0.2092
Epoch [3/50], Class Loss: 0.0432, JMMD Loss: 0.1602
Validation Loss: 0.2983
Epoch [4/50], Class Loss: 0.0149, JMMD Loss: 0.1040
Validation Loss: 2.8746
Epoch [5/50], Class Loss: 0.0576, JMMD Loss: 0.1365
Validation Loss: 0.0263
Epoch [6/50], Class Loss: 0.0178, JMMD Loss: 0.1066
Validation Loss: 0.0329
Epoch [7/50], Class Loss: 0.0091, JMMD Loss: 0.0647
Validation Loss: 0.0818
Epoch [8/50], Class Loss: 0.0162, JMMD Loss: 0.0857
Validation Loss: 0.1275
Epoch [9/50], Class Loss: 0.0081, JMMD Loss: 0.0758
Validation Loss: 0.2732
Epoch [10/50], Class Loss: 0.0122, JMMD Loss: 0.0737
Validation Loss: 0.0838
Early stopping!
Source Domain Performance - Accuracy: 97.30%, Precision: 97.63%, Recall: 97.33%, F1 Score: 97.35%
Target Domain Performance - Accuracy: 93.11%, Precision: 94.34%, Recall: 93.28%, F1 Score: 93.19%

Run 5/10
Epoch [1/50], Class Loss: 0.3417, JMMD Loss: 0.2778
Validation Loss: 0.5732
Epoch [2/50], Class Loss: 0.1099, JMMD Loss: 0.1788
Validation Loss: 8.4587
Epoch [3/50], Class Loss: 0.1215, JMMD Loss: 0.1533
Validation Loss: 0.3192
Epoch [4/50], Class Loss: 0.0685, JMMD Loss: 0.1331
Validation Loss: 0.5867
Epoch [5/50], Class Loss: 0.1970, JMMD Loss: 0.2474
Validation Loss: 0.8867
Epoch [6/50], Class Loss: 0.0370, JMMD Loss: 0.1593
Validation Loss: 0.1724
Epoch [7/50], Class Loss: 0.0404, JMMD Loss: 0.1234
Validation Loss: 6.0651
Epoch [8/50], Class Loss: 0.0554, JMMD Loss: 0.1278
Validation Loss: 0.0625
Epoch [9/50], Class Loss: 0.0332, JMMD Loss: 0.1182
Validation Loss: 0.0427
Epoch [10/50], Class Loss: 0.0205, JMMD Loss: 0.1021
Validation Loss: 0.2143
Epoch [11/50], Class Loss: 0.0196, JMMD Loss: 0.0991
Validation Loss: 0.0154
Epoch [12/50], Class Loss: 0.0092, JMMD Loss: 0.0901
Validation Loss: 0.0422
Epoch [13/50], Class Loss: 0.0114, JMMD Loss: 0.0867
Validation Loss: 0.0616
Epoch [14/50], Class Loss: 0.0089, JMMD Loss: 0.0810
Validation Loss: 0.0668
Epoch [15/50], Class Loss: 0.0092, JMMD Loss: 0.0823
Validation Loss: 0.0222
Epoch [16/50], Class Loss: 0.0263, JMMD Loss: 0.0840
Validation Loss: 0.0513
Early stopping!
Source Domain Performance - Accuracy: 98.14%, Precision: 98.33%, Recall: 98.16%, F1 Score: 98.18%
Target Domain Performance - Accuracy: 85.13%, Precision: 89.62%, Recall: 85.47%, F1 Score: 84.52%

Run 6/10
Epoch [1/50], Class Loss: 0.3558, JMMD Loss: 0.1748
Validation Loss: 7.1296
Epoch [2/50], Class Loss: 0.0693, JMMD Loss: 0.1445
Validation Loss: 0.7705
Epoch [3/50], Class Loss: 0.1671, JMMD Loss: 0.1601
Validation Loss: 0.0375
Epoch [4/50], Class Loss: 0.0408, JMMD Loss: 0.1316
Validation Loss: 0.0619
Epoch [5/50], Class Loss: 0.0273, JMMD Loss: 0.1059
Validation Loss: 0.1481
Epoch [6/50], Class Loss: 0.0316, JMMD Loss: 0.1076
Validation Loss: 0.0302
Epoch [7/50], Class Loss: 0.0164, JMMD Loss: 0.1002
Validation Loss: 3.9795
Epoch [8/50], Class Loss: 0.0480, JMMD Loss: 0.1173
Validation Loss: 0.0195
Epoch [9/50], Class Loss: 0.0150, JMMD Loss: 0.0928
Validation Loss: 0.0740
Epoch [10/50], Class Loss: 0.0182, JMMD Loss: 0.0789
Validation Loss: 0.0254
Epoch [11/50], Class Loss: 0.0099, JMMD Loss: 0.0868
Validation Loss: 0.0177
Epoch [12/50], Class Loss: 0.0438, JMMD Loss: 0.0782
Validation Loss: 0.0179
Epoch [13/50], Class Loss: 0.0309, JMMD Loss: 0.0782
Validation Loss: 0.0159
Epoch [14/50], Class Loss: 0.0057, JMMD Loss: 0.0762
Validation Loss: 0.0137
Epoch [15/50], Class Loss: 0.0074, JMMD Loss: 0.0770
Validation Loss: 0.0143
Epoch [16/50], Class Loss: 0.0047, JMMD Loss: 0.0789
Validation Loss: 0.0210
Epoch [17/50], Class Loss: 0.0095, JMMD Loss: 0.0669
Validation Loss: 0.0097
Epoch [18/50], Class Loss: 0.0049, JMMD Loss: 0.0786
Validation Loss: 0.0198
Epoch [19/50], Class Loss: 0.0077, JMMD Loss: 0.0638
Validation Loss: 0.0120
Epoch [20/50], Class Loss: 0.0060, JMMD Loss: 0.0677
Validation Loss: 0.0150
Epoch [21/50], Class Loss: 0.0053, JMMD Loss: 0.0603
Validation Loss: 0.0147
Epoch [22/50], Class Loss: 0.0054, JMMD Loss: 0.0650
Validation Loss: 0.0195
Early stopping!
Source Domain Performance - Accuracy: 99.28%, Precision: 99.32%, Recall: 99.29%, F1 Score: 99.29%
Target Domain Performance - Accuracy: 94.06%, Precision: 95.11%, Recall: 94.25%, F1 Score: 94.25%

Run 7/10
Epoch [1/50], Class Loss: 0.5900, JMMD Loss: 0.1960
Validation Loss: 4.5786
Epoch [2/50], Class Loss: 0.2113, JMMD Loss: 0.2518
Validation Loss: 0.8569
Epoch [3/50], Class Loss: 0.0345, JMMD Loss: 0.1535
Validation Loss: 0.0755
Epoch [4/50], Class Loss: 0.0301, JMMD Loss: 0.0930
Validation Loss: 6.1342
Epoch [5/50], Class Loss: 0.0380, JMMD Loss: 0.1194
Validation Loss: 0.5802
Epoch [6/50], Class Loss: 0.0351, JMMD Loss: 0.0951
Validation Loss: 1.8481
Epoch [7/50], Class Loss: 0.1340, JMMD Loss: 0.1253
Validation Loss: 3.0923
Epoch [8/50], Class Loss: 0.1240, JMMD Loss: 0.1293
Validation Loss: 0.3457
Early stopping!
Source Domain Performance - Accuracy: 89.93%, Precision: 92.13%, Recall: 90.02%, F1 Score: 89.80%
Target Domain Performance - Accuracy: 74.82%, Precision: 83.31%, Recall: 75.38%, F1 Score: 72.36%

Run 8/10
Epoch [1/50], Class Loss: 0.4168, JMMD Loss: 0.2347
Validation Loss: 0.6697
Epoch [2/50], Class Loss: 0.0471, JMMD Loss: 0.1830
Validation Loss: 0.0282
Epoch [3/50], Class Loss: 0.0340, JMMD Loss: 0.1194
Validation Loss: 0.0297
Epoch [4/50], Class Loss: 0.0124, JMMD Loss: 0.0821
Validation Loss: 0.0334
Epoch [5/50], Class Loss: 0.0496, JMMD Loss: 0.1023
Validation Loss: 1.9727
Epoch [6/50], Class Loss: 0.0384, JMMD Loss: 0.1138
Validation Loss: 3.8788
Epoch [7/50], Class Loss: 0.1078, JMMD Loss: 0.1764
Validation Loss: 2.2557
Early stopping!
Source Domain Performance - Accuracy: 54.86%, Precision: 66.35%, Recall: 54.61%, F1 Score: 48.45%
Target Domain Performance - Accuracy: 73.92%, Precision: 84.29%, Recall: 75.24%, F1 Score: 70.37%

Run 9/10
Epoch [1/50], Class Loss: 0.3317, JMMD Loss: 0.2535
Validation Loss: 0.1143
Epoch [2/50], Class Loss: 0.0423, JMMD Loss: 0.1591
Validation Loss: 3.0406
Epoch [3/50], Class Loss: 0.0419, JMMD Loss: 0.1582
Validation Loss: 0.0760
Epoch [4/50], Class Loss: 0.0787, JMMD Loss: 0.1338
Validation Loss: 0.0445
Epoch [5/50], Class Loss: 0.0332, JMMD Loss: 0.1548
Validation Loss: 0.0742
Epoch [6/50], Class Loss: 0.0234, JMMD Loss: 0.1024
Validation Loss: 1.0098
Epoch [7/50], Class Loss: 0.0879, JMMD Loss: 0.1104
Validation Loss: 0.2843
Epoch [8/50], Class Loss: 0.0852, JMMD Loss: 0.1456
Validation Loss: 0.1059
Epoch [9/50], Class Loss: 0.0235, JMMD Loss: 0.0939
Validation Loss: 0.0721
Early stopping!
Source Domain Performance - Accuracy: 98.26%, Precision: 98.39%, Recall: 98.28%, F1 Score: 98.28%
Target Domain Performance - Accuracy: 73.98%, Precision: 79.68%, Recall: 74.39%, F1 Score: 73.22%

Run 10/10
Epoch [1/50], Class Loss: 0.3241, JMMD Loss: 0.2069
Validation Loss: 0.5892
Epoch [2/50], Class Loss: 0.0559, JMMD Loss: 0.1514
Validation Loss: 0.9134
Epoch [3/50], Class Loss: 0.0454, JMMD Loss: 0.1519
Validation Loss: 0.1072
Epoch [4/50], Class Loss: 0.0230, JMMD Loss: 0.0968
Validation Loss: 1.8228
Epoch [5/50], Class Loss: 0.0685, JMMD Loss: 0.1118
Validation Loss: 1.6819
Epoch [6/50], Class Loss: 0.0485, JMMD Loss: 0.1278
Validation Loss: 2.1454
Epoch [7/50], Class Loss: 0.1069, JMMD Loss: 0.1277
Validation Loss: 1.1289
Epoch [8/50], Class Loss: 0.0344, JMMD Loss: 0.1307
Validation Loss: 0.0274
Epoch [9/50], Class Loss: 0.0186, JMMD Loss: 0.0938
Validation Loss: 0.0801
Epoch [10/50], Class Loss: 0.0147, JMMD Loss: 0.0918
Validation Loss: 0.0309
Epoch [11/50], Class Loss: 0.0111, JMMD Loss: 0.0820
Validation Loss: 0.0470
Epoch [12/50], Class Loss: 0.0063, JMMD Loss: 0.0816
Validation Loss: 0.0902
Epoch [13/50], Class Loss: 0.0105, JMMD Loss: 0.0742
Validation Loss: 0.0176
Epoch [14/50], Class Loss: 0.0081, JMMD Loss: 0.0701
Validation Loss: 0.0532
Epoch [15/50], Class Loss: 0.0072, JMMD Loss: 0.0731
Validation Loss: 0.0593
Epoch [16/50], Class Loss: 0.0077, JMMD Loss: 0.0678
Validation Loss: 0.0557
Epoch [17/50], Class Loss: 0.0045, JMMD Loss: 0.0683
Validation Loss: 0.0359
Epoch [18/50], Class Loss: 0.0044, JMMD Loss: 0.0662
Validation Loss: 0.0271
Early stopping!
Source Domain Performance - Accuracy: 98.86%, Precision: 98.95%, Recall: 98.86%, F1 Score: 98.89%
Target Domain Performance - Accuracy: 81.53%, Precision: 87.91%, Recall: 81.97%, F1 Score: 80.09%

Source performance: 91.38% 93.67% 91.40% 90.42%
Target performance: 81.83% 87.14% 82.27% 80.42%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 99.97%
  Class 1: 55.49%
  Class 2: 83.84%
  Class 3: 89.77%
Base

Run 1/10
Epoch 1/50, Train Loss: 0.3374, Train Acc: 0.8892, Val Loss: 2.6321, Val Acc: 0.5192
Epoch 2/50, Train Loss: 0.0498, Train Acc: 0.9856, Val Loss: 1.0016, Val Acc: 0.7884
Epoch 3/50, Train Loss: 0.0240, Train Acc: 0.9936, Val Loss: 0.0305, Val Acc: 0.9868
Epoch 4/50, Train Loss: 0.0135, Train Acc: 0.9964, Val Loss: 0.5420, Val Acc: 0.8555
Epoch 5/50, Train Loss: 0.0132, Train Acc: 0.9961, Val Loss: 0.0468, Val Acc: 0.9820
Epoch 6/50, Train Loss: 0.0164, Train Acc: 0.9949, Val Loss: 0.0588, Val Acc: 0.9766
Epoch 7/50, Train Loss: 0.0107, Train Acc: 0.9973, Val Loss: 0.1627, Val Acc: 0.9460
Epoch 8/50, Train Loss: 0.0052, Train Acc: 0.9981, Val Loss: 0.1809, Val Acc: 0.9418
Early stopping!

Run 2/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3657, Train Acc: 0.8775, Val Loss: 3.2182, Val Acc: 0.5024
Epoch 2/50, Train Loss: 0.0414, Train Acc: 0.9876, Val Loss: 0.2756, Val Acc: 0.8933
Epoch 3/50, Train Loss: 0.0235, Train Acc: 0.9940, Val Loss: 0.2158, Val Acc: 0.9376
Epoch 4/50, Train Loss: 0.0163, Train Acc: 0.9957, Val Loss: 0.6261, Val Acc: 0.8363
Epoch 5/50, Train Loss: 0.0153, Train Acc: 0.9961, Val Loss: 0.1350, Val Acc: 0.9496
Epoch 6/50, Train Loss: 0.0167, Train Acc: 0.9946, Val Loss: 1.8982, Val Acc: 0.7860
Epoch 7/50, Train Loss: 0.0212, Train Acc: 0.9930, Val Loss: 1.4513, Val Acc: 0.7422
Epoch 8/50, Train Loss: 0.0142, Train Acc: 0.9958, Val Loss: 2.1091, Val Acc: 0.7656
Epoch 9/50, Train Loss: 0.0117, Train Acc: 0.9961, Val Loss: 0.0056, Val Acc: 0.9976
Epoch 10/50, Train Loss: 0.0116, Train Acc: 0.9958, Val Loss: 1.1589, Val Acc: 0.7446
Epoch 11/50, Train Loss: 0.0074, Train Acc: 0.9976, Val Loss: 0.0065, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0040, Train Acc: 0.9994, Val Loss: 0.0053, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0036, Train Acc: 0.9993, Val Loss: 0.0049, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0067, Train Acc: 0.9991, Val Loss: 0.0044, Val Acc: 0.9982
Epoch 15/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0031, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0025, Train Acc: 0.9994, Val Loss: 0.0034, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0016, Train Acc: 0.9996, Val Loss: 0.0032, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0029, Train Acc: 0.9994, Val Loss: 0.0028, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0028, Val Acc: 0.9988
Epoch 20/50, Train Loss: 0.0013, Train Acc: 0.9996, Val Loss: 0.0021, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0038, Val Acc: 0.9988
Epoch 22/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0030, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0048, Val Acc: 0.9988
Epoch 24/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0037, Val Acc: 0.9988
Epoch 25/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0020, Val Acc: 0.9994
Epoch 26/50, Train Loss: 0.0054, Train Acc: 0.9993, Val Loss: 0.0026, Val Acc: 0.9994
Epoch 27/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0027, Val Acc: 0.9994
Epoch 28/50, Train Loss: 0.0019, Train Acc: 0.9996, Val Loss: 0.0047, Val Acc: 0.9988
Epoch 29/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0034, Val Acc: 0.9988
Epoch 30/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0038, Val Acc: 0.9988
Early stopping!

Run 3/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3011, Train Acc: 0.9033, Val Loss: 1.0544, Val Acc: 0.6343
Epoch 2/50, Train Loss: 0.0256, Train Acc: 0.9939, Val Loss: 0.0206, Val Acc: 0.9922
Epoch 3/50, Train Loss: 0.0268, Train Acc: 0.9937, Val Loss: 0.1087, Val Acc: 0.9580
Epoch 4/50, Train Loss: 0.0315, Train Acc: 0.9906, Val Loss: 0.2870, Val Acc: 0.8789
Epoch 5/50, Train Loss: 0.0210, Train Acc: 0.9949, Val Loss: 0.0088, Val Acc: 0.9976
Epoch 6/50, Train Loss: 0.0163, Train Acc: 0.9961, Val Loss: 1.0369, Val Acc: 0.7824
Epoch 7/50, Train Loss: 0.0093, Train Acc: 0.9976, Val Loss: 0.0018, Val Acc: 0.9988
Epoch 8/50, Train Loss: 0.0063, Train Acc: 0.9987, Val Loss: 0.1438, Val Acc: 0.9293
Epoch 9/50, Train Loss: 0.0064, Train Acc: 0.9985, Val Loss: 0.0830, Val Acc: 0.9832
Epoch 10/50, Train Loss: 0.0095, Train Acc: 0.9975, Val Loss: 1.6202, Val Acc: 0.7278
Epoch 11/50, Train Loss: 0.0061, Train Acc: 0.9981, Val Loss: 0.0135, Val Acc: 0.9946
Epoch 12/50, Train Loss: 0.0018, Train Acc: 0.9997, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0014, Train Acc: 0.9996, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0023, Val Acc: 0.9982
Epoch 17/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0021, Val Acc: 0.9982
Epoch 19/50, Train Loss: 0.0016, Train Acc: 0.9996, Val Loss: 0.0025, Val Acc: 0.9988
Early stopping!

Run 4/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3621, Train Acc: 0.8833, Val Loss: 0.8837, Val Acc: 0.7038
Epoch 2/50, Train Loss: 0.0491, Train Acc: 0.9861, Val Loss: 0.0355, Val Acc: 0.9892
Epoch 3/50, Train Loss: 0.0214, Train Acc: 0.9940, Val Loss: 0.0096, Val Acc: 0.9964
Epoch 4/50, Train Loss: 0.0228, Train Acc: 0.9928, Val Loss: 0.5439, Val Acc: 0.8543
Epoch 5/50, Train Loss: 0.0161, Train Acc: 0.9955, Val Loss: 0.0030, Val Acc: 1.0000
Epoch 6/50, Train Loss: 0.0084, Train Acc: 0.9985, Val Loss: 0.6078, Val Acc: 0.8112
Epoch 7/50, Train Loss: 0.0239, Train Acc: 0.9940, Val Loss: 0.0474, Val Acc: 0.9784
Epoch 8/50, Train Loss: 0.0143, Train Acc: 0.9946, Val Loss: 0.0736, Val Acc: 0.9802
Epoch 9/50, Train Loss: 0.0109, Train Acc: 0.9966, Val Loss: 0.0038, Val Acc: 0.9988
Epoch 10/50, Train Loss: 0.0044, Train Acc: 0.9985, Val Loss: 0.0206, Val Acc: 0.9934
Early stopping!

Run 5/10
Epoch 1/50, Train Loss: 0.3248, Train Acc: 0.8938, Val Loss: 2.0768, Val Acc: 0.5156
Epoch 2/50, Train Loss: 0.0399, Train Acc: 0.9871, Val Loss: 0.6581, Val Acc: 0.8369
Epoch 3/50, Train Loss: 0.0288, Train Acc: 0.9922, Val Loss: 0.0269, Val Acc: 0.9940
Epoch 4/50, Train Loss: 0.0182, Train Acc: 0.9945, Val Loss: 0.0046, Val Acc: 0.9994
Epoch 5/50, Train Loss: 0.0057, Train Acc: 0.9984, Val Loss: 0.4759, Val Acc: 0.8243
Epoch 6/50, Train Loss: 0.0085, Train Acc: 0.9982, Val Loss: 0.0062, Val Acc: 0.9988
Epoch 7/50, Train Loss: 0.0279, Train Acc: 0.9924, Val Loss: 0.0066, Val Acc: 0.9982
Epoch 8/50, Train Loss: 0.0142, Train Acc: 0.9963, Val Loss: 1.3919, Val Acc: 0.6313
Epoch 9/50, Train Loss: 0.0077, Train Acc: 0.9976, Val Loss: 1.1310, Val Acc: 0.7020
Early stopping!

Run 6/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3754, Train Acc: 0.8746, Val Loss: 2.3148, Val Acc: 0.4958
Epoch 2/50, Train Loss: 0.0299, Train Acc: 0.9922, Val Loss: 0.0345, Val Acc: 0.9880
Epoch 3/50, Train Loss: 0.0424, Train Acc: 0.9867, Val Loss: 0.1305, Val Acc: 0.9490
Epoch 4/50, Train Loss: 0.0171, Train Acc: 0.9951, Val Loss: 0.0665, Val Acc: 0.9742
Epoch 5/50, Train Loss: 0.0140, Train Acc: 0.9963, Val Loss: 0.0089, Val Acc: 0.9964
Epoch 6/50, Train Loss: 0.0159, Train Acc: 0.9957, Val Loss: 0.9896, Val Acc: 0.7692
Epoch 7/50, Train Loss: 0.0215, Train Acc: 0.9927, Val Loss: 0.0424, Val Acc: 0.9856
Epoch 8/50, Train Loss: 0.0112, Train Acc: 0.9969, Val Loss: 0.0220, Val Acc: 0.9934
Epoch 9/50, Train Loss: 0.0068, Train Acc: 0.9982, Val Loss: 0.0126, Val Acc: 0.9940
Epoch 10/50, Train Loss: 0.0130, Train Acc: 0.9963, Val Loss: 0.1381, Val Acc: 0.9670
Early stopping!

Run 7/10
Epoch 1/50, Train Loss: 0.3258, Train Acc: 0.8980, Val Loss: 0.4702, Val Acc: 0.8189
Epoch 2/50, Train Loss: 0.0269, Train Acc: 0.9928, Val Loss: 0.1734, Val Acc: 0.9371
Epoch 3/50, Train Loss: 0.0254, Train Acc: 0.9927, Val Loss: 0.1124, Val Acc: 0.9616
Epoch 4/50, Train Loss: 0.0186, Train Acc: 0.9954, Val Loss: 0.0071, Val Acc: 0.9970
Epoch 5/50, Train Loss: 0.0181, Train Acc: 0.9955, Val Loss: 0.0853, Val Acc: 0.9604
Epoch 6/50, Train Loss: 0.0228, Train Acc: 0.9942, Val Loss: 0.6736, Val Acc: 0.7944
Epoch 7/50, Train Loss: 0.0134, Train Acc: 0.9958, Val Loss: 0.0164, Val Acc: 0.9934
Epoch 8/50, Train Loss: 0.0093, Train Acc: 0.9976, Val Loss: 0.0792, Val Acc: 0.9760
Epoch 9/50, Train Loss: 0.0267, Train Acc: 0.9927, Val Loss: 0.0827, Val Acc: 0.9664
Early stopping!

Run 8/10
Epoch 1/50, Train Loss: 0.3626, Train Acc: 0.8787, Val Loss: 2.1946, Val Acc: 0.4952
Epoch 2/50, Train Loss: 0.0385, Train Acc: 0.9897, Val Loss: 0.3356, Val Acc: 0.8777
Epoch 3/50, Train Loss: 0.0181, Train Acc: 0.9952, Val Loss: 2.8033, Val Acc: 0.7548
Epoch 4/50, Train Loss: 0.0226, Train Acc: 0.9937, Val Loss: 1.3158, Val Acc: 0.7902
Epoch 5/50, Train Loss: 0.0235, Train Acc: 0.9933, Val Loss: 0.0590, Val Acc: 0.9802
Epoch 6/50, Train Loss: 0.0201, Train Acc: 0.9949, Val Loss: 0.0055, Val Acc: 0.9982
Epoch 7/50, Train Loss: 0.0118, Train Acc: 0.9963, Val Loss: 0.0036, Val Acc: 0.9994
Epoch 8/50, Train Loss: 0.0078, Train Acc: 0.9978, Val Loss: 0.0067, Val Acc: 0.9988
Epoch 9/50, Train Loss: 0.0073, Train Acc: 0.9982, Val Loss: 0.0900, Val Acc: 0.9712
Epoch 10/50, Train Loss: 0.0057, Train Acc: 0.9988, Val Loss: 0.0470, Val Acc: 0.9838
Epoch 11/50, Train Loss: 0.0060, Train Acc: 0.9985, Val Loss: 0.0012, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0052, Train Acc: 0.9988, Val Loss: 0.0028, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0033, Train Acc: 0.9993, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0018, Train Acc: 0.9996, Val Loss: 0.0025, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0073, Train Acc: 0.9993, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0028, Train Acc: 0.9994, Val Loss: 0.0020, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0021, Train Acc: 0.9994, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0025, Train Acc: 0.9993, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0024, Val Acc: 0.9988
Early stopping!

Run 9/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3059, Train Acc: 0.9009, Val Loss: 3.3254, Val Acc: 0.4952
Epoch 2/50, Train Loss: 0.0697, Train Acc: 0.9808, Val Loss: 1.3547, Val Acc: 0.6235
Epoch 3/50, Train Loss: 0.0328, Train Acc: 0.9894, Val Loss: 0.0768, Val Acc: 0.9682
Epoch 4/50, Train Loss: 0.0165, Train Acc: 0.9951, Val Loss: 0.0162, Val Acc: 0.9958
Epoch 5/50, Train Loss: 0.0169, Train Acc: 0.9951, Val Loss: 0.7425, Val Acc: 0.8046
Epoch 6/50, Train Loss: 0.0166, Train Acc: 0.9957, Val Loss: 0.0353, Val Acc: 0.9880
Epoch 7/50, Train Loss: 0.0185, Train Acc: 0.9943, Val Loss: 0.6180, Val Acc: 0.8303
Epoch 8/50, Train Loss: 0.0154, Train Acc: 0.9946, Val Loss: 0.0308, Val Acc: 0.9922
Epoch 9/50, Train Loss: 0.0184, Train Acc: 0.9946, Val Loss: 0.0066, Val Acc: 0.9988
Epoch 10/50, Train Loss: 0.0183, Train Acc: 0.9951, Val Loss: 1.7295, Val Acc: 0.7092
Epoch 11/50, Train Loss: 0.0104, Train Acc: 0.9990, Val Loss: 0.0013, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0020, Train Acc: 0.9997, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0022, Train Acc: 0.9997, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0024, Train Acc: 0.9996, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0020, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0015, Train Acc: 0.9996, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0035, Train Acc: 0.9994, Val Loss: 0.0019, Val Acc: 0.9988
Epoch 19/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0022, Val Acc: 0.9988
Early stopping!

Run 10/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3742, Train Acc: 0.8838, Val Loss: 1.6108, Val Acc: 0.5695
Epoch 2/50, Train Loss: 0.0547, Train Acc: 0.9852, Val Loss: 2.3695, Val Acc: 0.7290
Epoch 3/50, Train Loss: 0.0435, Train Acc: 0.9871, Val Loss: 0.1865, Val Acc: 0.9329
Epoch 4/50, Train Loss: 0.0198, Train Acc: 0.9946, Val Loss: 0.0268, Val Acc: 0.9892
Epoch 5/50, Train Loss: 0.0110, Train Acc: 0.9970, Val Loss: 0.1933, Val Acc: 0.9203
Epoch 6/50, Train Loss: 0.0064, Train Acc: 0.9981, Val Loss: 2.5303, Val Acc: 0.7494
Epoch 7/50, Train Loss: 0.0111, Train Acc: 0.9969, Val Loss: 0.0067, Val Acc: 0.9976
Epoch 8/50, Train Loss: 0.0090, Train Acc: 0.9982, Val Loss: 0.0207, Val Acc: 0.9940
Epoch 9/50, Train Loss: 0.0111, Train Acc: 0.9964, Val Loss: 0.4951, Val Acc: 0.8453
Epoch 10/50, Train Loss: 0.0102, Train Acc: 0.9976, Val Loss: 0.0207, Val Acc: 0.9946
Epoch 11/50, Train Loss: 0.0043, Train Acc: 0.9991, Val Loss: 0.0050, Val Acc: 0.9976
Epoch 12/50, Train Loss: 0.0026, Train Acc: 0.9993, Val Loss: 0.0013, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0031, Train Acc: 0.9993, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0027, Train Acc: 0.9996, Val Loss: 0.0571, Val Acc: 0.9754
Epoch 19/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0021, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0012, Val Acc: 0.9994
Early stopping!

Source performance: 95.65 94.90 95.66 94.91
Target performance: 59.63 55.59 57.77 51.08

bpsk: 89.87
qpsk: 1.85
16qam: 40.47
8apsk: 98.90
DANN
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.4089, Domain Loss: 1.3890, Class Loss: 1.0199
Epoch 2/50, Loss: 1.6271, Domain Loss: 1.2755, Class Loss: 0.3516
Epoch 3/50, Loss: 1.4993, Domain Loss: 1.3171, Class Loss: 0.1821
Epoch 4/50, Loss: 2.3964, Domain Loss: 2.2621, Class Loss: 0.1343
Epoch 5/50, Loss: 4.8744, Domain Loss: 4.6865, Class Loss: 0.1879
Epoch 6/50, Loss: 7.5826, Domain Loss: 7.3326, Class Loss: 0.2500
Epoch 7/50, Loss: 3.8425, Domain Loss: 3.4686, Class Loss: 0.3739
Epoch 8/50, Loss: 2.2023, Domain Loss: 2.0039, Class Loss: 0.1984
Epoch 9/50, Loss: 2.0113, Domain Loss: 1.8531, Class Loss: 0.1582
Epoch 10/50, Loss: 4.5089, Domain Loss: 3.5090, Class Loss: 0.9999
Epoch 11/50, Loss: 8.7990, Domain Loss: 7.9701, Class Loss: 0.8289
Epoch 12/50, Loss: 3.5762, Domain Loss: 3.1234, Class Loss: 0.4529
Epoch 13/50, Loss: 1.9196, Domain Loss: 1.6310, Class Loss: 0.2886
Epoch 14/50, Loss: 1.6011, Domain Loss: 1.4134, Class Loss: 0.1876
Epoch 15/50, Loss: 1.5287, Domain Loss: 1.3991, Class Loss: 0.1296
Epoch 16/50, Loss: 1.5009, Domain Loss: 1.4002, Class Loss: 0.1007
Epoch 17/50, Loss: 1.4647, Domain Loss: 1.3847, Class Loss: 0.0800
Epoch 18/50, Loss: 1.4461, Domain Loss: 1.3919, Class Loss: 0.0542
Epoch 19/50, Loss: 1.4346, Domain Loss: 1.3877, Class Loss: 0.0468
Epoch 20/50, Loss: 1.4570, Domain Loss: 1.3942, Class Loss: 0.0628
Epoch 21/50, Loss: 1.4668, Domain Loss: 1.3986, Class Loss: 0.0683
Epoch 22/50, Loss: 1.4466, Domain Loss: 1.3964, Class Loss: 0.0502
Epoch 23/50, Loss: 1.4601, Domain Loss: 1.4168, Class Loss: 0.0433
Epoch 24/50, Loss: 1.5211, Domain Loss: 1.4759, Class Loss: 0.0452
Epoch 25/50, Loss: 1.7709, Domain Loss: 1.7364, Class Loss: 0.0344
Epoch 26/50, Loss: 2.9921, Domain Loss: 2.7627, Class Loss: 0.2293
Epoch 27/50, Loss: 2.1611, Domain Loss: 1.8543, Class Loss: 0.3068
Epoch 28/50, Loss: 1.6913, Domain Loss: 1.4822, Class Loss: 0.2091
Epoch 29/50, Loss: 1.4559, Domain Loss: 1.3390, Class Loss: 0.1168
Epoch 30/50, Loss: 1.4763, Domain Loss: 1.3854, Class Loss: 0.0910
Epoch 31/50, Loss: 1.4460, Domain Loss: 1.3705, Class Loss: 0.0755
Epoch 32/50, Loss: 1.4504, Domain Loss: 1.3782, Class Loss: 0.0721
Epoch 33/50, Loss: 1.3978, Domain Loss: 1.3482, Class Loss: 0.0496
Epoch 34/50, Loss: 1.4154, Domain Loss: 1.3622, Class Loss: 0.0532
Epoch 35/50, Loss: 1.4556, Domain Loss: 1.3803, Class Loss: 0.0753
Epoch 36/50, Loss: 1.4341, Domain Loss: 1.3771, Class Loss: 0.0570
Epoch 37/50, Loss: 1.4210, Domain Loss: 1.3740, Class Loss: 0.0469
Epoch 38/50, Loss: 1.4867, Domain Loss: 1.4203, Class Loss: 0.0664
Epoch 39/50, Loss: 1.5252, Domain Loss: 1.4505, Class Loss: 0.0747
Epoch 40/50, Loss: 1.4872, Domain Loss: 1.4284, Class Loss: 0.0588
Epoch 41/50, Loss: 1.4822, Domain Loss: 1.4212, Class Loss: 0.0610
Epoch 42/50, Loss: 1.4590, Domain Loss: 1.4128, Class Loss: 0.0462
Epoch 43/50, Loss: 1.4383, Domain Loss: 1.3956, Class Loss: 0.0427
Epoch 44/50, Loss: 1.4330, Domain Loss: 1.3985, Class Loss: 0.0345
Epoch 45/50, Loss: 1.4229, Domain Loss: 1.3915, Class Loss: 0.0314
Epoch 46/50, Loss: 1.4189, Domain Loss: 1.3877, Class Loss: 0.0312
Epoch 47/50, Loss: 1.4279, Domain Loss: 1.4021, Class Loss: 0.0258
Epoch 48/50, Loss: 1.4225, Domain Loss: 1.3950, Class Loss: 0.0275
Epoch 49/50, Loss: 1.4032, Domain Loss: 1.3886, Class Loss: 0.0145
Epoch 50/50, Loss: 1.4151, Domain Loss: 1.3974, Class Loss: 0.0177
97.18


Epoch 1/50, Loss: 2.4122, Domain Loss: 1.3933, Class Loss: 1.0189
Epoch 2/50, Loss: 1.6869, Domain Loss: 1.2849, Class Loss: 0.4020
Epoch 3/50, Loss: 1.4351, Domain Loss: 1.2197, Class Loss: 0.2154
Epoch 4/50, Loss: 1.5073, Domain Loss: 1.3264, Class Loss: 0.1809
Epoch 5/50, Loss: 2.1397, Domain Loss: 1.8072, Class Loss: 0.3324
Epoch 6/50, Loss: 2.2285, Domain Loss: 1.9968, Class Loss: 0.2317
Epoch 7/50, Loss: 9.5156, Domain Loss: 9.1564, Class Loss: 0.3592
Epoch 8/50, Loss: 12.6901, Domain Loss: 12.3025, Class Loss: 0.3876
Epoch 9/50, Loss: 4.5338, Domain Loss: 4.1898, Class Loss: 0.3440
Epoch 10/50, Loss: 3.5261, Domain Loss: 3.2937, Class Loss: 0.2324
Epoch 11/50, Loss: 8.4954, Domain Loss: 8.0571, Class Loss: 0.4383
Epoch 12/50, Loss: 8.1850, Domain Loss: 7.5980, Class Loss: 0.5869
Epoch 13/50, Loss: 22.3321, Domain Loss: 21.3224, Class Loss: 1.0098
Epoch 14/50, Loss: 16.9985, Domain Loss: 16.2921, Class Loss: 0.7064
Epoch 15/50, Loss: 10.8533, Domain Loss: 9.3760, Class Loss: 1.4773
Epoch 16/50, Loss: 6.5868, Domain Loss: 4.8373, Class Loss: 1.7495
Epoch 17/50, Loss: 3.1682, Domain Loss: 2.3631, Class Loss: 0.8050
Epoch 18/50, Loss: 2.2746, Domain Loss: 1.5589, Class Loss: 0.7157
Epoch 19/50, Loss: 2.0944, Domain Loss: 1.4439, Class Loss: 0.6504
Epoch 20/50, Loss: 2.1530, Domain Loss: 1.5166, Class Loss: 0.6364
Epoch 21/50, Loss: 3.4116, Domain Loss: 2.5939, Class Loss: 0.8177
Epoch 22/50, Loss: 6.5934, Domain Loss: 5.0778, Class Loss: 1.5156
Epoch 23/50, Loss: 5.5576, Domain Loss: 4.6079, Class Loss: 0.9497
Epoch 24/50, Loss: 2.7527, Domain Loss: 2.0421, Class Loss: 0.7106
Epoch 25/50, Loss: 2.0196, Domain Loss: 1.4969, Class Loss: 0.5227
Epoch 26/50, Loss: 2.0770, Domain Loss: 1.5527, Class Loss: 0.5243
Epoch 27/50, Loss: 1.8468, Domain Loss: 1.4788, Class Loss: 0.3679
Epoch 28/50, Loss: 1.8245, Domain Loss: 1.4936, Class Loss: 0.3309
Epoch 29/50, Loss: 1.8102, Domain Loss: 1.4825, Class Loss: 0.3277
Epoch 30/50, Loss: 1.7970, Domain Loss: 1.4979, Class Loss: 0.2992
Epoch 31/50, Loss: 1.6698, Domain Loss: 1.4750, Class Loss: 0.1948
Epoch 32/50, Loss: 1.7161, Domain Loss: 1.4897, Class Loss: 0.2265
Epoch 33/50, Loss: 1.6070, Domain Loss: 1.4594, Class Loss: 0.1475
Epoch 34/50, Loss: 1.5947, Domain Loss: 1.4260, Class Loss: 0.1687
Epoch 35/50, Loss: 1.6055, Domain Loss: 1.4466, Class Loss: 0.1589
Epoch 36/50, Loss: 1.5632, Domain Loss: 1.4162, Class Loss: 0.1470
Epoch 37/50, Loss: 1.5334, Domain Loss: 1.4063, Class Loss: 0.1271
Epoch 38/50, Loss: 1.5586, Domain Loss: 1.4259, Class Loss: 0.1328
Epoch 39/50, Loss: 1.4869, Domain Loss: 1.4048, Class Loss: 0.0821
Epoch 40/50, Loss: 1.5191, Domain Loss: 1.4149, Class Loss: 0.1042
Epoch 41/50, Loss: 1.5115, Domain Loss: 1.4133, Class Loss: 0.0981
Epoch 42/50, Loss: 1.4978, Domain Loss: 1.4227, Class Loss: 0.0751
Epoch 43/50, Loss: 1.4999, Domain Loss: 1.4286, Class Loss: 0.0713
Epoch 44/50, Loss: 1.5012, Domain Loss: 1.4250, Class Loss: 0.0761
Epoch 45/50, Loss: 1.5172, Domain Loss: 1.4425, Class Loss: 0.0747
Epoch 46/50, Loss: 1.5482, Domain Loss: 1.4447, Class Loss: 0.1035
Epoch 47/50, Loss: 1.5063, Domain Loss: 1.4252, Class Loss: 0.0811
Epoch 48/50, Loss: 1.4913, Domain Loss: 1.4264, Class Loss: 0.0649
Epoch 49/50, Loss: 1.5068, Domain Loss: 1.4414, Class Loss: 0.0654
Epoch 50/50, Loss: 1.5208, Domain Loss: 1.4578, Class Loss: 0.0630
93.17


Epoch 1/50, Loss: 2.3437, Domain Loss: 1.3965, Class Loss: 0.9471
Epoch 2/50, Loss: 1.6459, Domain Loss: 1.2932, Class Loss: 0.3527
Epoch 3/50, Loss: 1.4602, Domain Loss: 1.2970, Class Loss: 0.1632
Epoch 4/50, Loss: 2.7861, Domain Loss: 2.5721, Class Loss: 0.2140
Epoch 5/50, Loss: 10.4412, Domain Loss: 10.1426, Class Loss: 0.2986
Epoch 6/50, Loss: 10.4943, Domain Loss: 10.0038, Class Loss: 0.4904
Epoch 7/50, Loss: 5.3517, Domain Loss: 4.9267, Class Loss: 0.4250
Epoch 8/50, Loss: 3.4662, Domain Loss: 3.0847, Class Loss: 0.3815
Epoch 9/50, Loss: 3.0155, Domain Loss: 2.5571, Class Loss: 0.4583
Epoch 10/50, Loss: 2.2943, Domain Loss: 2.0303, Class Loss: 0.2640
Epoch 11/50, Loss: 1.7107, Domain Loss: 1.5527, Class Loss: 0.1580
Epoch 12/50, Loss: 1.6536, Domain Loss: 1.5434, Class Loss: 0.1101
Epoch 13/50, Loss: 2.3218, Domain Loss: 2.1740, Class Loss: 0.1479
Epoch 14/50, Loss: 7.3335, Domain Loss: 6.8843, Class Loss: 0.4492
Epoch 15/50, Loss: 4.6413, Domain Loss: 4.1448, Class Loss: 0.4966
Epoch 16/50, Loss: 3.2100, Domain Loss: 2.8176, Class Loss: 0.3924
Epoch 17/50, Loss: 2.5455, Domain Loss: 2.3388, Class Loss: 0.2067
Epoch 18/50, Loss: 2.8998, Domain Loss: 2.5710, Class Loss: 0.3288
Epoch 19/50, Loss: 1.9569, Domain Loss: 1.6875, Class Loss: 0.2694
Epoch 20/50, Loss: 1.6279, Domain Loss: 1.4742, Class Loss: 0.1537
Epoch 21/50, Loss: 1.4983, Domain Loss: 1.3918, Class Loss: 0.1064
Epoch 22/50, Loss: 1.5361, Domain Loss: 1.4648, Class Loss: 0.0713
Epoch 23/50, Loss: 1.5217, Domain Loss: 1.4502, Class Loss: 0.0715
Epoch 24/50, Loss: 1.7898, Domain Loss: 1.6429, Class Loss: 0.1469
Epoch 25/50, Loss: 1.8455, Domain Loss: 1.7400, Class Loss: 0.1054
Epoch 26/50, Loss: 1.8445, Domain Loss: 1.6787, Class Loss: 0.1658
Epoch 27/50, Loss: 1.7003, Domain Loss: 1.5545, Class Loss: 0.1458
Epoch 28/50, Loss: 1.7680, Domain Loss: 1.6384, Class Loss: 0.1296
Epoch 29/50, Loss: 2.0905, Domain Loss: 1.5902, Class Loss: 0.5002
Epoch 30/50, Loss: 1.5775, Domain Loss: 1.3685, Class Loss: 0.2089
Epoch 31/50, Loss: 1.5156, Domain Loss: 1.4038, Class Loss: 0.1118
Epoch 32/50, Loss: 1.5113, Domain Loss: 1.4331, Class Loss: 0.0782
Epoch 33/50, Loss: 1.5208, Domain Loss: 1.4547, Class Loss: 0.0661
Epoch 34/50, Loss: 1.4789, Domain Loss: 1.4159, Class Loss: 0.0629
Epoch 35/50, Loss: 1.5411, Domain Loss: 1.4704, Class Loss: 0.0707
Epoch 36/50, Loss: 1.5487, Domain Loss: 1.4765, Class Loss: 0.0721
Epoch 37/50, Loss: 1.7315, Domain Loss: 1.5866, Class Loss: 0.1450
Epoch 38/50, Loss: 1.5852, Domain Loss: 1.5000, Class Loss: 0.0852
Epoch 39/50, Loss: 1.6121, Domain Loss: 1.5269, Class Loss: 0.0852
Epoch 40/50, Loss: 1.5495, Domain Loss: 1.4622, Class Loss: 0.0874
Epoch 41/50, Loss: 1.5737, Domain Loss: 1.4857, Class Loss: 0.0880
Epoch 42/50, Loss: 1.4501, Domain Loss: 1.3972, Class Loss: 0.0529
Epoch 43/50, Loss: 1.4336, Domain Loss: 1.3967, Class Loss: 0.0370
Epoch 44/50, Loss: 1.4760, Domain Loss: 1.4105, Class Loss: 0.0655
Epoch 45/50, Loss: 1.5744, Domain Loss: 1.5317, Class Loss: 0.0427
Epoch 46/50, Loss: 1.6707, Domain Loss: 1.6025, Class Loss: 0.0682
Epoch 47/50, Loss: 1.6173, Domain Loss: 1.5669, Class Loss: 0.0503
Epoch 48/50, Loss: 2.3967, Domain Loss: 2.2105, Class Loss: 0.1862
Epoch 49/50, Loss: 1.7669, Domain Loss: 1.6682, Class Loss: 0.0986
Epoch 50/50, Loss: 1.5877, Domain Loss: 1.5403, Class Loss: 0.0473
93.59


Epoch 1/50, Loss: 2.3595, Domain Loss: 1.3948, Class Loss: 0.9647
Epoch 2/50, Loss: 1.7994, Domain Loss: 1.3285, Class Loss: 0.4709
Epoch 3/50, Loss: 1.4133, Domain Loss: 1.1906, Class Loss: 0.2227
Epoch 4/50, Loss: 1.4485, Domain Loss: 1.3034, Class Loss: 0.1451
Epoch 5/50, Loss: 4.3045, Domain Loss: 4.1591, Class Loss: 0.1454
Epoch 6/50, Loss: 3.6410, Domain Loss: 3.5221, Class Loss: 0.1189
Epoch 7/50, Loss: 14.3024, Domain Loss: 13.9518, Class Loss: 0.3506
Epoch 8/50, Loss: 6.3658, Domain Loss: 6.0137, Class Loss: 0.3521
Epoch 9/50, Loss: 5.0546, Domain Loss: 4.3396, Class Loss: 0.7150
Epoch 10/50, Loss: 5.0663, Domain Loss: 4.5236, Class Loss: 0.5427
Epoch 11/50, Loss: 3.0633, Domain Loss: 2.7711, Class Loss: 0.2922
Epoch 12/50, Loss: 1.8485, Domain Loss: 1.6850, Class Loss: 0.1635
Epoch 13/50, Loss: 1.6091, Domain Loss: 1.4797, Class Loss: 0.1294
Epoch 14/50, Loss: 1.5158, Domain Loss: 1.3854, Class Loss: 0.1304
Epoch 15/50, Loss: 1.4364, Domain Loss: 1.3684, Class Loss: 0.0680
Epoch 16/50, Loss: 1.4427, Domain Loss: 1.3903, Class Loss: 0.0524
Epoch 17/50, Loss: 1.5407, Domain Loss: 1.4833, Class Loss: 0.0574
Epoch 18/50, Loss: 1.6021, Domain Loss: 1.5067, Class Loss: 0.0954
Epoch 19/50, Loss: 1.6560, Domain Loss: 1.5539, Class Loss: 0.1021
Epoch 20/50, Loss: 2.0065, Domain Loss: 1.9274, Class Loss: 0.0791
Epoch 21/50, Loss: 2.2443, Domain Loss: 2.1762, Class Loss: 0.0682
Epoch 22/50, Loss: 2.1974, Domain Loss: 2.0921, Class Loss: 0.1053
Epoch 23/50, Loss: 4.5529, Domain Loss: 4.1305, Class Loss: 0.4224
Epoch 24/50, Loss: 3.7974, Domain Loss: 3.3834, Class Loss: 0.4140
Epoch 25/50, Loss: 2.1164, Domain Loss: 1.9570, Class Loss: 0.1594
Epoch 26/50, Loss: 1.5957, Domain Loss: 1.5045, Class Loss: 0.0911
Epoch 27/50, Loss: 1.4359, Domain Loss: 1.3429, Class Loss: 0.0930
Epoch 28/50, Loss: 1.4253, Domain Loss: 1.3685, Class Loss: 0.0569
Epoch 29/50, Loss: 1.4137, Domain Loss: 1.3665, Class Loss: 0.0473
Epoch 30/50, Loss: 1.4357, Domain Loss: 1.3823, Class Loss: 0.0534
Epoch 31/50, Loss: 1.4774, Domain Loss: 1.3946, Class Loss: 0.0828
Epoch 32/50, Loss: 1.4551, Domain Loss: 1.3968, Class Loss: 0.0583
Epoch 33/50, Loss: 1.5007, Domain Loss: 1.4300, Class Loss: 0.0707
Epoch 34/50, Loss: 1.4692, Domain Loss: 1.4267, Class Loss: 0.0425
Epoch 35/50, Loss: 1.4496, Domain Loss: 1.4125, Class Loss: 0.0371
Epoch 36/50, Loss: 1.4742, Domain Loss: 1.4303, Class Loss: 0.0438
Epoch 37/50, Loss: 1.4880, Domain Loss: 1.4473, Class Loss: 0.0406
Epoch 38/50, Loss: 1.5050, Domain Loss: 1.4583, Class Loss: 0.0468
Epoch 39/50, Loss: 1.5096, Domain Loss: 1.4707, Class Loss: 0.0389
Epoch 40/50, Loss: 1.5274, Domain Loss: 1.4847, Class Loss: 0.0427
Epoch 41/50, Loss: 1.5838, Domain Loss: 1.5238, Class Loss: 0.0600
Epoch 42/50, Loss: 1.5889, Domain Loss: 1.5505, Class Loss: 0.0384
Epoch 43/50, Loss: 1.6325, Domain Loss: 1.5814, Class Loss: 0.0511
Epoch 44/50, Loss: 1.6886, Domain Loss: 1.6570, Class Loss: 0.0316
Epoch 45/50, Loss: 1.7240, Domain Loss: 1.6695, Class Loss: 0.0545
Epoch 46/50, Loss: 1.5377, Domain Loss: 1.4735, Class Loss: 0.0642
Epoch 47/50, Loss: 1.6937, Domain Loss: 1.5384, Class Loss: 0.1552
Epoch 48/50, Loss: 1.5536, Domain Loss: 1.4594, Class Loss: 0.0942
Epoch 49/50, Loss: 1.5890, Domain Loss: 1.5087, Class Loss: 0.0803
Epoch 50/50, Loss: 1.5160, Domain Loss: 1.4630, Class Loss: 0.0530
96.22


Epoch 1/50, Loss: 2.2450, Domain Loss: 1.3751, Class Loss: 0.8698
Epoch 2/50, Loss: 1.5478, Domain Loss: 1.2977, Class Loss: 0.2500
Epoch 3/50, Loss: 1.4910, Domain Loss: 1.3559, Class Loss: 0.1351
Epoch 4/50, Loss: 3.2687, Domain Loss: 3.1078, Class Loss: 0.1609
Epoch 5/50, Loss: 5.2134, Domain Loss: 4.9395, Class Loss: 0.2739
Epoch 6/50, Loss: 8.2229, Domain Loss: 7.5162, Class Loss: 0.7068
Epoch 7/50, Loss: 13.5240, Domain Loss: 12.9730, Class Loss: 0.5510
Epoch 8/50, Loss: 15.4349, Domain Loss: 15.0394, Class Loss: 0.3955
Epoch 9/50, Loss: 4.9587, Domain Loss: 4.6431, Class Loss: 0.3156
Epoch 10/50, Loss: 2.2363, Domain Loss: 2.0498, Class Loss: 0.1865
Epoch 11/50, Loss: 1.8955, Domain Loss: 1.7011, Class Loss: 0.1944
Epoch 12/50, Loss: 1.9824, Domain Loss: 1.6583, Class Loss: 0.3242
Epoch 13/50, Loss: 1.5968, Domain Loss: 1.4814, Class Loss: 0.1153
Epoch 14/50, Loss: 1.4419, Domain Loss: 1.3720, Class Loss: 0.0699
Epoch 15/50, Loss: 1.4115, Domain Loss: 1.3485, Class Loss: 0.0631
Epoch 16/50, Loss: 1.4220, Domain Loss: 1.3477, Class Loss: 0.0743
Epoch 17/50, Loss: 1.4341, Domain Loss: 1.3369, Class Loss: 0.0971
Epoch 18/50, Loss: 1.4148, Domain Loss: 1.3592, Class Loss: 0.0556
Epoch 19/50, Loss: 1.4131, Domain Loss: 1.3649, Class Loss: 0.0483
Epoch 20/50, Loss: 1.4490, Domain Loss: 1.3981, Class Loss: 0.0509
Epoch 21/50, Loss: 1.4938, Domain Loss: 1.4321, Class Loss: 0.0617
Epoch 22/50, Loss: 1.5408, Domain Loss: 1.4761, Class Loss: 0.0647
Epoch 23/50, Loss: 1.4572, Domain Loss: 1.4019, Class Loss: 0.0553
Epoch 24/50, Loss: 1.4363, Domain Loss: 1.3922, Class Loss: 0.0441
Epoch 25/50, Loss: 1.4046, Domain Loss: 1.3849, Class Loss: 0.0197
Epoch 26/50, Loss: 1.4543, Domain Loss: 1.4020, Class Loss: 0.0522
Epoch 27/50, Loss: 1.4716, Domain Loss: 1.4171, Class Loss: 0.0544
Epoch 28/50, Loss: 1.4454, Domain Loss: 1.4087, Class Loss: 0.0368
Epoch 29/50, Loss: 1.4320, Domain Loss: 1.4060, Class Loss: 0.0261
Epoch 30/50, Loss: 1.4336, Domain Loss: 1.4173, Class Loss: 0.0163
Epoch 31/50, Loss: 1.4356, Domain Loss: 1.4187, Class Loss: 0.0169
Epoch 32/50, Loss: 1.5262, Domain Loss: 1.4799, Class Loss: 0.0462
Epoch 33/50, Loss: 1.5783, Domain Loss: 1.5155, Class Loss: 0.0628
Epoch 34/50, Loss: 2.0951, Domain Loss: 1.9221, Class Loss: 0.1731
Epoch 35/50, Loss: 2.2127, Domain Loss: 2.1095, Class Loss: 0.1031
Epoch 36/50, Loss: 2.7880, Domain Loss: 2.6636, Class Loss: 0.1244
Epoch 37/50, Loss: 1.6613, Domain Loss: 1.5586, Class Loss: 0.1027
Epoch 38/50, Loss: 1.6132, Domain Loss: 1.4958, Class Loss: 0.1174
Epoch 39/50, Loss: 1.4363, Domain Loss: 1.3849, Class Loss: 0.0514
Epoch 40/50, Loss: 1.4445, Domain Loss: 1.3978, Class Loss: 0.0466
Epoch 41/50, Loss: 1.4020, Domain Loss: 1.3771, Class Loss: 0.0250
Epoch 42/50, Loss: 1.4486, Domain Loss: 1.4088, Class Loss: 0.0398
Epoch 43/50, Loss: 1.4204, Domain Loss: 1.3867, Class Loss: 0.0337
Epoch 44/50, Loss: 1.4274, Domain Loss: 1.3972, Class Loss: 0.0302
Epoch 45/50, Loss: 1.4311, Domain Loss: 1.4024, Class Loss: 0.0287
Epoch 46/50, Loss: 1.4190, Domain Loss: 1.3938, Class Loss: 0.0252
Epoch 47/50, Loss: 1.4280, Domain Loss: 1.4028, Class Loss: 0.0252
Epoch 48/50, Loss: 1.4374, Domain Loss: 1.4086, Class Loss: 0.0288
Epoch 49/50, Loss: 1.4180, Domain Loss: 1.3966, Class Loss: 0.0214
Epoch 50/50, Loss: 1.4140, Domain Loss: 1.3947, Class Loss: 0.0194
97.78


Epoch 1/50, Loss: 2.3995, Domain Loss: 1.3890, Class Loss: 1.0104
Epoch 2/50, Loss: 1.6713, Domain Loss: 1.3391, Class Loss: 0.3321
Epoch 3/50, Loss: 1.4874, Domain Loss: 1.2818, Class Loss: 0.2056
Epoch 4/50, Loss: 1.7413, Domain Loss: 1.5340, Class Loss: 0.2074
Epoch 5/50, Loss: 2.2699, Domain Loss: 2.1169, Class Loss: 0.1530
Epoch 6/50, Loss: 1.6891, Domain Loss: 1.6150, Class Loss: 0.0742
Epoch 7/50, Loss: 3.1103, Domain Loss: 2.8522, Class Loss: 0.2581
Epoch 8/50, Loss: 17.7144, Domain Loss: 16.7445, Class Loss: 0.9699
Epoch 9/50, Loss: 13.9303, Domain Loss: 13.3047, Class Loss: 0.6256
Epoch 10/50, Loss: 14.2763, Domain Loss: 13.7859, Class Loss: 0.4903
Epoch 11/50, Loss: 17.9156, Domain Loss: 16.9901, Class Loss: 0.9256
Epoch 12/50, Loss: 6.8939, Domain Loss: 6.1498, Class Loss: 0.7440
Epoch 13/50, Loss: 3.5353, Domain Loss: 3.0032, Class Loss: 0.5321
Epoch 14/50, Loss: 2.1424, Domain Loss: 1.7968, Class Loss: 0.3456
Epoch 15/50, Loss: 2.3497, Domain Loss: 1.6721, Class Loss: 0.6776
Epoch 16/50, Loss: 2.0845, Domain Loss: 1.5589, Class Loss: 0.5256
Epoch 17/50, Loss: 1.8532, Domain Loss: 1.5036, Class Loss: 0.3496
Epoch 18/50, Loss: 1.7636, Domain Loss: 1.4827, Class Loss: 0.2809
Epoch 19/50, Loss: 1.7411, Domain Loss: 1.4967, Class Loss: 0.2443
Epoch 20/50, Loss: 1.6428, Domain Loss: 1.4696, Class Loss: 0.1731
Epoch 21/50, Loss: 1.5784, Domain Loss: 1.4326, Class Loss: 0.1459
Epoch 22/50, Loss: 1.5791, Domain Loss: 1.4502, Class Loss: 0.1289
Epoch 23/50, Loss: 1.5444, Domain Loss: 1.4226, Class Loss: 0.1218
Epoch 24/50, Loss: 1.5260, Domain Loss: 1.4235, Class Loss: 0.1025
Epoch 25/50, Loss: 1.4825, Domain Loss: 1.3993, Class Loss: 0.0832
Epoch 26/50, Loss: 1.5081, Domain Loss: 1.4290, Class Loss: 0.0791
Epoch 27/50, Loss: 1.5481, Domain Loss: 1.4259, Class Loss: 0.1222
Epoch 28/50, Loss: 1.4970, Domain Loss: 1.4119, Class Loss: 0.0851
Epoch 29/50, Loss: 1.5756, Domain Loss: 1.4792, Class Loss: 0.0964
Epoch 30/50, Loss: 1.5290, Domain Loss: 1.4564, Class Loss: 0.0727
Epoch 31/50, Loss: 1.5585, Domain Loss: 1.4808, Class Loss: 0.0777
Epoch 32/50, Loss: 1.6442, Domain Loss: 1.5682, Class Loss: 0.0760
Epoch 33/50, Loss: 1.8024, Domain Loss: 1.6883, Class Loss: 0.1141
Epoch 34/50, Loss: 1.7058, Domain Loss: 1.6225, Class Loss: 0.0833
Epoch 35/50, Loss: 1.7868, Domain Loss: 1.6239, Class Loss: 0.1630
Epoch 36/50, Loss: 1.5145, Domain Loss: 1.4272, Class Loss: 0.0873
Epoch 37/50, Loss: 1.5080, Domain Loss: 1.4161, Class Loss: 0.0920
Epoch 38/50, Loss: 1.5556, Domain Loss: 1.4689, Class Loss: 0.0867
Epoch 39/50, Loss: 1.5103, Domain Loss: 1.4384, Class Loss: 0.0719
Epoch 40/50, Loss: 1.5227, Domain Loss: 1.4243, Class Loss: 0.0984
Epoch 41/50, Loss: 1.5003, Domain Loss: 1.4379, Class Loss: 0.0624
Epoch 42/50, Loss: 1.5525, Domain Loss: 1.4603, Class Loss: 0.0922
Epoch 43/50, Loss: 1.5476, Domain Loss: 1.4835, Class Loss: 0.0641
Epoch 44/50, Loss: 1.6509, Domain Loss: 1.5810, Class Loss: 0.0699
Epoch 45/50, Loss: 1.5936, Domain Loss: 1.5215, Class Loss: 0.0720
Epoch 46/50, Loss: 1.5835, Domain Loss: 1.5068, Class Loss: 0.0767
Epoch 47/50, Loss: 1.5212, Domain Loss: 1.4624, Class Loss: 0.0588
Epoch 48/50, Loss: 1.5816, Domain Loss: 1.5156, Class Loss: 0.0659
Epoch 49/50, Loss: 1.6308, Domain Loss: 1.5711, Class Loss: 0.0597
Epoch 50/50, Loss: 1.6256, Domain Loss: 1.5409, Class Loss: 0.0847
60.85


Epoch 1/50, Loss: 2.4328, Domain Loss: 1.3954, Class Loss: 1.0373
Epoch 2/50, Loss: 1.7046, Domain Loss: 1.2829, Class Loss: 0.4218
Epoch 3/50, Loss: 1.3112, Domain Loss: 1.1539, Class Loss: 0.1573
Epoch 4/50, Loss: 1.5617, Domain Loss: 1.4548, Class Loss: 0.1069
Epoch 5/50, Loss: 4.0716, Domain Loss: 3.8687, Class Loss: 0.2029
Epoch 6/50, Loss: 5.6942, Domain Loss: 5.5033, Class Loss: 0.1909
Epoch 7/50, Loss: 19.0509, Domain Loss: 18.6609, Class Loss: 0.3900
Epoch 8/50, Loss: 6.9080, Domain Loss: 6.5511, Class Loss: 0.3568
Epoch 9/50, Loss: 3.1720, Domain Loss: 2.9576, Class Loss: 0.2144
Epoch 10/50, Loss: 2.0104, Domain Loss: 1.8791, Class Loss: 0.1313
Epoch 11/50, Loss: 1.9037, Domain Loss: 1.8120, Class Loss: 0.0917
Epoch 12/50, Loss: 2.5038, Domain Loss: 2.4188, Class Loss: 0.0850
Epoch 13/50, Loss: 3.5738, Domain Loss: 3.3159, Class Loss: 0.2579
Epoch 14/50, Loss: 4.7266, Domain Loss: 4.3100, Class Loss: 0.4166
Epoch 15/50, Loss: 4.2707, Domain Loss: 3.9284, Class Loss: 0.3423
Epoch 16/50, Loss: 7.8052, Domain Loss: 7.2797, Class Loss: 0.5255
Epoch 17/50, Loss: 3.1400, Domain Loss: 2.7370, Class Loss: 0.4030
Epoch 18/50, Loss: 1.9676, Domain Loss: 1.7761, Class Loss: 0.1915
Epoch 19/50, Loss: 1.7149, Domain Loss: 1.5918, Class Loss: 0.1231
Epoch 20/50, Loss: 2.2114, Domain Loss: 1.9190, Class Loss: 0.2924
Epoch 21/50, Loss: 1.8337, Domain Loss: 1.6039, Class Loss: 0.2298
Epoch 22/50, Loss: 1.7178, Domain Loss: 1.5960, Class Loss: 0.1218
Epoch 23/50, Loss: 1.7109, Domain Loss: 1.6153, Class Loss: 0.0956
Epoch 24/50, Loss: 1.7317, Domain Loss: 1.6474, Class Loss: 0.0843
Epoch 25/50, Loss: 1.7456, Domain Loss: 1.6834, Class Loss: 0.0622
Epoch 26/50, Loss: 2.0417, Domain Loss: 1.9043, Class Loss: 0.1374
Epoch 27/50, Loss: 2.1538, Domain Loss: 2.0189, Class Loss: 0.1349
Epoch 28/50, Loss: 2.4990, Domain Loss: 2.2015, Class Loss: 0.2975
Epoch 29/50, Loss: 3.5059, Domain Loss: 3.2143, Class Loss: 0.2916
Epoch 30/50, Loss: 2.5950, Domain Loss: 2.2254, Class Loss: 0.3696
Epoch 31/50, Loss: 2.2954, Domain Loss: 2.0676, Class Loss: 0.2278
Epoch 32/50, Loss: 1.8045, Domain Loss: 1.6109, Class Loss: 0.1936
Epoch 33/50, Loss: 1.4760, Domain Loss: 1.4048, Class Loss: 0.0712
Epoch 34/50, Loss: 1.5236, Domain Loss: 1.4555, Class Loss: 0.0681
Epoch 35/50, Loss: 1.5266, Domain Loss: 1.4272, Class Loss: 0.0995
Epoch 36/50, Loss: 1.4222, Domain Loss: 1.3716, Class Loss: 0.0506
Epoch 37/50, Loss: 1.4205, Domain Loss: 1.3612, Class Loss: 0.0593
Epoch 38/50, Loss: 1.4465, Domain Loss: 1.3975, Class Loss: 0.0490
Epoch 39/50, Loss: 1.5038, Domain Loss: 1.4459, Class Loss: 0.0579
Epoch 40/50, Loss: 1.5041, Domain Loss: 1.4371, Class Loss: 0.0670
Epoch 41/50, Loss: 1.4373, Domain Loss: 1.4062, Class Loss: 0.0312
Epoch 42/50, Loss: 1.4348, Domain Loss: 1.4020, Class Loss: 0.0328
Epoch 43/50, Loss: 1.4089, Domain Loss: 1.3849, Class Loss: 0.0239
Epoch 44/50, Loss: 1.4428, Domain Loss: 1.4061, Class Loss: 0.0367
Epoch 45/50, Loss: 1.4664, Domain Loss: 1.4195, Class Loss: 0.0469
Epoch 46/50, Loss: 1.4470, Domain Loss: 1.4179, Class Loss: 0.0291
Epoch 47/50, Loss: 1.4400, Domain Loss: 1.4122, Class Loss: 0.0278
Epoch 48/50, Loss: 1.4445, Domain Loss: 1.4030, Class Loss: 0.0415
Epoch 49/50, Loss: 1.4422, Domain Loss: 1.4138, Class Loss: 0.0284
Epoch 50/50, Loss: 1.4224, Domain Loss: 1.4048, Class Loss: 0.0176
96.04


Epoch 1/50, Loss: 2.4028, Domain Loss: 1.3731, Class Loss: 1.0296
Epoch 2/50, Loss: 1.6873, Domain Loss: 1.2910, Class Loss: 0.3963
Epoch 3/50, Loss: 1.3849, Domain Loss: 1.1829, Class Loss: 0.2020
Epoch 4/50, Loss: 1.5263, Domain Loss: 1.4029, Class Loss: 0.1233
Epoch 5/50, Loss: 3.2979, Domain Loss: 3.1714, Class Loss: 0.1265
Epoch 6/50, Loss: 4.5226, Domain Loss: 4.2471, Class Loss: 0.2756
Epoch 7/50, Loss: 7.8680, Domain Loss: 7.2226, Class Loss: 0.6454
Epoch 8/50, Loss: 8.0178, Domain Loss: 7.5453, Class Loss: 0.4725
Epoch 9/50, Loss: 5.1433, Domain Loss: 4.2085, Class Loss: 0.9348
Epoch 10/50, Loss: 6.7655, Domain Loss: 5.2248, Class Loss: 1.5407
Epoch 11/50, Loss: 8.8861, Domain Loss: 8.0979, Class Loss: 0.7882
Epoch 12/50, Loss: 3.7668, Domain Loss: 3.3860, Class Loss: 0.3808
Epoch 13/50, Loss: 2.3265, Domain Loss: 1.9295, Class Loss: 0.3970
Epoch 14/50, Loss: 1.9223, Domain Loss: 1.6253, Class Loss: 0.2970
Epoch 15/50, Loss: 1.6594, Domain Loss: 1.4929, Class Loss: 0.1665
Epoch 16/50, Loss: 1.5504, Domain Loss: 1.4270, Class Loss: 0.1234
Epoch 17/50, Loss: 1.5025, Domain Loss: 1.3952, Class Loss: 0.1073
Epoch 18/50, Loss: 1.4392, Domain Loss: 1.3765, Class Loss: 0.0627
Epoch 19/50, Loss: 1.5180, Domain Loss: 1.4445, Class Loss: 0.0735
Epoch 20/50, Loss: 1.6609, Domain Loss: 1.5563, Class Loss: 0.1046
Epoch 21/50, Loss: 1.8113, Domain Loss: 1.6812, Class Loss: 0.1301
Epoch 22/50, Loss: 1.6486, Domain Loss: 1.4976, Class Loss: 0.1510
Epoch 23/50, Loss: 1.4646, Domain Loss: 1.4020, Class Loss: 0.0626
Epoch 24/50, Loss: 1.4604, Domain Loss: 1.3972, Class Loss: 0.0632
Epoch 25/50, Loss: 1.4735, Domain Loss: 1.4143, Class Loss: 0.0591
Epoch 26/50, Loss: 1.4824, Domain Loss: 1.4269, Class Loss: 0.0555
Epoch 27/50, Loss: 1.4423, Domain Loss: 1.3909, Class Loss: 0.0514
Epoch 28/50, Loss: 1.4783, Domain Loss: 1.4308, Class Loss: 0.0475
Epoch 29/50, Loss: 1.4672, Domain Loss: 1.4282, Class Loss: 0.0390
Epoch 30/50, Loss: 1.5244, Domain Loss: 1.4663, Class Loss: 0.0581
Epoch 31/50, Loss: 1.5445, Domain Loss: 1.5107, Class Loss: 0.0338
Epoch 32/50, Loss: 1.5827, Domain Loss: 1.5291, Class Loss: 0.0535
Epoch 33/50, Loss: 1.7161, Domain Loss: 1.6530, Class Loss: 0.0631
Epoch 34/50, Loss: 1.9396, Domain Loss: 1.8931, Class Loss: 0.0465
Epoch 35/50, Loss: 2.0437, Domain Loss: 1.9795, Class Loss: 0.0641
Epoch 36/50, Loss: 2.1643, Domain Loss: 2.0783, Class Loss: 0.0859
Epoch 37/50, Loss: 2.3776, Domain Loss: 2.2601, Class Loss: 0.1175
Epoch 38/50, Loss: 1.6579, Domain Loss: 1.5936, Class Loss: 0.0644
Epoch 39/50, Loss: 1.5050, Domain Loss: 1.4647, Class Loss: 0.0403
Epoch 40/50, Loss: 1.5427, Domain Loss: 1.5073, Class Loss: 0.0354
Epoch 41/50, Loss: 1.5670, Domain Loss: 1.5330, Class Loss: 0.0339
Epoch 42/50, Loss: 1.5685, Domain Loss: 1.5354, Class Loss: 0.0331
Epoch 43/50, Loss: 1.8792, Domain Loss: 1.7775, Class Loss: 0.1016
Epoch 44/50, Loss: 1.8854, Domain Loss: 1.6798, Class Loss: 0.2056
Epoch 45/50, Loss: 1.6397, Domain Loss: 1.5371, Class Loss: 0.1027
Epoch 46/50, Loss: 1.5370, Domain Loss: 1.4926, Class Loss: 0.0444
Epoch 47/50, Loss: 1.5020, Domain Loss: 1.4635, Class Loss: 0.0386
Epoch 48/50, Loss: 1.4741, Domain Loss: 1.4147, Class Loss: 0.0595
Epoch 49/50, Loss: 1.4696, Domain Loss: 1.4229, Class Loss: 0.0468
Epoch 50/50, Loss: 1.4515, Domain Loss: 1.4165, Class Loss: 0.0350
97.24


Epoch 1/50, Loss: 2.3005, Domain Loss: 1.3744, Class Loss: 0.9261
Epoch 2/50, Loss: 1.5938, Domain Loss: 1.2416, Class Loss: 0.3522
Epoch 3/50, Loss: 1.5258, Domain Loss: 1.2948, Class Loss: 0.2310
Epoch 4/50, Loss: 2.5234, Domain Loss: 2.3471, Class Loss: 0.1763
Epoch 5/50, Loss: 6.1770, Domain Loss: 5.9230, Class Loss: 0.2541
Epoch 6/50, Loss: 4.1221, Domain Loss: 3.8972, Class Loss: 0.2250
Epoch 7/50, Loss: 3.3428, Domain Loss: 3.0785, Class Loss: 0.2643
Epoch 8/50, Loss: 3.2497, Domain Loss: 3.0476, Class Loss: 0.2021
Epoch 9/50, Loss: 2.4340, Domain Loss: 2.1847, Class Loss: 0.2492
Epoch 10/50, Loss: 1.8581, Domain Loss: 1.6994, Class Loss: 0.1586
Epoch 11/50, Loss: 2.2064, Domain Loss: 2.0524, Class Loss: 0.1540
Epoch 12/50, Loss: 6.0110, Domain Loss: 5.5926, Class Loss: 0.4184
Epoch 13/50, Loss: 9.0130, Domain Loss: 7.7593, Class Loss: 1.2537
Epoch 14/50, Loss: 2.5565, Domain Loss: 2.0647, Class Loss: 0.4918
Epoch 15/50, Loss: 2.0165, Domain Loss: 1.6999, Class Loss: 0.3166
Epoch 16/50, Loss: 1.8175, Domain Loss: 1.6007, Class Loss: 0.2168
Epoch 17/50, Loss: 1.9000, Domain Loss: 1.7025, Class Loss: 0.1975
Epoch 18/50, Loss: 1.5702, Domain Loss: 1.4241, Class Loss: 0.1461
Epoch 19/50, Loss: 1.6099, Domain Loss: 1.4676, Class Loss: 0.1423
Epoch 20/50, Loss: 1.6592, Domain Loss: 1.4995, Class Loss: 0.1597
Epoch 21/50, Loss: 1.5404, Domain Loss: 1.4407, Class Loss: 0.0997
Epoch 22/50, Loss: 1.6432, Domain Loss: 1.5501, Class Loss: 0.0930
Epoch 23/50, Loss: 2.3342, Domain Loss: 2.0168, Class Loss: 0.3173
Epoch 24/50, Loss: 1.9917, Domain Loss: 1.8413, Class Loss: 0.1504
Epoch 25/50, Loss: 1.7371, Domain Loss: 1.6335, Class Loss: 0.1036
Epoch 26/50, Loss: 1.6163, Domain Loss: 1.5240, Class Loss: 0.0923
Epoch 27/50, Loss: 1.6041, Domain Loss: 1.5024, Class Loss: 0.1016
Epoch 28/50, Loss: 1.6621, Domain Loss: 1.5919, Class Loss: 0.0702
Epoch 29/50, Loss: 1.7864, Domain Loss: 1.7039, Class Loss: 0.0825
Epoch 30/50, Loss: 3.5346, Domain Loss: 3.2373, Class Loss: 0.2973
Epoch 31/50, Loss: 5.2937, Domain Loss: 4.2896, Class Loss: 1.0042
Epoch 32/50, Loss: 3.6706, Domain Loss: 3.0859, Class Loss: 0.5847
Epoch 33/50, Loss: 2.2053, Domain Loss: 1.8492, Class Loss: 0.3560
Epoch 34/50, Loss: 1.8448, Domain Loss: 1.5648, Class Loss: 0.2800
Epoch 35/50, Loss: 1.8243, Domain Loss: 1.6282, Class Loss: 0.1961
Epoch 36/50, Loss: 1.9713, Domain Loss: 1.7692, Class Loss: 0.2021
Epoch 37/50, Loss: 1.9224, Domain Loss: 1.7248, Class Loss: 0.1975
Epoch 38/50, Loss: 1.9317, Domain Loss: 1.7171, Class Loss: 0.2146
Epoch 39/50, Loss: 1.7356, Domain Loss: 1.5623, Class Loss: 0.1732
Epoch 40/50, Loss: 1.7084, Domain Loss: 1.5872, Class Loss: 0.1212
Epoch 41/50, Loss: 1.7658, Domain Loss: 1.6731, Class Loss: 0.0928
Epoch 42/50, Loss: 1.9498, Domain Loss: 1.8496, Class Loss: 0.1002
Epoch 43/50, Loss: 1.7211, Domain Loss: 1.6382, Class Loss: 0.0829
Epoch 44/50, Loss: 1.6049, Domain Loss: 1.5229, Class Loss: 0.0820
Epoch 45/50, Loss: 1.5575, Domain Loss: 1.4955, Class Loss: 0.0620
Epoch 46/50, Loss: 1.5257, Domain Loss: 1.4368, Class Loss: 0.0889
Epoch 47/50, Loss: 1.5386, Domain Loss: 1.4684, Class Loss: 0.0702
Epoch 48/50, Loss: 1.6421, Domain Loss: 1.5987, Class Loss: 0.0433
Epoch 49/50, Loss: 1.5767, Domain Loss: 1.5236, Class Loss: 0.0531
Epoch 50/50, Loss: 1.5709, Domain Loss: 1.4986, Class Loss: 0.0723
96.04


Epoch 1/50, Loss: 2.3715, Domain Loss: 1.3881, Class Loss: 0.9834
Epoch 2/50, Loss: 1.7325, Domain Loss: 1.2802, Class Loss: 0.4523
Epoch 3/50, Loss: 1.4077, Domain Loss: 1.1659, Class Loss: 0.2418
Epoch 4/50, Loss: 3.1053, Domain Loss: 2.9249, Class Loss: 0.1804
Epoch 5/50, Loss: 4.4268, Domain Loss: 4.2617, Class Loss: 0.1651
Epoch 6/50, Loss: 7.3822, Domain Loss: 7.0205, Class Loss: 0.3618
Epoch 7/50, Loss: 6.9658, Domain Loss: 6.2994, Class Loss: 0.6664
Epoch 8/50, Loss: 5.0852, Domain Loss: 4.7040, Class Loss: 0.3811
Epoch 9/50, Loss: 5.4118, Domain Loss: 4.5176, Class Loss: 0.8941
Epoch 10/50, Loss: 3.4649, Domain Loss: 2.9801, Class Loss: 0.4848
Epoch 11/50, Loss: 2.9890, Domain Loss: 2.4747, Class Loss: 0.5143
Epoch 12/50, Loss: 6.6206, Domain Loss: 5.9950, Class Loss: 0.6256
Epoch 13/50, Loss: 11.9316, Domain Loss: 11.0083, Class Loss: 0.9233
Epoch 14/50, Loss: 4.8817, Domain Loss: 4.1360, Class Loss: 0.7458
Epoch 15/50, Loss: 3.6095, Domain Loss: 3.0474, Class Loss: 0.5621
Epoch 16/50, Loss: 2.2849, Domain Loss: 1.7791, Class Loss: 0.5058
Epoch 17/50, Loss: 2.3248, Domain Loss: 1.9636, Class Loss: 0.3612
Epoch 18/50, Loss: 2.1387, Domain Loss: 1.8104, Class Loss: 0.3282
Epoch 19/50, Loss: 1.8545, Domain Loss: 1.6768, Class Loss: 0.1778
Epoch 20/50, Loss: 1.7679, Domain Loss: 1.6134, Class Loss: 0.1545
Epoch 21/50, Loss: 1.6874, Domain Loss: 1.5379, Class Loss: 0.1495
Epoch 22/50, Loss: 1.8842, Domain Loss: 1.6318, Class Loss: 0.2525
Epoch 23/50, Loss: 1.7170, Domain Loss: 1.5566, Class Loss: 0.1604
Epoch 24/50, Loss: 1.5580, Domain Loss: 1.4453, Class Loss: 0.1127
Epoch 25/50, Loss: 1.5206, Domain Loss: 1.4201, Class Loss: 0.1005
Epoch 26/50, Loss: 1.5803, Domain Loss: 1.4844, Class Loss: 0.0958
Epoch 27/50, Loss: 1.6103, Domain Loss: 1.5105, Class Loss: 0.0998
Epoch 28/50, Loss: 1.6842, Domain Loss: 1.5622, Class Loss: 0.1220
Epoch 29/50, Loss: 1.5863, Domain Loss: 1.4771, Class Loss: 0.1092
Epoch 30/50, Loss: 1.5548, Domain Loss: 1.4542, Class Loss: 0.1006
Epoch 31/50, Loss: 1.4887, Domain Loss: 1.4304, Class Loss: 0.0583
Epoch 32/50, Loss: 1.4741, Domain Loss: 1.4183, Class Loss: 0.0558
Epoch 33/50, Loss: 1.4960, Domain Loss: 1.4418, Class Loss: 0.0542
Epoch 34/50, Loss: 1.4818, Domain Loss: 1.4343, Class Loss: 0.0476
Epoch 35/50, Loss: 1.5318, Domain Loss: 1.4859, Class Loss: 0.0459
Epoch 36/50, Loss: 1.6013, Domain Loss: 1.5456, Class Loss: 0.0558
Epoch 37/50, Loss: 1.6220, Domain Loss: 1.5685, Class Loss: 0.0535
Epoch 38/50, Loss: 1.5503, Domain Loss: 1.4886, Class Loss: 0.0617
Epoch 39/50, Loss: 1.4814, Domain Loss: 1.4245, Class Loss: 0.0569
Epoch 40/50, Loss: 1.5141, Domain Loss: 1.4555, Class Loss: 0.0586
Epoch 41/50, Loss: 1.4826, Domain Loss: 1.4505, Class Loss: 0.0321
Epoch 42/50, Loss: 1.5389, Domain Loss: 1.4956, Class Loss: 0.0433
Epoch 43/50, Loss: 1.5279, Domain Loss: 1.4546, Class Loss: 0.0733
Epoch 44/50, Loss: 1.5066, Domain Loss: 1.4640, Class Loss: 0.0426
Epoch 45/50, Loss: 1.4974, Domain Loss: 1.4643, Class Loss: 0.0330
Epoch 46/50, Loss: 1.5198, Domain Loss: 1.4738, Class Loss: 0.0460
Epoch 47/50, Loss: 1.4754, Domain Loss: 1.4200, Class Loss: 0.0554
Epoch 48/50, Loss: 1.4631, Domain Loss: 1.4117, Class Loss: 0.0514
Epoch 49/50, Loss: 1.4888, Domain Loss: 1.4103, Class Loss: 0.0786
Epoch 50/50, Loss: 1.4946, Domain Loss: 1.4420, Class Loss: 0.0527
92.69


Source performance:
94.74 95.43 94.64 94.21 
Target performance:
92.08 93.20 92.18 91.67 

Deep CORALtarget performance: 99.95 88.30 89.64 90.83 
Deep CORAL Run 1/10
Epoch 1: Source Val Acc = 0.5402, Target Val Acc = 0.7332
Epoch 2: Source Val Acc = 0.6541, Target Val Acc = 0.5659
Epoch 3: Source Val Acc = 0.8004, Target Val Acc = 0.9089
Epoch 4: Source Val Acc = 0.7230, Target Val Acc = 0.5222
Epoch 5: Source Val Acc = 0.9784, Target Val Acc = 0.7716
Epoch 6: Source Val Acc = 0.9688, Target Val Acc = 0.6906
Epoch 7: Source Val Acc = 0.7362, Target Val Acc = 0.5372
Epoch 8: Source Val Acc = 0.5594, Target Val Acc = 0.7560
Epoch 9: Source Val Acc = 0.9976, Target Val Acc = 0.5941
Epoch 10: Source Val Acc = 0.8489, Target Val Acc = 0.7974
Epoch 11: Source Val Acc = 0.9472, Target Val Acc = 0.8477
Epoch 12: Source Val Acc = 0.9586, Target Val Acc = 0.8082
Epoch 13: Source Val Acc = 0.8435, Target Val Acc = 0.6595
Epoch 14: Source Val Acc = 0.8591, Target Val Acc = 0.7932
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.8591, Target Val Acc = 0.7932

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.7632, Target Val Acc = 0.8028
Epoch 2: Source Val Acc = 0.6463, Target Val Acc = 0.8345
Epoch 3: Source Val Acc = 0.8933, Target Val Acc = 0.5785
Epoch 4: Source Val Acc = 0.8825, Target Val Acc = 0.6115
Epoch 5: Source Val Acc = 0.9664, Target Val Acc = 0.7974
Epoch 6: Source Val Acc = 0.9628, Target Val Acc = 0.7860
Epoch 7: Source Val Acc = 0.6289, Target Val Acc = 0.6115
Epoch 8: Source Val Acc = 0.9844, Target Val Acc = 0.6966
Epoch 9: Source Val Acc = 0.9155, Target Val Acc = 0.8657
Epoch 10: Source Val Acc = 0.7734, Target Val Acc = 0.8088
Epoch 11: Source Val Acc = 0.6091, Target Val Acc = 0.6703
Epoch 12: Source Val Acc = 0.9388, Target Val Acc = 0.7464
Epoch 13: Source Val Acc = 0.9826, Target Val Acc = 0.8153
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9826, Target Val Acc = 0.8153

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.4952, Target Val Acc = 0.6247
Epoch 2: Source Val Acc = 0.8945, Target Val Acc = 0.8279
Epoch 3: Source Val Acc = 0.8016, Target Val Acc = 0.9071
Epoch 4: Source Val Acc = 0.9400, Target Val Acc = 0.8297
Epoch 5: Source Val Acc = 0.5012, Target Val Acc = 0.6469
Epoch 6: Source Val Acc = 0.9946, Target Val Acc = 0.6025
Epoch 7: Source Val Acc = 0.6805, Target Val Acc = 0.8645
Epoch 8: Source Val Acc = 0.6709, Target Val Acc = 0.3459
Epoch 9: Source Val Acc = 0.9940, Target Val Acc = 0.7452
Epoch 10: Source Val Acc = 0.8381, Target Val Acc = 0.8447
Epoch 11: Source Val Acc = 0.8531, Target Val Acc = 0.7896
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.8531, Target Val Acc = 0.7896

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.7614, Target Val Acc = 0.7200
Epoch 2: Source Val Acc = 0.9658, Target Val Acc = 0.6595
Epoch 3: Source Val Acc = 0.7704, Target Val Acc = 0.5438
Epoch 4: Source Val Acc = 0.8399, Target Val Acc = 0.7896
Epoch 5: Source Val Acc = 0.9928, Target Val Acc = 0.6733
Epoch 6: Source Val Acc = 0.9418, Target Val Acc = 0.6445
Epoch 7: Source Val Acc = 0.9808, Target Val Acc = 0.6859
Epoch 8: Source Val Acc = 0.7638, Target Val Acc = 0.5516
Epoch 9: Source Val Acc = 0.9484, Target Val Acc = 0.8699
Epoch 10: Source Val Acc = 0.7566, Target Val Acc = 0.8028
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.7566, Target Val Acc = 0.8028

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.8753, Target Val Acc = 0.6049
Epoch 2: Source Val Acc = 0.9191, Target Val Acc = 0.9412
Epoch 3: Source Val Acc = 0.9754, Target Val Acc = 0.8597
Epoch 4: Source Val Acc = 0.5270, Target Val Acc = 0.7188
Epoch 5: Source Val Acc = 0.7626, Target Val Acc = 0.6457
Epoch 6: Source Val Acc = 0.9688, Target Val Acc = 0.8513
Epoch 7: Source Val Acc = 0.9221, Target Val Acc = 0.9412
Epoch 8: Source Val Acc = 0.5749, Target Val Acc = 0.7518
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.5749, Target Val Acc = 0.7518

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.5024, Target Val Acc = 0.6343
Epoch 2: Source Val Acc = 0.8537, Target Val Acc = 0.7242
Epoch 3: Source Val Acc = 0.5024, Target Val Acc = 0.6277
Epoch 4: Source Val Acc = 0.5785, Target Val Acc = 0.7704
Epoch 5: Source Val Acc = 0.8273, Target Val Acc = 0.7584
Epoch 6: Source Val Acc = 0.9400, Target Val Acc = 0.7356
Epoch 7: Source Val Acc = 0.6936, Target Val Acc = 0.6667
Epoch 8: Source Val Acc = 0.9964, Target Val Acc = 0.6739
Epoch 9: Source Val Acc = 0.9934, Target Val Acc = 0.7266
Epoch 10: Source Val Acc = 0.9880, Target Val Acc = 0.7266
Epoch 11: Source Val Acc = 0.9982, Target Val Acc = 0.5701
Epoch 12: Source Val Acc = 0.7314, Target Val Acc = 0.5336
Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 0.7032
Epoch 14: Source Val Acc = 0.6900, Target Val Acc = 0.7374
Epoch 15: Source Val Acc = 0.9886, Target Val Acc = 0.6936
Epoch 16: Source Val Acc = 0.9844, Target Val Acc = 0.8052
Epoch 17: Source Val Acc = 0.8663, Target Val Acc = 0.8237
Epoch 18: Source Val Acc = 0.9946, Target Val Acc = 0.6313
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 0.9946, Target Val Acc = 0.6313

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.8477, Target Val Acc = 0.8363
Epoch 2: Source Val Acc = 0.5006, Target Val Acc = 0.5450
Epoch 3: Source Val Acc = 0.5198, Target Val Acc = 0.6835
Epoch 4: Source Val Acc = 0.9604, Target Val Acc = 0.7236
Epoch 5: Source Val Acc = 0.9862, Target Val Acc = 0.7362
Epoch 6: Source Val Acc = 0.9814, Target Val Acc = 0.7026
Epoch 7: Source Val Acc = 0.9748, Target Val Acc = 0.5941
Epoch 8: Source Val Acc = 0.9892, Target Val Acc = 0.6853
Epoch 9: Source Val Acc = 0.8909, Target Val Acc = 0.6127
Epoch 10: Source Val Acc = 0.9299, Target Val Acc = 0.8046
Epoch 11: Source Val Acc = 0.9676, Target Val Acc = 0.8094
Epoch 12: Source Val Acc = 0.9928, Target Val Acc = 0.6145
Epoch 13: Source Val Acc = 0.7266, Target Val Acc = 0.3429
Epoch 14: Source Val Acc = 0.5857, Target Val Acc = 0.7158
Epoch 15: Source Val Acc = 0.9982, Target Val Acc = 0.7254
Epoch 16: Source Val Acc = 0.9574, Target Val Acc = 0.5923
Epoch 17: Source Val Acc = 0.9982, Target Val Acc = 0.6493
Epoch 18: Source Val Acc = 0.9832, Target Val Acc = 0.7728
Epoch 19: Source Val Acc = 0.5600, Target Val Acc = 0.7668
Epoch 20: Source Val Acc = 0.9784, Target Val Acc = 0.7830
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.9784, Target Val Acc = 0.7830

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.6996, Target Val Acc = 0.9305
Epoch 2: Source Val Acc = 0.9874, Target Val Acc = 0.6541
Epoch 3: Source Val Acc = 0.6912, Target Val Acc = 0.8016
Epoch 4: Source Val Acc = 0.9005, Target Val Acc = 0.7758
Epoch 5: Source Val Acc = 0.5366, Target Val Acc = 0.7350
Epoch 6: Source Val Acc = 0.6517, Target Val Acc = 0.8771
Epoch 7: Source Val Acc = 0.9916, Target Val Acc = 0.6145
Epoch 8: Source Val Acc = 0.9748, Target Val Acc = 0.8082
Epoch 9: Source Val Acc = 0.9664, Target Val Acc = 0.7170
Epoch 10: Source Val Acc = 0.9922, Target Val Acc = 0.7740
Epoch 11: Source Val Acc = 0.9784, Target Val Acc = 0.8112
Epoch 12: Source Val Acc = 0.9934, Target Val Acc = 0.5929
Epoch 13: Source Val Acc = 0.8447, Target Val Acc = 0.6259
Epoch 14: Source Val Acc = 0.9412, Target Val Acc = 0.8405
Epoch 15: Source Val Acc = 0.9976, Target Val Acc = 0.6109
Epoch 16: Source Val Acc = 0.6499, Target Val Acc = 0.6637
Epoch 17: Source Val Acc = 0.9568, Target Val Acc = 0.8507
Epoch 18: Source Val Acc = 0.9442, Target Val Acc = 0.8513
Epoch 19: Source Val Acc = 0.8141, Target Val Acc = 0.8585
Epoch 20: Source Val Acc = 0.9784, Target Val Acc = 0.7512
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.9784, Target Val Acc = 0.7512

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.9892, Target Val Acc = 0.6715
Epoch 2: Source Val Acc = 0.8879, Target Val Acc = 0.9083
Epoch 3: Source Val Acc = 0.8873, Target Val Acc = 0.6055
Epoch 4: Source Val Acc = 0.5917, Target Val Acc = 0.7758
Epoch 5: Source Val Acc = 0.9317, Target Val Acc = 0.8897
Epoch 6: Source Val Acc = 0.7230, Target Val Acc = 0.8477
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 0.7230, Target Val Acc = 0.8477

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.7440, Target Val Acc = 0.8747
Epoch 2: Source Val Acc = 0.9604, Target Val Acc = 0.5995
Epoch 3: Source Val Acc = 0.9209, Target Val Acc = 0.7866
Epoch 4: Source Val Acc = 0.9790, Target Val Acc = 0.8873
Epoch 5: Source Val Acc = 0.8939, Target Val Acc = 0.8819
Epoch 6: Source Val Acc = 0.9406, Target Val Acc = 0.8663
Epoch 7: Source Val Acc = 0.5126, Target Val Acc = 0.3052
Epoch 8: Source Val Acc = 0.8052, Target Val Acc = 0.6475
Epoch 9: Source Val Acc = 0.8909, Target Val Acc = 0.8058
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.8909, Target Val Acc = 0.8058

Deep CORAL: Average Source Val Acc = 0.8592, Average Target Val Acc = 0.7772
STAR

Run 1/10
Epoch [1/50], Class Loss: 1.5213, Discrepancy Loss: 0.1342
Epoch [2/50], Class Loss: 0.4205, Discrepancy Loss: 0.0595
Epoch [3/50], Class Loss: 0.1768, Discrepancy Loss: 0.0410
Epoch [4/50], Class Loss: 0.2982, Discrepancy Loss: 0.0425
Epoch [5/50], Class Loss: 0.0599, Discrepancy Loss: 0.0197
Epoch [6/50], Class Loss: 0.0996, Discrepancy Loss: 0.0228
Epoch [7/50], Class Loss: 0.0505, Discrepancy Loss: 0.0184
Epoch [8/50], Class Loss: 0.0314, Discrepancy Loss: 0.0138
Epoch [9/50], Class Loss: 0.0561, Discrepancy Loss: 0.0197
Epoch [10/50], Class Loss: 0.0210, Discrepancy Loss: 0.0171
Epoch [11/50], Class Loss: 0.0174, Discrepancy Loss: 0.0134
Epoch [12/50], Class Loss: 0.0181, Discrepancy Loss: 0.0111
Epoch [13/50], Class Loss: 0.0097, Discrepancy Loss: 0.0120
Epoch [14/50], Class Loss: 0.0356, Discrepancy Loss: 0.0092
Epoch [15/50], Class Loss: 0.0895, Discrepancy Loss: 0.0167
Epoch [16/50], Class Loss: 0.0112, Discrepancy Loss: 0.0166
Epoch [17/50], Class Loss: 0.1216, Discrepancy Loss: 0.0138
Epoch [18/50], Class Loss: 0.0180, Discrepancy Loss: 0.0172
Epoch [19/50], Class Loss: 0.0115, Discrepancy Loss: 0.0113
Epoch [20/50], Class Loss: 0.0135, Discrepancy Loss: 0.0128
Epoch [21/50], Class Loss: 0.0073, Discrepancy Loss: 0.0136
Epoch [22/50], Class Loss: 0.0106, Discrepancy Loss: 0.0125
Epoch [23/50], Class Loss: 0.0088, Discrepancy Loss: 0.0110
Epoch [24/50], Class Loss: 0.0081, Discrepancy Loss: 0.0105
Epoch [25/50], Class Loss: 0.0102, Discrepancy Loss: 0.0108
Epoch [26/50], Class Loss: 0.0080, Discrepancy Loss: 0.0130
Epoch [27/50], Class Loss: 0.0082, Discrepancy Loss: 0.0124
Epoch [28/50], Class Loss: 0.0053, Discrepancy Loss: 0.0110
Epoch [29/50], Class Loss: 0.0037, Discrepancy Loss: 0.0097
Epoch [30/50], Class Loss: 0.0091, Discrepancy Loss: 0.0136
Epoch [31/50], Class Loss: 0.0134, Discrepancy Loss: 0.0110
Epoch [32/50], Class Loss: 0.0048, Discrepancy Loss: 0.0112
Epoch [33/50], Class Loss: 0.0670, Discrepancy Loss: 0.0091
Epoch [34/50], Class Loss: 0.0066, Discrepancy Loss: 0.0112
Epoch [35/50], Class Loss: 0.0054, Discrepancy Loss: 0.0119
Epoch [36/50], Class Loss: 0.0132, Discrepancy Loss: 0.0108
Epoch [37/50], Class Loss: 0.0053, Discrepancy Loss: 0.0092
Epoch [38/50], Class Loss: 0.0056, Discrepancy Loss: 0.0119
Epoch [39/50], Class Loss: 0.0040, Discrepancy Loss: 0.0104
Epoch [40/50], Class Loss: 0.0098, Discrepancy Loss: 0.0105
Epoch [41/50], Class Loss: 0.0109, Discrepancy Loss: 0.0103
Epoch [42/50], Class Loss: 0.0089, Discrepancy Loss: 0.0122
Epoch [43/50], Class Loss: 0.0986, Discrepancy Loss: 0.0101
Epoch [44/50], Class Loss: 0.0061, Discrepancy Loss: 0.0095
Epoch [45/50], Class Loss: 0.0076, Discrepancy Loss: 0.0102
Epoch [46/50], Class Loss: 0.1164, Discrepancy Loss: 0.0098
Epoch [47/50], Class Loss: 0.0051, Discrepancy Loss: 0.0097
Epoch [48/50], Class Loss: 0.0102, Discrepancy Loss: 0.0108
Epoch [49/50], Class Loss: 0.0059, Discrepancy Loss: 0.0110
Epoch [50/50], Class Loss: 0.0269, Discrepancy Loss: 0.0096
Source Domain Performance - Accuracy: 95.56%, Precision: 96.17%, Recall: 95.39%, F1 Score: 95.41%
Target Domain Performance - Accuracy: 98.62%, Precision: 98.76%, Recall: 98.55%, F1 Score: 98.64%

Run 2/10
Epoch [1/50], Class Loss: 1.6709, Discrepancy Loss: 0.1094
Epoch [2/50], Class Loss: 0.4798, Discrepancy Loss: 0.0616
Epoch [3/50], Class Loss: 0.0868, Discrepancy Loss: 0.0396
Epoch [4/50], Class Loss: 0.1229, Discrepancy Loss: 0.0248
Epoch [5/50], Class Loss: 0.0511, Discrepancy Loss: 0.0195
Epoch [6/50], Class Loss: 0.0703, Discrepancy Loss: 0.0270
Epoch [7/50], Class Loss: 0.0595, Discrepancy Loss: 0.0210
Epoch [8/50], Class Loss: 0.0225, Discrepancy Loss: 0.0136
Epoch [9/50], Class Loss: 0.0302, Discrepancy Loss: 0.0158
Epoch [10/50], Class Loss: 0.0472, Discrepancy Loss: 0.0114
Epoch [11/50], Class Loss: 0.1309, Discrepancy Loss: 0.0260
Epoch [12/50], Class Loss: 0.0149, Discrepancy Loss: 0.0153
Epoch [13/50], Class Loss: 0.0136, Discrepancy Loss: 0.0127
Epoch [14/50], Class Loss: 0.0231, Discrepancy Loss: 0.0152
Epoch [15/50], Class Loss: 0.0233, Discrepancy Loss: 0.0113
Epoch [16/50], Class Loss: 0.0142, Discrepancy Loss: 0.0124
Epoch [17/50], Class Loss: 0.0100, Discrepancy Loss: 0.0110
Epoch [18/50], Class Loss: 0.0054, Discrepancy Loss: 0.0108
Epoch [19/50], Class Loss: 0.0047, Discrepancy Loss: 0.0120
Epoch [20/50], Class Loss: 0.0942, Discrepancy Loss: 0.0079
Epoch [21/50], Class Loss: 0.0242, Discrepancy Loss: 0.0106
Epoch [22/50], Class Loss: 0.0247, Discrepancy Loss: 0.0107
Epoch [23/50], Class Loss: 0.0055, Discrepancy Loss: 0.0123
Epoch [24/50], Class Loss: 0.0285, Discrepancy Loss: 0.0121
Epoch [25/50], Class Loss: 0.2165, Discrepancy Loss: 0.0088
Epoch [26/50], Class Loss: 0.0069, Discrepancy Loss: 0.0100
Epoch [27/50], Class Loss: 0.0057, Discrepancy Loss: 0.0109
Epoch [28/50], Class Loss: 0.0057, Discrepancy Loss: 0.0096
Epoch [29/50], Class Loss: 0.0651, Discrepancy Loss: 0.0100
Epoch [30/50], Class Loss: 0.0067, Discrepancy Loss: 0.0117
Epoch [31/50], Class Loss: 0.0089, Discrepancy Loss: 0.0114
Epoch [32/50], Class Loss: 0.0123, Discrepancy Loss: 0.0080
Epoch [33/50], Class Loss: 0.0052, Discrepancy Loss: 0.0081
Epoch [34/50], Class Loss: 0.0181, Discrepancy Loss: 0.0080
Epoch [35/50], Class Loss: 0.0239, Discrepancy Loss: 0.0119
Epoch [36/50], Class Loss: 0.0094, Discrepancy Loss: 0.0086
Epoch [37/50], Class Loss: 0.0047, Discrepancy Loss: 0.0127
Epoch [38/50], Class Loss: 0.0074, Discrepancy Loss: 0.0105
Epoch [39/50], Class Loss: 0.1005, Discrepancy Loss: 0.0112
Epoch [40/50], Class Loss: 0.0209, Discrepancy Loss: 0.0110
Epoch [41/50], Class Loss: 0.0047, Discrepancy Loss: 0.0101
Epoch [42/50], Class Loss: 0.0047, Discrepancy Loss: 0.0098
Epoch [43/50], Class Loss: 0.0032, Discrepancy Loss: 0.0099
Epoch [44/50], Class Loss: 0.0049, Discrepancy Loss: 0.0087
Epoch [45/50], Class Loss: 0.0076, Discrepancy Loss: 0.0088
Epoch [46/50], Class Loss: 0.0069, Discrepancy Loss: 0.0102
Epoch [47/50], Class Loss: 0.0033, Discrepancy Loss: 0.0112
Epoch [48/50], Class Loss: 0.1127, Discrepancy Loss: 0.0099
Epoch [49/50], Class Loss: 0.0041, Discrepancy Loss: 0.0101
Epoch [50/50], Class Loss: 0.0064, Discrepancy Loss: 0.0092
Source Domain Performance - Accuracy: 95.92%, Precision: 96.39%, Recall: 95.77%, F1 Score: 95.79%
Target Domain Performance - Accuracy: 97.96%, Precision: 98.05%, Recall: 97.88%, F1 Score: 97.95%

Run 3/10
Epoch [1/50], Class Loss: 1.9938, Discrepancy Loss: 0.1369
Epoch [2/50], Class Loss: 0.7184, Discrepancy Loss: 0.0700
Epoch [3/50], Class Loss: 0.2514, Discrepancy Loss: 0.0357
Epoch [4/50], Class Loss: 0.0932, Discrepancy Loss: 0.0246
Epoch [5/50], Class Loss: 0.0767, Discrepancy Loss: 0.0202
Epoch [6/50], Class Loss: 0.0360, Discrepancy Loss: 0.0129
Epoch [7/50], Class Loss: 0.1520, Discrepancy Loss: 0.0209
Epoch [8/50], Class Loss: 0.1009, Discrepancy Loss: 0.0212
Epoch [9/50], Class Loss: 0.0288, Discrepancy Loss: 0.0158
Epoch [10/50], Class Loss: 0.1194, Discrepancy Loss: 0.0104
Epoch [11/50], Class Loss: 0.1731, Discrepancy Loss: 0.0217
Epoch [12/50], Class Loss: 0.0239, Discrepancy Loss: 0.0158
Epoch [13/50], Class Loss: 0.0269, Discrepancy Loss: 0.0162
Epoch [14/50], Class Loss: 0.0209, Discrepancy Loss: 0.0165
Epoch [15/50], Class Loss: 0.0103, Discrepancy Loss: 0.0128
Epoch [16/50], Class Loss: 0.0112, Discrepancy Loss: 0.0112
Epoch [17/50], Class Loss: 0.0095, Discrepancy Loss: 0.0118
Epoch [18/50], Class Loss: 0.0079, Discrepancy Loss: 0.0144
Epoch [19/50], Class Loss: 0.0177, Discrepancy Loss: 0.0137
Epoch [20/50], Class Loss: 0.0144, Discrepancy Loss: 0.0115
Epoch [21/50], Class Loss: 0.0148, Discrepancy Loss: 0.0105
Epoch [22/50], Class Loss: 0.0057, Discrepancy Loss: 0.0094
Epoch [23/50], Class Loss: 0.0098, Discrepancy Loss: 0.0091
Epoch [24/50], Class Loss: 0.0077, Discrepancy Loss: 0.0102
Epoch [25/50], Class Loss: 0.0092, Discrepancy Loss: 0.0110
Epoch [26/50], Class Loss: 0.0053, Discrepancy Loss: 0.0089
Epoch [27/50], Class Loss: 0.0082, Discrepancy Loss: 0.0109
Epoch [28/50], Class Loss: 0.0043, Discrepancy Loss: 0.0091
Epoch [29/50], Class Loss: 0.0082, Discrepancy Loss: 0.0105
Epoch [30/50], Class Loss: 0.0130, Discrepancy Loss: 0.0102
Epoch [31/50], Class Loss: 0.0346, Discrepancy Loss: 0.0090
Epoch [32/50], Class Loss: 0.0043, Discrepancy Loss: 0.0099
Epoch [33/50], Class Loss: 0.0054, Discrepancy Loss: 0.0079
Epoch [34/50], Class Loss: 0.0094, Discrepancy Loss: 0.0080
Epoch [35/50], Class Loss: 0.0118, Discrepancy Loss: 0.0092
Epoch [36/50], Class Loss: 0.0108, Discrepancy Loss: 0.0093
Epoch [37/50], Class Loss: 0.0731, Discrepancy Loss: 0.0110
Epoch [38/50], Class Loss: 0.0296, Discrepancy Loss: 0.0094
Epoch [39/50], Class Loss: 0.0078, Discrepancy Loss: 0.0103
Epoch [40/50], Class Loss: 0.0091, Discrepancy Loss: 0.0094
Epoch [41/50], Class Loss: 0.0051, Discrepancy Loss: 0.0096
Epoch [42/50], Class Loss: 0.0051, Discrepancy Loss: 0.0092
Epoch [43/50], Class Loss: 0.0116, Discrepancy Loss: 0.0095
Epoch [44/50], Class Loss: 0.0083, Discrepancy Loss: 0.0103
Epoch [45/50], Class Loss: 0.0306, Discrepancy Loss: 0.0091
Epoch [46/50], Class Loss: 0.0090, Discrepancy Loss: 0.0112
Epoch [47/50], Class Loss: 0.0066, Discrepancy Loss: 0.0093
Epoch [48/50], Class Loss: 0.1334, Discrepancy Loss: 0.0127
Epoch [49/50], Class Loss: 0.0057, Discrepancy Loss: 0.0122
Epoch [50/50], Class Loss: 0.0135, Discrepancy Loss: 0.0099
Source Domain Performance - Accuracy: 95.56%, Precision: 96.17%, Recall: 95.39%, F1 Score: 95.41%
Target Domain Performance - Accuracy: 99.04%, Precision: 99.07%, Recall: 99.02%, F1 Score: 99.04%

Run 4/10
Epoch [1/50], Class Loss: 1.6935, Discrepancy Loss: 0.1272
Epoch [2/50], Class Loss: 1.0024, Discrepancy Loss: 0.0862
Epoch [3/50], Class Loss: 0.3536, Discrepancy Loss: 0.0458
Epoch [4/50], Class Loss: 0.1707, Discrepancy Loss: 0.0313
Epoch [5/50], Class Loss: 0.2111, Discrepancy Loss: 0.0314
Epoch [6/50], Class Loss: 0.0358, Discrepancy Loss: 0.0273
Epoch [7/50], Class Loss: 0.0661, Discrepancy Loss: 0.0246
Epoch [8/50], Class Loss: 0.0251, Discrepancy Loss: 0.0228
Epoch [9/50], Class Loss: 0.0348, Discrepancy Loss: 0.0154
Epoch [10/50], Class Loss: 0.1955, Discrepancy Loss: 0.0247
Epoch [11/50], Class Loss: 0.0845, Discrepancy Loss: 0.0213
Epoch [12/50], Class Loss: 0.0352, Discrepancy Loss: 0.0232
Epoch [13/50], Class Loss: 0.0289, Discrepancy Loss: 0.0172
Epoch [14/50], Class Loss: 0.1850, Discrepancy Loss: 0.0163
Epoch [15/50], Class Loss: 0.0378, Discrepancy Loss: 0.0180
Epoch [16/50], Class Loss: 0.0215, Discrepancy Loss: 0.0204
Epoch [17/50], Class Loss: 0.0309, Discrepancy Loss: 0.0168
Epoch [18/50], Class Loss: 0.0189, Discrepancy Loss: 0.0222
Epoch [19/50], Class Loss: 0.0209, Discrepancy Loss: 0.0189
Epoch [20/50], Class Loss: 0.0250, Discrepancy Loss: 0.0161
Epoch [21/50], Class Loss: 0.0235, Discrepancy Loss: 0.0214
Epoch [22/50], Class Loss: 0.0166, Discrepancy Loss: 0.0173
Epoch [23/50], Class Loss: 0.0131, Discrepancy Loss: 0.0239
Epoch [24/50], Class Loss: 0.0189, Discrepancy Loss: 0.0191
Epoch [25/50], Class Loss: 0.0498, Discrepancy Loss: 0.0162
Epoch [26/50], Class Loss: 0.0161, Discrepancy Loss: 0.0170
Epoch [27/50], Class Loss: 0.0512, Discrepancy Loss: 0.0118
Epoch [28/50], Class Loss: 0.0126, Discrepancy Loss: 0.0189
Epoch [29/50], Class Loss: 0.0246, Discrepancy Loss: 0.0132
Epoch [30/50], Class Loss: 0.0121, Discrepancy Loss: 0.0140
Epoch [31/50], Class Loss: 0.0143, Discrepancy Loss: 0.0124
Epoch [32/50], Class Loss: 0.0670, Discrepancy Loss: 0.0160
Epoch [33/50], Class Loss: 0.0125, Discrepancy Loss: 0.0125
Epoch [34/50], Class Loss: 0.0118, Discrepancy Loss: 0.0166
Epoch [35/50], Class Loss: 0.0232, Discrepancy Loss: 0.0150
Epoch [36/50], Class Loss: 0.0142, Discrepancy Loss: 0.0118
Epoch [37/50], Class Loss: 0.0144, Discrepancy Loss: 0.0162
Epoch [38/50], Class Loss: 0.0253, Discrepancy Loss: 0.0146
Epoch [39/50], Class Loss: 0.0273, Discrepancy Loss: 0.0157
Epoch [40/50], Class Loss: 0.0114, Discrepancy Loss: 0.0153
Epoch [41/50], Class Loss: 0.0122, Discrepancy Loss: 0.0141
Epoch [42/50], Class Loss: 0.0158, Discrepancy Loss: 0.0173
Epoch [43/50], Class Loss: 0.0141, Discrepancy Loss: 0.0145
Epoch [44/50], Class Loss: 0.0232, Discrepancy Loss: 0.0159
Epoch [45/50], Class Loss: 0.0144, Discrepancy Loss: 0.0120
Epoch [46/50], Class Loss: 0.0108, Discrepancy Loss: 0.0176
Epoch [47/50], Class Loss: 0.0549, Discrepancy Loss: 0.0132
Epoch [48/50], Class Loss: 0.0144, Discrepancy Loss: 0.0179
Epoch [49/50], Class Loss: 0.0120, Discrepancy Loss: 0.0160
Epoch [50/50], Class Loss: 0.0094, Discrepancy Loss: 0.0128
Source Domain Performance - Accuracy: 95.80%, Precision: 96.38%, Recall: 95.64%, F1 Score: 95.66%
Target Domain Performance - Accuracy: 98.80%, Precision: 98.81%, Recall: 98.80%, F1 Score: 98.80%

Run 5/10
Epoch [1/50], Class Loss: 1.7446, Discrepancy Loss: 0.1050
Epoch [2/50], Class Loss: 0.6221, Discrepancy Loss: 0.0641
Epoch [3/50], Class Loss: 0.1768, Discrepancy Loss: 0.0483
Epoch [4/50], Class Loss: 0.0905, Discrepancy Loss: 0.0334
Epoch [5/50], Class Loss: 0.0789, Discrepancy Loss: 0.0229
Epoch [6/50], Class Loss: 0.2161, Discrepancy Loss: 0.0271
Epoch [7/50], Class Loss: 0.1763, Discrepancy Loss: 0.0298
Epoch [8/50], Class Loss: 0.0641, Discrepancy Loss: 0.0188
Epoch [9/50], Class Loss: 0.1147, Discrepancy Loss: 0.0290
Epoch [10/50], Class Loss: 0.1392, Discrepancy Loss: 0.0155
Epoch [11/50], Class Loss: 0.0214, Discrepancy Loss: 0.0158
Epoch [12/50], Class Loss: 0.0239, Discrepancy Loss: 0.0154
Epoch [13/50], Class Loss: 0.0304, Discrepancy Loss: 0.0135
Epoch [14/50], Class Loss: 0.0212, Discrepancy Loss: 0.0160
Epoch [15/50], Class Loss: 0.1591, Discrepancy Loss: 0.0183
Epoch [16/50], Class Loss: 0.0449, Discrepancy Loss: 0.0167
Epoch [17/50], Class Loss: 0.0298, Discrepancy Loss: 0.0215
Epoch [18/50], Class Loss: 0.0329, Discrepancy Loss: 0.0143
Epoch [19/50], Class Loss: 0.0182, Discrepancy Loss: 0.0128
Epoch [20/50], Class Loss: 0.0198, Discrepancy Loss: 0.0115
Epoch [21/50], Class Loss: 0.0704, Discrepancy Loss: 0.0160
Epoch [22/50], Class Loss: 0.0124, Discrepancy Loss: 0.0129
Epoch [23/50], Class Loss: 0.0114, Discrepancy Loss: 0.0166
Epoch [24/50], Class Loss: 0.1767, Discrepancy Loss: 0.0144
Epoch [25/50], Class Loss: 0.0159, Discrepancy Loss: 0.0118
Epoch [26/50], Class Loss: 0.0166, Discrepancy Loss: 0.0165
Epoch [27/50], Class Loss: 0.0101, Discrepancy Loss: 0.0112
Epoch [28/50], Class Loss: 0.0157, Discrepancy Loss: 0.0129
Epoch [29/50], Class Loss: 0.0170, Discrepancy Loss: 0.0113
Epoch [30/50], Class Loss: 0.0116, Discrepancy Loss: 0.0097
Epoch [31/50], Class Loss: 0.0275, Discrepancy Loss: 0.0127
Epoch [32/50], Class Loss: 0.0154, Discrepancy Loss: 0.0183
Epoch [33/50], Class Loss: 0.0133, Discrepancy Loss: 0.0142
Epoch [34/50], Class Loss: 0.0196, Discrepancy Loss: 0.0113
Epoch [35/50], Class Loss: 0.0114, Discrepancy Loss: 0.0133
Epoch [36/50], Class Loss: 0.0166, Discrepancy Loss: 0.0121
Epoch [37/50], Class Loss: 0.0109, Discrepancy Loss: 0.0109
Epoch [38/50], Class Loss: 0.0333, Discrepancy Loss: 0.0135
Epoch [39/50], Class Loss: 0.0116, Discrepancy Loss: 0.0122
Epoch [40/50], Class Loss: 0.0851, Discrepancy Loss: 0.0128
Epoch [41/50], Class Loss: 0.0191, Discrepancy Loss: 0.0135
Epoch [42/50], Class Loss: 0.0634, Discrepancy Loss: 0.0123
Epoch [43/50], Class Loss: 0.0157, Discrepancy Loss: 0.0107
Epoch [44/50], Class Loss: 0.0176, Discrepancy Loss: 0.0147
Epoch [45/50], Class Loss: 0.0123, Discrepancy Loss: 0.0103
Epoch [46/50], Class Loss: 0.0268, Discrepancy Loss: 0.0149
Epoch [47/50], Class Loss: 0.0094, Discrepancy Loss: 0.0143
Epoch [48/50], Class Loss: 0.0182, Discrepancy Loss: 0.0129
Epoch [49/50], Class Loss: 0.0102, Discrepancy Loss: 0.0133
Epoch [50/50], Class Loss: 0.0259, Discrepancy Loss: 0.0132
Source Domain Performance - Accuracy: 96.82%, Precision: 97.13%, Recall: 96.70%, F1 Score: 96.72%
Target Domain Performance - Accuracy: 98.08%, Precision: 98.11%, Recall: 98.05%, F1 Score: 98.07%

Run 6/10
Epoch [1/50], Class Loss: 1.9308, Discrepancy Loss: 0.1117
Epoch [2/50], Class Loss: 1.2626, Discrepancy Loss: 0.0427
Epoch [3/50], Class Loss: 0.1036, Discrepancy Loss: 0.0390
Epoch [4/50], Class Loss: 0.0444, Discrepancy Loss: 0.0193
Epoch [5/50], Class Loss: 0.0945, Discrepancy Loss: 0.0261
Epoch [6/50], Class Loss: 0.0360, Discrepancy Loss: 0.0205
Epoch [7/50], Class Loss: 0.0409, Discrepancy Loss: 0.0131
Epoch [8/50], Class Loss: 0.1189, Discrepancy Loss: 0.0235
Epoch [9/50], Class Loss: 0.0182, Discrepancy Loss: 0.0167
Epoch [10/50], Class Loss: 0.0632, Discrepancy Loss: 0.0159
Epoch [11/50], Class Loss: 0.0124, Discrepancy Loss: 0.0161
Epoch [12/50], Class Loss: 0.0095, Discrepancy Loss: 0.0137
Epoch [13/50], Class Loss: 0.0226, Discrepancy Loss: 0.0109
Epoch [14/50], Class Loss: 0.0087, Discrepancy Loss: 0.0194
Epoch [15/50], Class Loss: 0.0072, Discrepancy Loss: 0.0155
Epoch [16/50], Class Loss: 0.0084, Discrepancy Loss: 0.0110
Epoch [17/50], Class Loss: 0.0079, Discrepancy Loss: 0.0133
Epoch [18/50], Class Loss: 0.0078, Discrepancy Loss: 0.0109
Epoch [19/50], Class Loss: 0.0099, Discrepancy Loss: 0.0124
Epoch [20/50], Class Loss: 0.0091, Discrepancy Loss: 0.0102
Epoch [21/50], Class Loss: 0.0293, Discrepancy Loss: 0.0130
Epoch [22/50], Class Loss: 0.0082, Discrepancy Loss: 0.0129
Epoch [23/50], Class Loss: 0.0074, Discrepancy Loss: 0.0148
Epoch [24/50], Class Loss: 0.0361, Discrepancy Loss: 0.0113
Epoch [25/50], Class Loss: 0.0053, Discrepancy Loss: 0.0110
Epoch [26/50], Class Loss: 0.0080, Discrepancy Loss: 0.0102
Epoch [27/50], Class Loss: 0.0158, Discrepancy Loss: 0.0116
Epoch [28/50], Class Loss: 0.0069, Discrepancy Loss: 0.0102
Epoch [29/50], Class Loss: 0.0072, Discrepancy Loss: 0.0103
Epoch [30/50], Class Loss: 0.0045, Discrepancy Loss: 0.0089
Epoch [31/50], Class Loss: 0.0053, Discrepancy Loss: 0.0097
Epoch [32/50], Class Loss: 0.0319, Discrepancy Loss: 0.0111
Epoch [33/50], Class Loss: 0.0346, Discrepancy Loss: 0.0110
Epoch [34/50], Class Loss: 0.0089, Discrepancy Loss: 0.0108
Epoch [35/50], Class Loss: 0.0039, Discrepancy Loss: 0.0115
Epoch [36/50], Class Loss: 0.0046, Discrepancy Loss: 0.0109
Epoch [37/50], Class Loss: 0.0144, Discrepancy Loss: 0.0123
Epoch [38/50], Class Loss: 0.0061, Discrepancy Loss: 0.0119
Epoch [39/50], Class Loss: 0.0067, Discrepancy Loss: 0.0122
Epoch [40/50], Class Loss: 0.0720, Discrepancy Loss: 0.0112
Epoch [41/50], Class Loss: 0.0035, Discrepancy Loss: 0.0097
Epoch [42/50], Class Loss: 0.0062, Discrepancy Loss: 0.0115
Epoch [43/50], Class Loss: 0.0060, Discrepancy Loss: 0.0093
Epoch [44/50], Class Loss: 0.0077, Discrepancy Loss: 0.0102
Epoch [45/50], Class Loss: 0.0034, Discrepancy Loss: 0.0115
Epoch [46/50], Class Loss: 0.0055, Discrepancy Loss: 0.0115
Epoch [47/50], Class Loss: 0.0067, Discrepancy Loss: 0.0111
Epoch [48/50], Class Loss: 0.0348, Discrepancy Loss: 0.0098
Epoch [49/50], Class Loss: 0.0056, Discrepancy Loss: 0.0111
Epoch [50/50], Class Loss: 0.0046, Discrepancy Loss: 0.0126
Source Domain Performance - Accuracy: 98.62%, Precision: 98.66%, Recall: 98.58%, F1 Score: 98.59%
Target Domain Performance - Accuracy: 92.27%, Precision: 92.91%, Recall: 91.91%, F1 Score: 92.05%

Run 7/10
Epoch [1/50], Class Loss: 1.7183, Discrepancy Loss: 0.1027
Epoch [2/50], Class Loss: 0.3933, Discrepancy Loss: 0.0498
Epoch [3/50], Class Loss: 0.2053, Discrepancy Loss: 0.0327
Epoch [4/50], Class Loss: 0.0361, Discrepancy Loss: 0.0216
Epoch [5/50], Class Loss: 0.0764, Discrepancy Loss: 0.0176
Epoch [6/50], Class Loss: 0.0465, Discrepancy Loss: 0.0163
Epoch [7/50], Class Loss: 0.0370, Discrepancy Loss: 0.0169
Epoch [8/50], Class Loss: 0.0192, Discrepancy Loss: 0.0101
Epoch [9/50], Class Loss: 0.0324, Discrepancy Loss: 0.0113
Epoch [10/50], Class Loss: 0.1148, Discrepancy Loss: 0.0215
Epoch [11/50], Class Loss: 0.0233, Discrepancy Loss: 0.0189
Epoch [12/50], Class Loss: 0.0085, Discrepancy Loss: 0.0122
Epoch [13/50], Class Loss: 0.0074, Discrepancy Loss: 0.0117
Epoch [14/50], Class Loss: 0.0075, Discrepancy Loss: 0.0125
Epoch [15/50], Class Loss: 0.1240, Discrepancy Loss: 0.0091
Epoch [16/50], Class Loss: 0.0137, Discrepancy Loss: 0.0098
Epoch [17/50], Class Loss: 0.0063, Discrepancy Loss: 0.0110
Epoch [18/50], Class Loss: 0.0058, Discrepancy Loss: 0.0097
Epoch [19/50], Class Loss: 0.0095, Discrepancy Loss: 0.0077
Epoch [20/50], Class Loss: 0.0119, Discrepancy Loss: 0.0108
Epoch [21/50], Class Loss: 0.0090, Discrepancy Loss: 0.0100
Epoch [22/50], Class Loss: 0.0063, Discrepancy Loss: 0.0089
Epoch [23/50], Class Loss: 0.0199, Discrepancy Loss: 0.0105
Epoch [24/50], Class Loss: 0.0100, Discrepancy Loss: 0.0110
Epoch [25/50], Class Loss: 0.0044, Discrepancy Loss: 0.0095
Epoch [26/50], Class Loss: 0.0033, Discrepancy Loss: 0.0111
Epoch [27/50], Class Loss: 0.0044, Discrepancy Loss: 0.0090
Epoch [28/50], Class Loss: 0.0276, Discrepancy Loss: 0.0097
Epoch [29/50], Class Loss: 0.0037, Discrepancy Loss: 0.0083
Epoch [30/50], Class Loss: 0.0064, Discrepancy Loss: 0.0102
Epoch [31/50], Class Loss: 0.0042, Discrepancy Loss: 0.0089
Epoch [32/50], Class Loss: 0.0079, Discrepancy Loss: 0.0104
Epoch [33/50], Class Loss: 0.0118, Discrepancy Loss: 0.0094
Epoch [34/50], Class Loss: 0.0047, Discrepancy Loss: 0.0097
Epoch [35/50], Class Loss: 0.0028, Discrepancy Loss: 0.0089
Epoch [36/50], Class Loss: 0.0041, Discrepancy Loss: 0.0088
Epoch [37/50], Class Loss: 0.1277, Discrepancy Loss: 0.0093
Epoch [38/50], Class Loss: 0.0027, Discrepancy Loss: 0.0076
Epoch [39/50], Class Loss: 0.0032, Discrepancy Loss: 0.0070
Epoch [40/50], Class Loss: 0.0025, Discrepancy Loss: 0.0078
Epoch [41/50], Class Loss: 0.0450, Discrepancy Loss: 0.0081
Epoch [42/50], Class Loss: 0.0036, Discrepancy Loss: 0.0082
Epoch [43/50], Class Loss: 0.0035, Discrepancy Loss: 0.0075
Epoch [44/50], Class Loss: 0.0081, Discrepancy Loss: 0.0093
Epoch [45/50], Class Loss: 0.0064, Discrepancy Loss: 0.0085
Epoch [46/50], Class Loss: 0.0083, Discrepancy Loss: 0.0094
Epoch [47/50], Class Loss: 0.0621, Discrepancy Loss: 0.0079
Epoch [48/50], Class Loss: 0.1228, Discrepancy Loss: 0.0074
Epoch [49/50], Class Loss: 0.0031, Discrepancy Loss: 0.0084
Epoch [50/50], Class Loss: 0.0051, Discrepancy Loss: 0.0077
Source Domain Performance - Accuracy: 97.42%, Precision: 97.64%, Recall: 97.32%, F1 Score: 97.35%
Target Domain Performance - Accuracy: 96.22%, Precision: 96.38%, Recall: 96.08%, F1 Score: 96.13%

Run 8/10
Epoch [1/50], Class Loss: 1.8659, Discrepancy Loss: 0.1410
Epoch [2/50], Class Loss: 0.5713, Discrepancy Loss: 0.0711
Epoch [3/50], Class Loss: 0.2191, Discrepancy Loss: 0.0343
Epoch [4/50], Class Loss: 0.1507, Discrepancy Loss: 0.0303
Epoch [5/50], Class Loss: 0.2157, Discrepancy Loss: 0.0353
Epoch [6/50], Class Loss: 0.0638, Discrepancy Loss: 0.0194
Epoch [7/50], Class Loss: 0.0459, Discrepancy Loss: 0.0186
Epoch [8/50], Class Loss: 0.0556, Discrepancy Loss: 0.0132
Epoch [9/50], Class Loss: 0.0483, Discrepancy Loss: 0.0204
Epoch [10/50], Class Loss: 0.0708, Discrepancy Loss: 0.0147
Epoch [11/50], Class Loss: 0.1154, Discrepancy Loss: 0.0241
Epoch [12/50], Class Loss: 0.0167, Discrepancy Loss: 0.0171
Epoch [13/50], Class Loss: 0.0173, Discrepancy Loss: 0.0159
Epoch [14/50], Class Loss: 0.0215, Discrepancy Loss: 0.0124
Epoch [15/50], Class Loss: 0.0188, Discrepancy Loss: 0.0109
Epoch [16/50], Class Loss: 0.0120, Discrepancy Loss: 0.0089
Epoch [17/50], Class Loss: 0.0083, Discrepancy Loss: 0.0136
Epoch [18/50], Class Loss: 0.0837, Discrepancy Loss: 0.0104
Epoch [19/50], Class Loss: 0.0337, Discrepancy Loss: 0.0141
Epoch [20/50], Class Loss: 0.0093, Discrepancy Loss: 0.0101
Epoch [21/50], Class Loss: 0.0081, Discrepancy Loss: 0.0093
Epoch [22/50], Class Loss: 0.0102, Discrepancy Loss: 0.0096
Epoch [23/50], Class Loss: 0.0132, Discrepancy Loss: 0.0094
Epoch [24/50], Class Loss: 0.0052, Discrepancy Loss: 0.0094
Epoch [25/50], Class Loss: 0.0083, Discrepancy Loss: 0.0097
Epoch [26/50], Class Loss: 0.0262, Discrepancy Loss: 0.0095
Epoch [27/50], Class Loss: 0.0075, Discrepancy Loss: 0.0093
Epoch [28/50], Class Loss: 0.0087, Discrepancy Loss: 0.0106
Epoch [29/50], Class Loss: 0.0075, Discrepancy Loss: 0.0091
Epoch [30/50], Class Loss: 0.0970, Discrepancy Loss: 0.0128
Epoch [31/50], Class Loss: 0.0328, Discrepancy Loss: 0.0122
Epoch [32/50], Class Loss: 0.0078, Discrepancy Loss: 0.0092
Epoch [33/50], Class Loss: 0.0061, Discrepancy Loss: 0.0090
Epoch [34/50], Class Loss: 0.0096, Discrepancy Loss: 0.0088
Epoch [35/50], Class Loss: 0.0102, Discrepancy Loss: 0.0097
Epoch [36/50], Class Loss: 0.0059, Discrepancy Loss: 0.0095
Epoch [37/50], Class Loss: 0.0081, Discrepancy Loss: 0.0122
Epoch [38/50], Class Loss: 0.0093, Discrepancy Loss: 0.0093
Epoch [39/50], Class Loss: 0.0114, Discrepancy Loss: 0.0095
Epoch [40/50], Class Loss: 0.0097, Discrepancy Loss: 0.0118
Epoch [41/50], Class Loss: 0.0091, Discrepancy Loss: 0.0091
Epoch [42/50], Class Loss: 0.0239, Discrepancy Loss: 0.0083
Epoch [43/50], Class Loss: 0.0075, Discrepancy Loss: 0.0126
Epoch [44/50], Class Loss: 0.0148, Discrepancy Loss: 0.0099
Epoch [45/50], Class Loss: 0.0066, Discrepancy Loss: 0.0110
Epoch [46/50], Class Loss: 0.0121, Discrepancy Loss: 0.0096
Epoch [47/50], Class Loss: 0.0069, Discrepancy Loss: 0.0106
Epoch [48/50], Class Loss: 0.0680, Discrepancy Loss: 0.0095
Epoch [49/50], Class Loss: 0.0114, Discrepancy Loss: 0.0103
Epoch [50/50], Class Loss: 0.0284, Discrepancy Loss: 0.0116
Source Domain Performance - Accuracy: 95.80%, Precision: 96.38%, Recall: 95.64%, F1 Score: 95.66%
Target Domain Performance - Accuracy: 98.74%, Precision: 98.84%, Recall: 98.70%, F1 Score: 98.76%

Run 9/10
Epoch [1/50], Class Loss: 1.5617, Discrepancy Loss: 0.1229
Epoch [2/50], Class Loss: 0.4528, Discrepancy Loss: 0.0700
Epoch [3/50], Class Loss: 0.2714, Discrepancy Loss: 0.0476
Epoch [4/50], Class Loss: 0.1571, Discrepancy Loss: 0.0346
Epoch [5/50], Class Loss: 0.0890, Discrepancy Loss: 0.0246
Epoch [6/50], Class Loss: 0.0593, Discrepancy Loss: 0.0265
Epoch [7/50], Class Loss: 0.2420, Discrepancy Loss: 0.0317
Epoch [8/50], Class Loss: 0.0960, Discrepancy Loss: 0.0252
Epoch [9/50], Class Loss: 0.0747, Discrepancy Loss: 0.0216
Epoch [10/50], Class Loss: 0.0323, Discrepancy Loss: 0.0236
Epoch [11/50], Class Loss: 0.0322, Discrepancy Loss: 0.0162
Epoch [12/50], Class Loss: 0.0153, Discrepancy Loss: 0.0157
Epoch [13/50], Class Loss: 0.0270, Discrepancy Loss: 0.0149
Epoch [14/50], Class Loss: 0.1235, Discrepancy Loss: 0.0136
Epoch [15/50], Class Loss: 0.0234, Discrepancy Loss: 0.0140
Epoch [16/50], Class Loss: 0.0134, Discrepancy Loss: 0.0103
Epoch [17/50], Class Loss: 0.0158, Discrepancy Loss: 0.0117
Epoch [18/50], Class Loss: 0.0168, Discrepancy Loss: 0.0135
Epoch [19/50], Class Loss: 0.1092, Discrepancy Loss: 0.0100
Epoch [20/50], Class Loss: 0.0405, Discrepancy Loss: 0.0173
Epoch [21/50], Class Loss: 0.0135, Discrepancy Loss: 0.0149
Epoch [22/50], Class Loss: 0.0228, Discrepancy Loss: 0.0145
Epoch [23/50], Class Loss: 0.0146, Discrepancy Loss: 0.0168
Epoch [24/50], Class Loss: 0.0819, Discrepancy Loss: 0.0117
Epoch [25/50], Class Loss: 0.1046, Discrepancy Loss: 0.0140
Epoch [26/50], Class Loss: 0.0112, Discrepancy Loss: 0.0134
Epoch [27/50], Class Loss: 0.0230, Discrepancy Loss: 0.0103
Epoch [28/50], Class Loss: 0.0203, Discrepancy Loss: 0.0145
Epoch [29/50], Class Loss: 0.0148, Discrepancy Loss: 0.0121
Epoch [30/50], Class Loss: 0.0112, Discrepancy Loss: 0.0153
Epoch [31/50], Class Loss: 0.1907, Discrepancy Loss: 0.0106
Epoch [32/50], Class Loss: 0.0137, Discrepancy Loss: 0.0128
Epoch [33/50], Class Loss: 0.0146, Discrepancy Loss: 0.0133
Epoch [34/50], Class Loss: 0.0113, Discrepancy Loss: 0.0142
Epoch [35/50], Class Loss: 0.0120, Discrepancy Loss: 0.0141
Epoch [36/50], Class Loss: 0.1638, Discrepancy Loss: 0.0168
Epoch [37/50], Class Loss: 0.0168, Discrepancy Loss: 0.0128
Epoch [38/50], Class Loss: 0.0160, Discrepancy Loss: 0.0144
Epoch [39/50], Class Loss: 0.0105, Discrepancy Loss: 0.0128
Epoch [40/50], Class Loss: 0.0169, Discrepancy Loss: 0.0181
Epoch [41/50], Class Loss: 0.0121, Discrepancy Loss: 0.0143
Epoch [42/50], Class Loss: 0.0359, Discrepancy Loss: 0.0128
Epoch [43/50], Class Loss: 0.0122, Discrepancy Loss: 0.0138
Epoch [44/50], Class Loss: 0.0220, Discrepancy Loss: 0.0110
Epoch [45/50], Class Loss: 0.0083, Discrepancy Loss: 0.0190
Epoch [46/50], Class Loss: 0.0124, Discrepancy Loss: 0.0134
Epoch [47/50], Class Loss: 0.0662, Discrepancy Loss: 0.0138
Epoch [48/50], Class Loss: 0.0616, Discrepancy Loss: 0.0167
Epoch [49/50], Class Loss: 0.0145, Discrepancy Loss: 0.0117
Epoch [50/50], Class Loss: 0.0123, Discrepancy Loss: 0.0130
Source Domain Performance - Accuracy: 97.84%, Precision: 98.00%, Recall: 97.76%, F1 Score: 97.78%
Target Domain Performance - Accuracy: 96.64%, Precision: 96.80%, Recall: 96.48%, F1 Score: 96.59%

Run 10/10
Epoch [1/50], Class Loss: 1.9401, Discrepancy Loss: 0.1212
Epoch [2/50], Class Loss: 0.5503, Discrepancy Loss: 0.0689
Epoch [3/50], Class Loss: 0.2837, Discrepancy Loss: 0.0326
Epoch [4/50], Class Loss: 0.0893, Discrepancy Loss: 0.0223
Epoch [5/50], Class Loss: 0.0499, Discrepancy Loss: 0.0184
Epoch [6/50], Class Loss: 0.1564, Discrepancy Loss: 0.0292
Epoch [7/50], Class Loss: 0.3779, Discrepancy Loss: 0.0481
Epoch [8/50], Class Loss: 0.0753, Discrepancy Loss: 0.0328
Epoch [9/50], Class Loss: 0.0476, Discrepancy Loss: 0.0197
Epoch [10/50], Class Loss: 0.0340, Discrepancy Loss: 0.0168
Epoch [11/50], Class Loss: 0.0735, Discrepancy Loss: 0.0207
Epoch [12/50], Class Loss: 0.0303, Discrepancy Loss: 0.0114
Epoch [13/50], Class Loss: 0.0214, Discrepancy Loss: 0.0138
Epoch [14/50], Class Loss: 0.0148, Discrepancy Loss: 0.0126
Epoch [15/50], Class Loss: 0.0890, Discrepancy Loss: 0.0115
Epoch [16/50], Class Loss: 0.0158, Discrepancy Loss: 0.0140
Epoch [17/50], Class Loss: 0.0544, Discrepancy Loss: 0.0128
Epoch [18/50], Class Loss: 0.0207, Discrepancy Loss: 0.0153
Epoch [19/50], Class Loss: 0.0503, Discrepancy Loss: 0.0119
Epoch [20/50], Class Loss: 0.0239, Discrepancy Loss: 0.0235
Epoch [21/50], Class Loss: 0.0436, Discrepancy Loss: 0.0145
Epoch [22/50], Class Loss: 0.0109, Discrepancy Loss: 0.0127
Epoch [23/50], Class Loss: 0.0432, Discrepancy Loss: 0.0108
Epoch [24/50], Class Loss: 0.0100, Discrepancy Loss: 0.0129
Epoch [25/50], Class Loss: 0.0142, Discrepancy Loss: 0.0112
Epoch [26/50], Class Loss: 0.0159, Discrepancy Loss: 0.0122
Epoch [27/50], Class Loss: 0.0119, Discrepancy Loss: 0.0157
Epoch [28/50], Class Loss: 0.0074, Discrepancy Loss: 0.0160
Epoch [29/50], Class Loss: 0.0212, Discrepancy Loss: 0.0150
Epoch [30/50], Class Loss: 0.0099, Discrepancy Loss: 0.0121
Epoch [31/50], Class Loss: 0.0131, Discrepancy Loss: 0.0122
Epoch [32/50], Class Loss: 0.0179, Discrepancy Loss: 0.0127
Epoch [33/50], Class Loss: 0.0083, Discrepancy Loss: 0.0131
Epoch [34/50], Class Loss: 0.1163, Discrepancy Loss: 0.0140
Epoch [35/50], Class Loss: 0.0695, Discrepancy Loss: 0.0137
Epoch [36/50], Class Loss: 0.0216, Discrepancy Loss: 0.0110
Epoch [37/50], Class Loss: 0.0118, Discrepancy Loss: 0.0136
Epoch [38/50], Class Loss: 0.0071, Discrepancy Loss: 0.0143
Epoch [39/50], Class Loss: 0.0062, Discrepancy Loss: 0.0148
Epoch [40/50], Class Loss: 0.0235, Discrepancy Loss: 0.0118
Epoch [41/50], Class Loss: 0.0119, Discrepancy Loss: 0.0138
Epoch [42/50], Class Loss: 0.0074, Discrepancy Loss: 0.0128
Epoch [43/50], Class Loss: 0.0188, Discrepancy Loss: 0.0128
Epoch [44/50], Class Loss: 0.0134, Discrepancy Loss: 0.0106
Epoch [45/50], Class Loss: 0.0147, Discrepancy Loss: 0.0121
Epoch [46/50], Class Loss: 0.0538, Discrepancy Loss: 0.0129
Epoch [47/50], Class Loss: 0.0092, Discrepancy Loss: 0.0128
Epoch [48/50], Class Loss: 0.0097, Discrepancy Loss: 0.0121
Epoch [49/50], Class Loss: 0.0101, Discrepancy Loss: 0.0117
Epoch [50/50], Class Loss: 0.0085, Discrepancy Loss: 0.0128
Source Domain Performance - Accuracy: 97.66%, Precision: 97.84%, Recall: 97.57%, F1 Score: 97.59%
Target Domain Performance - Accuracy: 97.54%, Precision: 97.70%, Recall: 97.41%, F1 Score: 97.53%

Source performance: 96.70% 97.07% 96.57% 96.59%
Target performance: 97.39% 97.54% 97.29% 97.36%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 94.75%
16qam: 94.97%
8apsk: 99.42%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.5111, Discrepancy Loss: 0.0266
Validation Loss: 4.2144
Epoch [2/50], Class Loss: 0.0952, Discrepancy Loss: 0.0407
Validation Loss: 0.3423
Epoch [3/50], Class Loss: 0.0488, Discrepancy Loss: 0.0294
Validation Loss: 0.0103
Epoch [4/50], Class Loss: 0.0454, Discrepancy Loss: 0.0388
Validation Loss: 0.7890
Epoch [5/50], Class Loss: 0.1061, Discrepancy Loss: 0.0720
Validation Loss: 0.0206
Epoch [6/50], Class Loss: 0.0345, Discrepancy Loss: 0.0673
Validation Loss: 0.9342
Epoch [7/50], Class Loss: 0.0765, Discrepancy Loss: 0.0462
Validation Loss: 1.2254
Epoch [8/50], Class Loss: 0.0574, Discrepancy Loss: 0.0256
Validation Loss: 0.0475
Early stopping!
Source Domain Performance - Accuracy: 99.04%, Precision: 99.03%, Recall: 99.07%, F1 Score: 99.04%
Target Domain Performance - Accuracy: 74.04%, Precision: 82.33%, Recall: 73.05%, F1 Score: 69.60%

Run 2/10
Epoch [1/50], Class Loss: 0.6443, Discrepancy Loss: 0.0286
Validation Loss: 4.6616
Epoch [2/50], Class Loss: 0.0742, Discrepancy Loss: 0.0233
Validation Loss: 1.2201
Epoch [3/50], Class Loss: 0.0471, Discrepancy Loss: 0.0554
Validation Loss: 0.4000
Epoch [4/50], Class Loss: 0.0821, Discrepancy Loss: 0.0375
Validation Loss: 0.1458
Epoch [5/50], Class Loss: 0.0484, Discrepancy Loss: 0.0822
Validation Loss: 1.9925
Epoch [6/50], Class Loss: 0.0903, Discrepancy Loss: 0.0459
Validation Loss: 0.1041
Epoch [7/50], Class Loss: 0.0958, Discrepancy Loss: 0.0607
Validation Loss: 7.0261
Epoch [8/50], Class Loss: 0.1888, Discrepancy Loss: 0.0620
Validation Loss: 1.9629
Epoch [9/50], Class Loss: 0.0537, Discrepancy Loss: 0.0634
Validation Loss: 1.4018
Epoch [10/50], Class Loss: 0.0572, Discrepancy Loss: 0.0676
Validation Loss: 0.1904
Epoch [11/50], Class Loss: 0.0211, Discrepancy Loss: 0.0473
Validation Loss: 0.0209
Epoch [12/50], Class Loss: 0.0091, Discrepancy Loss: 0.0521
Validation Loss: 0.0297
Epoch [13/50], Class Loss: 0.0125, Discrepancy Loss: 0.0408
Validation Loss: 0.0125
Epoch [14/50], Class Loss: 0.0089, Discrepancy Loss: 0.0634
Validation Loss: 0.0153
Epoch [15/50], Class Loss: 0.0087, Discrepancy Loss: 0.1165
Validation Loss: 0.0400
Epoch [16/50], Class Loss: 0.0154, Discrepancy Loss: 0.1603
Validation Loss: 0.0279
Epoch [17/50], Class Loss: 0.0140, Discrepancy Loss: 0.1426
Validation Loss: 0.0274
Epoch [18/50], Class Loss: 0.0092, Discrepancy Loss: 0.1536
Validation Loss: 0.0142
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 72.84%, Precision: 80.15%, Recall: 71.96%, F1 Score: 63.68%

Run 3/10
Epoch [1/50], Class Loss: 0.5165, Discrepancy Loss: 0.0263
Validation Loss: 1.6233
Epoch [2/50], Class Loss: 0.0761, Discrepancy Loss: 0.0112
Validation Loss: 0.0672
Epoch [3/50], Class Loss: 0.0922, Discrepancy Loss: 0.0257
Validation Loss: 0.1239
Epoch [4/50], Class Loss: 0.1141, Discrepancy Loss: 0.0479
Validation Loss: 0.1949
Epoch [5/50], Class Loss: 0.0656, Discrepancy Loss: 0.0530
Validation Loss: 1.2197
Epoch [6/50], Class Loss: 0.0553, Discrepancy Loss: 0.0615
Validation Loss: 0.9955
Epoch [7/50], Class Loss: 0.0621, Discrepancy Loss: 0.0389
Validation Loss: 0.1365
Early stopping!
Source Domain Performance - Accuracy: 98.74%, Precision: 98.73%, Recall: 98.73%, F1 Score: 98.72%
Target Domain Performance - Accuracy: 93.23%, Precision: 94.08%, Recall: 93.06%, F1 Score: 92.91%

Run 4/10
Epoch [1/50], Class Loss: 0.5165, Discrepancy Loss: 0.0291
Validation Loss: 5.0116
Epoch [2/50], Class Loss: 0.0811, Discrepancy Loss: 0.0175
Validation Loss: 4.8409
Epoch [3/50], Class Loss: 0.0484, Discrepancy Loss: 0.0156
Validation Loss: 2.0075
Epoch [4/50], Class Loss: 0.0671, Discrepancy Loss: 0.0227
Validation Loss: 0.0488
Epoch [5/50], Class Loss: 0.0302, Discrepancy Loss: 0.0151
Validation Loss: 4.0210
Epoch [6/50], Class Loss: 0.0883, Discrepancy Loss: 0.0557
Validation Loss: 0.2687
Epoch [7/50], Class Loss: 0.1030, Discrepancy Loss: 0.0955
Validation Loss: 0.0843
Epoch [8/50], Class Loss: 0.0527, Discrepancy Loss: 0.0424
Validation Loss: 6.4790
Epoch [9/50], Class Loss: 0.0526, Discrepancy Loss: 0.0312
Validation Loss: 6.5929
Early stopping!
Source Domain Performance - Accuracy: 66.91%, Precision: 53.89%, Recall: 66.52%, F1 Score: 58.92%
Target Domain Performance - Accuracy: 32.25%, Precision: 40.52%, Recall: 28.43%, F1 Score: 22.10%

Run 5/10
Epoch [1/50], Class Loss: 0.7506, Discrepancy Loss: 0.0337
Validation Loss: 5.3136
Epoch [2/50], Class Loss: 0.0685, Discrepancy Loss: 0.0305
Validation Loss: 0.0509
Epoch [3/50], Class Loss: 0.0501, Discrepancy Loss: 0.0334
Validation Loss: 0.0574
Epoch [4/50], Class Loss: 0.0417, Discrepancy Loss: 0.0379
Validation Loss: 1.0467
Epoch [5/50], Class Loss: 0.1291, Discrepancy Loss: 0.0393
Validation Loss: 0.3462
Epoch [6/50], Class Loss: 0.1635, Discrepancy Loss: 0.0334
Validation Loss: 0.0212
Epoch [7/50], Class Loss: 0.0151, Discrepancy Loss: 0.0310
Validation Loss: 0.0300
Epoch [8/50], Class Loss: 0.0223, Discrepancy Loss: 0.0307
Validation Loss: 0.1201
Epoch [9/50], Class Loss: 0.0186, Discrepancy Loss: 0.0315
Validation Loss: 2.1678
Epoch [10/50], Class Loss: 0.0802, Discrepancy Loss: 0.0851
Validation Loss: 5.0135
Epoch [11/50], Class Loss: 0.0039, Discrepancy Loss: 0.0142
Validation Loss: 0.0023
Epoch [12/50], Class Loss: 0.0011, Discrepancy Loss: 0.0066
Validation Loss: 0.0032
Epoch [13/50], Class Loss: 0.0012, Discrepancy Loss: 0.0117
Validation Loss: 0.0056
Epoch [14/50], Class Loss: 0.0091, Discrepancy Loss: 0.0495
Validation Loss: 0.0015
Epoch [15/50], Class Loss: 0.0069, Discrepancy Loss: 0.0582
Validation Loss: 0.0076
Epoch [16/50], Class Loss: 0.0069, Discrepancy Loss: 0.0248
Validation Loss: 0.0078
Epoch [17/50], Class Loss: 0.0014, Discrepancy Loss: 0.0169
Validation Loss: 0.0017
Epoch [18/50], Class Loss: 0.0005, Discrepancy Loss: 0.0192
Validation Loss: 0.0059
Epoch [19/50], Class Loss: 0.0030, Discrepancy Loss: 0.0298
Validation Loss: 0.0039
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 55.46%, Precision: 60.36%, Recall: 53.01%, F1 Score: 51.63%

Run 6/10
Epoch [1/50], Class Loss: 0.4371, Discrepancy Loss: 0.0262
Validation Loss: 0.8980
Epoch [2/50], Class Loss: 0.0736, Discrepancy Loss: 0.0196
Validation Loss: 0.0462
Epoch [3/50], Class Loss: 0.0468, Discrepancy Loss: 0.0184
Validation Loss: 0.0290
Epoch [4/50], Class Loss: 0.0192, Discrepancy Loss: 0.0173
Validation Loss: 0.5717
Epoch [5/50], Class Loss: 0.0830, Discrepancy Loss: 0.0249
Validation Loss: 2.1062
Epoch [6/50], Class Loss: 0.0679, Discrepancy Loss: 0.0308
Validation Loss: 0.1174
Epoch [7/50], Class Loss: 0.0264, Discrepancy Loss: 0.0215
Validation Loss: 1.2295
Epoch [8/50], Class Loss: 0.0607, Discrepancy Loss: 0.0176
Validation Loss: 3.5592
Early stopping!
Source Domain Performance - Accuracy: 79.14%, Precision: 87.44%, Recall: 80.05%, F1 Score: 76.52%
Target Domain Performance - Accuracy: 71.58%, Precision: 86.22%, Recall: 73.71%, F1 Score: 72.66%

Run 7/10
Epoch [1/50], Class Loss: 0.4874, Discrepancy Loss: 0.0235
Validation Loss: 1.0970
Epoch [2/50], Class Loss: 0.0902, Discrepancy Loss: 0.0171
Validation Loss: 0.9284
Epoch [3/50], Class Loss: 0.0596, Discrepancy Loss: 0.0176
Validation Loss: 0.0750
Epoch [4/50], Class Loss: 0.0615, Discrepancy Loss: 0.0246
Validation Loss: 1.6838
Epoch [5/50], Class Loss: 0.0466, Discrepancy Loss: 0.0362
Validation Loss: 5.7779
Epoch [6/50], Class Loss: 0.0782, Discrepancy Loss: 0.0599
Validation Loss: 0.3756
Epoch [7/50], Class Loss: 0.0343, Discrepancy Loss: 0.0390
Validation Loss: 4.1895
Epoch [8/50], Class Loss: 0.0392, Discrepancy Loss: 0.0382
Validation Loss: 0.0397
Epoch [9/50], Class Loss: 0.0740, Discrepancy Loss: 0.0562
Validation Loss: 0.1893
Epoch [10/50], Class Loss: 0.0291, Discrepancy Loss: 0.0350
Validation Loss: 0.1086
Epoch [11/50], Class Loss: 0.0252, Discrepancy Loss: 0.0156
Validation Loss: 0.0163
Epoch [12/50], Class Loss: 0.0016, Discrepancy Loss: 0.0090
Validation Loss: 0.0104
Epoch [13/50], Class Loss: 0.0010, Discrepancy Loss: 0.0082
Validation Loss: 0.0132
Epoch [14/50], Class Loss: 0.0007, Discrepancy Loss: 0.0094
Validation Loss: 0.0114
Epoch [15/50], Class Loss: 0.0076, Discrepancy Loss: 0.0170
Validation Loss: 0.0147
Epoch [16/50], Class Loss: 0.0130, Discrepancy Loss: 0.0427
Validation Loss: 0.2624
Epoch [17/50], Class Loss: 0.0125, Discrepancy Loss: 0.0503
Validation Loss: 0.0954
Early stopping!
Source Domain Performance - Accuracy: 98.80%, Precision: 98.80%, Recall: 98.80%, F1 Score: 98.78%
Target Domain Performance - Accuracy: 45.08%, Precision: 55.29%, Recall: 42.29%, F1 Score: 35.03%

Run 8/10
Epoch [1/50], Class Loss: 0.4376, Discrepancy Loss: 0.0278
Validation Loss: 2.5321
Epoch [2/50], Class Loss: 0.0742, Discrepancy Loss: 0.0280
Validation Loss: 0.1144
Epoch [3/50], Class Loss: 0.0392, Discrepancy Loss: 0.0249
Validation Loss: 3.9072
Epoch [4/50], Class Loss: 0.0321, Discrepancy Loss: 0.0325
Validation Loss: 0.4287
Epoch [5/50], Class Loss: 0.0791, Discrepancy Loss: 0.0449
Validation Loss: 1.4828
Epoch [6/50], Class Loss: 0.0371, Discrepancy Loss: 0.0177
Validation Loss: 0.0702
Epoch [7/50], Class Loss: 0.0265, Discrepancy Loss: 0.0447
Validation Loss: 0.0369
Epoch [8/50], Class Loss: 0.0354, Discrepancy Loss: 0.0407
Validation Loss: 2.1556
Epoch [9/50], Class Loss: 0.0460, Discrepancy Loss: 0.0202
Validation Loss: 0.6088
Epoch [10/50], Class Loss: 0.1035, Discrepancy Loss: 0.0738
Validation Loss: 0.1739
Epoch [11/50], Class Loss: 0.0125, Discrepancy Loss: 0.0181
Validation Loss: 0.0673
Epoch [12/50], Class Loss: 0.0053, Discrepancy Loss: 0.0186
Validation Loss: 0.0027
Epoch [13/50], Class Loss: 0.0012, Discrepancy Loss: 0.0196
Validation Loss: 0.0029
Epoch [14/50], Class Loss: 0.0016, Discrepancy Loss: 0.0204
Validation Loss: 0.0079
Epoch [15/50], Class Loss: 0.0019, Discrepancy Loss: 0.0253
Validation Loss: 0.0045
Epoch [16/50], Class Loss: 0.0027, Discrepancy Loss: 0.0331
Validation Loss: 0.0112
Epoch [17/50], Class Loss: 0.0069, Discrepancy Loss: 0.0730
Validation Loss: 1.2880
Early stopping!
Source Domain Performance - Accuracy: 82.67%, Precision: 89.34%, Recall: 82.63%, F1 Score: 80.15%
Target Domain Performance - Accuracy: 48.86%, Precision: 56.32%, Recall: 46.07%, F1 Score: 41.03%

Run 9/10
Epoch [1/50], Class Loss: 0.5055, Discrepancy Loss: 0.0307
Validation Loss: 7.5831
Epoch [2/50], Class Loss: 0.0690, Discrepancy Loss: 0.0252
Validation Loss: 2.2982
Epoch [3/50], Class Loss: 0.0849, Discrepancy Loss: 0.0223
Validation Loss: 1.3473
Epoch [4/50], Class Loss: 0.0437, Discrepancy Loss: 0.0130
Validation Loss: 1.3057
Epoch [5/50], Class Loss: 0.0437, Discrepancy Loss: 0.0152
Validation Loss: 1.8756
Epoch [6/50], Class Loss: 0.0396, Discrepancy Loss: 0.0189
Validation Loss: 0.0189
Epoch [7/50], Class Loss: 0.0397, Discrepancy Loss: 0.0287
Validation Loss: 1.5233
Epoch [8/50], Class Loss: 0.0363, Discrepancy Loss: 0.0219
Validation Loss: 0.1541
Epoch [9/50], Class Loss: 0.0456, Discrepancy Loss: 0.0671
Validation Loss: 0.0737
Epoch [10/50], Class Loss: 0.0353, Discrepancy Loss: 0.0859
Validation Loss: 0.1544
Epoch [11/50], Class Loss: 0.0174, Discrepancy Loss: 0.0881
Validation Loss: 0.1577
Early stopping!
Source Domain Performance - Accuracy: 96.46%, Precision: 96.79%, Recall: 96.45%, F1 Score: 96.39%
Target Domain Performance - Accuracy: 57.07%, Precision: 77.17%, Recall: 55.04%, F1 Score: 50.03%

Run 10/10
Epoch [1/50], Class Loss: 0.5322, Discrepancy Loss: 0.0236
Validation Loss: 7.0483
Epoch [2/50], Class Loss: 0.0782, Discrepancy Loss: 0.0179
Validation Loss: 0.1458
Epoch [3/50], Class Loss: 0.0605, Discrepancy Loss: 0.0268
Validation Loss: 12.5813
Epoch [4/50], Class Loss: 0.0574, Discrepancy Loss: 0.0413
Validation Loss: 0.0317
Epoch [5/50], Class Loss: 0.0588, Discrepancy Loss: 0.0267
Validation Loss: 12.2670
Epoch [6/50], Class Loss: 0.0569, Discrepancy Loss: 0.0468
Validation Loss: 0.1781
Epoch [7/50], Class Loss: 0.0444, Discrepancy Loss: 0.0777
Validation Loss: 0.0774
Epoch [8/50], Class Loss: 0.0607, Discrepancy Loss: 0.0336
Validation Loss: 0.0171
Epoch [9/50], Class Loss: 0.0253, Discrepancy Loss: 0.0206
Validation Loss: 0.2050
Epoch [10/50], Class Loss: 0.0305, Discrepancy Loss: 0.0364
Validation Loss: 0.4254
Epoch [11/50], Class Loss: 0.0052, Discrepancy Loss: 0.0102
Validation Loss: 0.0047
Epoch [12/50], Class Loss: 0.0026, Discrepancy Loss: 0.0086
Validation Loss: 0.0066
Epoch [13/50], Class Loss: 0.0019, Discrepancy Loss: 0.0201
Validation Loss: 0.0035
Epoch [14/50], Class Loss: 0.0046, Discrepancy Loss: 0.0259
Validation Loss: 0.0076
Epoch [15/50], Class Loss: 0.0042, Discrepancy Loss: 0.0371
Validation Loss: 0.0105
Epoch [16/50], Class Loss: 0.0170, Discrepancy Loss: 0.0343
Validation Loss: 0.0054
Epoch [17/50], Class Loss: 0.0058, Discrepancy Loss: 0.0355
Validation Loss: 0.0309
Epoch [18/50], Class Loss: 0.0266, Discrepancy Loss: 0.0582
Validation Loss: 0.2237
Early stopping!
Source Domain Performance - Accuracy: 96.16%, Precision: 96.72%, Recall: 96.15%, F1 Score: 96.19%
Target Domain Performance - Accuracy: 48.32%, Precision: 71.93%, Recall: 45.57%, F1 Score: 37.73%

Source performance: 91.79% 92.07% 91.83% 90.46%
Target performance: 59.87% 70.44% 58.22% 53.64%

Per-Class Accuracy on Target Domain:
bpsk: 44.23%
qpsk: 26.65%
16qam: 68.29%
8apsk: 93.70%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.4515, JMMD Loss: 0.1370
Validation Loss: 0.0998
Epoch [2/50], Class Loss: 0.0578, JMMD Loss: 0.1069
Validation Loss: 3.2217
Epoch [3/50], Class Loss: 0.0423, JMMD Loss: 0.0953
Validation Loss: 0.1050
Epoch [4/50], Class Loss: 0.0364, JMMD Loss: 0.0829
Validation Loss: 0.0391
Epoch [5/50], Class Loss: 0.0396, JMMD Loss: 0.0773
Validation Loss: 0.1107
Epoch [6/50], Class Loss: 0.1112, JMMD Loss: 0.0921
Validation Loss: 0.0340
Epoch [7/50], Class Loss: 0.0189, JMMD Loss: 0.0821
Validation Loss: 0.0111
Epoch [8/50], Class Loss: 0.0072, JMMD Loss: 0.0715
Validation Loss: 0.0144
Epoch [9/50], Class Loss: 0.0080, JMMD Loss: 0.0580
Validation Loss: 0.0131
Epoch [10/50], Class Loss: 0.0047, JMMD Loss: 0.0564
Validation Loss: 0.0162
Epoch [11/50], Class Loss: 0.0034, JMMD Loss: 0.0551
Validation Loss: 0.0066
Epoch [12/50], Class Loss: 0.0028, JMMD Loss: 0.0546
Validation Loss: 0.0082
Epoch [13/50], Class Loss: 0.0024, JMMD Loss: 0.0482
Validation Loss: 0.0025
Epoch [14/50], Class Loss: 0.0020, JMMD Loss: 0.0499
Validation Loss: 0.0027
Epoch [15/50], Class Loss: 0.0030, JMMD Loss: 0.0565
Validation Loss: 0.0099
Epoch [16/50], Class Loss: 0.0023, JMMD Loss: 0.0622
Validation Loss: 0.0030
Epoch [17/50], Class Loss: 0.0044, JMMD Loss: 0.0485
Validation Loss: 0.0063
Epoch [18/50], Class Loss: 0.0059, JMMD Loss: 0.0470
Validation Loss: 0.0089
Early stopping!
Source Domain Performance - Accuracy: 99.64%, Precision: 99.64%, Recall: 99.63%, F1 Score: 99.63%
Target Domain Performance - Accuracy: 98.44%, Precision: 98.43%, Recall: 98.41%, F1 Score: 98.39%

Run 2/10
Epoch [1/50], Class Loss: 0.3938, JMMD Loss: 0.1170
Validation Loss: 1.9157
Epoch [2/50], Class Loss: 0.1060, JMMD Loss: 0.1165
Validation Loss: 0.0277
Epoch [3/50], Class Loss: 0.0702, JMMD Loss: 0.0949
Validation Loss: 4.7212
Epoch [4/50], Class Loss: 0.0518, JMMD Loss: 0.1069
Validation Loss: 4.1512
Epoch [5/50], Class Loss: 0.1475, JMMD Loss: 0.0902
Validation Loss: 0.2988
Epoch [6/50], Class Loss: 0.1417, JMMD Loss: 0.0781
Validation Loss: 0.0653
Epoch [7/50], Class Loss: 0.0319, JMMD Loss: 0.0964
Validation Loss: 0.0269
Epoch [8/50], Class Loss: 0.0218, JMMD Loss: 0.1019
Validation Loss: 0.0141
Epoch [9/50], Class Loss: 0.0318, JMMD Loss: 0.0882
Validation Loss: 0.0601
Epoch [10/50], Class Loss: 0.0239, JMMD Loss: 0.0760
Validation Loss: 0.0264
Epoch [11/50], Class Loss: 0.0177, JMMD Loss: 0.0852
Validation Loss: 0.0160
Epoch [12/50], Class Loss: 0.0134, JMMD Loss: 0.0836
Validation Loss: 0.0193
Epoch [13/50], Class Loss: 0.0119, JMMD Loss: 0.0906
Validation Loss: 0.0140
Epoch [14/50], Class Loss: 0.0145, JMMD Loss: 0.0755
Validation Loss: 0.0148
Epoch [15/50], Class Loss: 0.0187, JMMD Loss: 0.0769
Validation Loss: 0.0258
Epoch [16/50], Class Loss: 0.0091, JMMD Loss: 0.0811
Validation Loss: 0.0131
Epoch [17/50], Class Loss: 0.0093, JMMD Loss: 0.0753
Validation Loss: 0.0168
Epoch [18/50], Class Loss: 0.0077, JMMD Loss: 0.0803
Validation Loss: 0.0120
Epoch [19/50], Class Loss: 0.0324, JMMD Loss: 0.0805
Validation Loss: 0.0127
Epoch [20/50], Class Loss: 0.0217, JMMD Loss: 0.0788
Validation Loss: 0.0093
Epoch [21/50], Class Loss: 0.0126, JMMD Loss: 0.0746
Validation Loss: 0.0107
Epoch [22/50], Class Loss: 0.0089, JMMD Loss: 0.0753
Validation Loss: 0.0157
Epoch [23/50], Class Loss: 0.0129, JMMD Loss: 0.0742
Validation Loss: 0.0077
Epoch [24/50], Class Loss: 0.0900, JMMD Loss: 0.0759
Validation Loss: 0.0407
Epoch [25/50], Class Loss: 0.0284, JMMD Loss: 0.0739
Validation Loss: 0.0100
Epoch [26/50], Class Loss: 0.0147, JMMD Loss: 0.0704
Validation Loss: 0.0198
Epoch [27/50], Class Loss: 0.0066, JMMD Loss: 0.0740
Validation Loss: 0.0086
Epoch [28/50], Class Loss: 0.0088, JMMD Loss: 0.0744
Validation Loss: 0.0051
Epoch [29/50], Class Loss: 0.0068, JMMD Loss: 0.0665
Validation Loss: 0.0069
Epoch [30/50], Class Loss: 0.0069, JMMD Loss: 0.0670
Validation Loss: 0.0053
Epoch [31/50], Class Loss: 0.0220, JMMD Loss: 0.0720
Validation Loss: 0.0238
Epoch [32/50], Class Loss: 0.0063, JMMD Loss: 0.0775
Validation Loss: 0.0107
Epoch [33/50], Class Loss: 0.0068, JMMD Loss: 0.0672
Validation Loss: 0.0139
Early stopping!
Source Domain Performance - Accuracy: 99.64%, Precision: 99.64%, Recall: 99.63%, F1 Score: 99.63%
Target Domain Performance - Accuracy: 97.12%, Precision: 97.22%, Recall: 97.10%, F1 Score: 97.04%

Run 3/10
Epoch [1/50], Class Loss: 0.3369, JMMD Loss: 0.1435
Validation Loss: 0.5074
Epoch [2/50], Class Loss: 0.0509, JMMD Loss: 0.0920
Validation Loss: 1.3984
Epoch [3/50], Class Loss: 0.0569, JMMD Loss: 0.1003
Validation Loss: 0.2137
Epoch [4/50], Class Loss: 0.0742, JMMD Loss: 0.0859
Validation Loss: 1.6472
Epoch [5/50], Class Loss: 0.1258, JMMD Loss: 0.1103
Validation Loss: 0.0195
Epoch [6/50], Class Loss: 0.0465, JMMD Loss: 0.0941
Validation Loss: 0.0233
Epoch [7/50], Class Loss: 0.1189, JMMD Loss: 0.1109
Validation Loss: 2.5694
Epoch [8/50], Class Loss: 0.0934, JMMD Loss: 0.1038
Validation Loss: 0.0325
Epoch [9/50], Class Loss: 0.0137, JMMD Loss: 0.0906
Validation Loss: 0.1222
Epoch [10/50], Class Loss: 0.0344, JMMD Loss: 0.0854
Validation Loss: 0.1475
Early stopping!
Source Domain Performance - Accuracy: 93.17%, Precision: 93.47%, Recall: 93.25%, F1 Score: 93.12%
Target Domain Performance - Accuracy: 93.17%, Precision: 94.26%, Recall: 93.72%, F1 Score: 93.42%

Run 4/10
Epoch [1/50], Class Loss: 0.2844, JMMD Loss: 0.1045
Validation Loss: 0.0770
Epoch [2/50], Class Loss: 0.0531, JMMD Loss: 0.1197
Validation Loss: 0.0155
Epoch [3/50], Class Loss: 0.0403, JMMD Loss: 0.0863
Validation Loss: 0.0495
Epoch [4/50], Class Loss: 0.2128, JMMD Loss: 0.0936
Validation Loss: 2.8469
Epoch [5/50], Class Loss: 0.0972, JMMD Loss: 0.0971
Validation Loss: 0.3710
Epoch [6/50], Class Loss: 0.0316, JMMD Loss: 0.1076
Validation Loss: 0.0408
Epoch [7/50], Class Loss: 0.0214, JMMD Loss: 0.0884
Validation Loss: 0.0127
Epoch [8/50], Class Loss: 0.0441, JMMD Loss: 0.0917
Validation Loss: 0.1554
Epoch [9/50], Class Loss: 0.0209, JMMD Loss: 0.0861
Validation Loss: 0.4257
Epoch [10/50], Class Loss: 0.0694, JMMD Loss: 0.0916
Validation Loss: 0.3323
Epoch [11/50], Class Loss: 0.0271, JMMD Loss: 0.0968
Validation Loss: 0.0392
Epoch [12/50], Class Loss: 0.1036, JMMD Loss: 0.0908
Validation Loss: 0.1564
Early stopping!
Source Domain Performance - Accuracy: 96.10%, Precision: 96.60%, Recall: 95.95%, F1 Score: 95.97%
Target Domain Performance - Accuracy: 96.40%, Precision: 96.56%, Recall: 96.25%, F1 Score: 96.34%

Run 5/10
Epoch [1/50], Class Loss: 0.3604, JMMD Loss: 0.0942
Validation Loss: 3.6554
Epoch [2/50], Class Loss: 0.0535, JMMD Loss: 0.0996
Validation Loss: 0.2821
Epoch [3/50], Class Loss: 0.0595, JMMD Loss: 0.0876
Validation Loss: 2.3416
Epoch [4/50], Class Loss: 0.0331, JMMD Loss: 0.0775
Validation Loss: 1.4882
Epoch [5/50], Class Loss: 0.1940, JMMD Loss: 0.0860
Validation Loss: 0.0288
Epoch [6/50], Class Loss: 0.0397, JMMD Loss: 0.0869
Validation Loss: 0.0846
Epoch [7/50], Class Loss: 0.0191, JMMD Loss: 0.0762
Validation Loss: 0.0087
Epoch [8/50], Class Loss: 0.0180, JMMD Loss: 0.0731
Validation Loss: 0.0108
Epoch [9/50], Class Loss: 0.0060, JMMD Loss: 0.0695
Validation Loss: 0.0059
Epoch [10/50], Class Loss: 0.0107, JMMD Loss: 0.0603
Validation Loss: 1.1169
Epoch [11/50], Class Loss: 0.0151, JMMD Loss: 0.0668
Validation Loss: 0.0113
Epoch [12/50], Class Loss: 0.0100, JMMD Loss: 0.0623
Validation Loss: 0.0112
Epoch [13/50], Class Loss: 0.0050, JMMD Loss: 0.0606
Validation Loss: 0.0063
Epoch [14/50], Class Loss: 0.0046, JMMD Loss: 0.0568
Validation Loss: 0.0039
Epoch [15/50], Class Loss: 0.0164, JMMD Loss: 0.0596
Validation Loss: 0.0050
Epoch [16/50], Class Loss: 0.0131, JMMD Loss: 0.0604
Validation Loss: 0.0055
Epoch [17/50], Class Loss: 0.0042, JMMD Loss: 0.0604
Validation Loss: 0.0026
Epoch [18/50], Class Loss: 0.0047, JMMD Loss: 0.0652
Validation Loss: 0.0094
Epoch [19/50], Class Loss: 0.0038, JMMD Loss: 0.0648
Validation Loss: 0.0027
Epoch [20/50], Class Loss: 0.0033, JMMD Loss: 0.0533
Validation Loss: 0.0040
Epoch [21/50], Class Loss: 0.0027, JMMD Loss: 0.0561
Validation Loss: 0.0140
Epoch [22/50], Class Loss: 0.0023, JMMD Loss: 0.0556
Validation Loss: 0.0049
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 96.94%, Precision: 97.05%, Recall: 96.85%, F1 Score: 96.81%

Run 6/10
Epoch [1/50], Class Loss: 0.3474, JMMD Loss: 0.1292
Validation Loss: 0.1311
Epoch [2/50], Class Loss: 0.0441, JMMD Loss: 0.0989
Validation Loss: 3.0996
Epoch [3/50], Class Loss: 0.0568, JMMD Loss: 0.1023
Validation Loss: 1.0006
Epoch [4/50], Class Loss: 0.0208, JMMD Loss: 0.0982
Validation Loss: 0.1947
Epoch [5/50], Class Loss: 0.0931, JMMD Loss: 0.0963
Validation Loss: 0.0376
Epoch [6/50], Class Loss: 0.0334, JMMD Loss: 0.1197
Validation Loss: 0.0320
Epoch [7/50], Class Loss: 0.0141, JMMD Loss: 0.0853
Validation Loss: 0.0916
Epoch [8/50], Class Loss: 0.0236, JMMD Loss: 0.0835
Validation Loss: 0.0259
Epoch [9/50], Class Loss: 0.0525, JMMD Loss: 0.0793
Validation Loss: 0.1397
Epoch [10/50], Class Loss: 0.0922, JMMD Loss: 0.0916
Validation Loss: 0.1829
Epoch [11/50], Class Loss: 0.0334, JMMD Loss: 0.0862
Validation Loss: 0.0943
Epoch [12/50], Class Loss: 0.0101, JMMD Loss: 0.0950
Validation Loss: 0.1094
Epoch [13/50], Class Loss: 0.0097, JMMD Loss: 0.0858
Validation Loss: 0.0320
Early stopping!
Source Domain Performance - Accuracy: 99.04%, Precision: 99.03%, Recall: 99.02%, F1 Score: 99.02%
Target Domain Performance - Accuracy: 94.72%, Precision: 95.07%, Recall: 94.55%, F1 Score: 94.61%

Run 7/10
Epoch [1/50], Class Loss: 0.3845, JMMD Loss: 0.1060
Validation Loss: 3.6242
Epoch [2/50], Class Loss: 0.0983, JMMD Loss: 0.1048
Validation Loss: 0.2231
Epoch [3/50], Class Loss: 0.0780, JMMD Loss: 0.1013
Validation Loss: 0.0159
Epoch [4/50], Class Loss: 0.0169, JMMD Loss: 0.0883
Validation Loss: 0.9504
Epoch [5/50], Class Loss: 0.0488, JMMD Loss: 0.0894
Validation Loss: 0.1102
Epoch [6/50], Class Loss: 0.0519, JMMD Loss: 0.0828
Validation Loss: 2.4810
Epoch [7/50], Class Loss: 0.1632, JMMD Loss: 0.1223
Validation Loss: 0.0220
Epoch [8/50], Class Loss: 0.0413, JMMD Loss: 0.1159
Validation Loss: 0.3193
Early stopping!
Source Domain Performance - Accuracy: 88.49%, Precision: 89.94%, Recall: 88.74%, F1 Score: 88.40%
Target Domain Performance - Accuracy: 88.61%, Precision: 91.75%, Recall: 89.77%, F1 Score: 89.19%

Run 8/10
Epoch [1/50], Class Loss: 0.2919, JMMD Loss: 0.1138
Validation Loss: 0.1609
Epoch [2/50], Class Loss: 0.0427, JMMD Loss: 0.0943
Validation Loss: 0.2422
Epoch [3/50], Class Loss: 0.0998, JMMD Loss: 0.0891
Validation Loss: 3.7593
Epoch [4/50], Class Loss: 0.1858, JMMD Loss: 0.0809
Validation Loss: 0.0343
Epoch [5/50], Class Loss: 0.0428, JMMD Loss: 0.0828
Validation Loss: 0.0318
Epoch [6/50], Class Loss: 0.0165, JMMD Loss: 0.0774
Validation Loss: 0.0467
Epoch [7/50], Class Loss: 0.0189, JMMD Loss: 0.0648
Validation Loss: 0.0107
Epoch [8/50], Class Loss: 0.0223, JMMD Loss: 0.0790
Validation Loss: 0.0980
Epoch [9/50], Class Loss: 0.2104, JMMD Loss: 0.1109
Validation Loss: 0.6170
Epoch [10/50], Class Loss: 0.0527, JMMD Loss: 0.1059
Validation Loss: 1.8427
Epoch [11/50], Class Loss: 0.0279, JMMD Loss: 0.0950
Validation Loss: 0.1210
Epoch [12/50], Class Loss: 0.0174, JMMD Loss: 0.1045
Validation Loss: 0.0438
Early stopping!
Source Domain Performance - Accuracy: 98.44%, Precision: 98.43%, Recall: 98.47%, F1 Score: 98.44%
Target Domain Performance - Accuracy: 89.57%, Precision: 91.82%, Recall: 89.38%, F1 Score: 89.06%

Run 9/10
Epoch [1/50], Class Loss: 0.3342, JMMD Loss: 0.1133
Validation Loss: 4.7081
Epoch [2/50], Class Loss: 0.0562, JMMD Loss: 0.1226
Validation Loss: 0.4946
Epoch [3/50], Class Loss: 0.0353, JMMD Loss: 0.1054
Validation Loss: 2.0715
Epoch [4/50], Class Loss: 0.1255, JMMD Loss: 0.0940
Validation Loss: 1.2293
Epoch [5/50], Class Loss: 0.0172, JMMD Loss: 0.1012
Validation Loss: 0.0632
Epoch [6/50], Class Loss: 0.0311, JMMD Loss: 0.0853
Validation Loss: 0.0041
Epoch [7/50], Class Loss: 0.1525, JMMD Loss: 0.1017
Validation Loss: 0.0611
Epoch [8/50], Class Loss: 0.0114, JMMD Loss: 0.1021
Validation Loss: 0.0491
Epoch [9/50], Class Loss: 0.0079, JMMD Loss: 0.0842
Validation Loss: 0.6715
Epoch [10/50], Class Loss: 0.0089, JMMD Loss: 0.0812
Validation Loss: 0.0039
Epoch [11/50], Class Loss: 0.0062, JMMD Loss: 0.0724
Validation Loss: 0.0040
Epoch [12/50], Class Loss: 0.0035, JMMD Loss: 0.0676
Validation Loss: 0.0131
Epoch [13/50], Class Loss: 0.0116, JMMD Loss: 0.0657
Validation Loss: 0.0164
Epoch [14/50], Class Loss: 0.0231, JMMD Loss: 0.0589
Validation Loss: 0.0136
Epoch [15/50], Class Loss: 0.0102, JMMD Loss: 0.0597
Validation Loss: 0.0097
Early stopping!
Source Domain Performance - Accuracy: 99.64%, Precision: 99.64%, Recall: 99.63%, F1 Score: 99.63%
Target Domain Performance - Accuracy: 96.10%, Precision: 96.35%, Recall: 95.98%, F1 Score: 95.91%

Run 10/10
Epoch [1/50], Class Loss: 0.4421, JMMD Loss: 0.1503
Validation Loss: 0.3136
Epoch [2/50], Class Loss: 0.1006, JMMD Loss: 0.1149
Validation Loss: 0.1595
Epoch [3/50], Class Loss: 0.0498, JMMD Loss: 0.1044
Validation Loss: 1.5118
Epoch [4/50], Class Loss: 0.0852, JMMD Loss: 0.1057
Validation Loss: 0.3500
Epoch [5/50], Class Loss: 0.0363, JMMD Loss: 0.0933
Validation Loss: 0.0177
Epoch [6/50], Class Loss: 0.0288, JMMD Loss: 0.0815
Validation Loss: 1.6763
Epoch [7/50], Class Loss: 0.0748, JMMD Loss: 0.0736
Validation Loss: 0.7210
Epoch [8/50], Class Loss: 0.0247, JMMD Loss: 0.0690
Validation Loss: 0.5804
Epoch [9/50], Class Loss: 0.0961, JMMD Loss: 0.0879
Validation Loss: 0.0928
Epoch [10/50], Class Loss: 0.1881, JMMD Loss: 0.0852
Validation Loss: 0.0467
Early stopping!
Source Domain Performance - Accuracy: 98.92%, Precision: 98.91%, Recall: 98.91%, F1 Score: 98.91%
Target Domain Performance - Accuracy: 83.39%, Precision: 86.74%, Recall: 82.77%, F1 Score: 82.77%

Source performance: 97.30% 97.52% 97.31% 97.26%
Target performance: 93.45% 94.53% 93.48% 93.35%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 99.39%
  Class 1: 83.84%
  Class 2: 96.14%
  Class 3: 94.55%
Base

Run 1/10
Epoch 1/50, Train Loss: 0.3659, Train Acc: 0.8775, Val Loss: 2.7128, Val Acc: 0.2974
Epoch 2/50, Train Loss: 0.0469, Train Acc: 0.9858, Val Loss: 0.4157, Val Acc: 0.8729
Epoch 3/50, Train Loss: 0.0256, Train Acc: 0.9946, Val Loss: 0.0890, Val Acc: 0.9652
Epoch 4/50, Train Loss: 0.0227, Train Acc: 0.9937, Val Loss: 0.0252, Val Acc: 0.9952
Epoch 5/50, Train Loss: 0.0093, Train Acc: 0.9975, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 6/50, Train Loss: 0.0114, Train Acc: 0.9975, Val Loss: 0.3119, Val Acc: 0.9029
Epoch 7/50, Train Loss: 0.0128, Train Acc: 0.9972, Val Loss: 0.0078, Val Acc: 0.9982
Epoch 8/50, Train Loss: 0.0060, Train Acc: 0.9985, Val Loss: 0.2956, Val Acc: 0.9065
Epoch 9/50, Train Loss: 0.0062, Train Acc: 0.9982, Val Loss: 0.8775, Val Acc: 0.7554
Epoch 10/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0049, Val Acc: 0.9988
Early stopping!

Run 2/10
Epoch 1/50, Train Loss: 0.3206, Train Acc: 0.8956, Val Loss: 1.1140, Val Acc: 0.7146
Epoch 2/50, Train Loss: 0.0360, Train Acc: 0.9898, Val Loss: 0.0912, Val Acc: 0.9604
Epoch 3/50, Train Loss: 0.0305, Train Acc: 0.9907, Val Loss: 0.0273, Val Acc: 0.9928
Epoch 4/50, Train Loss: 0.0127, Train Acc: 0.9963, Val Loss: 0.3239, Val Acc: 0.9089
Epoch 5/50, Train Loss: 0.0177, Train Acc: 0.9943, Val Loss: 0.0603, Val Acc: 0.9790
Epoch 6/50, Train Loss: 0.0136, Train Acc: 0.9960, Val Loss: 0.1002, Val Acc: 0.9538
Epoch 7/50, Train Loss: 0.0157, Train Acc: 0.9951, Val Loss: 1.4074, Val Acc: 0.6109
Epoch 8/50, Train Loss: 0.0097, Train Acc: 0.9970, Val Loss: 0.0243, Val Acc: 0.9904
Epoch 9/50, Train Loss: 0.0064, Train Acc: 0.9982, Val Loss: 0.0159, Val Acc: 0.9964
Epoch 10/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0019, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0012, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0025, Train Acc: 0.9996, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0023, Train Acc: 0.9996, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0010, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0048, Train Acc: 0.9997, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 0.9994
Early stopping!

Run 3/10
Epoch 1/50, Train Loss: 0.3100, Train Acc: 0.8911, Val Loss: 2.6241, Val Acc: 0.5072
Epoch 2/50, Train Loss: 0.0304, Train Acc: 0.9937, Val Loss: 0.6227, Val Acc: 0.8195
Epoch 3/50, Train Loss: 0.0398, Train Acc: 0.9886, Val Loss: 3.2402, Val Acc: 0.5312
Epoch 4/50, Train Loss: 0.0226, Train Acc: 0.9943, Val Loss: 0.1732, Val Acc: 0.9365
Epoch 5/50, Train Loss: 0.0174, Train Acc: 0.9961, Val Loss: 0.0668, Val Acc: 0.9904
Epoch 6/50, Train Loss: 0.0197, Train Acc: 0.9942, Val Loss: 2.8710, Val Acc: 0.6685
Epoch 7/50, Train Loss: 0.0344, Train Acc: 0.9912, Val Loss: 0.0271, Val Acc: 0.9904
Epoch 8/50, Train Loss: 0.0105, Train Acc: 0.9970, Val Loss: 0.0087, Val Acc: 0.9964
Epoch 9/50, Train Loss: 0.0056, Train Acc: 0.9979, Val Loss: 0.5369, Val Acc: 0.8369
Epoch 10/50, Train Loss: 0.0121, Train Acc: 0.9961, Val Loss: 0.4507, Val Acc: 0.8201
Epoch 11/50, Train Loss: 0.0042, Train Acc: 0.9988, Val Loss: 0.0031, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0030, Train Acc: 0.9997, Val Loss: 0.0056, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0035, Train Acc: 0.9990, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0026, Train Acc: 0.9993, Val Loss: 0.0014, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0031, Train Acc: 0.9990, Val Loss: 0.0047, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0017, Train Acc: 0.9996, Val Loss: 0.4677, Val Acc: 0.8010
Epoch 19/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0032, Val Acc: 0.9982
Epoch 20/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0013, Val Acc: 0.9988
Epoch 22/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0016, Val Acc: 0.9988
Epoch 24/50, Train Loss: 0.0024, Train Acc: 0.9997, Val Loss: 0.0043, Val Acc: 0.9982
Epoch 25/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 26/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0008, Train Acc: 0.9997, Val Loss: 0.0014, Val Acc: 0.9988
Early stopping!

Run 4/10
Epoch 1/50, Train Loss: 0.3395, Train Acc: 0.8806, Val Loss: 3.7994, Val Acc: 0.4892
Epoch 2/50, Train Loss: 0.0379, Train Acc: 0.9909, Val Loss: 0.0148, Val Acc: 0.9964
Epoch 3/50, Train Loss: 0.0413, Train Acc: 0.9889, Val Loss: 0.2030, Val Acc: 0.9448
Epoch 4/50, Train Loss: 0.0187, Train Acc: 0.9937, Val Loss: 0.0055, Val Acc: 0.9994
Epoch 5/50, Train Loss: 0.0115, Train Acc: 0.9969, Val Loss: 0.0112, Val Acc: 0.9970
Epoch 6/50, Train Loss: 0.0145, Train Acc: 0.9958, Val Loss: 2.1890, Val Acc: 0.6679
Epoch 7/50, Train Loss: 0.0218, Train Acc: 0.9936, Val Loss: 0.0263, Val Acc: 0.9958
Epoch 8/50, Train Loss: 0.0122, Train Acc: 0.9966, Val Loss: 0.1105, Val Acc: 0.9634
Epoch 9/50, Train Loss: 0.0198, Train Acc: 0.9934, Val Loss: 0.0127, Val Acc: 0.9964
Early stopping!

Run 5/10
Epoch 1/50, Train Loss: 0.3441, Train Acc: 0.8860, Val Loss: 0.4611, Val Acc: 0.8483
Epoch 2/50, Train Loss: 0.0446, Train Acc: 0.9892, Val Loss: 0.7156, Val Acc: 0.8052
Epoch 3/50, Train Loss: 0.0251, Train Acc: 0.9933, Val Loss: 0.2547, Val Acc: 0.8993
Epoch 4/50, Train Loss: 0.0264, Train Acc: 0.9918, Val Loss: 0.1242, Val Acc: 0.9550
Epoch 5/50, Train Loss: 0.0285, Train Acc: 0.9927, Val Loss: 2.0638, Val Acc: 0.5767
Epoch 6/50, Train Loss: 0.0128, Train Acc: 0.9958, Val Loss: 0.0188, Val Acc: 0.9934
Epoch 7/50, Train Loss: 0.0066, Train Acc: 0.9984, Val Loss: 0.0337, Val Acc: 0.9898
Epoch 8/50, Train Loss: 0.0097, Train Acc: 0.9969, Val Loss: 1.8307, Val Acc: 0.7446
Epoch 9/50, Train Loss: 0.0052, Train Acc: 0.9996, Val Loss: 2.4665, Val Acc: 0.6001
Epoch 10/50, Train Loss: 0.0138, Train Acc: 0.9957, Val Loss: 0.2745, Val Acc: 0.9227
Epoch 11/50, Train Loss: 0.0115, Train Acc: 0.9970, Val Loss: 0.0080, Val Acc: 0.9964
Epoch 12/50, Train Loss: 0.0041, Train Acc: 0.9990, Val Loss: 0.0043, Val Acc: 0.9982
Epoch 13/50, Train Loss: 0.0036, Train Acc: 0.9991, Val Loss: 0.0057, Val Acc: 0.9976
Epoch 14/50, Train Loss: 0.0061, Train Acc: 0.9990, Val Loss: 0.0023, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0018, Train Acc: 0.9999, Val Loss: 0.0022, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0023, Train Acc: 0.9993, Val Loss: 0.0030, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0021, Train Acc: 0.9997, Val Loss: 0.0014, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0018, Train Acc: 0.9999, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 19/50, Train Loss: 0.0021, Train Acc: 0.9997, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0015, Train Acc: 0.9996, Val Loss: 0.0013, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0021, Val Acc: 0.9988
Epoch 22/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0022, Val Acc: 0.9988
Epoch 23/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0022, Val Acc: 0.9988
Epoch 24/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 26/50, Train Loss: 0.0018, Train Acc: 0.9994, Val Loss: 0.0012, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0041, Train Acc: 0.9994, Val Loss: 0.0022, Val Acc: 0.9988
Epoch 29/50, Train Loss: 0.0016, Train Acc: 0.9994, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0015, Train Acc: 0.9996, Val Loss: 0.0025, Val Acc: 0.9994
Epoch 32/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 33/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 34/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0015, Val Acc: 1.0000
Epoch 35/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 36/50, Train Loss: 0.0008, Train Acc: 0.9997, Val Loss: 0.0022, Val Acc: 0.9994
Epoch 37/50, Train Loss: 0.0023, Train Acc: 0.9993, Val Loss: 0.0009, Val Acc: 1.0000
Early stopping!

Run 6/10
Epoch 1/50, Train Loss: 0.3862, Train Acc: 0.8649, Val Loss: 2.6517, Val Acc: 0.4910
Epoch 2/50, Train Loss: 0.0395, Train Acc: 0.9888, Val Loss: 0.2936, Val Acc: 0.8987
Epoch 3/50, Train Loss: 0.0247, Train Acc: 0.9931, Val Loss: 0.7475, Val Acc: 0.7764
Epoch 4/50, Train Loss: 0.0268, Train Acc: 0.9928, Val Loss: 2.3762, Val Acc: 0.6049
Epoch 5/50, Train Loss: 0.0130, Train Acc: 0.9967, Val Loss: 0.0290, Val Acc: 0.9934
Epoch 6/50, Train Loss: 0.0168, Train Acc: 0.9948, Val Loss: 2.2516, Val Acc: 0.7368
Epoch 7/50, Train Loss: 0.0047, Train Acc: 0.9987, Val Loss: 0.8284, Val Acc: 0.7548
Epoch 8/50, Train Loss: 0.0125, Train Acc: 0.9966, Val Loss: 0.1759, Val Acc: 0.9418
Epoch 9/50, Train Loss: 0.0182, Train Acc: 0.9948, Val Loss: 1.1377, Val Acc: 0.6841
Epoch 10/50, Train Loss: 0.0109, Train Acc: 0.9979, Val Loss: 0.0405, Val Acc: 0.9880
Early stopping!

Run 7/10
Epoch 1/50, Train Loss: 0.3384, Train Acc: 0.8845, Val Loss: 1.0093, Val Acc: 0.7398
Epoch 2/50, Train Loss: 0.0473, Train Acc: 0.9864, Val Loss: 3.3561, Val Acc: 0.7296
Epoch 3/50, Train Loss: 0.0349, Train Acc: 0.9898, Val Loss: 0.2236, Val Acc: 0.9191
Epoch 4/50, Train Loss: 0.0171, Train Acc: 0.9957, Val Loss: 0.1076, Val Acc: 0.9634
Epoch 5/50, Train Loss: 0.0167, Train Acc: 0.9949, Val Loss: 0.0146, Val Acc: 0.9958
Epoch 6/50, Train Loss: 0.0958, Train Acc: 0.9822, Val Loss: 0.2282, Val Acc: 0.9299
Epoch 7/50, Train Loss: 0.0150, Train Acc: 0.9960, Val Loss: 0.0180, Val Acc: 0.9952
Epoch 8/50, Train Loss: 0.0124, Train Acc: 0.9970, Val Loss: 0.0267, Val Acc: 0.9916
Epoch 9/50, Train Loss: 0.0198, Train Acc: 0.9942, Val Loss: 0.0093, Val Acc: 0.9958
Epoch 10/50, Train Loss: 0.0049, Train Acc: 0.9985, Val Loss: 0.0129, Val Acc: 0.9970
Epoch 11/50, Train Loss: 0.0027, Train Acc: 0.9994, Val Loss: 0.0027, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0039, Train Acc: 0.9991, Val Loss: 0.0024, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0028, Train Acc: 0.9996, Val Loss: 0.0026, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0022, Train Acc: 0.9996, Val Loss: 0.0025, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0042, Train Acc: 0.9991, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0023, Train Acc: 0.9996, Val Loss: 0.0022, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0026, Train Acc: 0.9993, Val Loss: 0.0026, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0017, Train Acc: 0.9994, Val Loss: 0.0030, Val Acc: 0.9988
Epoch 19/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0026, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9988
Early stopping!

Run 8/10
Epoch 1/50, Train Loss: 0.3614, Train Acc: 0.8796, Val Loss: 1.2211, Val Acc: 0.6307
Epoch 2/50, Train Loss: 0.0457, Train Acc: 0.9874, Val Loss: 0.2305, Val Acc: 0.9059
Epoch 3/50, Train Loss: 0.0348, Train Acc: 0.9907, Val Loss: 0.1041, Val Acc: 0.9694
Epoch 4/50, Train Loss: 0.0222, Train Acc: 0.9930, Val Loss: 0.0821, Val Acc: 0.9748
Epoch 5/50, Train Loss: 0.0245, Train Acc: 0.9922, Val Loss: 0.1194, Val Acc: 0.9574
Epoch 6/50, Train Loss: 0.0123, Train Acc: 0.9960, Val Loss: 1.0945, Val Acc: 0.7506
Epoch 7/50, Train Loss: 0.0153, Train Acc: 0.9955, Val Loss: 0.5194, Val Acc: 0.8687
Epoch 8/50, Train Loss: 0.0106, Train Acc: 0.9978, Val Loss: 0.3290, Val Acc: 0.8951
Epoch 9/50, Train Loss: 0.0084, Train Acc: 0.9982, Val Loss: 0.0120, Val Acc: 0.9976
Epoch 10/50, Train Loss: 0.0039, Train Acc: 0.9993, Val Loss: 0.3386, Val Acc: 0.8975
Epoch 11/50, Train Loss: 0.0029, Train Acc: 0.9993, Val Loss: 0.0025, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0066, Train Acc: 0.9993, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0018, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0016, Train Acc: 0.9996, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0044, Train Acc: 0.9993, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0050, Train Acc: 0.9994, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0025, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 9/10
Epoch 1/50, Train Loss: 0.4015, Train Acc: 0.8616, Val Loss: 0.6536, Val Acc: 0.8118
Epoch 2/50, Train Loss: 0.0523, Train Acc: 0.9837, Val Loss: 3.1635, Val Acc: 0.5384
Epoch 3/50, Train Loss: 0.0474, Train Acc: 0.9877, Val Loss: 0.1009, Val Acc: 0.9868
Epoch 4/50, Train Loss: 0.0223, Train Acc: 0.9942, Val Loss: 0.0971, Val Acc: 0.9634
Epoch 5/50, Train Loss: 0.0171, Train Acc: 0.9952, Val Loss: 0.0558, Val Acc: 0.9886
Epoch 6/50, Train Loss: 0.0072, Train Acc: 0.9978, Val Loss: 0.0741, Val Acc: 0.9814
Epoch 7/50, Train Loss: 0.0120, Train Acc: 0.9967, Val Loss: 0.0059, Val Acc: 0.9988
Epoch 8/50, Train Loss: 0.0086, Train Acc: 0.9979, Val Loss: 1.0145, Val Acc: 0.6841
Epoch 9/50, Train Loss: 0.0108, Train Acc: 0.9967, Val Loss: 0.2647, Val Acc: 0.8867
Epoch 10/50, Train Loss: 0.0135, Train Acc: 0.9955, Val Loss: 0.0390, Val Acc: 0.9874
Epoch 11/50, Train Loss: 0.0036, Train Acc: 0.9987, Val Loss: 0.0013, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0023, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0032, Train Acc: 0.9988, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0020, Train Acc: 0.9996, Val Loss: 0.0020, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0026, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0014, Train Acc: 0.9996, Val Loss: 0.0009, Val Acc: 1.0000
Early stopping!

Run 10/10
Epoch 1/50, Train Loss: 0.3641, Train Acc: 0.8724, Val Loss: 1.9354, Val Acc: 0.5222
Epoch 2/50, Train Loss: 0.0622, Train Acc: 0.9838, Val Loss: 0.1598, Val Acc: 0.9376
Epoch 3/50, Train Loss: 0.0209, Train Acc: 0.9955, Val Loss: 0.0267, Val Acc: 0.9898
Epoch 4/50, Train Loss: 0.0169, Train Acc: 0.9960, Val Loss: 0.7639, Val Acc: 0.8070
Epoch 5/50, Train Loss: 0.0165, Train Acc: 0.9964, Val Loss: 0.0104, Val Acc: 0.9982
Epoch 6/50, Train Loss: 0.0219, Train Acc: 0.9945, Val Loss: 0.0238, Val Acc: 0.9934
Epoch 7/50, Train Loss: 0.0162, Train Acc: 0.9963, Val Loss: 0.0562, Val Acc: 0.9814
Epoch 8/50, Train Loss: 0.0090, Train Acc: 0.9976, Val Loss: 1.6179, Val Acc: 0.7440
Epoch 9/50, Train Loss: 0.0032, Train Acc: 0.9994, Val Loss: 0.0021, Val Acc: 0.9994
Epoch 10/50, Train Loss: 0.0178, Train Acc: 0.9957, Val Loss: 0.3168, Val Acc: 0.9167
Epoch 11/50, Train Loss: 0.0047, Train Acc: 0.9988, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0023, Train Acc: 0.9993, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0019, Train Acc: 0.9996, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0022, Train Acc: 0.9994, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0025, Train Acc: 0.9994, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0016, Train Acc: 0.9999, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0022, Train Acc: 0.9990, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0035, Train Acc: 0.9996, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 24/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Early stopping!

Source performance: 99.80 99.81 99.80 99.80
Target performance: 90.74 92.50 91.09 90.77

bpsk: 99.85
qpsk: 73.98
16qam: 90.63
8apsk: 99.90
DANN
Epoch 1/50, Loss: 2.3873, Domain Loss: 1.3892, Class Loss: 0.9981
Epoch 2/50, Loss: 1.7466, Domain Loss: 1.3416, Class Loss: 0.4050
Epoch 3/50, Loss: 1.5429, Domain Loss: 1.3724, Class Loss: 0.1705
Epoch 4/50, Loss: 2.5816, Domain Loss: 2.4577, Class Loss: 0.1239
Epoch 5/50, Loss: 3.9165, Domain Loss: 3.7074, Class Loss: 0.2091
Epoch 6/50, Loss: 5.5705, Domain Loss: 5.3719, Class Loss: 0.1986
Epoch 7/50, Loss: 5.5434, Domain Loss: 5.2998, Class Loss: 0.2436
Epoch 8/50, Loss: 20.3530, Domain Loss: 19.8515, Class Loss: 0.5015
Epoch 9/50, Loss: 8.4711, Domain Loss: 8.0449, Class Loss: 0.4262
Epoch 10/50, Loss: 9.4917, Domain Loss: 8.9326, Class Loss: 0.5591
Epoch 11/50, Loss: 5.3009, Domain Loss: 4.7803, Class Loss: 0.5206
Epoch 12/50, Loss: 3.2239, Domain Loss: 2.6713, Class Loss: 0.5526
Epoch 13/50, Loss: 2.2630, Domain Loss: 1.9530, Class Loss: 0.3100
Epoch 14/50, Loss: 1.6932, Domain Loss: 1.4717, Class Loss: 0.2215
Epoch 15/50, Loss: 1.5180, Domain Loss: 1.3689, Class Loss: 0.1490
Epoch 16/50, Loss: 1.4888, Domain Loss: 1.3882, Class Loss: 0.1006
Epoch 17/50, Loss: 1.5230, Domain Loss: 1.4231, Class Loss: 0.0999
Epoch 18/50, Loss: 1.7807, Domain Loss: 1.6856, Class Loss: 0.0951
Epoch 19/50, Loss: 1.7711, Domain Loss: 1.6833, Class Loss: 0.0878
Epoch 20/50, Loss: 1.5376, Domain Loss: 1.4761, Class Loss: 0.0614
Epoch 21/50, Loss: 1.4876, Domain Loss: 1.4498, Class Loss: 0.0377
Epoch 22/50, Loss: 1.5215, Domain Loss: 1.4730, Class Loss: 0.0485
Epoch 23/50, Loss: 1.5552, Domain Loss: 1.5121, Class Loss: 0.0431
Epoch 24/50, Loss: 1.6994, Domain Loss: 1.6433, Class Loss: 0.0561
Epoch 25/50, Loss: 1.9843, Domain Loss: 1.8476, Class Loss: 0.1368
Epoch 26/50, Loss: 1.7568, Domain Loss: 1.6455, Class Loss: 0.1114
Epoch 27/50, Loss: 1.6730, Domain Loss: 1.6172, Class Loss: 0.0558
Epoch 28/50, Loss: 1.6510, Domain Loss: 1.6013, Class Loss: 0.0497
Epoch 29/50, Loss: 1.5639, Domain Loss: 1.5128, Class Loss: 0.0512
Epoch 30/50, Loss: 1.6150, Domain Loss: 1.5683, Class Loss: 0.0467
Epoch 31/50, Loss: 2.1022, Domain Loss: 1.9150, Class Loss: 0.1872
Epoch 32/50, Loss: 1.6921, Domain Loss: 1.6003, Class Loss: 0.0918
Epoch 33/50, Loss: 1.4826, Domain Loss: 1.4391, Class Loss: 0.0435
Epoch 34/50, Loss: 1.6269, Domain Loss: 1.5920, Class Loss: 0.0349
Epoch 35/50, Loss: 3.6879, Domain Loss: 3.5617, Class Loss: 0.1262
Epoch 36/50, Loss: 2.6539, Domain Loss: 2.5911, Class Loss: 0.0628
Epoch 37/50, Loss: 3.8923, Domain Loss: 3.7677, Class Loss: 0.1245
Epoch 38/50, Loss: 2.3327, Domain Loss: 2.2522, Class Loss: 0.0805
Epoch 39/50, Loss: 1.5388, Domain Loss: 1.5039, Class Loss: 0.0349
Epoch 40/50, Loss: 1.4083, Domain Loss: 1.3821, Class Loss: 0.0262
Epoch 41/50, Loss: 1.4500, Domain Loss: 1.4080, Class Loss: 0.0421
Epoch 42/50, Loss: 1.4908, Domain Loss: 1.4589, Class Loss: 0.0319
Epoch 43/50, Loss: 1.4867, Domain Loss: 1.4392, Class Loss: 0.0475
Epoch 44/50, Loss: 1.4550, Domain Loss: 1.4184, Class Loss: 0.0365
Epoch 45/50, Loss: 1.4048, Domain Loss: 1.3884, Class Loss: 0.0163
Epoch 46/50, Loss: 1.4112, Domain Loss: 1.3841, Class Loss: 0.0271
Epoch 47/50, Loss: 1.4068, Domain Loss: 1.3913, Class Loss: 0.0155
Epoch 48/50, Loss: 1.4027, Domain Loss: 1.3873, Class Loss: 0.0154
Epoch 49/50, Loss: 1.4115, Domain Loss: 1.3959, Class Loss: 0.0157
Epoch 50/50, Loss: 1.4137, Domain Loss: 1.3895, Class Loss: 0.0242
97.54


Epoch 1/50, Loss: 2.4116, Domain Loss: 1.3937, Class Loss: 1.0179
Epoch 2/50, Loss: 1.6874, Domain Loss: 1.3425, Class Loss: 0.3449
Epoch 3/50, Loss: 1.5319, Domain Loss: 1.3687, Class Loss: 0.1632
Epoch 4/50, Loss: 1.6337, Domain Loss: 1.5255, Class Loss: 0.1083
Epoch 5/50, Loss: 4.3783, Domain Loss: 4.2206, Class Loss: 0.1576
Epoch 6/50, Loss: 2.3090, Domain Loss: 2.2075, Class Loss: 0.1015
Epoch 7/50, Loss: 1.9970, Domain Loss: 1.9211, Class Loss: 0.0759
Epoch 8/50, Loss: 1.6093, Domain Loss: 1.5301, Class Loss: 0.0792
Epoch 9/50, Loss: 1.4697, Domain Loss: 1.4258, Class Loss: 0.0439
Epoch 10/50, Loss: 1.8568, Domain Loss: 1.7801, Class Loss: 0.0768
Epoch 11/50, Loss: 2.4747, Domain Loss: 2.2685, Class Loss: 0.2061
Epoch 12/50, Loss: 1.5977, Domain Loss: 1.5040, Class Loss: 0.0937
Epoch 13/50, Loss: 1.4399, Domain Loss: 1.3751, Class Loss: 0.0648
Epoch 14/50, Loss: 1.5426, Domain Loss: 1.4531, Class Loss: 0.0895
Epoch 15/50, Loss: 1.4457, Domain Loss: 1.4055, Class Loss: 0.0402
Epoch 16/50, Loss: 1.4224, Domain Loss: 1.3849, Class Loss: 0.0375
Epoch 17/50, Loss: 1.4264, Domain Loss: 1.3912, Class Loss: 0.0352
Epoch 18/50, Loss: 1.4435, Domain Loss: 1.4111, Class Loss: 0.0323
Epoch 19/50, Loss: 1.4474, Domain Loss: 1.4291, Class Loss: 0.0183
Epoch 20/50, Loss: 1.4599, Domain Loss: 1.4386, Class Loss: 0.0213
Epoch 21/50, Loss: 1.4575, Domain Loss: 1.4126, Class Loss: 0.0449
Epoch 22/50, Loss: 1.5215, Domain Loss: 1.4885, Class Loss: 0.0331
Epoch 23/50, Loss: 1.9737, Domain Loss: 1.8810, Class Loss: 0.0928
Epoch 24/50, Loss: 3.1288, Domain Loss: 2.8878, Class Loss: 0.2410
Epoch 25/50, Loss: 1.8707, Domain Loss: 1.7963, Class Loss: 0.0744
Epoch 26/50, Loss: 1.4642, Domain Loss: 1.4173, Class Loss: 0.0469
Epoch 27/50, Loss: 1.4261, Domain Loss: 1.3957, Class Loss: 0.0305
Epoch 28/50, Loss: 1.4298, Domain Loss: 1.3973, Class Loss: 0.0325
Epoch 29/50, Loss: 1.4255, Domain Loss: 1.4005, Class Loss: 0.0249
Epoch 30/50, Loss: 1.4177, Domain Loss: 1.3833, Class Loss: 0.0344
Epoch 31/50, Loss: 1.4308, Domain Loss: 1.4028, Class Loss: 0.0280
Epoch 32/50, Loss: 1.3970, Domain Loss: 1.3783, Class Loss: 0.0187
Epoch 33/50, Loss: 1.4938, Domain Loss: 1.4029, Class Loss: 0.0909
Epoch 34/50, Loss: 1.4084, Domain Loss: 1.3643, Class Loss: 0.0441
Epoch 35/50, Loss: 1.4041, Domain Loss: 1.3732, Class Loss: 0.0309
Epoch 36/50, Loss: 1.4406, Domain Loss: 1.3891, Class Loss: 0.0516
Epoch 37/50, Loss: 1.4093, Domain Loss: 1.3830, Class Loss: 0.0263
Epoch 38/50, Loss: 1.4153, Domain Loss: 1.3814, Class Loss: 0.0339
Epoch 39/50, Loss: 1.4615, Domain Loss: 1.3965, Class Loss: 0.0650
Epoch 40/50, Loss: 1.4822, Domain Loss: 1.4250, Class Loss: 0.0571
Epoch 41/50, Loss: 1.4391, Domain Loss: 1.4192, Class Loss: 0.0199
Epoch 42/50, Loss: 1.4361, Domain Loss: 1.4185, Class Loss: 0.0176
Epoch 43/50, Loss: 1.4525, Domain Loss: 1.4336, Class Loss: 0.0190
Epoch 44/50, Loss: 1.4379, Domain Loss: 1.4231, Class Loss: 0.0148
Epoch 45/50, Loss: 1.4266, Domain Loss: 1.4093, Class Loss: 0.0173
Epoch 46/50, Loss: 1.4203, Domain Loss: 1.4054, Class Loss: 0.0149
Epoch 47/50, Loss: 1.4525, Domain Loss: 1.4174, Class Loss: 0.0351
Epoch 48/50, Loss: 1.4202, Domain Loss: 1.3921, Class Loss: 0.0281
Epoch 49/50, Loss: 1.4606, Domain Loss: 1.4377, Class Loss: 0.0229
Epoch 50/50, Loss: 1.4687, Domain Loss: 1.4456, Class Loss: 0.0231
98.26


Epoch 1/50, Loss: 2.3084, Domain Loss: 1.3939, Class Loss: 0.9145
Epoch 2/50, Loss: 1.6465, Domain Loss: 1.3345, Class Loss: 0.3120
Epoch 3/50, Loss: 1.4852, Domain Loss: 1.3267, Class Loss: 0.1585
Epoch 4/50, Loss: 3.6390, Domain Loss: 3.5059, Class Loss: 0.1331
Epoch 5/50, Loss: 10.4559, Domain Loss: 10.2983, Class Loss: 0.1576
Epoch 6/50, Loss: 20.3118, Domain Loss: 19.9931, Class Loss: 0.3187
Epoch 7/50, Loss: 6.9837, Domain Loss: 6.7638, Class Loss: 0.2199
Epoch 8/50, Loss: 2.5909, Domain Loss: 2.2919, Class Loss: 0.2990
Epoch 9/50, Loss: 1.9417, Domain Loss: 1.7654, Class Loss: 0.1763
Epoch 10/50, Loss: 1.5340, Domain Loss: 1.4419, Class Loss: 0.0922
Epoch 11/50, Loss: 1.4849, Domain Loss: 1.3981, Class Loss: 0.0868
Epoch 12/50, Loss: 1.4944, Domain Loss: 1.4231, Class Loss: 0.0713
Epoch 13/50, Loss: 1.4645, Domain Loss: 1.4094, Class Loss: 0.0551
Epoch 14/50, Loss: 1.4935, Domain Loss: 1.4324, Class Loss: 0.0611
Epoch 15/50, Loss: 1.4740, Domain Loss: 1.4238, Class Loss: 0.0502
Epoch 16/50, Loss: 1.4668, Domain Loss: 1.4130, Class Loss: 0.0538
Epoch 17/50, Loss: 1.4678, Domain Loss: 1.4161, Class Loss: 0.0517
Epoch 18/50, Loss: 1.4580, Domain Loss: 1.4171, Class Loss: 0.0409
Epoch 19/50, Loss: 1.4244, Domain Loss: 1.4018, Class Loss: 0.0226
Epoch 20/50, Loss: 1.4360, Domain Loss: 1.4137, Class Loss: 0.0223
Epoch 21/50, Loss: 1.4272, Domain Loss: 1.4019, Class Loss: 0.0253
Epoch 22/50, Loss: 1.4120, Domain Loss: 1.3999, Class Loss: 0.0120
Epoch 23/50, Loss: 1.4087, Domain Loss: 1.3949, Class Loss: 0.0137
Epoch 24/50, Loss: 1.4123, Domain Loss: 1.3944, Class Loss: 0.0178
Epoch 25/50, Loss: 1.4138, Domain Loss: 1.3970, Class Loss: 0.0168
Epoch 26/50, Loss: 1.4043, Domain Loss: 1.3904, Class Loss: 0.0139
Epoch 27/50, Loss: 1.3939, Domain Loss: 1.3869, Class Loss: 0.0070
Epoch 28/50, Loss: 1.3945, Domain Loss: 1.3851, Class Loss: 0.0094
Epoch 29/50, Loss: 1.3922, Domain Loss: 1.3875, Class Loss: 0.0047
Epoch 30/50, Loss: 1.4234, Domain Loss: 1.3983, Class Loss: 0.0251
Epoch 31/50, Loss: 1.4090, Domain Loss: 1.3899, Class Loss: 0.0191
Epoch 32/50, Loss: 1.4132, Domain Loss: 1.3983, Class Loss: 0.0150
Epoch 33/50, Loss: 1.4083, Domain Loss: 1.3956, Class Loss: 0.0127
Epoch 34/50, Loss: 1.4075, Domain Loss: 1.3956, Class Loss: 0.0120
Epoch 35/50, Loss: 1.4117, Domain Loss: 1.4035, Class Loss: 0.0082
Epoch 36/50, Loss: 1.4117, Domain Loss: 1.4033, Class Loss: 0.0084
Epoch 37/50, Loss: 1.4238, Domain Loss: 1.4157, Class Loss: 0.0081
Epoch 38/50, Loss: 1.4254, Domain Loss: 1.4195, Class Loss: 0.0059
Epoch 39/50, Loss: 1.4302, Domain Loss: 1.4224, Class Loss: 0.0078
Epoch 40/50, Loss: 1.4372, Domain Loss: 1.4281, Class Loss: 0.0091
Epoch 41/50, Loss: 1.4101, Domain Loss: 1.3996, Class Loss: 0.0105
Epoch 42/50, Loss: 1.4019, Domain Loss: 1.3885, Class Loss: 0.0134
Epoch 43/50, Loss: 1.3956, Domain Loss: 1.3886, Class Loss: 0.0070
Epoch 44/50, Loss: 1.3924, Domain Loss: 1.3855, Class Loss: 0.0069
Epoch 45/50, Loss: 1.3948, Domain Loss: 1.3854, Class Loss: 0.0094
Epoch 46/50, Loss: 1.3950, Domain Loss: 1.3834, Class Loss: 0.0116
Epoch 47/50, Loss: 1.4030, Domain Loss: 1.3860, Class Loss: 0.0170
Epoch 48/50, Loss: 1.4007, Domain Loss: 1.3890, Class Loss: 0.0118
Epoch 49/50, Loss: 1.4001, Domain Loss: 1.3907, Class Loss: 0.0094
Epoch 50/50, Loss: 1.4015, Domain Loss: 1.3884, Class Loss: 0.0131
98.56


Epoch 1/50, Loss: 2.4255, Domain Loss: 1.3875, Class Loss: 1.0380
Epoch 2/50, Loss: 1.7991, Domain Loss: 1.3509, Class Loss: 0.4482
Epoch 3/50, Loss: 1.5789, Domain Loss: 1.3136, Class Loss: 0.2654
Epoch 4/50, Loss: 1.5140, Domain Loss: 1.3667, Class Loss: 0.1473
Epoch 5/50, Loss: 4.5903, Domain Loss: 4.4528, Class Loss: 0.1375
Epoch 6/50, Loss: 23.6790, Domain Loss: 23.1695, Class Loss: 0.5095
Epoch 7/50, Loss: 17.2840, Domain Loss: 16.8001, Class Loss: 0.4839
Epoch 8/50, Loss: 8.4821, Domain Loss: 7.9435, Class Loss: 0.5386
Epoch 9/50, Loss: 7.8203, Domain Loss: 7.4360, Class Loss: 0.3843
Epoch 10/50, Loss: 4.5188, Domain Loss: 4.2349, Class Loss: 0.2840
Epoch 11/50, Loss: 3.0483, Domain Loss: 2.6840, Class Loss: 0.3643
Epoch 12/50, Loss: 2.0885, Domain Loss: 1.9160, Class Loss: 0.1725
Epoch 13/50, Loss: 1.6293, Domain Loss: 1.5085, Class Loss: 0.1208
Epoch 14/50, Loss: 1.5119, Domain Loss: 1.4503, Class Loss: 0.0616
Epoch 15/50, Loss: 1.6138, Domain Loss: 1.5182, Class Loss: 0.0956
Epoch 16/50, Loss: 1.7566, Domain Loss: 1.6502, Class Loss: 0.1063
Epoch 17/50, Loss: 2.2376, Domain Loss: 1.9934, Class Loss: 0.2443
Epoch 18/50, Loss: 2.6060, Domain Loss: 2.3545, Class Loss: 0.2515
Epoch 19/50, Loss: 2.2495, Domain Loss: 2.0990, Class Loss: 0.1505
Epoch 20/50, Loss: 2.3516, Domain Loss: 2.1006, Class Loss: 0.2509
Epoch 21/50, Loss: 1.9012, Domain Loss: 1.7628, Class Loss: 0.1384
Epoch 22/50, Loss: 1.7423, Domain Loss: 1.5641, Class Loss: 0.1782
Epoch 23/50, Loss: 1.5616, Domain Loss: 1.4932, Class Loss: 0.0685
Epoch 24/50, Loss: 1.5169, Domain Loss: 1.4594, Class Loss: 0.0575
Epoch 25/50, Loss: 1.4583, Domain Loss: 1.4049, Class Loss: 0.0534
Epoch 26/50, Loss: 1.4407, Domain Loss: 1.3893, Class Loss: 0.0515
Epoch 27/50, Loss: 1.4721, Domain Loss: 1.4091, Class Loss: 0.0630
Epoch 28/50, Loss: 1.4660, Domain Loss: 1.4169, Class Loss: 0.0491
Epoch 29/50, Loss: 1.4286, Domain Loss: 1.4011, Class Loss: 0.0275
Epoch 30/50, Loss: 1.5113, Domain Loss: 1.4291, Class Loss: 0.0823
Epoch 31/50, Loss: 1.4560, Domain Loss: 1.4025, Class Loss: 0.0535
Epoch 32/50, Loss: 1.4331, Domain Loss: 1.3882, Class Loss: 0.0449
Epoch 33/50, Loss: 1.4397, Domain Loss: 1.4027, Class Loss: 0.0370
Epoch 34/50, Loss: 1.4173, Domain Loss: 1.3888, Class Loss: 0.0286
Epoch 35/50, Loss: 1.4292, Domain Loss: 1.3974, Class Loss: 0.0318
Epoch 36/50, Loss: 1.4372, Domain Loss: 1.4019, Class Loss: 0.0353
Epoch 37/50, Loss: 1.4328, Domain Loss: 1.4078, Class Loss: 0.0251
Epoch 38/50, Loss: 1.4233, Domain Loss: 1.3968, Class Loss: 0.0266
Epoch 39/50, Loss: 1.4117, Domain Loss: 1.3983, Class Loss: 0.0134
Epoch 40/50, Loss: 1.4203, Domain Loss: 1.3921, Class Loss: 0.0282
Epoch 41/50, Loss: 1.4094, Domain Loss: 1.3839, Class Loss: 0.0255
Epoch 42/50, Loss: 1.4058, Domain Loss: 1.3892, Class Loss: 0.0166
Epoch 43/50, Loss: 1.3929, Domain Loss: 1.3834, Class Loss: 0.0095
Epoch 44/50, Loss: 1.4054, Domain Loss: 1.3883, Class Loss: 0.0171
Epoch 45/50, Loss: 1.4137, Domain Loss: 1.3923, Class Loss: 0.0214
Epoch 46/50, Loss: 1.4014, Domain Loss: 1.3890, Class Loss: 0.0124
Epoch 47/50, Loss: 1.4171, Domain Loss: 1.3872, Class Loss: 0.0299
Epoch 48/50, Loss: 1.3884, Domain Loss: 1.3799, Class Loss: 0.0084
Epoch 49/50, Loss: 1.3959, Domain Loss: 1.3825, Class Loss: 0.0135
Epoch 50/50, Loss: 1.4204, Domain Loss: 1.3963, Class Loss: 0.0241
98.56


Epoch 1/50, Loss: 2.4347, Domain Loss: 1.3874, Class Loss: 1.0473
Epoch 2/50, Loss: 1.7024, Domain Loss: 1.3226, Class Loss: 0.3798
Epoch 3/50, Loss: 1.4994, Domain Loss: 1.3145, Class Loss: 0.1849
Epoch 4/50, Loss: 1.7487, Domain Loss: 1.6420, Class Loss: 0.1067
Epoch 5/50, Loss: 1.7355, Domain Loss: 1.6529, Class Loss: 0.0826
Epoch 6/50, Loss: 6.4274, Domain Loss: 6.1860, Class Loss: 0.2414
Epoch 7/50, Loss: 5.5482, Domain Loss: 5.3252, Class Loss: 0.2230
Epoch 8/50, Loss: 6.6097, Domain Loss: 6.1245, Class Loss: 0.4852
Epoch 9/50, Loss: 5.6723, Domain Loss: 5.0476, Class Loss: 0.6248
Epoch 10/50, Loss: 4.5681, Domain Loss: 4.1005, Class Loss: 0.4675
Epoch 11/50, Loss: 5.2902, Domain Loss: 4.8140, Class Loss: 0.4762
Epoch 12/50, Loss: 8.8378, Domain Loss: 8.4045, Class Loss: 0.4333
Epoch 13/50, Loss: 11.4991, Domain Loss: 10.9601, Class Loss: 0.5391
Epoch 14/50, Loss: 9.1283, Domain Loss: 7.7025, Class Loss: 1.4257
Epoch 15/50, Loss: 11.7381, Domain Loss: 10.8746, Class Loss: 0.8635
Epoch 16/50, Loss: 5.6259, Domain Loss: 4.9646, Class Loss: 0.6613
Epoch 17/50, Loss: 3.9799, Domain Loss: 3.3882, Class Loss: 0.5917
Epoch 18/50, Loss: 3.6134, Domain Loss: 3.2848, Class Loss: 0.3286
Epoch 19/50, Loss: 2.4666, Domain Loss: 2.1987, Class Loss: 0.2679
Epoch 20/50, Loss: 3.3708, Domain Loss: 2.7012, Class Loss: 0.6696
Epoch 21/50, Loss: 2.5626, Domain Loss: 2.1066, Class Loss: 0.4560
Epoch 22/50, Loss: 1.9068, Domain Loss: 1.5537, Class Loss: 0.3531
Epoch 23/50, Loss: 1.7788, Domain Loss: 1.4537, Class Loss: 0.3251
Epoch 24/50, Loss: 1.5840, Domain Loss: 1.3601, Class Loss: 0.2239
Epoch 25/50, Loss: 1.5535, Domain Loss: 1.4062, Class Loss: 0.1473
Epoch 26/50, Loss: 1.5983, Domain Loss: 1.4933, Class Loss: 0.1050
Epoch 27/50, Loss: 1.5272, Domain Loss: 1.4165, Class Loss: 0.1107
Epoch 28/50, Loss: 1.5114, Domain Loss: 1.4174, Class Loss: 0.0940
Epoch 29/50, Loss: 1.5110, Domain Loss: 1.4242, Class Loss: 0.0867
Epoch 30/50, Loss: 1.5320, Domain Loss: 1.4452, Class Loss: 0.0868
Epoch 31/50, Loss: 1.5300, Domain Loss: 1.4546, Class Loss: 0.0753
Epoch 32/50, Loss: 1.5301, Domain Loss: 1.4612, Class Loss: 0.0689
Epoch 33/50, Loss: 1.5072, Domain Loss: 1.4522, Class Loss: 0.0551
Epoch 34/50, Loss: 1.4748, Domain Loss: 1.4257, Class Loss: 0.0491
Epoch 35/50, Loss: 1.5767, Domain Loss: 1.5216, Class Loss: 0.0551
Epoch 36/50, Loss: 1.6329, Domain Loss: 1.5742, Class Loss: 0.0587
Epoch 37/50, Loss: 1.6077, Domain Loss: 1.5644, Class Loss: 0.0433
Epoch 38/50, Loss: 1.6417, Domain Loss: 1.6030, Class Loss: 0.0387
Epoch 39/50, Loss: 1.6596, Domain Loss: 1.6104, Class Loss: 0.0491
Epoch 40/50, Loss: 1.6346, Domain Loss: 1.5968, Class Loss: 0.0378
Epoch 41/50, Loss: 1.5991, Domain Loss: 1.5725, Class Loss: 0.0267
Epoch 42/50, Loss: 1.5069, Domain Loss: 1.4699, Class Loss: 0.0370
Epoch 43/50, Loss: 1.5645, Domain Loss: 1.4946, Class Loss: 0.0699
Epoch 44/50, Loss: 1.6069, Domain Loss: 1.5512, Class Loss: 0.0557
Epoch 45/50, Loss: 1.7044, Domain Loss: 1.6509, Class Loss: 0.0535
Epoch 46/50, Loss: 1.8572, Domain Loss: 1.8152, Class Loss: 0.0420
Epoch 47/50, Loss: 1.6945, Domain Loss: 1.6467, Class Loss: 0.0479
Epoch 48/50, Loss: 1.7456, Domain Loss: 1.7135, Class Loss: 0.0321
Epoch 49/50, Loss: 1.5493, Domain Loss: 1.5200, Class Loss: 0.0293
Epoch 50/50, Loss: 1.5021, Domain Loss: 1.4787, Class Loss: 0.0233
98.98


Epoch 1/50, Loss: 2.2888, Domain Loss: 1.3887, Class Loss: 0.9002
Epoch 2/50, Loss: 1.6946, Domain Loss: 1.3294, Class Loss: 0.3653
Epoch 3/50, Loss: 1.6511, Domain Loss: 1.4658, Class Loss: 0.1853
Epoch 4/50, Loss: 3.7731, Domain Loss: 3.6336, Class Loss: 0.1395
Epoch 5/50, Loss: 4.7987, Domain Loss: 4.5881, Class Loss: 0.2106
Epoch 6/50, Loss: 11.2627, Domain Loss: 11.1033, Class Loss: 0.1595
Epoch 7/50, Loss: 32.6547, Domain Loss: 32.0365, Class Loss: 0.6182
Epoch 8/50, Loss: 12.1380, Domain Loss: 11.3484, Class Loss: 0.7896
Epoch 9/50, Loss: 11.8622, Domain Loss: 11.0196, Class Loss: 0.8426
Epoch 10/50, Loss: 7.2211, Domain Loss: 6.4058, Class Loss: 0.8153
Epoch 11/50, Loss: 5.1803, Domain Loss: 4.4906, Class Loss: 0.6896
Epoch 12/50, Loss: 7.2822, Domain Loss: 6.0377, Class Loss: 1.2445
Epoch 13/50, Loss: 10.0233, Domain Loss: 9.0028, Class Loss: 1.0205
Epoch 14/50, Loss: 12.1023, Domain Loss: 11.2384, Class Loss: 0.8639
Epoch 15/50, Loss: 4.5099, Domain Loss: 3.7843, Class Loss: 0.7256
Epoch 16/50, Loss: 2.7502, Domain Loss: 2.0441, Class Loss: 0.7061
Epoch 17/50, Loss: 2.1518, Domain Loss: 1.5019, Class Loss: 0.6499
Epoch 18/50, Loss: 1.9624, Domain Loss: 1.4472, Class Loss: 0.5152
Epoch 19/50, Loss: 1.9528, Domain Loss: 1.5054, Class Loss: 0.4474
Epoch 20/50, Loss: 2.3755, Domain Loss: 1.8707, Class Loss: 0.5048
Epoch 21/50, Loss: 2.7018, Domain Loss: 1.9889, Class Loss: 0.7129
Epoch 22/50, Loss: 2.0217, Domain Loss: 1.5818, Class Loss: 0.4399
Epoch 23/50, Loss: 1.8432, Domain Loss: 1.4775, Class Loss: 0.3657
Epoch 24/50, Loss: 1.6992, Domain Loss: 1.4573, Class Loss: 0.2419
Epoch 25/50, Loss: 1.6694, Domain Loss: 1.4604, Class Loss: 0.2090
Epoch 26/50, Loss: 1.6742, Domain Loss: 1.4639, Class Loss: 0.2103
Epoch 27/50, Loss: 1.6136, Domain Loss: 1.4597, Class Loss: 0.1539
Epoch 28/50, Loss: 1.6308, Domain Loss: 1.4801, Class Loss: 0.1507
Epoch 29/50, Loss: 1.6551, Domain Loss: 1.5014, Class Loss: 0.1537
Epoch 30/50, Loss: 1.6402, Domain Loss: 1.5113, Class Loss: 0.1290
Epoch 31/50, Loss: 1.6177, Domain Loss: 1.5336, Class Loss: 0.0841
Epoch 32/50, Loss: 1.6915, Domain Loss: 1.6058, Class Loss: 0.0857
Epoch 33/50, Loss: 2.0010, Domain Loss: 1.8147, Class Loss: 0.1863
Epoch 34/50, Loss: 2.0519, Domain Loss: 1.9456, Class Loss: 0.1063
Epoch 35/50, Loss: 2.0462, Domain Loss: 1.9075, Class Loss: 0.1387
Epoch 36/50, Loss: 2.1535, Domain Loss: 2.0321, Class Loss: 0.1214
Epoch 37/50, Loss: 2.0808, Domain Loss: 1.9519, Class Loss: 0.1290
Epoch 38/50, Loss: 1.7967, Domain Loss: 1.6518, Class Loss: 0.1449
Epoch 39/50, Loss: 1.5990, Domain Loss: 1.5204, Class Loss: 0.0786
Epoch 40/50, Loss: 1.5078, Domain Loss: 1.4546, Class Loss: 0.0531
Epoch 41/50, Loss: 1.4773, Domain Loss: 1.4298, Class Loss: 0.0475
Epoch 42/50, Loss: 1.4490, Domain Loss: 1.4045, Class Loss: 0.0446
Epoch 43/50, Loss: 1.4674, Domain Loss: 1.4138, Class Loss: 0.0536
Epoch 44/50, Loss: 1.4206, Domain Loss: 1.3894, Class Loss: 0.0312
Epoch 45/50, Loss: 1.4161, Domain Loss: 1.3817, Class Loss: 0.0344
Epoch 46/50, Loss: 1.4053, Domain Loss: 1.3812, Class Loss: 0.0241
Epoch 47/50, Loss: 1.4457, Domain Loss: 1.4131, Class Loss: 0.0325
Epoch 48/50, Loss: 1.3953, Domain Loss: 1.3674, Class Loss: 0.0278
Epoch 49/50, Loss: 1.4314, Domain Loss: 1.3996, Class Loss: 0.0318
Epoch 50/50, Loss: 1.4631, Domain Loss: 1.4364, Class Loss: 0.0267
92.51


Epoch 1/50, Loss: 2.4525, Domain Loss: 1.4031, Class Loss: 1.0495
Epoch 2/50, Loss: 1.7462, Domain Loss: 1.3433, Class Loss: 0.4029
Epoch 3/50, Loss: 1.5223, Domain Loss: 1.3328, Class Loss: 0.1895
Epoch 4/50, Loss: 1.4920, Domain Loss: 1.3907, Class Loss: 0.1013
Epoch 5/50, Loss: 3.1560, Domain Loss: 3.0252, Class Loss: 0.1308
Epoch 6/50, Loss: 7.8952, Domain Loss: 7.6429, Class Loss: 0.2522
Epoch 7/50, Loss: 8.9361, Domain Loss: 8.6704, Class Loss: 0.2657
Epoch 8/50, Loss: 22.9080, Domain Loss: 22.1751, Class Loss: 0.7330
Epoch 9/50, Loss: 24.5484, Domain Loss: 23.3116, Class Loss: 1.2369
Epoch 10/50, Loss: 10.8490, Domain Loss: 10.4300, Class Loss: 0.4191
Epoch 11/50, Loss: 3.9350, Domain Loss: 3.6563, Class Loss: 0.2787
Epoch 12/50, Loss: 2.4600, Domain Loss: 2.1053, Class Loss: 0.3548
Epoch 13/50, Loss: 2.0040, Domain Loss: 1.7352, Class Loss: 0.2688
Epoch 14/50, Loss: 1.7441, Domain Loss: 1.6073, Class Loss: 0.1368
Epoch 15/50, Loss: 1.7049, Domain Loss: 1.5716, Class Loss: 0.1333
Epoch 16/50, Loss: 1.7318, Domain Loss: 1.6361, Class Loss: 0.0957
Epoch 17/50, Loss: 1.6954, Domain Loss: 1.6293, Class Loss: 0.0660
Epoch 18/50, Loss: 1.5986, Domain Loss: 1.5261, Class Loss: 0.0725
Epoch 19/50, Loss: 1.5686, Domain Loss: 1.4789, Class Loss: 0.0896
Epoch 20/50, Loss: 1.7304, Domain Loss: 1.6261, Class Loss: 0.1043
Epoch 21/50, Loss: 1.6706, Domain Loss: 1.5873, Class Loss: 0.0832
Epoch 22/50, Loss: 1.8290, Domain Loss: 1.6962, Class Loss: 0.1328
Epoch 23/50, Loss: 1.5627, Domain Loss: 1.5036, Class Loss: 0.0591
Epoch 24/50, Loss: 1.4477, Domain Loss: 1.3835, Class Loss: 0.0643
Epoch 25/50, Loss: 1.5536, Domain Loss: 1.4749, Class Loss: 0.0787
Epoch 26/50, Loss: 1.6183, Domain Loss: 1.5463, Class Loss: 0.0720
Epoch 27/50, Loss: 1.5009, Domain Loss: 1.4495, Class Loss: 0.0514
Epoch 28/50, Loss: 1.4371, Domain Loss: 1.4052, Class Loss: 0.0319
Epoch 29/50, Loss: 1.4545, Domain Loss: 1.4168, Class Loss: 0.0377
Epoch 30/50, Loss: 1.4691, Domain Loss: 1.4269, Class Loss: 0.0423
Epoch 31/50, Loss: 1.4357, Domain Loss: 1.4040, Class Loss: 0.0317
Epoch 32/50, Loss: 1.4058, Domain Loss: 1.3875, Class Loss: 0.0183
Epoch 33/50, Loss: 1.4229, Domain Loss: 1.3792, Class Loss: 0.0437
Epoch 34/50, Loss: 1.4223, Domain Loss: 1.3754, Class Loss: 0.0469
Epoch 35/50, Loss: 1.4117, Domain Loss: 1.3759, Class Loss: 0.0358
Epoch 36/50, Loss: 1.4071, Domain Loss: 1.3706, Class Loss: 0.0365
Epoch 37/50, Loss: 1.4085, Domain Loss: 1.3801, Class Loss: 0.0284
Epoch 38/50, Loss: 1.4124, Domain Loss: 1.3739, Class Loss: 0.0385
Epoch 39/50, Loss: 1.3860, Domain Loss: 1.3698, Class Loss: 0.0162
Epoch 40/50, Loss: 1.3921, Domain Loss: 1.3777, Class Loss: 0.0144
Epoch 41/50, Loss: 1.4026, Domain Loss: 1.3798, Class Loss: 0.0228
Epoch 42/50, Loss: 1.4281, Domain Loss: 1.3815, Class Loss: 0.0466
Epoch 43/50, Loss: 1.4615, Domain Loss: 1.4246, Class Loss: 0.0369
Epoch 44/50, Loss: 1.5289, Domain Loss: 1.4533, Class Loss: 0.0756
Epoch 45/50, Loss: 1.4233, Domain Loss: 1.3970, Class Loss: 0.0263
Epoch 46/50, Loss: 1.4183, Domain Loss: 1.3976, Class Loss: 0.0207
Epoch 47/50, Loss: 1.4233, Domain Loss: 1.4005, Class Loss: 0.0227
Epoch 48/50, Loss: 1.4375, Domain Loss: 1.4148, Class Loss: 0.0227
Epoch 49/50, Loss: 1.4603, Domain Loss: 1.4268, Class Loss: 0.0336
Epoch 50/50, Loss: 1.4832, Domain Loss: 1.4485, Class Loss: 0.0347
96.04


Epoch 1/50, Loss: 2.3497, Domain Loss: 1.3862, Class Loss: 0.9635
Epoch 2/50, Loss: 1.6547, Domain Loss: 1.3519, Class Loss: 0.3028
Epoch 3/50, Loss: 1.4773, Domain Loss: 1.3595, Class Loss: 0.1177
Epoch 4/50, Loss: 1.6487, Domain Loss: 1.5571, Class Loss: 0.0916
Epoch 5/50, Loss: 3.9084, Domain Loss: 3.7896, Class Loss: 0.1188
Epoch 6/50, Loss: 5.7845, Domain Loss: 5.6159, Class Loss: 0.1686
Epoch 7/50, Loss: 8.4112, Domain Loss: 8.0596, Class Loss: 0.3516
Epoch 8/50, Loss: 5.0372, Domain Loss: 4.7422, Class Loss: 0.2949
Epoch 9/50, Loss: 3.6403, Domain Loss: 3.2692, Class Loss: 0.3711
Epoch 10/50, Loss: 1.8973, Domain Loss: 1.7409, Class Loss: 0.1564
Epoch 11/50, Loss: 1.6454, Domain Loss: 1.5585, Class Loss: 0.0869
Epoch 12/50, Loss: 1.6255, Domain Loss: 1.5511, Class Loss: 0.0744
Epoch 13/50, Loss: 2.1332, Domain Loss: 2.0219, Class Loss: 0.1113
Epoch 14/50, Loss: 3.8221, Domain Loss: 3.4402, Class Loss: 0.3819
Epoch 15/50, Loss: 2.4819, Domain Loss: 2.1770, Class Loss: 0.3049
Epoch 16/50, Loss: 1.6120, Domain Loss: 1.4998, Class Loss: 0.1122
Epoch 17/50, Loss: 1.4326, Domain Loss: 1.3696, Class Loss: 0.0631
Epoch 18/50, Loss: 1.4137, Domain Loss: 1.3713, Class Loss: 0.0424
Epoch 19/50, Loss: 1.3901, Domain Loss: 1.3548, Class Loss: 0.0353
Epoch 20/50, Loss: 1.3873, Domain Loss: 1.3582, Class Loss: 0.0290
Epoch 21/50, Loss: 1.3868, Domain Loss: 1.3574, Class Loss: 0.0293
Epoch 22/50, Loss: 1.3961, Domain Loss: 1.3673, Class Loss: 0.0288
Epoch 23/50, Loss: 1.5774, Domain Loss: 1.3920, Class Loss: 0.1854
Epoch 24/50, Loss: 1.4849, Domain Loss: 1.3902, Class Loss: 0.0947
Epoch 25/50, Loss: 1.5577, Domain Loss: 1.4798, Class Loss: 0.0779
Epoch 26/50, Loss: 1.6845, Domain Loss: 1.5875, Class Loss: 0.0971
Epoch 27/50, Loss: 1.5006, Domain Loss: 1.4494, Class Loss: 0.0513
Epoch 28/50, Loss: 1.4669, Domain Loss: 1.3955, Class Loss: 0.0714
Epoch 29/50, Loss: 1.4193, Domain Loss: 1.3778, Class Loss: 0.0414
Epoch 30/50, Loss: 1.4010, Domain Loss: 1.3723, Class Loss: 0.0287
Epoch 31/50, Loss: 1.4134, Domain Loss: 1.3768, Class Loss: 0.0366
Epoch 32/50, Loss: 1.4008, Domain Loss: 1.3762, Class Loss: 0.0246
Epoch 33/50, Loss: 1.4045, Domain Loss: 1.3800, Class Loss: 0.0245
Epoch 34/50, Loss: 1.4260, Domain Loss: 1.3819, Class Loss: 0.0441
Epoch 35/50, Loss: 1.4063, Domain Loss: 1.3824, Class Loss: 0.0239
Epoch 36/50, Loss: 1.4111, Domain Loss: 1.3829, Class Loss: 0.0282
Epoch 37/50, Loss: 1.4294, Domain Loss: 1.3903, Class Loss: 0.0391
Epoch 38/50, Loss: 1.4216, Domain Loss: 1.4034, Class Loss: 0.0182
Epoch 39/50, Loss: 1.4362, Domain Loss: 1.4051, Class Loss: 0.0311
Epoch 40/50, Loss: 1.4359, Domain Loss: 1.4173, Class Loss: 0.0186
Epoch 41/50, Loss: 1.4331, Domain Loss: 1.4067, Class Loss: 0.0264
Epoch 42/50, Loss: 1.4639, Domain Loss: 1.4007, Class Loss: 0.0632
Epoch 43/50, Loss: 1.4206, Domain Loss: 1.4032, Class Loss: 0.0173
Epoch 44/50, Loss: 1.4176, Domain Loss: 1.3905, Class Loss: 0.0272
Epoch 45/50, Loss: 1.4029, Domain Loss: 1.3916, Class Loss: 0.0113
Epoch 46/50, Loss: 1.4102, Domain Loss: 1.3961, Class Loss: 0.0141
Epoch 47/50, Loss: 1.4144, Domain Loss: 1.3998, Class Loss: 0.0146
Epoch 48/50, Loss: 1.4086, Domain Loss: 1.4006, Class Loss: 0.0080
Epoch 49/50, Loss: 1.4151, Domain Loss: 1.4022, Class Loss: 0.0128
Epoch 50/50, Loss: 1.4161, Domain Loss: 1.4011, Class Loss: 0.0150
99.04


Epoch 1/50, Loss: 2.3764, Domain Loss: 1.3948, Class Loss: 0.9816
Epoch 2/50, Loss: 1.8144, Domain Loss: 1.3654, Class Loss: 0.4491
Epoch 3/50, Loss: 1.6154, Domain Loss: 1.3626, Class Loss: 0.2528
Epoch 4/50, Loss: 1.5542, Domain Loss: 1.3885, Class Loss: 0.1658
Epoch 5/50, Loss: 1.7866, Domain Loss: 1.6836, Class Loss: 0.1030
Epoch 6/50, Loss: 4.9069, Domain Loss: 4.7396, Class Loss: 0.1673
Epoch 7/50, Loss: 9.4225, Domain Loss: 9.0650, Class Loss: 0.3575
Epoch 8/50, Loss: 3.3893, Domain Loss: 3.1174, Class Loss: 0.2718
Epoch 9/50, Loss: 2.8855, Domain Loss: 2.5205, Class Loss: 0.3651
Epoch 10/50, Loss: 3.2471, Domain Loss: 2.7814, Class Loss: 0.4657
Epoch 11/50, Loss: 2.3850, Domain Loss: 2.1763, Class Loss: 0.2087
Epoch 12/50, Loss: 2.6279, Domain Loss: 2.3689, Class Loss: 0.2591
Epoch 13/50, Loss: 7.6090, Domain Loss: 7.1218, Class Loss: 0.4872
Epoch 14/50, Loss: 7.0311, Domain Loss: 6.4090, Class Loss: 0.6221
Epoch 15/50, Loss: 5.2990, Domain Loss: 4.7432, Class Loss: 0.5557
Epoch 16/50, Loss: 3.0473, Domain Loss: 2.6149, Class Loss: 0.4325
Epoch 17/50, Loss: 2.8362, Domain Loss: 2.6035, Class Loss: 0.2327
Epoch 18/50, Loss: 1.7459, Domain Loss: 1.5779, Class Loss: 0.1680
Epoch 19/50, Loss: 1.6485, Domain Loss: 1.5436, Class Loss: 0.1049
Epoch 20/50, Loss: 1.7123, Domain Loss: 1.6010, Class Loss: 0.1113
Epoch 21/50, Loss: 1.7024, Domain Loss: 1.5891, Class Loss: 0.1133
Epoch 22/50, Loss: 1.8614, Domain Loss: 1.7132, Class Loss: 0.1482
Epoch 23/50, Loss: 2.9477, Domain Loss: 2.8259, Class Loss: 0.1219
Epoch 24/50, Loss: 5.7419, Domain Loss: 5.4476, Class Loss: 0.2943
Epoch 25/50, Loss: 2.8353, Domain Loss: 2.5932, Class Loss: 0.2421
Epoch 26/50, Loss: 2.0937, Domain Loss: 1.9134, Class Loss: 0.1803
Epoch 27/50, Loss: 1.7757, Domain Loss: 1.5973, Class Loss: 0.1784
Epoch 28/50, Loss: 1.5264, Domain Loss: 1.4572, Class Loss: 0.0692
Epoch 29/50, Loss: 1.4870, Domain Loss: 1.4268, Class Loss: 0.0601
Epoch 30/50, Loss: 1.4609, Domain Loss: 1.4232, Class Loss: 0.0378
Epoch 31/50, Loss: 1.4469, Domain Loss: 1.4068, Class Loss: 0.0401
Epoch 32/50, Loss: 1.4129, Domain Loss: 1.3903, Class Loss: 0.0226
Epoch 33/50, Loss: 1.4333, Domain Loss: 1.4105, Class Loss: 0.0227
Epoch 34/50, Loss: 1.4456, Domain Loss: 1.4049, Class Loss: 0.0407
Epoch 35/50, Loss: 1.4238, Domain Loss: 1.3944, Class Loss: 0.0294
Epoch 36/50, Loss: 1.4441, Domain Loss: 1.4079, Class Loss: 0.0362
Epoch 37/50, Loss: 1.4815, Domain Loss: 1.4283, Class Loss: 0.0532
Epoch 38/50, Loss: 1.4686, Domain Loss: 1.4336, Class Loss: 0.0350
Epoch 39/50, Loss: 1.4311, Domain Loss: 1.4105, Class Loss: 0.0206
Epoch 40/50, Loss: 1.4799, Domain Loss: 1.4436, Class Loss: 0.0363
Epoch 41/50, Loss: 1.4873, Domain Loss: 1.4507, Class Loss: 0.0366
Epoch 42/50, Loss: 1.5615, Domain Loss: 1.5013, Class Loss: 0.0602
Epoch 43/50, Loss: 1.5416, Domain Loss: 1.4691, Class Loss: 0.0725
Epoch 44/50, Loss: 1.5459, Domain Loss: 1.5079, Class Loss: 0.0380
Epoch 45/50, Loss: 1.5499, Domain Loss: 1.5089, Class Loss: 0.0410
Epoch 46/50, Loss: 1.5101, Domain Loss: 1.4344, Class Loss: 0.0757
Epoch 47/50, Loss: 1.4899, Domain Loss: 1.4588, Class Loss: 0.0311
Epoch 48/50, Loss: 1.5397, Domain Loss: 1.5012, Class Loss: 0.0385
Epoch 49/50, Loss: 1.4197, Domain Loss: 1.3987, Class Loss: 0.0211
Epoch 50/50, Loss: 1.5565, Domain Loss: 1.4934, Class Loss: 0.0631
84.29


Epoch 1/50, Loss: 2.4473, Domain Loss: 1.3960, Class Loss: 1.0513
Epoch 2/50, Loss: 1.8535, Domain Loss: 1.3351, Class Loss: 0.5184
Epoch 3/50, Loss: 1.5630, Domain Loss: 1.2700, Class Loss: 0.2930
Epoch 4/50, Loss: 1.9012, Domain Loss: 1.7035, Class Loss: 0.1977
Epoch 5/50, Loss: 5.0001, Domain Loss: 4.7919, Class Loss: 0.2082
Epoch 6/50, Loss: 17.6745, Domain Loss: 17.4561, Class Loss: 0.2184
Epoch 7/50, Loss: 8.6238, Domain Loss: 8.3103, Class Loss: 0.3134
Epoch 8/50, Loss: 6.5340, Domain Loss: 5.9681, Class Loss: 0.5660
Epoch 9/50, Loss: 2.9176, Domain Loss: 2.5147, Class Loss: 0.4029
Epoch 10/50, Loss: 2.4547, Domain Loss: 2.0655, Class Loss: 0.3892
Epoch 11/50, Loss: 2.0976, Domain Loss: 1.8359, Class Loss: 0.2617
Epoch 12/50, Loss: 1.8201, Domain Loss: 1.6613, Class Loss: 0.1588
Epoch 13/50, Loss: 1.9404, Domain Loss: 1.7728, Class Loss: 0.1676
Epoch 14/50, Loss: 1.9986, Domain Loss: 1.8392, Class Loss: 0.1594
Epoch 15/50, Loss: 1.7623, Domain Loss: 1.6334, Class Loss: 0.1289
Epoch 16/50, Loss: 1.7756, Domain Loss: 1.6988, Class Loss: 0.0768
Epoch 17/50, Loss: 3.6209, Domain Loss: 3.4396, Class Loss: 0.1813
Epoch 18/50, Loss: 16.6836, Domain Loss: 16.3385, Class Loss: 0.3451
Epoch 19/50, Loss: 4.9374, Domain Loss: 4.8138, Class Loss: 0.1236
Epoch 20/50, Loss: 2.1130, Domain Loss: 2.0130, Class Loss: 0.1000
Epoch 21/50, Loss: 2.0728, Domain Loss: 1.9736, Class Loss: 0.0991
Epoch 22/50, Loss: 2.3221, Domain Loss: 2.1419, Class Loss: 0.1802
Epoch 23/50, Loss: 3.5969, Domain Loss: 3.3085, Class Loss: 0.2884
Epoch 24/50, Loss: 2.6778, Domain Loss: 2.4245, Class Loss: 0.2534
Epoch 25/50, Loss: 1.9521, Domain Loss: 1.8177, Class Loss: 0.1345
Epoch 26/50, Loss: 2.2874, Domain Loss: 1.9475, Class Loss: 0.3399
Epoch 27/50, Loss: 3.0625, Domain Loss: 2.8393, Class Loss: 0.2232
Epoch 28/50, Loss: 2.1345, Domain Loss: 2.0272, Class Loss: 0.1073
Epoch 29/50, Loss: 1.9117, Domain Loss: 1.8013, Class Loss: 0.1104
Epoch 30/50, Loss: 1.6572, Domain Loss: 1.5522, Class Loss: 0.1050
Epoch 31/50, Loss: 1.5696, Domain Loss: 1.4815, Class Loss: 0.0881
Epoch 32/50, Loss: 1.5909, Domain Loss: 1.5006, Class Loss: 0.0903
Epoch 33/50, Loss: 1.5563, Domain Loss: 1.4742, Class Loss: 0.0820
Epoch 34/50, Loss: 1.5035, Domain Loss: 1.4371, Class Loss: 0.0664
Epoch 35/50, Loss: 1.5267, Domain Loss: 1.4587, Class Loss: 0.0680
Epoch 36/50, Loss: 1.4792, Domain Loss: 1.4315, Class Loss: 0.0477
Epoch 37/50, Loss: 1.5043, Domain Loss: 1.4601, Class Loss: 0.0441
Epoch 38/50, Loss: 1.5319, Domain Loss: 1.4876, Class Loss: 0.0443
Epoch 39/50, Loss: 1.5346, Domain Loss: 1.4932, Class Loss: 0.0414
Epoch 40/50, Loss: 1.5488, Domain Loss: 1.5096, Class Loss: 0.0392
Epoch 41/50, Loss: 1.5120, Domain Loss: 1.4869, Class Loss: 0.0251
Epoch 42/50, Loss: 1.5240, Domain Loss: 1.4966, Class Loss: 0.0274
Epoch 43/50, Loss: 1.5198, Domain Loss: 1.4827, Class Loss: 0.0371
Epoch 44/50, Loss: 1.5137, Domain Loss: 1.4745, Class Loss: 0.0392
Epoch 45/50, Loss: 1.4966, Domain Loss: 1.4655, Class Loss: 0.0312
Epoch 46/50, Loss: 1.4829, Domain Loss: 1.4326, Class Loss: 0.0503
Epoch 47/50, Loss: 1.4773, Domain Loss: 1.4346, Class Loss: 0.0427
Epoch 48/50, Loss: 1.4350, Domain Loss: 1.4124, Class Loss: 0.0226
Epoch 49/50, Loss: 1.4292, Domain Loss: 1.4040, Class Loss: 0.0252
Epoch 50/50, Loss: 1.4520, Domain Loss: 1.4268, Class Loss: 0.0252
98.74


Source performance:
95.90 96.72 95.96 95.73 
Target performance:
96.25 96.99 96.15 96.03 

Per-class target performance: 99.98 99.31 96.34 88.96 Deep CORAL
Deep CORAL Run 1/10
Epoch 1: Source Val Acc = 0.4892, Target Val Acc = 0.5048
Epoch 2: Source Val Acc = 0.2938, Target Val Acc = 0.4670
Epoch 3: Source Val Acc = 0.9862, Target Val Acc = 0.9149
Epoch 4: Source Val Acc = 0.9652, Target Val Acc = 0.9724
Epoch 5: Source Val Acc = 0.2818, Target Val Acc = 0.5743
Epoch 6: Source Val Acc = 0.7494, Target Val Acc = 0.5174
Epoch 7: Source Val Acc = 0.8315, Target Val Acc = 0.8945
Epoch 8: Source Val Acc = 0.9952, Target Val Acc = 0.9814
Epoch 9: Source Val Acc = 0.9946, Target Val Acc = 0.9946
Epoch 10: Source Val Acc = 0.9994, Target Val Acc = 0.9376
Epoch 11: Source Val Acc = 0.9994, Target Val Acc = 0.9652
Epoch 12: Source Val Acc = 0.9982, Target Val Acc = 0.9820
Epoch 13: Source Val Acc = 0.9916, Target Val Acc = 0.9892
Epoch 14: Source Val Acc = 0.8735, Target Val Acc = 0.9814
Epoch 15: Source Val Acc = 0.9958, Target Val Acc = 0.9934
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.9958, Target Val Acc = 0.9934

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.8177, Target Val Acc = 0.8681
Epoch 2: Source Val Acc = 0.9664, Target Val Acc = 0.9694
Epoch 3: Source Val Acc = 0.9670, Target Val Acc = 0.9155
Epoch 4: Source Val Acc = 0.9772, Target Val Acc = 0.9832
Epoch 5: Source Val Acc = 0.7332, Target Val Acc = 0.6517
Epoch 6: Source Val Acc = 0.7452, Target Val Acc = 0.8447
Epoch 7: Source Val Acc = 0.9940, Target Val Acc = 0.9041
Epoch 8: Source Val Acc = 0.9622, Target Val Acc = 0.9682
Epoch 9: Source Val Acc = 0.9886, Target Val Acc = 0.9946
Epoch 10: Source Val Acc = 0.9946, Target Val Acc = 0.9203
Epoch 11: Source Val Acc = 0.8873, Target Val Acc = 0.9329
Epoch 12: Source Val Acc = 0.9970, Target Val Acc = 0.8999
Epoch 13: Source Val Acc = 0.9880, Target Val Acc = 0.9910
Epoch 14: Source Val Acc = 0.9778, Target Val Acc = 0.9760
Epoch 15: Source Val Acc = 0.9814, Target Val Acc = 0.9838
Epoch 16: Source Val Acc = 0.9970, Target Val Acc = 0.8915
Epoch 17: Source Val Acc = 0.9976, Target Val Acc = 0.9844
Epoch 18: Source Val Acc = 0.9988, Target Val Acc = 0.9790
Epoch 19: Source Val Acc = 0.9922, Target Val Acc = 0.9179
Epoch 20: Source Val Acc = 0.9928, Target Val Acc = 0.9934
Epoch 21: Source Val Acc = 0.9994, Target Val Acc = 0.9718
Epoch 22: Source Val Acc = 0.9994, Target Val Acc = 0.9862
Epoch 23: Source Val Acc = 0.9982, Target Val Acc = 0.9862
Epoch 24: Source Val Acc = 0.9976, Target Val Acc = 0.9916
Epoch 25: Source Val Acc = 0.9610, Target Val Acc = 0.9694
Epoch 26: Source Val Acc = 0.9784, Target Val Acc = 0.7938
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9784, Target Val Acc = 0.7938

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.9598, Target Val Acc = 0.9508
Epoch 2: Source Val Acc = 0.4928, Target Val Acc = 0.5582
Epoch 3: Source Val Acc = 0.5060, Target Val Acc = 0.5719
Epoch 4: Source Val Acc = 0.9934, Target Val Acc = 0.9544
Epoch 5: Source Val Acc = 0.5084, Target Val Acc = 0.5492
Epoch 6: Source Val Acc = 0.9802, Target Val Acc = 0.9287
Epoch 7: Source Val Acc = 0.8681, Target Val Acc = 0.9089
Epoch 8: Source Val Acc = 0.9982, Target Val Acc = 0.9874
Epoch 9: Source Val Acc = 0.9898, Target Val Acc = 0.9910
Epoch 10: Source Val Acc = 0.9688, Target Val Acc = 0.9880
Epoch 11: Source Val Acc = 0.9976, Target Val Acc = 0.9784
Epoch 12: Source Val Acc = 0.7968, Target Val Acc = 0.8711
Epoch 13: Source Val Acc = 0.9958, Target Val Acc = 0.9388
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.9958, Target Val Acc = 0.9388

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.9520, Target Val Acc = 0.8741
Epoch 2: Source Val Acc = 0.8255, Target Val Acc = 0.7926
Epoch 3: Source Val Acc = 0.9700, Target Val Acc = 0.9412
Epoch 4: Source Val Acc = 0.9161, Target Val Acc = 0.9418
Epoch 5: Source Val Acc = 0.8513, Target Val Acc = 0.9353
Epoch 6: Source Val Acc = 0.3028, Target Val Acc = 0.4586
Epoch 7: Source Val Acc = 0.8034, Target Val Acc = 0.7434
Epoch 8: Source Val Acc = 0.9904, Target Val Acc = 0.9778
Epoch 9: Source Val Acc = 0.7926, Target Val Acc = 0.6271
Epoch 10: Source Val Acc = 0.9940, Target Val Acc = 0.9790
Epoch 11: Source Val Acc = 0.9964, Target Val Acc = 0.9940
Epoch 12: Source Val Acc = 0.9658, Target Val Acc = 0.9820
Epoch 13: Source Val Acc = 0.9958, Target Val Acc = 0.9976
Epoch 14: Source Val Acc = 0.9964, Target Val Acc = 0.9724
Epoch 15: Source Val Acc = 0.9934, Target Val Acc = 0.9592
Epoch 16: Source Val Acc = 0.9976, Target Val Acc = 0.9874
Epoch 17: Source Val Acc = 0.9958, Target Val Acc = 0.9838
Epoch 18: Source Val Acc = 0.9550, Target Val Acc = 0.9760
Epoch 19: Source Val Acc = 0.7362, Target Val Acc = 0.7770
Epoch 20: Source Val Acc = 0.8519, Target Val Acc = 0.6133
Epoch 21: Source Val Acc = 0.9946, Target Val Acc = 0.9832
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.9946, Target Val Acc = 0.9832

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.2764, Target Val Acc = 0.4143
Epoch 2: Source Val Acc = 0.8627, Target Val Acc = 0.9622
Epoch 3: Source Val Acc = 0.3981, Target Val Acc = 0.4844
Epoch 4: Source Val Acc = 0.7608, Target Val Acc = 0.8339
Epoch 5: Source Val Acc = 0.8855, Target Val Acc = 0.9179
Epoch 6: Source Val Acc = 0.9802, Target Val Acc = 0.9706
Epoch 7: Source Val Acc = 0.7476, Target Val Acc = 0.7608
Epoch 8: Source Val Acc = 0.9730, Target Val Acc = 0.9778
Epoch 9: Source Val Acc = 0.9005, Target Val Acc = 0.9538
Epoch 10: Source Val Acc = 0.9934, Target Val Acc = 0.9772
Epoch 11: Source Val Acc = 0.8034, Target Val Acc = 0.8435
Epoch 12: Source Val Acc = 0.9712, Target Val Acc = 0.9281
Epoch 13: Source Val Acc = 0.9892, Target Val Acc = 0.9634
Epoch 14: Source Val Acc = 0.6133, Target Val Acc = 0.6007
Epoch 15: Source Val Acc = 0.9946, Target Val Acc = 0.9802
Epoch 16: Source Val Acc = 0.9934, Target Val Acc = 0.9868
Epoch 17: Source Val Acc = 0.8789, Target Val Acc = 0.9155
Epoch 18: Source Val Acc = 0.9958, Target Val Acc = 0.9880
Epoch 19: Source Val Acc = 0.9934, Target Val Acc = 0.9934
Epoch 20: Source Val Acc = 0.5564, Target Val Acc = 0.7314
Epoch 21: Source Val Acc = 0.9970, Target Val Acc = 0.9904
Epoch 22: Source Val Acc = 0.9904, Target Val Acc = 0.9772
Epoch 23: Source Val Acc = 0.9970, Target Val Acc = 0.9886
Epoch 24: Source Val Acc = 0.9982, Target Val Acc = 0.9922
Epoch 25: Source Val Acc = 0.9958, Target Val Acc = 0.9718
Epoch 26: Source Val Acc = 0.9988, Target Val Acc = 0.9916
Epoch 27: Source Val Acc = 0.9988, Target Val Acc = 0.9934
Epoch 28: Source Val Acc = 0.9898, Target Val Acc = 0.9868
Epoch 29: Source Val Acc = 0.9994, Target Val Acc = 0.9718
Epoch 30: Source Val Acc = 0.9976, Target Val Acc = 0.9736
Epoch 31: Source Val Acc = 0.9958, Target Val Acc = 0.9371
Epoch 32: Source Val Acc = 0.8267, Target Val Acc = 0.9424
Epoch 33: Source Val Acc = 0.9047, Target Val Acc = 0.9400
Epoch 34: Source Val Acc = 0.9898, Target Val Acc = 0.9862
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.9898, Target Val Acc = 0.9862

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.2656, Target Val Acc = 0.3393
Epoch 2: Source Val Acc = 0.9299, Target Val Acc = 0.9388
Epoch 3: Source Val Acc = 0.5887, Target Val Acc = 0.7596
Epoch 4: Source Val Acc = 0.9718, Target Val Acc = 0.8711
Epoch 5: Source Val Acc = 0.9958, Target Val Acc = 0.8495
Epoch 6: Source Val Acc = 0.9970, Target Val Acc = 0.9155
Epoch 7: Source Val Acc = 0.7452, Target Val Acc = 0.8010
Epoch 8: Source Val Acc = 0.9976, Target Val Acc = 0.9928
Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.9832
Epoch 10: Source Val Acc = 0.9089, Target Val Acc = 0.8135
Epoch 11: Source Val Acc = 0.8951, Target Val Acc = 0.9215
Epoch 12: Source Val Acc = 0.9982, Target Val Acc = 0.9640
Epoch 13: Source Val Acc = 0.7338, Target Val Acc = 0.8327
Epoch 14: Source Val Acc = 0.9616, Target Val Acc = 0.9053
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 0.9616, Target Val Acc = 0.9053

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.7794, Target Val Acc = 0.7434
Epoch 2: Source Val Acc = 0.8237, Target Val Acc = 0.6679
Epoch 3: Source Val Acc = 0.9886, Target Val Acc = 0.9400
Epoch 4: Source Val Acc = 0.9844, Target Val Acc = 0.9898
Epoch 5: Source Val Acc = 0.9916, Target Val Acc = 0.9359
Epoch 6: Source Val Acc = 0.7194, Target Val Acc = 0.6882
Epoch 7: Source Val Acc = 0.9766, Target Val Acc = 0.9688
Epoch 8: Source Val Acc = 0.9832, Target Val Acc = 0.8951
Epoch 9: Source Val Acc = 0.9412, Target Val Acc = 0.9646
Epoch 10: Source Val Acc = 0.9850, Target Val Acc = 0.9892
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.9850, Target Val Acc = 0.9892

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.8837, Target Val Acc = 0.8795
Epoch 2: Source Val Acc = 0.5504, Target Val Acc = 0.6397
Epoch 3: Source Val Acc = 0.9982, Target Val Acc = 0.9730
Epoch 4: Source Val Acc = 0.7356, Target Val Acc = 0.6697
Epoch 5: Source Val Acc = 0.9952, Target Val Acc = 0.9580
Epoch 6: Source Val Acc = 0.9766, Target Val Acc = 0.9724
Epoch 7: Source Val Acc = 0.8237, Target Val Acc = 0.9544
Epoch 8: Source Val Acc = 0.9790, Target Val Acc = 0.9928
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.9790, Target Val Acc = 0.9928

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.8135, Target Val Acc = 0.8735
Epoch 2: Source Val Acc = 0.9490, Target Val Acc = 0.9454
Epoch 3: Source Val Acc = 0.7896, Target Val Acc = 0.8459
Epoch 4: Source Val Acc = 0.9934, Target Val Acc = 0.9862
Epoch 5: Source Val Acc = 0.7518, Target Val Acc = 0.7506
Epoch 6: Source Val Acc = 0.9928, Target Val Acc = 0.9856
Epoch 7: Source Val Acc = 0.5426, Target Val Acc = 0.6978
Epoch 8: Source Val Acc = 0.9694, Target Val Acc = 0.8573
Epoch 9: Source Val Acc = 0.9376, Target Val Acc = 0.9730
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 0.9376, Target Val Acc = 0.9730

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.8699, Target Val Acc = 0.8597
Epoch 2: Source Val Acc = 0.6679, Target Val Acc = 0.5174
Epoch 3: Source Val Acc = 0.9412, Target Val Acc = 0.9430
Epoch 4: Source Val Acc = 0.9958, Target Val Acc = 0.9484
Epoch 5: Source Val Acc = 0.9874, Target Val Acc = 0.9940
Epoch 6: Source Val Acc = 0.7140, Target Val Acc = 0.8207
Epoch 7: Source Val Acc = 0.9946, Target Val Acc = 0.9670
Epoch 8: Source Val Acc = 0.9868, Target Val Acc = 0.9754
Epoch 9: Source Val Acc = 0.9940, Target Val Acc = 0.9778
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.9940, Target Val Acc = 0.9778

Deep CORAL: Average Source Val Acc = 0.9812, Average Target Val Acc = 0.9534
STAR

Run 1/10
Epoch [1/50], Class Loss: 1.7404, Discrepancy Loss: 0.1296
Epoch [2/50], Class Loss: 0.4995, Discrepancy Loss: 0.0504
Epoch [3/50], Class Loss: 0.1216, Discrepancy Loss: 0.0202
Epoch [4/50], Class Loss: 0.1628, Discrepancy Loss: 0.0154
Epoch [5/50], Class Loss: 0.2385, Discrepancy Loss: 0.0299
Epoch [6/50], Class Loss: 0.0585, Discrepancy Loss: 0.0140
Epoch [7/50], Class Loss: 0.0933, Discrepancy Loss: 0.0104
Epoch [8/50], Class Loss: 0.2657, Discrepancy Loss: 0.0329
Epoch [9/50], Class Loss: 0.0653, Discrepancy Loss: 0.0127
Epoch [10/50], Class Loss: 0.0605, Discrepancy Loss: 0.0096
Epoch [11/50], Class Loss: 0.0299, Discrepancy Loss: 0.0070
Epoch [12/50], Class Loss: 0.0148, Discrepancy Loss: 0.0064
Epoch [13/50], Class Loss: 0.0176, Discrepancy Loss: 0.0070
Epoch [14/50], Class Loss: 0.0225, Discrepancy Loss: 0.0057
Epoch [15/50], Class Loss: 0.0121, Discrepancy Loss: 0.0070
Epoch [16/50], Class Loss: 0.0220, Discrepancy Loss: 0.0055
Epoch [17/50], Class Loss: 0.0156, Discrepancy Loss: 0.0083
Epoch [18/50], Class Loss: 0.0115, Discrepancy Loss: 0.0052
Epoch [19/50], Class Loss: 0.0126, Discrepancy Loss: 0.0058
Epoch [20/50], Class Loss: 0.0162, Discrepancy Loss: 0.0065
Epoch [21/50], Class Loss: 0.0130, Discrepancy Loss: 0.0063
Epoch [22/50], Class Loss: 0.0146, Discrepancy Loss: 0.0066
Epoch [23/50], Class Loss: 0.0116, Discrepancy Loss: 0.0070
Epoch [24/50], Class Loss: 0.0093, Discrepancy Loss: 0.0044
Epoch [25/50], Class Loss: 0.1270, Discrepancy Loss: 0.0056
Epoch [26/50], Class Loss: 0.0092, Discrepancy Loss: 0.0056
Epoch [27/50], Class Loss: 0.0498, Discrepancy Loss: 0.0063
Epoch [28/50], Class Loss: 0.0149, Discrepancy Loss: 0.0064
Epoch [29/50], Class Loss: 0.0099, Discrepancy Loss: 0.0050
Epoch [30/50], Class Loss: 0.0188, Discrepancy Loss: 0.0068
Epoch [31/50], Class Loss: 0.0110, Discrepancy Loss: 0.0046
Epoch [32/50], Class Loss: 0.0969, Discrepancy Loss: 0.0050
Epoch [33/50], Class Loss: 0.0149, Discrepancy Loss: 0.0055
Epoch [34/50], Class Loss: 0.0286, Discrepancy Loss: 0.0050
Epoch [35/50], Class Loss: 0.0113, Discrepancy Loss: 0.0062
Epoch [36/50], Class Loss: 0.0095, Discrepancy Loss: 0.0059
Epoch [37/50], Class Loss: 0.0118, Discrepancy Loss: 0.0054
Epoch [38/50], Class Loss: 0.0088, Discrepancy Loss: 0.0051
Epoch [39/50], Class Loss: 0.0097, Discrepancy Loss: 0.0052
Epoch [40/50], Class Loss: 0.0366, Discrepancy Loss: 0.0063
Epoch [41/50], Class Loss: 0.0093, Discrepancy Loss: 0.0043
Epoch [42/50], Class Loss: 0.0166, Discrepancy Loss: 0.0058
Epoch [43/50], Class Loss: 0.0093, Discrepancy Loss: 0.0052
Epoch [44/50], Class Loss: 0.0116, Discrepancy Loss: 0.0063
Epoch [45/50], Class Loss: 0.0134, Discrepancy Loss: 0.0058
Epoch [46/50], Class Loss: 0.0131, Discrepancy Loss: 0.0053
Epoch [47/50], Class Loss: 0.0123, Discrepancy Loss: 0.0056
Epoch [48/50], Class Loss: 0.0092, Discrepancy Loss: 0.0069
Epoch [49/50], Class Loss: 0.0095, Discrepancy Loss: 0.0068
Epoch [50/50], Class Loss: 0.0197, Discrepancy Loss: 0.0053
Source Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%
Target Domain Performance - Accuracy: 99.52%, Precision: 99.51%, Recall: 99.52%, F1 Score: 99.52%

Run 2/10
Epoch [1/50], Class Loss: 1.6360, Discrepancy Loss: 0.1291
Epoch [2/50], Class Loss: 0.4380, Discrepancy Loss: 0.0476
Epoch [3/50], Class Loss: 0.1470, Discrepancy Loss: 0.0192
Epoch [4/50], Class Loss: 0.0852, Discrepancy Loss: 0.0156
Epoch [5/50], Class Loss: 0.1310, Discrepancy Loss: 0.0161
Epoch [6/50], Class Loss: 0.1358, Discrepancy Loss: 0.0218
Epoch [7/50], Class Loss: 0.0343, Discrepancy Loss: 0.0130
Epoch [8/50], Class Loss: 0.0148, Discrepancy Loss: 0.0093
Epoch [9/50], Class Loss: 0.0246, Discrepancy Loss: 0.0133
Epoch [10/50], Class Loss: 0.0181, Discrepancy Loss: 0.0088
Epoch [11/50], Class Loss: 0.0064, Discrepancy Loss: 0.0061
Epoch [12/50], Class Loss: 0.0117, Discrepancy Loss: 0.0068
Epoch [13/50], Class Loss: 0.0087, Discrepancy Loss: 0.0077
Epoch [14/50], Class Loss: 0.0074, Discrepancy Loss: 0.0055
Epoch [15/50], Class Loss: 0.0041, Discrepancy Loss: 0.0048
Epoch [16/50], Class Loss: 0.0344, Discrepancy Loss: 0.0049
Epoch [17/50], Class Loss: 0.0328, Discrepancy Loss: 0.0079
Epoch [18/50], Class Loss: 0.0117, Discrepancy Loss: 0.0049
Epoch [19/50], Class Loss: 0.0129, Discrepancy Loss: 0.0041
Epoch [20/50], Class Loss: 0.0106, Discrepancy Loss: 0.0038
Epoch [21/50], Class Loss: 0.0072, Discrepancy Loss: 0.0050
Epoch [22/50], Class Loss: 0.0159, Discrepancy Loss: 0.0048
Epoch [23/50], Class Loss: 0.0051, Discrepancy Loss: 0.0037
Epoch [24/50], Class Loss: 0.0068, Discrepancy Loss: 0.0028
Epoch [25/50], Class Loss: 0.0405, Discrepancy Loss: 0.0045
Epoch [26/50], Class Loss: 0.0066, Discrepancy Loss: 0.0071
Epoch [27/50], Class Loss: 0.0028, Discrepancy Loss: 0.0052
Epoch [28/50], Class Loss: 0.0048, Discrepancy Loss: 0.0037
Epoch [29/50], Class Loss: 0.0746, Discrepancy Loss: 0.0052
Epoch [30/50], Class Loss: 0.0066, Discrepancy Loss: 0.0029
Epoch [31/50], Class Loss: 0.0045, Discrepancy Loss: 0.0026
Epoch [32/50], Class Loss: 0.0048, Discrepancy Loss: 0.0022
Epoch [33/50], Class Loss: 0.0040, Discrepancy Loss: 0.0021
Epoch [34/50], Class Loss: 0.0036, Discrepancy Loss: 0.0037
Epoch [35/50], Class Loss: 0.0331, Discrepancy Loss: 0.0046
Epoch [36/50], Class Loss: 0.0073, Discrepancy Loss: 0.0046
Epoch [37/50], Class Loss: 0.0045, Discrepancy Loss: 0.0046
Epoch [38/50], Class Loss: 0.0071, Discrepancy Loss: 0.0028
Epoch [39/50], Class Loss: 0.0053, Discrepancy Loss: 0.0029
Epoch [40/50], Class Loss: 0.0039, Discrepancy Loss: 0.0043
Epoch [41/50], Class Loss: 0.0045, Discrepancy Loss: 0.0037
Epoch [42/50], Class Loss: 0.0051, Discrepancy Loss: 0.0039
Epoch [43/50], Class Loss: 0.0050, Discrepancy Loss: 0.0028
Epoch [44/50], Class Loss: 0.0243, Discrepancy Loss: 0.0029
Epoch [45/50], Class Loss: 0.0043, Discrepancy Loss: 0.0033
Epoch [46/50], Class Loss: 0.0050, Discrepancy Loss: 0.0035
Epoch [47/50], Class Loss: 0.0024, Discrepancy Loss: 0.0045
Epoch [48/50], Class Loss: 0.0038, Discrepancy Loss: 0.0045
Epoch [49/50], Class Loss: 0.0053, Discrepancy Loss: 0.0041
Epoch [50/50], Class Loss: 0.0351, Discrepancy Loss: 0.0033
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 99.64%, Precision: 99.63%, Recall: 99.65%, F1 Score: 99.64%

Run 3/10
Epoch [1/50], Class Loss: 1.5889, Discrepancy Loss: 0.1385
Epoch [2/50], Class Loss: 0.5573, Discrepancy Loss: 0.0529
Epoch [3/50], Class Loss: 0.2862, Discrepancy Loss: 0.0380
Epoch [4/50], Class Loss: 0.3317, Discrepancy Loss: 0.0439
Epoch [5/50], Class Loss: 0.1089, Discrepancy Loss: 0.0156
Epoch [6/50], Class Loss: 0.1228, Discrepancy Loss: 0.0238
Epoch [7/50], Class Loss: 0.0741, Discrepancy Loss: 0.0164
Epoch [8/50], Class Loss: 0.0472, Discrepancy Loss: 0.0124
Epoch [9/50], Class Loss: 0.0618, Discrepancy Loss: 0.0100
Epoch [10/50], Class Loss: 0.1412, Discrepancy Loss: 0.0163
Epoch [11/50], Class Loss: 0.0313, Discrepancy Loss: 0.0117
Epoch [12/50], Class Loss: 0.0284, Discrepancy Loss: 0.0138
Epoch [13/50], Class Loss: 0.1143, Discrepancy Loss: 0.0130
Epoch [14/50], Class Loss: 0.0562, Discrepancy Loss: 0.0141
Epoch [15/50], Class Loss: 0.0379, Discrepancy Loss: 0.0129
Epoch [16/50], Class Loss: 0.0271, Discrepancy Loss: 0.0093
Epoch [17/50], Class Loss: 0.0251, Discrepancy Loss: 0.0083
Epoch [18/50], Class Loss: 0.0175, Discrepancy Loss: 0.0107
Epoch [19/50], Class Loss: 0.0242, Discrepancy Loss: 0.0078
Epoch [20/50], Class Loss: 0.0236, Discrepancy Loss: 0.0095
Epoch [21/50], Class Loss: 0.0209, Discrepancy Loss: 0.0063
Epoch [22/50], Class Loss: 0.1363, Discrepancy Loss: 0.0056
Epoch [23/50], Class Loss: 0.0339, Discrepancy Loss: 0.0083
Epoch [24/50], Class Loss: 0.0877, Discrepancy Loss: 0.0081
Epoch [25/50], Class Loss: 0.0215, Discrepancy Loss: 0.0083
Epoch [26/50], Class Loss: 0.0265, Discrepancy Loss: 0.0071
Epoch [27/50], Class Loss: 0.0199, Discrepancy Loss: 0.0055
Epoch [28/50], Class Loss: 0.0245, Discrepancy Loss: 0.0086
Epoch [29/50], Class Loss: 0.0598, Discrepancy Loss: 0.0079
Epoch [30/50], Class Loss: 0.0175, Discrepancy Loss: 0.0121
Epoch [31/50], Class Loss: 0.0164, Discrepancy Loss: 0.0069
Epoch [32/50], Class Loss: 0.0153, Discrepancy Loss: 0.0105
Epoch [33/50], Class Loss: 0.0142, Discrepancy Loss: 0.0089
Epoch [34/50], Class Loss: 0.0244, Discrepancy Loss: 0.0057
Epoch [35/50], Class Loss: 0.0237, Discrepancy Loss: 0.0100
Epoch [36/50], Class Loss: 0.0167, Discrepancy Loss: 0.0078
Epoch [37/50], Class Loss: 0.0239, Discrepancy Loss: 0.0069
Epoch [38/50], Class Loss: 0.0216, Discrepancy Loss: 0.0069
Epoch [39/50], Class Loss: 0.0172, Discrepancy Loss: 0.0077
Epoch [40/50], Class Loss: 0.0377, Discrepancy Loss: 0.0099
Epoch [41/50], Class Loss: 0.0232, Discrepancy Loss: 0.0082
Epoch [42/50], Class Loss: 0.0161, Discrepancy Loss: 0.0074
Epoch [43/50], Class Loss: 0.0189, Discrepancy Loss: 0.0085
Epoch [44/50], Class Loss: 0.0171, Discrepancy Loss: 0.0072
Epoch [45/50], Class Loss: 0.0144, Discrepancy Loss: 0.0094
Epoch [46/50], Class Loss: 0.0165, Discrepancy Loss: 0.0073
Epoch [47/50], Class Loss: 0.0209, Discrepancy Loss: 0.0066
Epoch [48/50], Class Loss: 0.0139, Discrepancy Loss: 0.0071
Epoch [49/50], Class Loss: 0.0148, Discrepancy Loss: 0.0057
Epoch [50/50], Class Loss: 0.0243, Discrepancy Loss: 0.0107
Source Domain Performance - Accuracy: 99.46%, Precision: 99.45%, Recall: 99.46%, F1 Score: 99.46%
Target Domain Performance - Accuracy: 99.52%, Precision: 99.52%, Recall: 99.52%, F1 Score: 99.52%

Run 4/10
Epoch [1/50], Class Loss: 1.9199, Discrepancy Loss: 0.1413
Epoch [2/50], Class Loss: 1.2660, Discrepancy Loss: 0.1086
Epoch [3/50], Class Loss: 0.4193, Discrepancy Loss: 0.0429
Epoch [4/50], Class Loss: 0.2259, Discrepancy Loss: 0.0278
Epoch [5/50], Class Loss: 0.3022, Discrepancy Loss: 0.0272
Epoch [6/50], Class Loss: 0.1547, Discrepancy Loss: 0.0297
Epoch [7/50], Class Loss: 0.0624, Discrepancy Loss: 0.0133
Epoch [8/50], Class Loss: 0.0613, Discrepancy Loss: 0.0165
Epoch [9/50], Class Loss: 0.0439, Discrepancy Loss: 0.0112
Epoch [10/50], Class Loss: 0.0810, Discrepancy Loss: 0.0129
Epoch [11/50], Class Loss: 0.0538, Discrepancy Loss: 0.0108
Epoch [12/50], Class Loss: 0.0421, Discrepancy Loss: 0.0083
Epoch [13/50], Class Loss: 0.0341, Discrepancy Loss: 0.0092
Epoch [14/50], Class Loss: 0.0195, Discrepancy Loss: 0.0077
Epoch [15/50], Class Loss: 0.0220, Discrepancy Loss: 0.0048
Epoch [16/50], Class Loss: 0.0198, Discrepancy Loss: 0.0079
Epoch [17/50], Class Loss: 0.0156, Discrepancy Loss: 0.0071
Epoch [18/50], Class Loss: 0.0156, Discrepancy Loss: 0.0070
Epoch [19/50], Class Loss: 0.0150, Discrepancy Loss: 0.0055
Epoch [20/50], Class Loss: 0.0231, Discrepancy Loss: 0.0048
Epoch [21/50], Class Loss: 0.0128, Discrepancy Loss: 0.0076
Epoch [22/50], Class Loss: 0.0164, Discrepancy Loss: 0.0059
Epoch [23/50], Class Loss: 0.0132, Discrepancy Loss: 0.0075
Epoch [24/50], Class Loss: 0.0087, Discrepancy Loss: 0.0057
Epoch [25/50], Class Loss: 0.0188, Discrepancy Loss: 0.0063
Epoch [26/50], Class Loss: 0.0063, Discrepancy Loss: 0.0044
Epoch [27/50], Class Loss: 0.0172, Discrepancy Loss: 0.0035
Epoch [28/50], Class Loss: 0.1881, Discrepancy Loss: 0.0052
Epoch [29/50], Class Loss: 0.0162, Discrepancy Loss: 0.0063
Epoch [30/50], Class Loss: 0.0135, Discrepancy Loss: 0.0044
Epoch [31/50], Class Loss: 0.0118, Discrepancy Loss: 0.0036
Epoch [32/50], Class Loss: 0.0140, Discrepancy Loss: 0.0049
Epoch [33/50], Class Loss: 0.0110, Discrepancy Loss: 0.0067
Epoch [34/50], Class Loss: 0.0528, Discrepancy Loss: 0.0061
Epoch [35/50], Class Loss: 0.0889, Discrepancy Loss: 0.0072
Epoch [36/50], Class Loss: 0.0106, Discrepancy Loss: 0.0037
Epoch [37/50], Class Loss: 0.0098, Discrepancy Loss: 0.0048
Epoch [38/50], Class Loss: 0.0140, Discrepancy Loss: 0.0070
Epoch [39/50], Class Loss: 0.0141, Discrepancy Loss: 0.0056
Epoch [40/50], Class Loss: 0.0099, Discrepancy Loss: 0.0066
Epoch [41/50], Class Loss: 0.0241, Discrepancy Loss: 0.0047
Epoch [42/50], Class Loss: 0.0084, Discrepancy Loss: 0.0058
Epoch [43/50], Class Loss: 0.0157, Discrepancy Loss: 0.0043
Epoch [44/50], Class Loss: 0.0129, Discrepancy Loss: 0.0050
Epoch [45/50], Class Loss: 0.0140, Discrepancy Loss: 0.0055
Epoch [46/50], Class Loss: 0.0094, Discrepancy Loss: 0.0072
Epoch [47/50], Class Loss: 0.0156, Discrepancy Loss: 0.0038
Epoch [48/50], Class Loss: 0.0114, Discrepancy Loss: 0.0047
Epoch [49/50], Class Loss: 0.0257, Discrepancy Loss: 0.0056
Epoch [50/50], Class Loss: 0.0113, Discrepancy Loss: 0.0042
Source Domain Performance - Accuracy: 99.58%, Precision: 99.58%, Recall: 99.58%, F1 Score: 99.58%
Target Domain Performance - Accuracy: 98.98%, Precision: 98.96%, Recall: 99.01%, F1 Score: 98.97%

Run 5/10
Epoch [1/50], Class Loss: 1.7709, Discrepancy Loss: 0.1249
Epoch [2/50], Class Loss: 0.3210, Discrepancy Loss: 0.0467
Epoch [3/50], Class Loss: 0.2872, Discrepancy Loss: 0.0303
Epoch [4/50], Class Loss: 0.1336, Discrepancy Loss: 0.0163
Epoch [5/50], Class Loss: 0.0938, Discrepancy Loss: 0.0164
Epoch [6/50], Class Loss: 0.0362, Discrepancy Loss: 0.0106
Epoch [7/50], Class Loss: 0.0586, Discrepancy Loss: 0.0116
Epoch [8/50], Class Loss: 0.0155, Discrepancy Loss: 0.0084
Epoch [9/50], Class Loss: 0.1941, Discrepancy Loss: 0.0159
Epoch [10/50], Class Loss: 0.1474, Discrepancy Loss: 0.0266
Epoch [11/50], Class Loss: 0.0388, Discrepancy Loss: 0.0100
Epoch [12/50], Class Loss: 0.0379, Discrepancy Loss: 0.0099
Epoch [13/50], Class Loss: 0.0299, Discrepancy Loss: 0.0082
Epoch [14/50], Class Loss: 0.0880, Discrepancy Loss: 0.0091
Epoch [15/50], Class Loss: 0.0218, Discrepancy Loss: 0.0081
Epoch [16/50], Class Loss: 0.0226, Discrepancy Loss: 0.0098
Epoch [17/50], Class Loss: 0.0221, Discrepancy Loss: 0.0112
Epoch [18/50], Class Loss: 0.0518, Discrepancy Loss: 0.0063
Epoch [19/50], Class Loss: 0.0314, Discrepancy Loss: 0.0098
Epoch [20/50], Class Loss: 0.0209, Discrepancy Loss: 0.0070
Epoch [21/50], Class Loss: 0.0139, Discrepancy Loss: 0.0064
Epoch [22/50], Class Loss: 0.0146, Discrepancy Loss: 0.0073
Epoch [23/50], Class Loss: 0.0114, Discrepancy Loss: 0.0059
Epoch [24/50], Class Loss: 0.0152, Discrepancy Loss: 0.0062
Epoch [25/50], Class Loss: 0.0289, Discrepancy Loss: 0.0066
Epoch [26/50], Class Loss: 0.0154, Discrepancy Loss: 0.0056
Epoch [27/50], Class Loss: 0.0139, Discrepancy Loss: 0.0061
Epoch [28/50], Class Loss: 0.0161, Discrepancy Loss: 0.0057
Epoch [29/50], Class Loss: 0.0148, Discrepancy Loss: 0.0049
Epoch [30/50], Class Loss: 0.0219, Discrepancy Loss: 0.0059
Epoch [31/50], Class Loss: 0.0227, Discrepancy Loss: 0.0058
Epoch [32/50], Class Loss: 0.0125, Discrepancy Loss: 0.0044
Epoch [33/50], Class Loss: 0.0194, Discrepancy Loss: 0.0058
Epoch [34/50], Class Loss: 0.0142, Discrepancy Loss: 0.0063
Epoch [35/50], Class Loss: 0.0155, Discrepancy Loss: 0.0069
Epoch [36/50], Class Loss: 0.0832, Discrepancy Loss: 0.0054
Epoch [37/50], Class Loss: 0.0142, Discrepancy Loss: 0.0046
Epoch [38/50], Class Loss: 0.0190, Discrepancy Loss: 0.0059
Epoch [39/50], Class Loss: 0.0320, Discrepancy Loss: 0.0056
Epoch [40/50], Class Loss: 0.0146, Discrepancy Loss: 0.0064
Epoch [41/50], Class Loss: 0.0137, Discrepancy Loss: 0.0070
Epoch [42/50], Class Loss: 0.0117, Discrepancy Loss: 0.0057
Epoch [43/50], Class Loss: 0.0114, Discrepancy Loss: 0.0058
Epoch [44/50], Class Loss: 0.0249, Discrepancy Loss: 0.0058
Epoch [45/50], Class Loss: 0.0231, Discrepancy Loss: 0.0079
Epoch [46/50], Class Loss: 0.0285, Discrepancy Loss: 0.0060
Epoch [47/50], Class Loss: 0.0204, Discrepancy Loss: 0.0048
Epoch [48/50], Class Loss: 0.0111, Discrepancy Loss: 0.0076
Epoch [49/50], Class Loss: 0.1309, Discrepancy Loss: 0.0047
Epoch [50/50], Class Loss: 0.0099, Discrepancy Loss: 0.0049
Source Domain Performance - Accuracy: 99.64%, Precision: 99.64%, Recall: 99.64%, F1 Score: 99.64%
Target Domain Performance - Accuracy: 99.58%, Precision: 99.59%, Recall: 99.57%, F1 Score: 99.58%

Run 6/10
Epoch [1/50], Class Loss: 1.9466, Discrepancy Loss: 0.1322
Epoch [2/50], Class Loss: 0.2717, Discrepancy Loss: 0.0416
Epoch [3/50], Class Loss: 0.2161, Discrepancy Loss: 0.0266
Epoch [4/50], Class Loss: 0.0812, Discrepancy Loss: 0.0134
Epoch [5/50], Class Loss: 0.0854, Discrepancy Loss: 0.0134
Epoch [6/50], Class Loss: 0.0857, Discrepancy Loss: 0.0080
Epoch [7/50], Class Loss: 0.1073, Discrepancy Loss: 0.0159
Epoch [8/50], Class Loss: 0.0263, Discrepancy Loss: 0.0089
Epoch [9/50], Class Loss: 0.0269, Discrepancy Loss: 0.0091
Epoch [10/50], Class Loss: 0.0276, Discrepancy Loss: 0.0076
Epoch [11/50], Class Loss: 0.0629, Discrepancy Loss: 0.0106
Epoch [12/50], Class Loss: 0.0329, Discrepancy Loss: 0.0062
Epoch [13/50], Class Loss: 0.0128, Discrepancy Loss: 0.0077
Epoch [14/50], Class Loss: 0.1199, Discrepancy Loss: 0.0053
Epoch [15/50], Class Loss: 0.0096, Discrepancy Loss: 0.0050
Epoch [16/50], Class Loss: 0.0079, Discrepancy Loss: 0.0058
Epoch [17/50], Class Loss: 0.0059, Discrepancy Loss: 0.0056
Epoch [18/50], Class Loss: 0.0138, Discrepancy Loss: 0.0055
Epoch [19/50], Class Loss: 0.0240, Discrepancy Loss: 0.0050
Epoch [20/50], Class Loss: 0.0059, Discrepancy Loss: 0.0060
Epoch [21/50], Class Loss: 0.0061, Discrepancy Loss: 0.0055
Epoch [22/50], Class Loss: 0.0072, Discrepancy Loss: 0.0064
Epoch [23/50], Class Loss: 0.0059, Discrepancy Loss: 0.0059
Epoch [24/50], Class Loss: 0.0041, Discrepancy Loss: 0.0056
Epoch [25/50], Class Loss: 0.0047, Discrepancy Loss: 0.0050
Epoch [26/50], Class Loss: 0.0099, Discrepancy Loss: 0.0044
Epoch [27/50], Class Loss: 0.0043, Discrepancy Loss: 0.0046
Epoch [28/50], Class Loss: 0.0121, Discrepancy Loss: 0.0042
Epoch [29/50], Class Loss: 0.0048, Discrepancy Loss: 0.0048
Epoch [30/50], Class Loss: 0.0034, Discrepancy Loss: 0.0063
Epoch [31/50], Class Loss: 0.0080, Discrepancy Loss: 0.0067
Epoch [32/50], Class Loss: 0.0052, Discrepancy Loss: 0.0040
Epoch [33/50], Class Loss: 0.0070, Discrepancy Loss: 0.0040
Epoch [34/50], Class Loss: 0.0097, Discrepancy Loss: 0.0060
Epoch [35/50], Class Loss: 0.0704, Discrepancy Loss: 0.0046
Epoch [36/50], Class Loss: 0.0070, Discrepancy Loss: 0.0043
Epoch [37/50], Class Loss: 0.0122, Discrepancy Loss: 0.0035
Epoch [38/50], Class Loss: 0.0038, Discrepancy Loss: 0.0043
Epoch [39/50], Class Loss: 0.0271, Discrepancy Loss: 0.0048
Epoch [40/50], Class Loss: 0.0071, Discrepancy Loss: 0.0042
Epoch [41/50], Class Loss: 0.0062, Discrepancy Loss: 0.0043
Epoch [42/50], Class Loss: 0.0038, Discrepancy Loss: 0.0061
Epoch [43/50], Class Loss: 0.0043, Discrepancy Loss: 0.0044
Epoch [44/50], Class Loss: 0.1309, Discrepancy Loss: 0.0054
Epoch [45/50], Class Loss: 0.0046, Discrepancy Loss: 0.0056
Epoch [46/50], Class Loss: 0.0042, Discrepancy Loss: 0.0040
Epoch [47/50], Class Loss: 0.0053, Discrepancy Loss: 0.0050
Epoch [48/50], Class Loss: 0.0032, Discrepancy Loss: 0.0047
Epoch [49/50], Class Loss: 0.0051, Discrepancy Loss: 0.0052
Epoch [50/50], Class Loss: 0.0057, Discrepancy Loss: 0.0037
Source Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.76%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 99.70%, Precision: 99.70%, Recall: 99.71%, F1 Score: 99.70%

Run 7/10
Epoch [1/50], Class Loss: 1.4273, Discrepancy Loss: 0.1073
Epoch [2/50], Class Loss: 0.1340, Discrepancy Loss: 0.0230
Epoch [3/50], Class Loss: 0.1722, Discrepancy Loss: 0.0215
Epoch [4/50], Class Loss: 0.0948, Discrepancy Loss: 0.0118
Epoch [5/50], Class Loss: 0.1563, Discrepancy Loss: 0.0175
Epoch [6/50], Class Loss: 0.1059, Discrepancy Loss: 0.0155
Epoch [7/50], Class Loss: 0.2453, Discrepancy Loss: 0.0125
Epoch [8/50], Class Loss: 0.0892, Discrepancy Loss: 0.0170
Epoch [9/50], Class Loss: 0.0311, Discrepancy Loss: 0.0122
Epoch [10/50], Class Loss: 0.0216, Discrepancy Loss: 0.0085
Epoch [11/50], Class Loss: 0.0127, Discrepancy Loss: 0.0080
Epoch [12/50], Class Loss: 0.0101, Discrepancy Loss: 0.0054
Epoch [13/50], Class Loss: 0.0372, Discrepancy Loss: 0.0071
Epoch [14/50], Class Loss: 0.0192, Discrepancy Loss: 0.0057
Epoch [15/50], Class Loss: 0.0175, Discrepancy Loss: 0.0054
Epoch [16/50], Class Loss: 0.1206, Discrepancy Loss: 0.0072
Epoch [17/50], Class Loss: 0.0273, Discrepancy Loss: 0.0063
Epoch [18/50], Class Loss: 0.0121, Discrepancy Loss: 0.0055
Epoch [19/50], Class Loss: 0.0152, Discrepancy Loss: 0.0082
Epoch [20/50], Class Loss: 0.0091, Discrepancy Loss: 0.0044
Epoch [21/50], Class Loss: 0.0084, Discrepancy Loss: 0.0046
Epoch [22/50], Class Loss: 0.0186, Discrepancy Loss: 0.0053
Epoch [23/50], Class Loss: 0.0123, Discrepancy Loss: 0.0044
Epoch [24/50], Class Loss: 0.0237, Discrepancy Loss: 0.0033
Epoch [25/50], Class Loss: 0.0051, Discrepancy Loss: 0.0044
Epoch [26/50], Class Loss: 0.0147, Discrepancy Loss: 0.0045
Epoch [27/50], Class Loss: 0.0129, Discrepancy Loss: 0.0053
Epoch [28/50], Class Loss: 0.0087, Discrepancy Loss: 0.0055
Epoch [29/50], Class Loss: 0.0094, Discrepancy Loss: 0.0076
Epoch [30/50], Class Loss: 0.0059, Discrepancy Loss: 0.0038
Epoch [31/50], Class Loss: 0.0663, Discrepancy Loss: 0.0052
Epoch [32/50], Class Loss: 0.0106, Discrepancy Loss: 0.0068
Epoch [33/50], Class Loss: 0.1062, Discrepancy Loss: 0.0046
Epoch [34/50], Class Loss: 0.0213, Discrepancy Loss: 0.0062
Epoch [35/50], Class Loss: 0.0175, Discrepancy Loss: 0.0045
Epoch [36/50], Class Loss: 0.0192, Discrepancy Loss: 0.0061
Epoch [37/50], Class Loss: 0.0069, Discrepancy Loss: 0.0058
Epoch [38/50], Class Loss: 0.0113, Discrepancy Loss: 0.0043
Epoch [39/50], Class Loss: 0.0084, Discrepancy Loss: 0.0046
Epoch [40/50], Class Loss: 0.0062, Discrepancy Loss: 0.0055
Epoch [41/50], Class Loss: 0.0146, Discrepancy Loss: 0.0060
Epoch [42/50], Class Loss: 0.0090, Discrepancy Loss: 0.0036
Epoch [43/50], Class Loss: 0.0089, Discrepancy Loss: 0.0046
Epoch [44/50], Class Loss: 0.0053, Discrepancy Loss: 0.0051
Epoch [45/50], Class Loss: 0.0083, Discrepancy Loss: 0.0056
Epoch [46/50], Class Loss: 0.0302, Discrepancy Loss: 0.0037
Epoch [47/50], Class Loss: 0.0045, Discrepancy Loss: 0.0040
Epoch [48/50], Class Loss: 0.0244, Discrepancy Loss: 0.0048
Epoch [49/50], Class Loss: 0.0101, Discrepancy Loss: 0.0050
Epoch [50/50], Class Loss: 0.0365, Discrepancy Loss: 0.0080
Source Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%

Run 8/10
Epoch [1/50], Class Loss: 1.8281, Discrepancy Loss: 0.1242
Epoch [2/50], Class Loss: 0.3071, Discrepancy Loss: 0.0474
Epoch [3/50], Class Loss: 0.1827, Discrepancy Loss: 0.0213
Epoch [4/50], Class Loss: 0.1304, Discrepancy Loss: 0.0228
Epoch [5/50], Class Loss: 0.0384, Discrepancy Loss: 0.0176
Epoch [6/50], Class Loss: 0.0746, Discrepancy Loss: 0.0086
Epoch [7/50], Class Loss: 0.1307, Discrepancy Loss: 0.0237
Epoch [8/50], Class Loss: 0.0258, Discrepancy Loss: 0.0115
Epoch [9/50], Class Loss: 0.0250, Discrepancy Loss: 0.0089
Epoch [10/50], Class Loss: 0.0232, Discrepancy Loss: 0.0059
Epoch [11/50], Class Loss: 0.0163, Discrepancy Loss: 0.0073
Epoch [12/50], Class Loss: 0.0081, Discrepancy Loss: 0.0052
Epoch [13/50], Class Loss: 0.0192, Discrepancy Loss: 0.0053
Epoch [14/50], Class Loss: 0.0120, Discrepancy Loss: 0.0053
Epoch [15/50], Class Loss: 0.1008, Discrepancy Loss: 0.0041
Epoch [16/50], Class Loss: 0.0120, Discrepancy Loss: 0.0089
Epoch [17/50], Class Loss: 0.0154, Discrepancy Loss: 0.0061
Epoch [18/50], Class Loss: 0.0143, Discrepancy Loss: 0.0075
Epoch [19/50], Class Loss: 0.0168, Discrepancy Loss: 0.0053
Epoch [20/50], Class Loss: 0.0117, Discrepancy Loss: 0.0054
Epoch [21/50], Class Loss: 0.0069, Discrepancy Loss: 0.0045
Epoch [22/50], Class Loss: 0.0119, Discrepancy Loss: 0.0052
Epoch [23/50], Class Loss: 0.1197, Discrepancy Loss: 0.0038
Epoch [24/50], Class Loss: 0.0812, Discrepancy Loss: 0.0050
Epoch [25/50], Class Loss: 0.0059, Discrepancy Loss: 0.0065
Epoch [26/50], Class Loss: 0.0070, Discrepancy Loss: 0.0038
Epoch [27/50], Class Loss: 0.0092, Discrepancy Loss: 0.0049
Epoch [28/50], Class Loss: 0.0055, Discrepancy Loss: 0.0055
Epoch [29/50], Class Loss: 0.0482, Discrepancy Loss: 0.0054
Epoch [30/50], Class Loss: 0.0086, Discrepancy Loss: 0.0056
Epoch [31/50], Class Loss: 0.0085, Discrepancy Loss: 0.0048
Epoch [32/50], Class Loss: 0.1580, Discrepancy Loss: 0.0038
Epoch [33/50], Class Loss: 0.0068, Discrepancy Loss: 0.0045
Epoch [34/50], Class Loss: 0.0065, Discrepancy Loss: 0.0040
Epoch [35/50], Class Loss: 0.0066, Discrepancy Loss: 0.0044
Epoch [36/50], Class Loss: 0.0118, Discrepancy Loss: 0.0034
Epoch [37/50], Class Loss: 0.0105, Discrepancy Loss: 0.0043
Epoch [38/50], Class Loss: 0.0054, Discrepancy Loss: 0.0065
Epoch [39/50], Class Loss: 0.0050, Discrepancy Loss: 0.0050
Epoch [40/50], Class Loss: 0.0239, Discrepancy Loss: 0.0042
Epoch [41/50], Class Loss: 0.0329, Discrepancy Loss: 0.0049
Epoch [42/50], Class Loss: 0.0059, Discrepancy Loss: 0.0052
Epoch [43/50], Class Loss: 0.0101, Discrepancy Loss: 0.0038
Epoch [44/50], Class Loss: 0.0091, Discrepancy Loss: 0.0049
Epoch [45/50], Class Loss: 0.1265, Discrepancy Loss: 0.0032
Epoch [46/50], Class Loss: 0.0121, Discrepancy Loss: 0.0059
Epoch [47/50], Class Loss: 0.0080, Discrepancy Loss: 0.0049
Epoch [48/50], Class Loss: 0.0101, Discrepancy Loss: 0.0040
Epoch [49/50], Class Loss: 0.0080, Discrepancy Loss: 0.0046
Epoch [50/50], Class Loss: 0.0055, Discrepancy Loss: 0.0048
Source Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.76%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 99.40%, Precision: 99.40%, Recall: 99.41%, F1 Score: 99.40%

Run 9/10
Epoch [1/50], Class Loss: 1.5534, Discrepancy Loss: 0.1135
Epoch [2/50], Class Loss: 0.2893, Discrepancy Loss: 0.0495
Epoch [3/50], Class Loss: 0.1900, Discrepancy Loss: 0.0322
Epoch [4/50], Class Loss: 0.1971, Discrepancy Loss: 0.0194
Epoch [5/50], Class Loss: 0.1111, Discrepancy Loss: 0.0236
Epoch [6/50], Class Loss: 0.0480, Discrepancy Loss: 0.0135
Epoch [7/50], Class Loss: 0.0335, Discrepancy Loss: 0.0104
Epoch [8/50], Class Loss: 0.1285, Discrepancy Loss: 0.0089
Epoch [9/50], Class Loss: 0.1881, Discrepancy Loss: 0.0315
Epoch [10/50], Class Loss: 0.1957, Discrepancy Loss: 0.0266
Epoch [11/50], Class Loss: 0.0536, Discrepancy Loss: 0.0105
Epoch [12/50], Class Loss: 0.0374, Discrepancy Loss: 0.0115
Epoch [13/50], Class Loss: 0.0234, Discrepancy Loss: 0.0145
Epoch [14/50], Class Loss: 0.0345, Discrepancy Loss: 0.0105
Epoch [15/50], Class Loss: 0.0268, Discrepancy Loss: 0.0107
Epoch [16/50], Class Loss: 0.0229, Discrepancy Loss: 0.0081
Epoch [17/50], Class Loss: 0.0206, Discrepancy Loss: 0.0069
Epoch [18/50], Class Loss: 0.0217, Discrepancy Loss: 0.0088
Epoch [19/50], Class Loss: 0.0175, Discrepancy Loss: 0.0089
Epoch [20/50], Class Loss: 0.0266, Discrepancy Loss: 0.0064
Epoch [21/50], Class Loss: 0.0158, Discrepancy Loss: 0.0090
Epoch [22/50], Class Loss: 0.0188, Discrepancy Loss: 0.0060
Epoch [23/50], Class Loss: 0.1562, Discrepancy Loss: 0.0068
Epoch [24/50], Class Loss: 0.0156, Discrepancy Loss: 0.0086
Epoch [25/50], Class Loss: 0.0383, Discrepancy Loss: 0.0055
Epoch [26/50], Class Loss: 0.0223, Discrepancy Loss: 0.0065
Epoch [27/50], Class Loss: 0.0231, Discrepancy Loss: 0.0059
Epoch [28/50], Class Loss: 0.0172, Discrepancy Loss: 0.0076
Epoch [29/50], Class Loss: 0.0146, Discrepancy Loss: 0.0052
Epoch [30/50], Class Loss: 0.0136, Discrepancy Loss: 0.0069
Epoch [31/50], Class Loss: 0.0181, Discrepancy Loss: 0.0080
Epoch [32/50], Class Loss: 0.0130, Discrepancy Loss: 0.0047
Epoch [33/50], Class Loss: 0.0154, Discrepancy Loss: 0.0040
Epoch [34/50], Class Loss: 0.0126, Discrepancy Loss: 0.0060
Epoch [35/50], Class Loss: 0.0189, Discrepancy Loss: 0.0050
Epoch [36/50], Class Loss: 0.0160, Discrepancy Loss: 0.0064
Epoch [37/50], Class Loss: 0.0167, Discrepancy Loss: 0.0055
Epoch [38/50], Class Loss: 0.1101, Discrepancy Loss: 0.0058
Epoch [39/50], Class Loss: 0.0187, Discrepancy Loss: 0.0087
Epoch [40/50], Class Loss: 0.0182, Discrepancy Loss: 0.0085
Epoch [41/50], Class Loss: 0.0171, Discrepancy Loss: 0.0062
Epoch [42/50], Class Loss: 0.0142, Discrepancy Loss: 0.0047
Epoch [43/50], Class Loss: 0.0151, Discrepancy Loss: 0.0052
Epoch [44/50], Class Loss: 0.0164, Discrepancy Loss: 0.0057
Epoch [45/50], Class Loss: 0.0138, Discrepancy Loss: 0.0068
Epoch [46/50], Class Loss: 0.0164, Discrepancy Loss: 0.0052
Epoch [47/50], Class Loss: 0.0176, Discrepancy Loss: 0.0066
Epoch [48/50], Class Loss: 0.0169, Discrepancy Loss: 0.0048
Epoch [49/50], Class Loss: 0.0250, Discrepancy Loss: 0.0055
Epoch [50/50], Class Loss: 0.0147, Discrepancy Loss: 0.0056
Source Domain Performance - Accuracy: 99.34%, Precision: 99.35%, Recall: 99.35%, F1 Score: 99.34%
Target Domain Performance - Accuracy: 99.58%, Precision: 99.58%, Recall: 99.57%, F1 Score: 99.58%

Run 10/10
Epoch [1/50], Class Loss: 1.7162, Discrepancy Loss: 0.0996
Epoch [2/50], Class Loss: 0.3864, Discrepancy Loss: 0.0419
Epoch [3/50], Class Loss: 0.6764, Discrepancy Loss: 0.0612
Epoch [4/50], Class Loss: 0.1207, Discrepancy Loss: 0.0231
Epoch [5/50], Class Loss: 0.0827, Discrepancy Loss: 0.0177
Epoch [6/50], Class Loss: 0.0496, Discrepancy Loss: 0.0171
Epoch [7/50], Class Loss: 0.0456, Discrepancy Loss: 0.0124
Epoch [8/50], Class Loss: 0.0431, Discrepancy Loss: 0.0163
Epoch [9/50], Class Loss: 0.0727, Discrepancy Loss: 0.0171
Epoch [10/50], Class Loss: 0.0391, Discrepancy Loss: 0.0084
Epoch [11/50], Class Loss: 0.0345, Discrepancy Loss: 0.0060
Epoch [12/50], Class Loss: 0.0215, Discrepancy Loss: 0.0083
Epoch [13/50], Class Loss: 0.0155, Discrepancy Loss: 0.0052
Epoch [14/50], Class Loss: 0.0155, Discrepancy Loss: 0.0042
Epoch [15/50], Class Loss: 0.0165, Discrepancy Loss: 0.0054
Epoch [16/50], Class Loss: 0.0093, Discrepancy Loss: 0.0080
Epoch [17/50], Class Loss: 0.0116, Discrepancy Loss: 0.0073
Epoch [18/50], Class Loss: 0.0093, Discrepancy Loss: 0.0071
Epoch [19/50], Class Loss: 0.0080, Discrepancy Loss: 0.0068
Epoch [20/50], Class Loss: 0.0079, Discrepancy Loss: 0.0050
Epoch [21/50], Class Loss: 0.0252, Discrepancy Loss: 0.0048
Epoch [22/50], Class Loss: 0.1211, Discrepancy Loss: 0.0063
Epoch [23/50], Class Loss: 0.0811, Discrepancy Loss: 0.0059
Epoch [24/50], Class Loss: 0.0118, Discrepancy Loss: 0.0087
Epoch [25/50], Class Loss: 0.1695, Discrepancy Loss: 0.0080
Epoch [26/50], Class Loss: 0.0246, Discrepancy Loss: 0.0053
Epoch [27/50], Class Loss: 0.0205, Discrepancy Loss: 0.0056
Epoch [28/50], Class Loss: 0.0096, Discrepancy Loss: 0.0107
Epoch [29/50], Class Loss: 0.0218, Discrepancy Loss: 0.0062
Epoch [30/50], Class Loss: 0.0058, Discrepancy Loss: 0.0071
Epoch [31/50], Class Loss: 0.0211, Discrepancy Loss: 0.0095
Epoch [32/50], Class Loss: 0.0048, Discrepancy Loss: 0.0068
Epoch [33/50], Class Loss: 0.0271, Discrepancy Loss: 0.0051
Epoch [34/50], Class Loss: 0.0295, Discrepancy Loss: 0.0097
Epoch [35/50], Class Loss: 0.0633, Discrepancy Loss: 0.0072
Epoch [36/50], Class Loss: 0.1881, Discrepancy Loss: 0.0067
Epoch [37/50], Class Loss: 0.0138, Discrepancy Loss: 0.0056
Epoch [38/50], Class Loss: 0.0094, Discrepancy Loss: 0.0055
Epoch [39/50], Class Loss: 0.0073, Discrepancy Loss: 0.0060
Epoch [40/50], Class Loss: 0.0532, Discrepancy Loss: 0.0068
Epoch [41/50], Class Loss: 0.0123, Discrepancy Loss: 0.0054
Epoch [42/50], Class Loss: 0.0083, Discrepancy Loss: 0.0078
Epoch [43/50], Class Loss: 0.0083, Discrepancy Loss: 0.0066
Epoch [44/50], Class Loss: 0.0111, Discrepancy Loss: 0.0051
Epoch [45/50], Class Loss: 0.0077, Discrepancy Loss: 0.0049
Epoch [46/50], Class Loss: 0.0069, Discrepancy Loss: 0.0071
Epoch [47/50], Class Loss: 0.0203, Discrepancy Loss: 0.0071
Epoch [48/50], Class Loss: 0.0182, Discrepancy Loss: 0.0076
Epoch [49/50], Class Loss: 0.0153, Discrepancy Loss: 0.0069
Epoch [50/50], Class Loss: 0.0227, Discrepancy Loss: 0.0052
Source Domain Performance - Accuracy: 99.46%, Precision: 99.45%, Recall: 99.47%, F1 Score: 99.46%
Target Domain Performance - Accuracy: 99.70%, Precision: 99.71%, Recall: 99.68%, F1 Score: 99.70%

Source performance: 99.65% 99.65% 99.65% 99.65%
Target performance: 99.54% 99.54% 99.55% 99.54%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 99.79%
16qam: 98.97%
8apsk: 99.42%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.4782, Discrepancy Loss: 0.0201
Validation Loss: 0.4783
Epoch [2/50], Class Loss: 0.0844, Discrepancy Loss: 0.0220
Validation Loss: 0.0699
Epoch [3/50], Class Loss: 0.0705, Discrepancy Loss: 0.0108
Validation Loss: 0.0722
Epoch [4/50], Class Loss: 0.0397, Discrepancy Loss: 0.0150
Validation Loss: 0.1813
Epoch [5/50], Class Loss: 0.0412, Discrepancy Loss: 0.0160
Validation Loss: 1.1873
Epoch [6/50], Class Loss: 0.0701, Discrepancy Loss: 0.0129
Validation Loss: 3.1246
Epoch [7/50], Class Loss: 0.0374, Discrepancy Loss: 0.0186
Validation Loss: 0.0409
Epoch [8/50], Class Loss: 0.0196, Discrepancy Loss: 0.0226
Validation Loss: 0.4020
Epoch [9/50], Class Loss: 0.0694, Discrepancy Loss: 0.0357
Validation Loss: 1.1861
Epoch [10/50], Class Loss: 0.0571, Discrepancy Loss: 0.0440
Validation Loss: 1.8169
Epoch [11/50], Class Loss: 0.0053, Discrepancy Loss: 0.0120
Validation Loss: 0.0017
Epoch [12/50], Class Loss: 0.0015, Discrepancy Loss: 0.0109
Validation Loss: 0.0031
Epoch [13/50], Class Loss: 0.0037, Discrepancy Loss: 0.0113
Validation Loss: 0.0014
Epoch [14/50], Class Loss: 0.0010, Discrepancy Loss: 0.0123
Validation Loss: 0.0032
Epoch [15/50], Class Loss: 0.0231, Discrepancy Loss: 0.0109
Validation Loss: 0.0032
Epoch [16/50], Class Loss: 0.0023, Discrepancy Loss: 0.0133
Validation Loss: 0.0019
Epoch [17/50], Class Loss: 0.0025, Discrepancy Loss: 0.0116
Validation Loss: 0.0060
Epoch [18/50], Class Loss: 0.0129, Discrepancy Loss: 0.0126
Validation Loss: 0.0020
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 86.87%, Precision: 89.09%, Recall: 87.36%, F1 Score: 86.95%

Run 2/10
Epoch [1/50], Class Loss: 0.5452, Discrepancy Loss: 0.0369
Validation Loss: 12.5613
Epoch [2/50], Class Loss: 0.0582, Discrepancy Loss: 0.0277
Validation Loss: 1.5198
Epoch [3/50], Class Loss: 0.0518, Discrepancy Loss: 0.0163
Validation Loss: 0.1594
Epoch [4/50], Class Loss: 0.0783, Discrepancy Loss: 0.0230
Validation Loss: 1.8243
Epoch [5/50], Class Loss: 0.0555, Discrepancy Loss: 0.0181
Validation Loss: 0.5534
Epoch [6/50], Class Loss: 0.0503, Discrepancy Loss: 0.0082
Validation Loss: 1.3924
Epoch [7/50], Class Loss: 0.0251, Discrepancy Loss: 0.0111
Validation Loss: 2.4413
Epoch [8/50], Class Loss: 0.0375, Discrepancy Loss: 0.0181
Validation Loss: 0.0602
Epoch [9/50], Class Loss: 0.3845, Discrepancy Loss: 0.0365
Validation Loss: 0.2081
Epoch [10/50], Class Loss: 0.0513, Discrepancy Loss: 0.0335
Validation Loss: 0.0161
Epoch [11/50], Class Loss: 0.0053, Discrepancy Loss: 0.0345
Validation Loss: 0.0118
Epoch [12/50], Class Loss: 0.0018, Discrepancy Loss: 0.0066
Validation Loss: 0.0044
Epoch [13/50], Class Loss: 0.0018, Discrepancy Loss: 0.0054
Validation Loss: 0.0035
Epoch [14/50], Class Loss: 0.0011, Discrepancy Loss: 0.0059
Validation Loss: 0.0043
Epoch [15/50], Class Loss: 0.0007, Discrepancy Loss: 0.0075
Validation Loss: 0.0020
Epoch [16/50], Class Loss: 0.0010, Discrepancy Loss: 0.0109
Validation Loss: 0.0073
Epoch [17/50], Class Loss: 0.0011, Discrepancy Loss: 0.0125
Validation Loss: 0.0036
Epoch [18/50], Class Loss: 0.0023, Discrepancy Loss: 0.0146
Validation Loss: 0.0056
Epoch [19/50], Class Loss: 0.0028, Discrepancy Loss: 0.0159
Validation Loss: 0.0065
Epoch [20/50], Class Loss: 0.0107, Discrepancy Loss: 0.0181
Validation Loss: 0.0025
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 90.77%, Precision: 92.33%, Recall: 91.11%, F1 Score: 90.90%

Run 3/10
Epoch [1/50], Class Loss: 0.5611, Discrepancy Loss: 0.0278
Validation Loss: 8.6781
Epoch [2/50], Class Loss: 0.0963, Discrepancy Loss: 0.0240
Validation Loss: 0.2171
Epoch [3/50], Class Loss: 0.0722, Discrepancy Loss: 0.0154
Validation Loss: 1.7682
Epoch [4/50], Class Loss: 0.0399, Discrepancy Loss: 0.0181
Validation Loss: 0.3384
Epoch [5/50], Class Loss: 0.1602, Discrepancy Loss: 0.0393
Validation Loss: 0.2169
Epoch [6/50], Class Loss: 0.0405, Discrepancy Loss: 0.0258
Validation Loss: 0.1975
Epoch [7/50], Class Loss: 0.0456, Discrepancy Loss: 0.0228
Validation Loss: 6.1328
Epoch [8/50], Class Loss: 0.1604, Discrepancy Loss: 0.0191
Validation Loss: 1.2368
Epoch [9/50], Class Loss: 0.0141, Discrepancy Loss: 0.0207
Validation Loss: 0.0122
Epoch [10/50], Class Loss: 0.0117, Discrepancy Loss: 0.0305
Validation Loss: 1.4674
Epoch [11/50], Class Loss: 0.0047, Discrepancy Loss: 0.0152
Validation Loss: 0.0088
Epoch [12/50], Class Loss: 0.0023, Discrepancy Loss: 0.0034
Validation Loss: 0.0045
Epoch [13/50], Class Loss: 0.0014, Discrepancy Loss: 0.0041
Validation Loss: 0.0029
Epoch [14/50], Class Loss: 0.0006, Discrepancy Loss: 0.0046
Validation Loss: 0.0027
Epoch [15/50], Class Loss: 0.0010, Discrepancy Loss: 0.0040
Validation Loss: 0.0035
Epoch [16/50], Class Loss: 0.0135, Discrepancy Loss: 0.0028
Validation Loss: 0.0018
Epoch [17/50], Class Loss: 0.0009, Discrepancy Loss: 0.0046
Validation Loss: 0.0019
Epoch [18/50], Class Loss: 0.0006, Discrepancy Loss: 0.0049
Validation Loss: 0.0025
Epoch [19/50], Class Loss: 0.0003, Discrepancy Loss: 0.0062
Validation Loss: 0.0037
Epoch [20/50], Class Loss: 0.0005, Discrepancy Loss: 0.0080
Validation Loss: 0.0023
Epoch [21/50], Class Loss: 0.0002, Discrepancy Loss: 0.0081
Validation Loss: 0.0029
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 87.53%, Precision: 90.38%, Recall: 88.02%, F1 Score: 87.51%

Run 4/10
Epoch [1/50], Class Loss: 0.5179, Discrepancy Loss: 0.0267
Validation Loss: 8.0304
Epoch [2/50], Class Loss: 0.0661, Discrepancy Loss: 0.0087
Validation Loss: 5.2810
Epoch [3/50], Class Loss: 0.0534, Discrepancy Loss: 0.0185
Validation Loss: 4.2733
Epoch [4/50], Class Loss: 0.0287, Discrepancy Loss: 0.0245
Validation Loss: 0.6772
Epoch [5/50], Class Loss: 0.0397, Discrepancy Loss: 0.0402
Validation Loss: 0.0911
Epoch [6/50], Class Loss: 0.0278, Discrepancy Loss: 0.0515
Validation Loss: 0.9641
Epoch [7/50], Class Loss: 0.0423, Discrepancy Loss: 0.0193
Validation Loss: 1.2129
Epoch [8/50], Class Loss: 0.0305, Discrepancy Loss: 0.0396
Validation Loss: 1.3113
Epoch [9/50], Class Loss: 0.0395, Discrepancy Loss: 0.0271
Validation Loss: 0.1210
Epoch [10/50], Class Loss: 0.0299, Discrepancy Loss: 0.0322
Validation Loss: 12.4409
Early stopping!
Source Domain Performance - Accuracy: 58.45%, Precision: 71.20%, Recall: 59.15%, F1 Score: 52.19%
Target Domain Performance - Accuracy: 75.66%, Precision: 86.60%, Recall: 74.48%, F1 Score: 67.61%

Run 5/10
Epoch [1/50], Class Loss: 0.5420, Discrepancy Loss: 0.0243
Validation Loss: 1.8269
Epoch [2/50], Class Loss: 0.0592, Discrepancy Loss: 0.0207
Validation Loss: 0.0606
Epoch [3/50], Class Loss: 0.0429, Discrepancy Loss: 0.0158
Validation Loss: 1.4809
Epoch [4/50], Class Loss: 0.0947, Discrepancy Loss: 0.0108
Validation Loss: 0.2973
Epoch [5/50], Class Loss: 0.0328, Discrepancy Loss: 0.0157
Validation Loss: 0.3437
Epoch [6/50], Class Loss: 0.0364, Discrepancy Loss: 0.0111
Validation Loss: 0.3029
Epoch [7/50], Class Loss: 0.1013, Discrepancy Loss: 0.0117
Validation Loss: 0.0599
Epoch [8/50], Class Loss: 0.0298, Discrepancy Loss: 0.0142
Validation Loss: 4.8250
Epoch [9/50], Class Loss: 0.0160, Discrepancy Loss: 0.0074
Validation Loss: 0.0159
Epoch [10/50], Class Loss: 0.0193, Discrepancy Loss: 0.0097
Validation Loss: 2.5756
Epoch [11/50], Class Loss: 0.0121, Discrepancy Loss: 0.0035
Validation Loss: 0.0090
Epoch [12/50], Class Loss: 0.0576, Discrepancy Loss: 0.0020
Validation Loss: 0.0010
Epoch [13/50], Class Loss: 0.0013, Discrepancy Loss: 0.0029
Validation Loss: 0.0009
Epoch [14/50], Class Loss: 0.0004, Discrepancy Loss: 0.0029
Validation Loss: 0.0009
Epoch [15/50], Class Loss: 0.0007, Discrepancy Loss: 0.0026
Validation Loss: 0.0007
Epoch [16/50], Class Loss: 0.0011, Discrepancy Loss: 0.0032
Validation Loss: 0.0016
Epoch [17/50], Class Loss: 0.0023, Discrepancy Loss: 0.0051
Validation Loss: 0.0033
Epoch [18/50], Class Loss: 0.0003, Discrepancy Loss: 0.0049
Validation Loss: 0.0035
Epoch [19/50], Class Loss: 0.0008, Discrepancy Loss: 0.0072
Validation Loss: 0.0127
Epoch [20/50], Class Loss: 0.0023, Discrepancy Loss: 0.0067
Validation Loss: 0.0027
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 95.38%, Precision: 95.79%, Recall: 95.56%, F1 Score: 95.49%

Run 6/10
Epoch [1/50], Class Loss: 0.4118, Discrepancy Loss: 0.0217
Validation Loss: 0.9405
Epoch [2/50], Class Loss: 0.1422, Discrepancy Loss: 0.0179
Validation Loss: 2.2145
Epoch [3/50], Class Loss: 0.0760, Discrepancy Loss: 0.0157
Validation Loss: 0.0414
Epoch [4/50], Class Loss: 0.0891, Discrepancy Loss: 0.0087
Validation Loss: 0.3524
Epoch [5/50], Class Loss: 0.0953, Discrepancy Loss: 0.0172
Validation Loss: 0.1803
Epoch [6/50], Class Loss: 0.0237, Discrepancy Loss: 0.0138
Validation Loss: 0.7013
Epoch [7/50], Class Loss: 0.0823, Discrepancy Loss: 0.0265
Validation Loss: 6.8872
Epoch [8/50], Class Loss: 0.0355, Discrepancy Loss: 0.0215
Validation Loss: 0.0193
Epoch [9/50], Class Loss: 0.0274, Discrepancy Loss: 0.0442
Validation Loss: 1.9695
Epoch [10/50], Class Loss: 0.0291, Discrepancy Loss: 0.0387
Validation Loss: 4.0394
Epoch [11/50], Class Loss: 0.0048, Discrepancy Loss: 0.0167
Validation Loss: 0.0294
Epoch [12/50], Class Loss: 0.0035, Discrepancy Loss: 0.0124
Validation Loss: 0.0218
Epoch [13/50], Class Loss: 0.0020, Discrepancy Loss: 0.0091
Validation Loss: 0.0183
Epoch [14/50], Class Loss: 0.0014, Discrepancy Loss: 0.0095
Validation Loss: 0.0130
Epoch [15/50], Class Loss: 0.0006, Discrepancy Loss: 0.0100
Validation Loss: 0.0200
Epoch [16/50], Class Loss: 0.0013, Discrepancy Loss: 0.0090
Validation Loss: 0.0149
Epoch [17/50], Class Loss: 0.0020, Discrepancy Loss: 0.0094
Validation Loss: 0.0073
Epoch [18/50], Class Loss: 0.0010, Discrepancy Loss: 0.0075
Validation Loss: 0.0171
Epoch [19/50], Class Loss: 0.0017, Discrepancy Loss: 0.0064
Validation Loss: 0.0129
Epoch [20/50], Class Loss: 0.0011, Discrepancy Loss: 0.0043
Validation Loss: 0.0129
Epoch [21/50], Class Loss: 0.0009, Discrepancy Loss: 0.0036
Validation Loss: 0.0109
Epoch [22/50], Class Loss: 0.0015, Discrepancy Loss: 0.0030
Validation Loss: 0.0127
Early stopping!
Source Domain Performance - Accuracy: 99.70%, Precision: 99.71%, Recall: 99.70%, F1 Score: 99.70%
Target Domain Performance - Accuracy: 82.43%, Precision: 86.81%, Recall: 83.11%, F1 Score: 82.06%

Run 7/10
Epoch [1/50], Class Loss: 0.4730, Discrepancy Loss: 0.0454
Validation Loss: 18.3168
Epoch [2/50], Class Loss: 0.1455, Discrepancy Loss: 0.0185
Validation Loss: 2.3837
Epoch [3/50], Class Loss: 0.0689, Discrepancy Loss: 0.0135
Validation Loss: 0.0340
Epoch [4/50], Class Loss: 0.0638, Discrepancy Loss: 0.0128
Validation Loss: 0.0490
Epoch [5/50], Class Loss: 0.0295, Discrepancy Loss: 0.0221
Validation Loss: 0.2232
Epoch [6/50], Class Loss: 0.0358, Discrepancy Loss: 0.0265
Validation Loss: 1.2875
Epoch [7/50], Class Loss: 0.0286, Discrepancy Loss: 0.0128
Validation Loss: 18.0657
Epoch [8/50], Class Loss: 0.0248, Discrepancy Loss: 0.0145
Validation Loss: 0.1375
Early stopping!
Source Domain Performance - Accuracy: 98.62%, Precision: 98.62%, Recall: 98.64%, F1 Score: 98.62%
Target Domain Performance - Accuracy: 97.42%, Precision: 97.52%, Recall: 97.42%, F1 Score: 97.42%

Run 8/10
Epoch [1/50], Class Loss: 0.4718, Discrepancy Loss: 0.0229
Validation Loss: 1.1480
Epoch [2/50], Class Loss: 0.0870, Discrepancy Loss: 0.0097
Validation Loss: 0.0471
Epoch [3/50], Class Loss: 0.0781, Discrepancy Loss: 0.0109
Validation Loss: 0.3436
Epoch [4/50], Class Loss: 0.0297, Discrepancy Loss: 0.0125
Validation Loss: 1.3306
Epoch [5/50], Class Loss: 0.0496, Discrepancy Loss: 0.0235
Validation Loss: 2.8684
Epoch [6/50], Class Loss: 0.0257, Discrepancy Loss: 0.0252
Validation Loss: 0.5856
Epoch [7/50], Class Loss: 0.0608, Discrepancy Loss: 0.0186
Validation Loss: 2.3347
Early stopping!
Source Domain Performance - Accuracy: 74.52%, Precision: 58.41%, Recall: 74.65%, F1 Score: 64.81%
Target Domain Performance - Accuracy: 72.72%, Precision: 59.79%, Recall: 72.27%, F1 Score: 64.29%

Run 9/10
Epoch [1/50], Class Loss: 0.5279, Discrepancy Loss: 0.0379
Validation Loss: 8.1075
Epoch [2/50], Class Loss: 0.1019, Discrepancy Loss: 0.0162
Validation Loss: 0.8092
Epoch [3/50], Class Loss: 0.0330, Discrepancy Loss: 0.0237
Validation Loss: 0.0551
Epoch [4/50], Class Loss: 0.0511, Discrepancy Loss: 0.0238
Validation Loss: 0.3488
Epoch [5/50], Class Loss: 0.0414, Discrepancy Loss: 0.0230
Validation Loss: 0.0799
Epoch [6/50], Class Loss: 0.0900, Discrepancy Loss: 0.0224
Validation Loss: 1.9129
Epoch [7/50], Class Loss: 0.0143, Discrepancy Loss: 0.0473
Validation Loss: 0.1513
Epoch [8/50], Class Loss: 0.0124, Discrepancy Loss: 0.0231
Validation Loss: 0.0357
Epoch [9/50], Class Loss: 0.0077, Discrepancy Loss: 0.0384
Validation Loss: 1.0309
Epoch [10/50], Class Loss: 0.0217, Discrepancy Loss: 0.0212
Validation Loss: 0.0296
Epoch [11/50], Class Loss: 0.0098, Discrepancy Loss: 0.0147
Validation Loss: 0.0015
Epoch [12/50], Class Loss: 0.0016, Discrepancy Loss: 0.0092
Validation Loss: 0.0030
Epoch [13/50], Class Loss: 0.0013, Discrepancy Loss: 0.0121
Validation Loss: 0.0036
Epoch [14/50], Class Loss: 0.0011, Discrepancy Loss: 0.0132
Validation Loss: 0.0027
Epoch [15/50], Class Loss: 0.0014, Discrepancy Loss: 0.0166
Validation Loss: 0.0034
Epoch [16/50], Class Loss: 0.0022, Discrepancy Loss: 0.0174
Validation Loss: 0.0109
Early stopping!
Source Domain Performance - Accuracy: 99.76%, Precision: 99.77%, Recall: 99.76%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 72.84%, Precision: 80.69%, Recall: 73.86%, F1 Score: 71.05%

Run 10/10
Epoch [1/50], Class Loss: 0.4439, Discrepancy Loss: 0.0355
Validation Loss: 6.4262
Epoch [2/50], Class Loss: 0.0820, Discrepancy Loss: 0.0188
Validation Loss: 0.3518
Epoch [3/50], Class Loss: 0.0494, Discrepancy Loss: 0.0188
Validation Loss: 11.5549
Epoch [4/50], Class Loss: 0.0903, Discrepancy Loss: 0.0363
Validation Loss: 0.5178
Epoch [5/50], Class Loss: 0.0473, Discrepancy Loss: 0.0165
Validation Loss: 3.2664
Epoch [6/50], Class Loss: 0.0356, Discrepancy Loss: 0.0130
Validation Loss: 0.1195
Epoch [7/50], Class Loss: 0.0243, Discrepancy Loss: 0.0098
Validation Loss: 0.4105
Epoch [8/50], Class Loss: 0.0143, Discrepancy Loss: 0.0159
Validation Loss: 0.6771
Epoch [9/50], Class Loss: 0.0310, Discrepancy Loss: 0.0175
Validation Loss: 0.4416
Epoch [10/50], Class Loss: 0.0187, Discrepancy Loss: 0.0219
Validation Loss: 3.3838
Epoch [11/50], Class Loss: 0.0027, Discrepancy Loss: 0.0068
Validation Loss: 0.0056
Epoch [12/50], Class Loss: 0.0015, Discrepancy Loss: 0.0096
Validation Loss: 0.0063
Epoch [13/50], Class Loss: 0.0034, Discrepancy Loss: 0.0099
Validation Loss: 0.0010
Epoch [14/50], Class Loss: 0.0023, Discrepancy Loss: 0.0120
Validation Loss: 0.0024
Epoch [15/50], Class Loss: 0.0013, Discrepancy Loss: 0.0131
Validation Loss: 0.0036
Epoch [16/50], Class Loss: 0.0011, Discrepancy Loss: 0.0136
Validation Loss: 0.0058
Epoch [17/50], Class Loss: 0.0008, Discrepancy Loss: 0.0160
Validation Loss: 0.0075
Epoch [18/50], Class Loss: 0.0007, Discrepancy Loss: 0.0187
Validation Loss: 0.0094
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 76.68%, Precision: 83.23%, Recall: 77.54%, F1 Score: 75.66%

Source performance: 93.07% 92.74% 93.15% 91.47%
Target performance: 83.83% 86.22% 84.07% 81.89%

Per-Class Accuracy on Target Domain:
bpsk: 89.24%
qpsk: 68.60%
16qam: 88.25%
8apsk: 90.20%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.3219, JMMD Loss: 0.0605
Validation Loss: 0.0351
Epoch [2/50], Class Loss: 0.0200, JMMD Loss: 0.0679
Validation Loss: 0.1145
Epoch [3/50], Class Loss: 0.0843, JMMD Loss: 0.0639
Validation Loss: 0.9045
Epoch [4/50], Class Loss: 0.1121, JMMD Loss: 0.0521
Validation Loss: 2.6485
Epoch [5/50], Class Loss: 0.0801, JMMD Loss: 0.0455
Validation Loss: 0.0252
Epoch [6/50], Class Loss: 0.0435, JMMD Loss: 0.0527
Validation Loss: 0.0783
Epoch [7/50], Class Loss: 0.0374, JMMD Loss: 0.0393
Validation Loss: 0.0178
Epoch [8/50], Class Loss: 0.0283, JMMD Loss: 0.0448
Validation Loss: 0.9431
Epoch [9/50], Class Loss: 0.0938, JMMD Loss: 0.0507
Validation Loss: 0.0200
Epoch [10/50], Class Loss: 0.0136, JMMD Loss: 0.0495
Validation Loss: 0.0549
Epoch [11/50], Class Loss: 0.0476, JMMD Loss: 0.0559
Validation Loss: 0.0063
Epoch [12/50], Class Loss: 0.0194, JMMD Loss: 0.0424
Validation Loss: 0.0130
Epoch [13/50], Class Loss: 0.0093, JMMD Loss: 0.0486
Validation Loss: 0.0068
Epoch [14/50], Class Loss: 0.0084, JMMD Loss: 0.0488
Validation Loss: 0.0064
Epoch [15/50], Class Loss: 0.0094, JMMD Loss: 0.0455
Validation Loss: 0.0062
Epoch [16/50], Class Loss: 0.0094, JMMD Loss: 0.0512
Validation Loss: 0.0091
Epoch [17/50], Class Loss: 0.0051, JMMD Loss: 0.0461
Validation Loss: 0.0038
Epoch [18/50], Class Loss: 0.0050, JMMD Loss: 0.0573
Validation Loss: 0.0038
Epoch [19/50], Class Loss: 0.0055, JMMD Loss: 0.0443
Validation Loss: 0.0036
Epoch [20/50], Class Loss: 0.0034, JMMD Loss: 0.0425
Validation Loss: 0.0047
Epoch [21/50], Class Loss: 0.0045, JMMD Loss: 0.0474
Validation Loss: 0.0044
Epoch [22/50], Class Loss: 0.0058, JMMD Loss: 0.0465
Validation Loss: 0.0156
Epoch [23/50], Class Loss: 0.0051, JMMD Loss: 0.0438
Validation Loss: 0.0034
Epoch [24/50], Class Loss: 0.0225, JMMD Loss: 0.0480
Validation Loss: 0.0090
Epoch [25/50], Class Loss: 0.0171, JMMD Loss: 0.0456
Validation Loss: 0.0040
Epoch [26/50], Class Loss: 0.0064, JMMD Loss: 0.0554
Validation Loss: 0.0202
Epoch [27/50], Class Loss: 0.0044, JMMD Loss: 0.0488
Validation Loss: 0.0110
Epoch [28/50], Class Loss: 0.0474, JMMD Loss: 0.0539
Validation Loss: 0.0094
Early stopping!
Source Domain Performance - Accuracy: 99.76%, Precision: 99.75%, Recall: 99.76%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 99.76%, Precision: 99.77%, Recall: 99.75%, F1 Score: 99.76%

Run 2/10
Epoch [1/50], Class Loss: 0.2904, JMMD Loss: 0.0590
Validation Loss: 3.0144
Epoch [2/50], Class Loss: 0.0952, JMMD Loss: 0.0566
Validation Loss: 1.7046
Epoch [3/50], Class Loss: 0.0608, JMMD Loss: 0.0574
Validation Loss: 0.1173
Epoch [4/50], Class Loss: 0.0768, JMMD Loss: 0.0580
Validation Loss: 0.4449
Epoch [5/50], Class Loss: 0.0275, JMMD Loss: 0.0516
Validation Loss: 0.0396
Epoch [6/50], Class Loss: 0.0101, JMMD Loss: 0.0503
Validation Loss: 0.0119
Epoch [7/50], Class Loss: 0.0061, JMMD Loss: 0.0516
Validation Loss: 0.0534
Epoch [8/50], Class Loss: 0.0138, JMMD Loss: 0.0578
Validation Loss: 0.0130
Epoch [9/50], Class Loss: 0.0059, JMMD Loss: 0.0573
Validation Loss: 0.0406
Epoch [10/50], Class Loss: 0.0230, JMMD Loss: 0.0439
Validation Loss: 0.2852
Epoch [11/50], Class Loss: 0.0164, JMMD Loss: 0.0480
Validation Loss: 0.0075
Epoch [12/50], Class Loss: 0.0070, JMMD Loss: 0.0494
Validation Loss: 0.0022
Epoch [13/50], Class Loss: 0.0039, JMMD Loss: 0.0518
Validation Loss: 0.0061
Epoch [14/50], Class Loss: 0.0082, JMMD Loss: 0.0492
Validation Loss: 0.0099
Epoch [15/50], Class Loss: 0.0042, JMMD Loss: 0.0498
Validation Loss: 0.0060
Epoch [16/50], Class Loss: 0.0049, JMMD Loss: 0.0600
Validation Loss: 0.0013
Epoch [17/50], Class Loss: 0.0046, JMMD Loss: 0.0507
Validation Loss: 0.0076
Epoch [18/50], Class Loss: 0.0021, JMMD Loss: 0.0428
Validation Loss: 0.0061
Epoch [19/50], Class Loss: 0.0018, JMMD Loss: 0.0405
Validation Loss: 0.0051
Epoch [20/50], Class Loss: 0.0022, JMMD Loss: 0.0410
Validation Loss: 0.0060
Epoch [21/50], Class Loss: 0.0014, JMMD Loss: 0.0415
Validation Loss: 0.0042
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 99.70%, Precision: 99.71%, Recall: 99.71%, F1 Score: 99.71%

Run 3/10
Epoch [1/50], Class Loss: 0.4015, JMMD Loss: 0.0826
Validation Loss: 0.3521
Epoch [2/50], Class Loss: 0.0510, JMMD Loss: 0.0590
Validation Loss: 0.0817
Epoch [3/50], Class Loss: 0.0655, JMMD Loss: 0.0592
Validation Loss: 0.3860
Epoch [4/50], Class Loss: 0.0508, JMMD Loss: 0.0590
Validation Loss: 0.4682
Epoch [5/50], Class Loss: 0.0353, JMMD Loss: 0.0503
Validation Loss: 0.6396
Epoch [6/50], Class Loss: 0.0707, JMMD Loss: 0.0470
Validation Loss: 0.0354
Epoch [7/50], Class Loss: 0.0176, JMMD Loss: 0.0500
Validation Loss: 0.0428
Epoch [8/50], Class Loss: 0.0161, JMMD Loss: 0.0419
Validation Loss: 1.7544
Epoch [9/50], Class Loss: 0.0636, JMMD Loss: 0.0438
Validation Loss: 11.4104
Epoch [10/50], Class Loss: 0.0785, JMMD Loss: 0.0429
Validation Loss: 5.8264
Epoch [11/50], Class Loss: 0.0519, JMMD Loss: 0.0501
Validation Loss: 0.0391
Early stopping!
Source Domain Performance - Accuracy: 98.86%, Precision: 98.85%, Recall: 98.87%, F1 Score: 98.86%
Target Domain Performance - Accuracy: 98.38%, Precision: 98.46%, Recall: 98.41%, F1 Score: 98.41%

Run 4/10
Epoch [1/50], Class Loss: 0.4669, JMMD Loss: 0.0627
Validation Loss: 1.1353
Epoch [2/50], Class Loss: 0.0319, JMMD Loss: 0.0474
Validation Loss: 0.8065
Epoch [3/50], Class Loss: 0.0444, JMMD Loss: 0.0520
Validation Loss: 1.3090
Epoch [4/50], Class Loss: 0.0329, JMMD Loss: 0.0488
Validation Loss: 0.0630
Epoch [5/50], Class Loss: 0.0163, JMMD Loss: 0.0503
Validation Loss: 0.1074
Epoch [6/50], Class Loss: 0.0454, JMMD Loss: 0.0457
Validation Loss: 0.9891
Epoch [7/50], Class Loss: 0.1630, JMMD Loss: 0.0415
Validation Loss: 0.0168
Epoch [8/50], Class Loss: 0.0477, JMMD Loss: 0.0502
Validation Loss: 0.0804
Epoch [9/50], Class Loss: 0.0121, JMMD Loss: 0.0506
Validation Loss: 0.0864
Epoch [10/50], Class Loss: 0.0085, JMMD Loss: 0.0529
Validation Loss: 0.0225
Epoch [11/50], Class Loss: 0.0107, JMMD Loss: 0.0611
Validation Loss: 0.0089
Epoch [12/50], Class Loss: 0.0086, JMMD Loss: 0.0462
Validation Loss: 0.0036
Epoch [13/50], Class Loss: 0.0033, JMMD Loss: 0.0484
Validation Loss: 0.0032
Epoch [14/50], Class Loss: 0.0030, JMMD Loss: 0.0528
Validation Loss: 0.0045
Epoch [15/50], Class Loss: 0.0121, JMMD Loss: 0.0462
Validation Loss: 0.0068
Epoch [16/50], Class Loss: 0.0122, JMMD Loss: 0.0459
Validation Loss: 0.0043
Epoch [17/50], Class Loss: 0.0049, JMMD Loss: 0.0423
Validation Loss: 0.0050
Epoch [18/50], Class Loss: 0.0022, JMMD Loss: 0.0413
Validation Loss: 0.0030
Epoch [19/50], Class Loss: 0.0022, JMMD Loss: 0.0460
Validation Loss: 0.0061
Epoch [20/50], Class Loss: 0.0031, JMMD Loss: 0.0572
Validation Loss: 0.0306
Epoch [21/50], Class Loss: 0.0071, JMMD Loss: 0.0452
Validation Loss: 0.0040
Epoch [22/50], Class Loss: 0.0021, JMMD Loss: 0.0543
Validation Loss: 0.0031
Epoch [23/50], Class Loss: 0.0031, JMMD Loss: 0.0429
Validation Loss: 0.0013
Epoch [24/50], Class Loss: 0.0020, JMMD Loss: 0.0412
Validation Loss: 0.0018
Epoch [25/50], Class Loss: 0.0012, JMMD Loss: 0.0453
Validation Loss: 0.0044
Epoch [26/50], Class Loss: 0.0028, JMMD Loss: 0.0495
Validation Loss: 0.0037
Epoch [27/50], Class Loss: 0.0023, JMMD Loss: 0.0482
Validation Loss: 0.0054
Epoch [28/50], Class Loss: 0.0028, JMMD Loss: 0.0534
Validation Loss: 0.0022
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.64%, Precision: 99.65%, Recall: 99.65%, F1 Score: 99.65%

Run 5/10
Epoch [1/50], Class Loss: 0.3570, JMMD Loss: 0.0509
Validation Loss: 0.2644
Epoch [2/50], Class Loss: 0.0871, JMMD Loss: 0.0532
Validation Loss: 0.2058
Epoch [3/50], Class Loss: 0.0192, JMMD Loss: 0.0434
Validation Loss: 0.8463
Epoch [4/50], Class Loss: 0.0706, JMMD Loss: 0.0521
Validation Loss: 4.3540
Epoch [5/50], Class Loss: 0.0774, JMMD Loss: 0.0546
Validation Loss: 0.0154
Epoch [6/50], Class Loss: 0.0196, JMMD Loss: 0.0596
Validation Loss: 0.0482
Epoch [7/50], Class Loss: 0.0269, JMMD Loss: 0.0562
Validation Loss: 0.9928
Epoch [8/50], Class Loss: 0.1050, JMMD Loss: 0.0457
Validation Loss: 0.1453
Epoch [9/50], Class Loss: 0.1564, JMMD Loss: 0.0390
Validation Loss: 0.2818
Epoch [10/50], Class Loss: 0.0236, JMMD Loss: 0.0465
Validation Loss: 0.1117
Early stopping!
Source Domain Performance - Accuracy: 98.38%, Precision: 98.41%, Recall: 98.39%, F1 Score: 98.39%
Target Domain Performance - Accuracy: 98.86%, Precision: 98.91%, Recall: 98.83%, F1 Score: 98.86%

Run 6/10
Epoch [1/50], Class Loss: 0.3278, JMMD Loss: 0.0546
Validation Loss: 1.4001
Epoch [2/50], Class Loss: 0.0344, JMMD Loss: 0.0571
Validation Loss: 0.9478
Epoch [3/50], Class Loss: 0.0478, JMMD Loss: 0.0533
Validation Loss: 0.1361
Epoch [4/50], Class Loss: 0.0749, JMMD Loss: 0.0538
Validation Loss: 0.0526
Epoch [5/50], Class Loss: 0.0275, JMMD Loss: 0.0505
Validation Loss: 1.0771
Epoch [6/50], Class Loss: 0.0551, JMMD Loss: 0.0574
Validation Loss: 0.0089
Epoch [7/50], Class Loss: 0.0119, JMMD Loss: 0.0491
Validation Loss: 0.1196
Epoch [8/50], Class Loss: 0.0376, JMMD Loss: 0.0460
Validation Loss: 0.0211
Epoch [9/50], Class Loss: 0.0240, JMMD Loss: 0.0520
Validation Loss: 0.0136
Epoch [10/50], Class Loss: 0.0055, JMMD Loss: 0.0460
Validation Loss: 0.0015
Epoch [11/50], Class Loss: 0.0051, JMMD Loss: 0.0501
Validation Loss: 0.0040
Epoch [12/50], Class Loss: 0.0025, JMMD Loss: 0.0440
Validation Loss: 0.0033
Epoch [13/50], Class Loss: 0.0165, JMMD Loss: 0.0483
Validation Loss: 0.0009
Epoch [14/50], Class Loss: 0.0103, JMMD Loss: 0.0480
Validation Loss: 0.0035
Epoch [15/50], Class Loss: 0.0021, JMMD Loss: 0.0477
Validation Loss: 0.0023
Epoch [16/50], Class Loss: 0.0066, JMMD Loss: 0.0513
Validation Loss: 0.0046
Epoch [17/50], Class Loss: 0.0037, JMMD Loss: 0.0495
Validation Loss: 0.0031
Epoch [18/50], Class Loss: 0.0024, JMMD Loss: 0.0458
Validation Loss: 0.0012
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.58%, Precision: 99.59%, Recall: 99.59%, F1 Score: 99.59%

Run 7/10
Epoch [1/50], Class Loss: 0.3969, JMMD Loss: 0.0653
Validation Loss: 0.0982
Epoch [2/50], Class Loss: 0.0974, JMMD Loss: 0.0665
Validation Loss: 0.3123
Epoch [3/50], Class Loss: 0.0568, JMMD Loss: 0.0490
Validation Loss: 2.5117
Epoch [4/50], Class Loss: 0.0412, JMMD Loss: 0.0529
Validation Loss: 0.8974
Epoch [5/50], Class Loss: 0.0289, JMMD Loss: 0.0527
Validation Loss: 0.0283
Epoch [6/50], Class Loss: 0.0127, JMMD Loss: 0.0432
Validation Loss: 0.0391
Epoch [7/50], Class Loss: 0.0132, JMMD Loss: 0.0505
Validation Loss: 0.8777
Epoch [8/50], Class Loss: 0.0249, JMMD Loss: 0.0471
Validation Loss: 1.2903
Epoch [9/50], Class Loss: 0.0427, JMMD Loss: 0.0492
Validation Loss: 0.4255
Epoch [10/50], Class Loss: 0.0424, JMMD Loss: 0.0564
Validation Loss: 0.0707
Early stopping!
Source Domain Performance - Accuracy: 98.86%, Precision: 98.93%, Recall: 98.86%, F1 Score: 98.87%
Target Domain Performance - Accuracy: 98.98%, Precision: 99.01%, Recall: 98.99%, F1 Score: 98.99%

Run 8/10
Epoch [1/50], Class Loss: 0.2552, JMMD Loss: 0.0564
Validation Loss: 0.2105
Epoch [2/50], Class Loss: 0.0392, JMMD Loss: 0.0525
Validation Loss: 0.2799
Epoch [3/50], Class Loss: 0.0646, JMMD Loss: 0.0479
Validation Loss: 0.0714
Epoch [4/50], Class Loss: 0.0985, JMMD Loss: 0.0478
Validation Loss: 0.4127
Epoch [5/50], Class Loss: 0.2308, JMMD Loss: 0.0507
Validation Loss: 0.1161
Epoch [6/50], Class Loss: 0.0431, JMMD Loss: 0.0531
Validation Loss: 0.0287
Epoch [7/50], Class Loss: 0.0190, JMMD Loss: 0.0567
Validation Loss: 0.0807
Epoch [8/50], Class Loss: 0.0153, JMMD Loss: 0.0601
Validation Loss: 0.0201
Epoch [9/50], Class Loss: 0.0111, JMMD Loss: 0.0514
Validation Loss: 0.0054
Epoch [10/50], Class Loss: 0.0384, JMMD Loss: 0.0538
Validation Loss: 0.0413
Epoch [11/50], Class Loss: 0.0142, JMMD Loss: 0.0501
Validation Loss: 0.0255
Epoch [12/50], Class Loss: 0.0060, JMMD Loss: 0.0487
Validation Loss: 0.0036
Epoch [13/50], Class Loss: 0.0065, JMMD Loss: 0.0474
Validation Loss: 0.0054
Epoch [14/50], Class Loss: 0.0087, JMMD Loss: 0.0521
Validation Loss: 0.0041
Epoch [15/50], Class Loss: 0.0076, JMMD Loss: 0.0577
Validation Loss: 0.0056
Epoch [16/50], Class Loss: 0.0064, JMMD Loss: 0.0520
Validation Loss: 0.0039
Epoch [17/50], Class Loss: 0.0289, JMMD Loss: 0.0490
Validation Loss: 0.0079
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 99.16%, Precision: 99.18%, Recall: 99.18%, F1 Score: 99.17%

Run 9/10
Epoch [1/50], Class Loss: 0.3060, JMMD Loss: 0.0612
Validation Loss: 1.2291
Epoch [2/50], Class Loss: 0.0871, JMMD Loss: 0.0606
Validation Loss: 0.3401
Epoch [3/50], Class Loss: 0.0355, JMMD Loss: 0.0518
Validation Loss: 0.0173
Epoch [4/50], Class Loss: 0.0140, JMMD Loss: 0.0553
Validation Loss: 0.0418
Epoch [5/50], Class Loss: 0.0155, JMMD Loss: 0.0616
Validation Loss: 0.0530
Epoch [6/50], Class Loss: 0.0279, JMMD Loss: 0.0495
Validation Loss: 3.3675
Epoch [7/50], Class Loss: 0.0308, JMMD Loss: 0.0428
Validation Loss: 0.0540
Epoch [8/50], Class Loss: 0.0936, JMMD Loss: 0.0510
Validation Loss: 1.2758
Early stopping!
Source Domain Performance - Accuracy: 77.94%, Precision: 80.01%, Recall: 78.27%, F1 Score: 76.96%
Target Domain Performance - Accuracy: 93.35%, Precision: 93.87%, Recall: 93.22%, F1 Score: 93.31%

Run 10/10
Epoch [1/50], Class Loss: 0.4485, JMMD Loss: 0.0590
Validation Loss: 3.0081
Epoch [2/50], Class Loss: 0.1344, JMMD Loss: 0.0491
Validation Loss: 1.0689
Epoch [3/50], Class Loss: 0.0298, JMMD Loss: 0.0501
Validation Loss: 0.9525
Epoch [4/50], Class Loss: 0.0470, JMMD Loss: 0.0466
Validation Loss: 2.3831
Epoch [5/50], Class Loss: 0.0452, JMMD Loss: 0.0675
Validation Loss: 0.3525
Epoch [6/50], Class Loss: 0.0287, JMMD Loss: 0.0432
Validation Loss: 0.9249
Epoch [7/50], Class Loss: 0.0349, JMMD Loss: 0.0509
Validation Loss: 0.0263
Epoch [8/50], Class Loss: 0.0306, JMMD Loss: 0.0617
Validation Loss: 0.0194
Epoch [9/50], Class Loss: 0.0142, JMMD Loss: 0.0480
Validation Loss: 0.0186
Epoch [10/50], Class Loss: 0.0098, JMMD Loss: 0.0509
Validation Loss: 0.0129
Epoch [11/50], Class Loss: 0.0049, JMMD Loss: 0.0604
Validation Loss: 0.0259
Epoch [12/50], Class Loss: 0.0271, JMMD Loss: 0.0442
Validation Loss: 0.0180
Epoch [13/50], Class Loss: 0.0121, JMMD Loss: 0.0498
Validation Loss: 0.0131
Epoch [14/50], Class Loss: 0.0042, JMMD Loss: 0.0533
Validation Loss: 0.0071
Epoch [15/50], Class Loss: 0.0193, JMMD Loss: 0.0428
Validation Loss: 0.0066
Epoch [16/50], Class Loss: 0.0131, JMMD Loss: 0.0488
Validation Loss: 0.0131
Epoch [17/50], Class Loss: 0.0050, JMMD Loss: 0.0473
Validation Loss: 0.0085
Epoch [18/50], Class Loss: 0.0059, JMMD Loss: 0.0498
Validation Loss: 0.0057
Epoch [19/50], Class Loss: 0.0063, JMMD Loss: 0.0471
Validation Loss: 0.0077
Epoch [20/50], Class Loss: 0.0198, JMMD Loss: 0.0564
Validation Loss: 0.0145
Epoch [21/50], Class Loss: 0.0025, JMMD Loss: 0.0484
Validation Loss: 0.0117
Epoch [22/50], Class Loss: 0.0041, JMMD Loss: 0.0504
Validation Loss: 0.0101
Epoch [23/50], Class Loss: 0.0033, JMMD Loss: 0.0527
Validation Loss: 0.0060
Early stopping!
Source Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.76%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 99.46%, Precision: 99.47%, Recall: 99.47%, F1 Score: 99.47%

Source performance: 97.33% 97.54% 97.36% 97.23%
Target performance: 98.69% 98.76% 98.68% 98.69%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 99.78%
  Class 1: 98.90%
  Class 2: 98.48%
  Class 3: 97.55%
Base

Run 1/10
Epoch 1/50, Train Loss: 0.3159, Train Acc: 0.9009, Val Loss: 0.5417, Val Acc: 0.8159
Epoch 2/50, Train Loss: 0.0428, Train Acc: 0.9885, Val Loss: 0.0123, Val Acc: 0.9982
Epoch 3/50, Train Loss: 0.0233, Train Acc: 0.9943, Val Loss: 1.9211, Val Acc: 0.6151
Epoch 4/50, Train Loss: 0.0306, Train Acc: 0.9918, Val Loss: 0.0266, Val Acc: 0.9910
Epoch 5/50, Train Loss: 0.0128, Train Acc: 0.9976, Val Loss: 0.2444, Val Acc: 0.9293
Epoch 6/50, Train Loss: 0.0167, Train Acc: 0.9955, Val Loss: 0.0132, Val Acc: 0.9964
Epoch 7/50, Train Loss: 0.0133, Train Acc: 0.9949, Val Loss: 0.3666, Val Acc: 0.8777
Early stopping!

Run 2/10
Epoch 1/50, Train Loss: 0.3553, Train Acc: 0.8778, Val Loss: 0.5517, Val Acc: 0.8195
Epoch 2/50, Train Loss: 0.0472, Train Acc: 0.9858, Val Loss: 1.3883, Val Acc: 0.7272
Epoch 3/50, Train Loss: 0.0280, Train Acc: 0.9918, Val Loss: 0.4610, Val Acc: 0.8453
Epoch 4/50, Train Loss: 0.0148, Train Acc: 0.9963, Val Loss: 0.1879, Val Acc: 0.9143
Epoch 5/50, Train Loss: 0.0113, Train Acc: 0.9975, Val Loss: 0.0106, Val Acc: 0.9964
Epoch 6/50, Train Loss: 0.0216, Train Acc: 0.9922, Val Loss: 0.1842, Val Acc: 0.9311
Epoch 7/50, Train Loss: 0.0069, Train Acc: 0.9981, Val Loss: 0.0066, Val Acc: 1.0000
Epoch 8/50, Train Loss: 0.0047, Train Acc: 0.9990, Val Loss: 0.0096, Val Acc: 0.9982
Epoch 9/50, Train Loss: 0.0158, Train Acc: 0.9949, Val Loss: 1.2060, Val Acc: 0.6529
Epoch 10/50, Train Loss: 0.0100, Train Acc: 0.9960, Val Loss: 0.0082, Val Acc: 0.9982
Epoch 11/50, Train Loss: 0.0033, Train Acc: 0.9994, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0029, Train Acc: 0.9990, Val Loss: 0.0028, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0014, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0026, Train Acc: 0.9993, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0060, Train Acc: 0.9991, Val Loss: 0.0031, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0029, Train Acc: 0.9994, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0034, Train Acc: 0.9988, Val Loss: 0.0016, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0030, Train Acc: 0.9991, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 3/10
Epoch 1/50, Train Loss: 0.3620, Train Acc: 0.8821, Val Loss: 0.9833, Val Acc: 0.6451
Epoch 2/50, Train Loss: 0.0383, Train Acc: 0.9894, Val Loss: 0.8156, Val Acc: 0.7794
Epoch 3/50, Train Loss: 0.0325, Train Acc: 0.9907, Val Loss: 0.1981, Val Acc: 0.9239
Epoch 4/50, Train Loss: 0.0131, Train Acc: 0.9961, Val Loss: 0.8946, Val Acc: 0.7980
Epoch 5/50, Train Loss: 0.0307, Train Acc: 0.9922, Val Loss: 0.0131, Val Acc: 0.9964
Epoch 6/50, Train Loss: 0.0163, Train Acc: 0.9949, Val Loss: 0.0237, Val Acc: 0.9928
Epoch 7/50, Train Loss: 0.0121, Train Acc: 0.9963, Val Loss: 0.1080, Val Acc: 0.9610
Epoch 8/50, Train Loss: 0.0126, Train Acc: 0.9961, Val Loss: 0.5691, Val Acc: 0.8693
Epoch 9/50, Train Loss: 0.0162, Train Acc: 0.9940, Val Loss: 0.0086, Val Acc: 0.9958
Epoch 10/50, Train Loss: 0.0169, Train Acc: 0.9943, Val Loss: 0.0379, Val Acc: 0.9862
Epoch 11/50, Train Loss: 0.0061, Train Acc: 0.9988, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0027, Train Acc: 0.9993, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0020, Train Acc: 0.9996, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0059, Train Acc: 0.9990, Val Loss: 0.0021, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0013, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0019, Train Acc: 0.9993, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 24/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Early stopping!

Run 4/10
Epoch 1/50, Train Loss: 0.3715, Train Acc: 0.8698, Val Loss: 0.9697, Val Acc: 0.7458
Epoch 2/50, Train Loss: 0.0529, Train Acc: 0.9850, Val Loss: 1.9236, Val Acc: 0.6972
Epoch 3/50, Train Loss: 0.0233, Train Acc: 0.9928, Val Loss: 0.0062, Val Acc: 0.9988
Epoch 4/50, Train Loss: 0.0192, Train Acc: 0.9948, Val Loss: 0.0718, Val Acc: 0.9718
Epoch 5/50, Train Loss: 0.0148, Train Acc: 0.9957, Val Loss: 0.0331, Val Acc: 0.9898
Epoch 6/50, Train Loss: 0.0131, Train Acc: 0.9966, Val Loss: 1.0112, Val Acc: 0.7926
Epoch 7/50, Train Loss: 0.0157, Train Acc: 0.9960, Val Loss: 0.0253, Val Acc: 0.9922
Epoch 8/50, Train Loss: 0.0080, Train Acc: 0.9970, Val Loss: 0.0090, Val Acc: 0.9976
Early stopping!

Run 5/10
Epoch 1/50, Train Loss: 0.3307, Train Acc: 0.8919, Val Loss: 0.4774, Val Acc: 0.8129
Epoch 2/50, Train Loss: 0.0667, Train Acc: 0.9820, Val Loss: 0.6160, Val Acc: 0.8076
Epoch 3/50, Train Loss: 0.0296, Train Acc: 0.9907, Val Loss: 0.9304, Val Acc: 0.7644
Epoch 4/50, Train Loss: 0.0199, Train Acc: 0.9931, Val Loss: 0.0387, Val Acc: 0.9868
Epoch 5/50, Train Loss: 0.0138, Train Acc: 0.9964, Val Loss: 0.0864, Val Acc: 0.9622
Epoch 6/50, Train Loss: 0.0154, Train Acc: 0.9952, Val Loss: 0.0103, Val Acc: 0.9958
Epoch 7/50, Train Loss: 0.0124, Train Acc: 0.9961, Val Loss: 0.0278, Val Acc: 0.9904
Epoch 8/50, Train Loss: 0.0026, Train Acc: 0.9993, Val Loss: 0.0039, Val Acc: 0.9994
Epoch 9/50, Train Loss: 0.0067, Train Acc: 0.9990, Val Loss: 0.0175, Val Acc: 0.9928
Epoch 10/50, Train Loss: 0.0096, Train Acc: 0.9964, Val Loss: 0.0150, Val Acc: 0.9940
Epoch 11/50, Train Loss: 0.0026, Train Acc: 0.9991, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0037, Train Acc: 0.9990, Val Loss: 0.0398, Val Acc: 0.9874
Epoch 13/50, Train Loss: 0.0019, Train Acc: 0.9996, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994
Early stopping!

Run 6/10
Epoch 1/50, Train Loss: 0.3849, Train Acc: 0.8716, Val Loss: 0.8328, Val Acc: 0.7008
Epoch 2/50, Train Loss: 0.0547, Train Acc: 0.9862, Val Loss: 1.2648, Val Acc: 0.6918
Epoch 3/50, Train Loss: 0.0976, Train Acc: 0.9784, Val Loss: 0.0292, Val Acc: 0.9928
Epoch 4/50, Train Loss: 0.0215, Train Acc: 0.9936, Val Loss: 0.0055, Val Acc: 0.9988
Epoch 5/50, Train Loss: 0.0118, Train Acc: 0.9967, Val Loss: 0.1251, Val Acc: 0.9568
Epoch 6/50, Train Loss: 0.0193, Train Acc: 0.9940, Val Loss: 0.2086, Val Acc: 0.9071
Epoch 7/50, Train Loss: 0.0609, Train Acc: 0.9849, Val Loss: 0.1561, Val Acc: 0.9400
Epoch 8/50, Train Loss: 0.0123, Train Acc: 0.9972, Val Loss: 0.0038, Val Acc: 0.9982
Epoch 9/50, Train Loss: 0.0115, Train Acc: 0.9958, Val Loss: 0.0564, Val Acc: 0.9778
Epoch 10/50, Train Loss: 0.0072, Train Acc: 0.9982, Val Loss: 0.0611, Val Acc: 0.9808
Epoch 11/50, Train Loss: 0.0021, Train Acc: 0.9994, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0016, Train Acc: 1.0000, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0024, Train Acc: 0.9996, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0014, Train Acc: 0.9997, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0062, Train Acc: 0.9993, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0019, Train Acc: 0.9994, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0021, Train Acc: 0.9993, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0018, Train Acc: 0.9994, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0007, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Early stopping!

Run 7/10
Epoch 1/50, Train Loss: 0.3767, Train Acc: 0.8706, Val Loss: 1.8704, Val Acc: 0.5150
Epoch 2/50, Train Loss: 0.0384, Train Acc: 0.9898, Val Loss: 0.0989, Val Acc: 0.9622
Epoch 3/50, Train Loss: 0.0274, Train Acc: 0.9912, Val Loss: 0.0307, Val Acc: 0.9880
Epoch 4/50, Train Loss: 0.0239, Train Acc: 0.9925, Val Loss: 0.0187, Val Acc: 0.9958
Epoch 5/50, Train Loss: 0.0219, Train Acc: 0.9928, Val Loss: 0.0410, Val Acc: 0.9832
Epoch 6/50, Train Loss: 0.0272, Train Acc: 0.9924, Val Loss: 0.4892, Val Acc: 0.8741
Epoch 7/50, Train Loss: 0.0245, Train Acc: 0.9915, Val Loss: 0.0617, Val Acc: 0.9802
Epoch 8/50, Train Loss: 0.0097, Train Acc: 0.9969, Val Loss: 0.0340, Val Acc: 0.9928
Epoch 9/50, Train Loss: 0.0044, Train Acc: 0.9990, Val Loss: 0.0385, Val Acc: 0.9862
Early stopping!

Run 8/10
Epoch 1/50, Train Loss: 0.3730, Train Acc: 0.8797, Val Loss: 1.8343, Val Acc: 0.5683
Epoch 2/50, Train Loss: 0.0398, Train Acc: 0.9889, Val Loss: 2.3035, Val Acc: 0.5594
Epoch 3/50, Train Loss: 0.0200, Train Acc: 0.9945, Val Loss: 2.0637, Val Acc: 0.7476
Epoch 4/50, Train Loss: 0.0270, Train Acc: 0.9922, Val Loss: 0.2451, Val Acc: 0.9287
Epoch 5/50, Train Loss: 0.0105, Train Acc: 0.9976, Val Loss: 0.0175, Val Acc: 0.9976
Epoch 6/50, Train Loss: 0.0129, Train Acc: 0.9967, Val Loss: 0.0808, Val Acc: 0.9748
Epoch 7/50, Train Loss: 0.0197, Train Acc: 0.9946, Val Loss: 0.4384, Val Acc: 0.8297
Epoch 8/50, Train Loss: 0.0174, Train Acc: 0.9951, Val Loss: 0.0994, Val Acc: 0.9700
Epoch 9/50, Train Loss: 0.0176, Train Acc: 0.9945, Val Loss: 0.1381, Val Acc: 0.9544
Epoch 10/50, Train Loss: 0.0162, Train Acc: 0.9952, Val Loss: 0.1161, Val Acc: 0.9718
Early stopping!

Run 9/10
Epoch 1/50, Train Loss: 0.3903, Train Acc: 0.8685, Val Loss: 2.1659, Val Acc: 0.5018
Epoch 2/50, Train Loss: 0.0438, Train Acc: 0.9879, Val Loss: 0.8359, Val Acc: 0.7758
Epoch 3/50, Train Loss: 0.0278, Train Acc: 0.9931, Val Loss: 2.5309, Val Acc: 0.7218
Epoch 4/50, Train Loss: 0.0361, Train Acc: 0.9904, Val Loss: 0.3692, Val Acc: 0.8903
Epoch 5/50, Train Loss: 0.0204, Train Acc: 0.9943, Val Loss: 0.1681, Val Acc: 0.9382
Epoch 6/50, Train Loss: 0.0098, Train Acc: 0.9978, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 7/50, Train Loss: 0.0111, Train Acc: 0.9978, Val Loss: 0.0050, Val Acc: 0.9982
Epoch 8/50, Train Loss: 0.0112, Train Acc: 0.9972, Val Loss: 1.2852, Val Acc: 0.6397
Epoch 9/50, Train Loss: 0.0162, Train Acc: 0.9955, Val Loss: 2.5063, Val Acc: 0.7284
Epoch 10/50, Train Loss: 0.0125, Train Acc: 0.9963, Val Loss: 0.0020, Val Acc: 0.9994
Epoch 11/50, Train Loss: 0.0023, Train Acc: 0.9994, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0017, Train Acc: 0.9996, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Early stopping!

Run 10/10
Epoch 1/50, Train Loss: 0.3816, Train Acc: 0.8749, Val Loss: 0.9202, Val Acc: 0.7134
Epoch 2/50, Train Loss: 0.0783, Train Acc: 0.9784, Val Loss: 0.3972, Val Acc: 0.8579
Epoch 3/50, Train Loss: 0.0219, Train Acc: 0.9946, Val Loss: 0.0094, Val Acc: 0.9970
Epoch 4/50, Train Loss: 0.0259, Train Acc: 0.9927, Val Loss: 0.8874, Val Acc: 0.8112
Epoch 5/50, Train Loss: 0.0186, Train Acc: 0.9936, Val Loss: 0.1418, Val Acc: 0.9496
Epoch 6/50, Train Loss: 0.0391, Train Acc: 0.9886, Val Loss: 0.0197, Val Acc: 0.9934
Epoch 7/50, Train Loss: 0.0158, Train Acc: 0.9952, Val Loss: 0.0845, Val Acc: 0.9604
Epoch 8/50, Train Loss: 0.0074, Train Acc: 0.9979, Val Loss: 0.1040, Val Acc: 0.9562
Early stopping!

Source performance: 97.89 98.09 97.90 97.89
Target performance: 96.53 97.17 96.55 96.34

bpsk: 99.70
qpsk: 92.93
16qam: 94.95
8apsk: 98.61
DANN
Epoch 1/50, Loss: 2.4072, Domain Loss: 1.3931, Class Loss: 1.0141
Epoch 2/50, Loss: 1.6769, Domain Loss: 1.3936, Class Loss: 0.2833
Epoch 3/50, Loss: 1.5206, Domain Loss: 1.3881, Class Loss: 0.1325
Epoch 4/50, Loss: 1.5227, Domain Loss: 1.3784, Class Loss: 0.1443
Epoch 5/50, Loss: 1.4608, Domain Loss: 1.3777, Class Loss: 0.0831
Epoch 6/50, Loss: 1.4221, Domain Loss: 1.3800, Class Loss: 0.0421
Epoch 7/50, Loss: 2.1083, Domain Loss: 2.0020, Class Loss: 0.1063
Epoch 8/50, Loss: 5.1476, Domain Loss: 4.8847, Class Loss: 0.2629
Epoch 9/50, Loss: 4.3152, Domain Loss: 3.8971, Class Loss: 0.4181
Epoch 10/50, Loss: 3.2194, Domain Loss: 2.8747, Class Loss: 0.3447
Epoch 11/50, Loss: 2.2321, Domain Loss: 1.8414, Class Loss: 0.3907
Epoch 12/50, Loss: 1.8564, Domain Loss: 1.6463, Class Loss: 0.2100
Epoch 13/50, Loss: 1.7173, Domain Loss: 1.5776, Class Loss: 0.1397
Epoch 14/50, Loss: 2.1464, Domain Loss: 1.9901, Class Loss: 0.1564
Epoch 15/50, Loss: 5.6614, Domain Loss: 5.1865, Class Loss: 0.4750
Epoch 16/50, Loss: 5.4324, Domain Loss: 4.7881, Class Loss: 0.6443
Epoch 17/50, Loss: 2.5540, Domain Loss: 2.2879, Class Loss: 0.2661
Epoch 18/50, Loss: 2.0943, Domain Loss: 1.8730, Class Loss: 0.2213
Epoch 19/50, Loss: 1.7254, Domain Loss: 1.5472, Class Loss: 0.1782
Epoch 20/50, Loss: 1.8309, Domain Loss: 1.5444, Class Loss: 0.2865
Epoch 21/50, Loss: 1.7857, Domain Loss: 1.5618, Class Loss: 0.2239
Epoch 22/50, Loss: 1.6276, Domain Loss: 1.4445, Class Loss: 0.1831
Epoch 23/50, Loss: 1.6591, Domain Loss: 1.5142, Class Loss: 0.1449
Epoch 24/50, Loss: 1.6169, Domain Loss: 1.4720, Class Loss: 0.1449
Epoch 25/50, Loss: 1.5911, Domain Loss: 1.4511, Class Loss: 0.1399
Epoch 26/50, Loss: 1.5659, Domain Loss: 1.4853, Class Loss: 0.0805
Epoch 27/50, Loss: 1.5123, Domain Loss: 1.4416, Class Loss: 0.0707
Epoch 28/50, Loss: 1.5016, Domain Loss: 1.4338, Class Loss: 0.0678
Epoch 29/50, Loss: 1.5159, Domain Loss: 1.4504, Class Loss: 0.0655
Epoch 30/50, Loss: 1.4851, Domain Loss: 1.4230, Class Loss: 0.0621
Epoch 31/50, Loss: 1.4706, Domain Loss: 1.4182, Class Loss: 0.0525
Epoch 32/50, Loss: 1.4558, Domain Loss: 1.4185, Class Loss: 0.0373
Epoch 33/50, Loss: 1.5010, Domain Loss: 1.4255, Class Loss: 0.0755
Epoch 34/50, Loss: 1.4988, Domain Loss: 1.4302, Class Loss: 0.0687
Epoch 35/50, Loss: 1.5100, Domain Loss: 1.4503, Class Loss: 0.0597
Epoch 36/50, Loss: 1.5420, Domain Loss: 1.4745, Class Loss: 0.0675
Epoch 37/50, Loss: 1.5467, Domain Loss: 1.4732, Class Loss: 0.0735
Epoch 38/50, Loss: 1.5045, Domain Loss: 1.4558, Class Loss: 0.0487
Epoch 39/50, Loss: 1.7952, Domain Loss: 1.6254, Class Loss: 0.1698
Epoch 40/50, Loss: 1.6367, Domain Loss: 1.5447, Class Loss: 0.0920
Epoch 41/50, Loss: 1.5610, Domain Loss: 1.4857, Class Loss: 0.0753
Epoch 42/50, Loss: 1.5229, Domain Loss: 1.4765, Class Loss: 0.0464
Epoch 43/50, Loss: 1.5535, Domain Loss: 1.5107, Class Loss: 0.0428
Epoch 44/50, Loss: 1.6807, Domain Loss: 1.6074, Class Loss: 0.0733
Epoch 45/50, Loss: 1.7037, Domain Loss: 1.5735, Class Loss: 0.1302
Epoch 46/50, Loss: 1.7185, Domain Loss: 1.5843, Class Loss: 0.1342
Epoch 47/50, Loss: 1.7492, Domain Loss: 1.6498, Class Loss: 0.0994
Epoch 48/50, Loss: 1.5954, Domain Loss: 1.5157, Class Loss: 0.0798
Epoch 49/50, Loss: 1.4981, Domain Loss: 1.4579, Class Loss: 0.0402
Epoch 50/50, Loss: 1.4824, Domain Loss: 1.4422, Class Loss: 0.0402
99.70


Epoch 1/50, Loss: 2.4059, Domain Loss: 1.4107, Class Loss: 0.9952
Epoch 2/50, Loss: 1.8617, Domain Loss: 1.3862, Class Loss: 0.4756
Epoch 3/50, Loss: 1.6243, Domain Loss: 1.3777, Class Loss: 0.2465
Epoch 4/50, Loss: 1.5050, Domain Loss: 1.3748, Class Loss: 0.1302
Epoch 5/50, Loss: 1.4848, Domain Loss: 1.3789, Class Loss: 0.1059
Epoch 6/50, Loss: 1.4594, Domain Loss: 1.3940, Class Loss: 0.0654
Epoch 7/50, Loss: 4.5189, Domain Loss: 4.3652, Class Loss: 0.1537
Epoch 8/50, Loss: 7.6981, Domain Loss: 7.4370, Class Loss: 0.2611
Epoch 9/50, Loss: 2.5093, Domain Loss: 2.3516, Class Loss: 0.1577
Epoch 10/50, Loss: 1.5991, Domain Loss: 1.5424, Class Loss: 0.0567
Epoch 11/50, Loss: 1.9116, Domain Loss: 1.6298, Class Loss: 0.2819
Epoch 12/50, Loss: 2.0488, Domain Loss: 1.7250, Class Loss: 0.3238
Epoch 13/50, Loss: 1.7867, Domain Loss: 1.6253, Class Loss: 0.1614
Epoch 14/50, Loss: 7.7253, Domain Loss: 7.0852, Class Loss: 0.6401
Epoch 15/50, Loss: 11.6107, Domain Loss: 10.8757, Class Loss: 0.7351
Epoch 16/50, Loss: 8.0836, Domain Loss: 7.4527, Class Loss: 0.6309
Epoch 17/50, Loss: 3.1499, Domain Loss: 2.9116, Class Loss: 0.2383
Epoch 18/50, Loss: 1.8063, Domain Loss: 1.6401, Class Loss: 0.1661
Epoch 19/50, Loss: 1.7950, Domain Loss: 1.6697, Class Loss: 0.1253
Epoch 20/50, Loss: 3.4506, Domain Loss: 3.1123, Class Loss: 0.3383
Epoch 21/50, Loss: 2.5609, Domain Loss: 2.2319, Class Loss: 0.3290
Epoch 22/50, Loss: 1.7449, Domain Loss: 1.6285, Class Loss: 0.1164
Epoch 23/50, Loss: 1.5523, Domain Loss: 1.4628, Class Loss: 0.0895
Epoch 24/50, Loss: 1.5056, Domain Loss: 1.4388, Class Loss: 0.0668
Epoch 25/50, Loss: 1.4958, Domain Loss: 1.4393, Class Loss: 0.0565
Epoch 26/50, Loss: 1.4857, Domain Loss: 1.4364, Class Loss: 0.0494
Epoch 27/50, Loss: 1.4737, Domain Loss: 1.4310, Class Loss: 0.0427
Epoch 28/50, Loss: 1.5013, Domain Loss: 1.4430, Class Loss: 0.0583
Epoch 29/50, Loss: 1.5312, Domain Loss: 1.4906, Class Loss: 0.0406
Epoch 30/50, Loss: 1.7942, Domain Loss: 1.7221, Class Loss: 0.0722
Epoch 31/50, Loss: 1.8394, Domain Loss: 1.7405, Class Loss: 0.0989
Epoch 32/50, Loss: 1.7606, Domain Loss: 1.6604, Class Loss: 0.1003
Epoch 33/50, Loss: 1.7411, Domain Loss: 1.6790, Class Loss: 0.0620
Epoch 34/50, Loss: 1.7985, Domain Loss: 1.7548, Class Loss: 0.0438
Epoch 35/50, Loss: 1.8050, Domain Loss: 1.7279, Class Loss: 0.0770
Epoch 36/50, Loss: 1.5689, Domain Loss: 1.5214, Class Loss: 0.0474
Epoch 37/50, Loss: 1.8739, Domain Loss: 1.8196, Class Loss: 0.0543
Epoch 38/50, Loss: 1.7125, Domain Loss: 1.6157, Class Loss: 0.0969
Epoch 39/50, Loss: 1.5456, Domain Loss: 1.4845, Class Loss: 0.0610
Epoch 40/50, Loss: 1.6183, Domain Loss: 1.5663, Class Loss: 0.0520
Epoch 41/50, Loss: 1.5828, Domain Loss: 1.5378, Class Loss: 0.0450
Epoch 42/50, Loss: 1.6299, Domain Loss: 1.5893, Class Loss: 0.0406
Epoch 43/50, Loss: 1.5427, Domain Loss: 1.4769, Class Loss: 0.0658
Epoch 44/50, Loss: 1.4628, Domain Loss: 1.4132, Class Loss: 0.0495
Epoch 45/50, Loss: 1.4341, Domain Loss: 1.3892, Class Loss: 0.0449
Epoch 46/50, Loss: 1.4582, Domain Loss: 1.4053, Class Loss: 0.0530
Epoch 47/50, Loss: 1.4195, Domain Loss: 1.3927, Class Loss: 0.0269
Epoch 48/50, Loss: 1.4101, Domain Loss: 1.3916, Class Loss: 0.0184
Epoch 49/50, Loss: 1.4055, Domain Loss: 1.3886, Class Loss: 0.0169
Epoch 50/50, Loss: 1.4120, Domain Loss: 1.3940, Class Loss: 0.0180
69.78


Epoch 1/50, Loss: 2.3354, Domain Loss: 1.3966, Class Loss: 0.9389
Epoch 2/50, Loss: 1.7424, Domain Loss: 1.3931, Class Loss: 0.3493
Epoch 3/50, Loss: 1.5733, Domain Loss: 1.3831, Class Loss: 0.1902
Epoch 4/50, Loss: 1.4633, Domain Loss: 1.3766, Class Loss: 0.0867
Epoch 5/50, Loss: 1.4546, Domain Loss: 1.3844, Class Loss: 0.0702
Epoch 6/50, Loss: 1.4747, Domain Loss: 1.3765, Class Loss: 0.0982
Epoch 7/50, Loss: 1.4874, Domain Loss: 1.4000, Class Loss: 0.0874
Epoch 8/50, Loss: 2.0530, Domain Loss: 1.9798, Class Loss: 0.0732
Epoch 9/50, Loss: 1.8115, Domain Loss: 1.7343, Class Loss: 0.0772
Epoch 10/50, Loss: 5.9559, Domain Loss: 5.7288, Class Loss: 0.2271
Epoch 11/50, Loss: 10.6329, Domain Loss: 10.1504, Class Loss: 0.4825
Epoch 12/50, Loss: 12.2717, Domain Loss: 11.7591, Class Loss: 0.5126
Epoch 13/50, Loss: 25.1935, Domain Loss: 23.8463, Class Loss: 1.3472
Epoch 14/50, Loss: 20.5334, Domain Loss: 19.5890, Class Loss: 0.9444
Epoch 15/50, Loss: 6.1250, Domain Loss: 5.5043, Class Loss: 0.6207
Epoch 16/50, Loss: 2.8653, Domain Loss: 2.4154, Class Loss: 0.4498
Epoch 17/50, Loss: 2.7638, Domain Loss: 2.2666, Class Loss: 0.4972
Epoch 18/50, Loss: 3.4781, Domain Loss: 2.9514, Class Loss: 0.5267
Epoch 19/50, Loss: 2.1065, Domain Loss: 1.9092, Class Loss: 0.1973
Epoch 20/50, Loss: 1.8539, Domain Loss: 1.7077, Class Loss: 0.1462
Epoch 21/50, Loss: 1.6860, Domain Loss: 1.5586, Class Loss: 0.1274
Epoch 22/50, Loss: 1.6247, Domain Loss: 1.5126, Class Loss: 0.1121
Epoch 23/50, Loss: 1.5186, Domain Loss: 1.4438, Class Loss: 0.0749
Epoch 24/50, Loss: 1.4996, Domain Loss: 1.4397, Class Loss: 0.0599
Epoch 25/50, Loss: 1.4946, Domain Loss: 1.4209, Class Loss: 0.0736
Epoch 26/50, Loss: 1.5648, Domain Loss: 1.4862, Class Loss: 0.0786
Epoch 27/50, Loss: 1.6393, Domain Loss: 1.5225, Class Loss: 0.1167
Epoch 28/50, Loss: 1.5090, Domain Loss: 1.4297, Class Loss: 0.0794
Epoch 29/50, Loss: 1.4470, Domain Loss: 1.4003, Class Loss: 0.0467
Epoch 30/50, Loss: 1.4324, Domain Loss: 1.3999, Class Loss: 0.0326
Epoch 31/50, Loss: 1.4515, Domain Loss: 1.4026, Class Loss: 0.0489
Epoch 32/50, Loss: 1.4281, Domain Loss: 1.3958, Class Loss: 0.0323
Epoch 33/50, Loss: 1.4337, Domain Loss: 1.4024, Class Loss: 0.0312
Epoch 34/50, Loss: 1.4521, Domain Loss: 1.4138, Class Loss: 0.0383
Epoch 35/50, Loss: 1.4435, Domain Loss: 1.4020, Class Loss: 0.0415
Epoch 36/50, Loss: 1.4415, Domain Loss: 1.4054, Class Loss: 0.0361
Epoch 37/50, Loss: 1.4348, Domain Loss: 1.4054, Class Loss: 0.0294
Epoch 38/50, Loss: 1.4370, Domain Loss: 1.4038, Class Loss: 0.0332
Epoch 39/50, Loss: 1.4281, Domain Loss: 1.3958, Class Loss: 0.0323
Epoch 40/50, Loss: 1.4553, Domain Loss: 1.4016, Class Loss: 0.0537
Epoch 41/50, Loss: 1.4217, Domain Loss: 1.3996, Class Loss: 0.0221
Epoch 42/50, Loss: 1.4211, Domain Loss: 1.3914, Class Loss: 0.0297
Epoch 43/50, Loss: 1.4388, Domain Loss: 1.4026, Class Loss: 0.0361
Epoch 44/50, Loss: 1.4379, Domain Loss: 1.3966, Class Loss: 0.0413
Epoch 45/50, Loss: 1.4207, Domain Loss: 1.3912, Class Loss: 0.0296
Epoch 46/50, Loss: 1.4169, Domain Loss: 1.3895, Class Loss: 0.0274
Epoch 47/50, Loss: 1.4064, Domain Loss: 1.3862, Class Loss: 0.0202
Epoch 48/50, Loss: 1.4317, Domain Loss: 1.4078, Class Loss: 0.0239
Epoch 49/50, Loss: 1.4187, Domain Loss: 1.4008, Class Loss: 0.0179
Epoch 50/50, Loss: 1.4877, Domain Loss: 1.4156, Class Loss: 0.0720
96.52


Epoch 1/50, Loss: 2.4425, Domain Loss: 1.4035, Class Loss: 1.0390
Epoch 2/50, Loss: 1.8271, Domain Loss: 1.3915, Class Loss: 0.4356
Epoch 3/50, Loss: 1.5712, Domain Loss: 1.3827, Class Loss: 0.1885
Epoch 4/50, Loss: 1.4740, Domain Loss: 1.3828, Class Loss: 0.0911
Epoch 5/50, Loss: 1.6569, Domain Loss: 1.5333, Class Loss: 0.1237
Epoch 6/50, Loss: 2.4386, Domain Loss: 2.3018, Class Loss: 0.1367
Epoch 7/50, Loss: 4.2618, Domain Loss: 4.0332, Class Loss: 0.2286
Epoch 8/50, Loss: 3.9431, Domain Loss: 3.4657, Class Loss: 0.4775
Epoch 9/50, Loss: 6.0456, Domain Loss: 5.7770, Class Loss: 0.2685
Epoch 10/50, Loss: 3.1143, Domain Loss: 2.8008, Class Loss: 0.3135
Epoch 11/50, Loss: 3.9984, Domain Loss: 3.6105, Class Loss: 0.3879
Epoch 12/50, Loss: 2.5351, Domain Loss: 2.2497, Class Loss: 0.2853
Epoch 13/50, Loss: 1.7429, Domain Loss: 1.6005, Class Loss: 0.1424
Epoch 14/50, Loss: 1.8520, Domain Loss: 1.6441, Class Loss: 0.2079
Epoch 15/50, Loss: 2.1834, Domain Loss: 1.8194, Class Loss: 0.3640
Epoch 16/50, Loss: 2.5884, Domain Loss: 2.2191, Class Loss: 0.3693
Epoch 17/50, Loss: 1.9248, Domain Loss: 1.7396, Class Loss: 0.1851
Epoch 18/50, Loss: 1.6463, Domain Loss: 1.5120, Class Loss: 0.1343
Epoch 19/50, Loss: 1.5816, Domain Loss: 1.4566, Class Loss: 0.1250
Epoch 20/50, Loss: 1.5469, Domain Loss: 1.4584, Class Loss: 0.0885
Epoch 21/50, Loss: 1.5071, Domain Loss: 1.4172, Class Loss: 0.0899
Epoch 22/50, Loss: 1.4556, Domain Loss: 1.4092, Class Loss: 0.0464
Epoch 23/50, Loss: 1.4762, Domain Loss: 1.4179, Class Loss: 0.0583
Epoch 24/50, Loss: 1.4731, Domain Loss: 1.4140, Class Loss: 0.0591
Epoch 25/50, Loss: 1.4468, Domain Loss: 1.3875, Class Loss: 0.0594
Epoch 26/50, Loss: 1.4579, Domain Loss: 1.3934, Class Loss: 0.0645
Epoch 27/50, Loss: 1.4376, Domain Loss: 1.3952, Class Loss: 0.0424
Epoch 28/50, Loss: 1.4462, Domain Loss: 1.4019, Class Loss: 0.0443
Epoch 29/50, Loss: 1.4778, Domain Loss: 1.4318, Class Loss: 0.0460
Epoch 30/50, Loss: 1.4797, Domain Loss: 1.4361, Class Loss: 0.0436
Epoch 31/50, Loss: 1.5753, Domain Loss: 1.4996, Class Loss: 0.0757
Epoch 32/50, Loss: 1.5549, Domain Loss: 1.4926, Class Loss: 0.0623
Epoch 33/50, Loss: 1.4810, Domain Loss: 1.4336, Class Loss: 0.0474
Epoch 34/50, Loss: 1.4174, Domain Loss: 1.3872, Class Loss: 0.0302
Epoch 35/50, Loss: 1.4241, Domain Loss: 1.3848, Class Loss: 0.0392
Epoch 36/50, Loss: 1.4133, Domain Loss: 1.3868, Class Loss: 0.0265
Epoch 37/50, Loss: 1.4265, Domain Loss: 1.3851, Class Loss: 0.0414
Epoch 38/50, Loss: 1.4136, Domain Loss: 1.3848, Class Loss: 0.0288
Epoch 39/50, Loss: 1.4145, Domain Loss: 1.3934, Class Loss: 0.0211
Epoch 40/50, Loss: 1.4151, Domain Loss: 1.3941, Class Loss: 0.0210
Epoch 41/50, Loss: 1.4314, Domain Loss: 1.3906, Class Loss: 0.0407
Epoch 42/50, Loss: 1.3995, Domain Loss: 1.3869, Class Loss: 0.0125
Epoch 43/50, Loss: 1.4347, Domain Loss: 1.4035, Class Loss: 0.0312
Epoch 44/50, Loss: 1.7840, Domain Loss: 1.4808, Class Loss: 0.3032
Epoch 45/50, Loss: 1.5416, Domain Loss: 1.4541, Class Loss: 0.0875
Epoch 46/50, Loss: 1.5512, Domain Loss: 1.4716, Class Loss: 0.0796
Epoch 47/50, Loss: 1.7425, Domain Loss: 1.6617, Class Loss: 0.0808
Epoch 48/50, Loss: 2.1056, Domain Loss: 1.9442, Class Loss: 0.1614
Epoch 49/50, Loss: 1.8089, Domain Loss: 1.5932, Class Loss: 0.2157
Epoch 50/50, Loss: 1.5479, Domain Loss: 1.4574, Class Loss: 0.0905
80.82


Epoch 1/50, Loss: 2.4552, Domain Loss: 1.3939, Class Loss: 1.0613
Epoch 2/50, Loss: 1.7137, Domain Loss: 1.3909, Class Loss: 0.3228
Epoch 3/50, Loss: 1.5234, Domain Loss: 1.3857, Class Loss: 0.1377
Epoch 4/50, Loss: 1.4454, Domain Loss: 1.3706, Class Loss: 0.0748
Epoch 5/50, Loss: 1.4565, Domain Loss: 1.3830, Class Loss: 0.0735
Epoch 6/50, Loss: 1.5053, Domain Loss: 1.4021, Class Loss: 0.1031
Epoch 7/50, Loss: 3.1693, Domain Loss: 3.0799, Class Loss: 0.0895
Epoch 8/50, Loss: 9.9757, Domain Loss: 9.8156, Class Loss: 0.1601
Epoch 9/50, Loss: 5.0985, Domain Loss: 4.7721, Class Loss: 0.3265
Epoch 10/50, Loss: 2.9387, Domain Loss: 2.6282, Class Loss: 0.3105
Epoch 11/50, Loss: 4.7929, Domain Loss: 4.0491, Class Loss: 0.7438
Epoch 12/50, Loss: 9.3807, Domain Loss: 8.7237, Class Loss: 0.6570
Epoch 13/50, Loss: 3.0444, Domain Loss: 2.8101, Class Loss: 0.2342
Epoch 14/50, Loss: 1.5753, Domain Loss: 1.4565, Class Loss: 0.1188
Epoch 15/50, Loss: 1.5443, Domain Loss: 1.4487, Class Loss: 0.0956
Epoch 16/50, Loss: 1.4721, Domain Loss: 1.4030, Class Loss: 0.0691
Epoch 17/50, Loss: 1.4434, Domain Loss: 1.3867, Class Loss: 0.0566
Epoch 18/50, Loss: 1.4345, Domain Loss: 1.3892, Class Loss: 0.0454
Epoch 19/50, Loss: 1.4290, Domain Loss: 1.4005, Class Loss: 0.0286
Epoch 20/50, Loss: 1.4256, Domain Loss: 1.3923, Class Loss: 0.0333
Epoch 21/50, Loss: 1.4439, Domain Loss: 1.4113, Class Loss: 0.0326
Epoch 22/50, Loss: 1.4346, Domain Loss: 1.4017, Class Loss: 0.0330
Epoch 23/50, Loss: 1.4166, Domain Loss: 1.3951, Class Loss: 0.0214
Epoch 24/50, Loss: 1.4307, Domain Loss: 1.3975, Class Loss: 0.0331
Epoch 25/50, Loss: 1.4232, Domain Loss: 1.3904, Class Loss: 0.0328
Epoch 26/50, Loss: 1.4036, Domain Loss: 1.3842, Class Loss: 0.0195
Epoch 27/50, Loss: 1.4134, Domain Loss: 1.3874, Class Loss: 0.0260
Epoch 28/50, Loss: 1.4124, Domain Loss: 1.3939, Class Loss: 0.0185
Epoch 29/50, Loss: 1.4143, Domain Loss: 1.3859, Class Loss: 0.0285
Epoch 30/50, Loss: 1.4297, Domain Loss: 1.3923, Class Loss: 0.0374
Epoch 31/50, Loss: 1.4108, Domain Loss: 1.3908, Class Loss: 0.0200
Epoch 32/50, Loss: 1.4057, Domain Loss: 1.3899, Class Loss: 0.0158
Epoch 33/50, Loss: 1.4179, Domain Loss: 1.3934, Class Loss: 0.0245
Epoch 34/50, Loss: 1.4126, Domain Loss: 1.3926, Class Loss: 0.0200
Epoch 35/50, Loss: 1.4036, Domain Loss: 1.3938, Class Loss: 0.0098
Epoch 36/50, Loss: 1.4156, Domain Loss: 1.3968, Class Loss: 0.0188
Epoch 37/50, Loss: 1.4052, Domain Loss: 1.3937, Class Loss: 0.0114
Epoch 38/50, Loss: 1.4205, Domain Loss: 1.3992, Class Loss: 0.0214
Epoch 39/50, Loss: 1.4183, Domain Loss: 1.4020, Class Loss: 0.0163
Epoch 40/50, Loss: 1.4357, Domain Loss: 1.4019, Class Loss: 0.0338
Epoch 41/50, Loss: 1.4392, Domain Loss: 1.4136, Class Loss: 0.0256
Epoch 42/50, Loss: 1.4477, Domain Loss: 1.4346, Class Loss: 0.0132
Epoch 43/50, Loss: 1.4875, Domain Loss: 1.4808, Class Loss: 0.0067
Epoch 44/50, Loss: 1.6155, Domain Loss: 1.5548, Class Loss: 0.0607
Epoch 45/50, Loss: 1.5701, Domain Loss: 1.5436, Class Loss: 0.0266
Epoch 46/50, Loss: 1.5324, Domain Loss: 1.5084, Class Loss: 0.0240
Epoch 47/50, Loss: 1.4708, Domain Loss: 1.4565, Class Loss: 0.0144
Epoch 48/50, Loss: 1.4535, Domain Loss: 1.4290, Class Loss: 0.0245
Epoch 49/50, Loss: 1.4524, Domain Loss: 1.4235, Class Loss: 0.0288
Epoch 50/50, Loss: 1.4628, Domain Loss: 1.4396, Class Loss: 0.0232
98.74


Epoch 1/50, Loss: 2.4477, Domain Loss: 1.4014, Class Loss: 1.0463
Epoch 2/50, Loss: 1.8115, Domain Loss: 1.3931, Class Loss: 0.4184
Epoch 3/50, Loss: 1.5805, Domain Loss: 1.3903, Class Loss: 0.1903
Epoch 4/50, Loss: 1.4952, Domain Loss: 1.3838, Class Loss: 0.1113
Epoch 5/50, Loss: 1.4341, Domain Loss: 1.3674, Class Loss: 0.0667
Epoch 6/50, Loss: 3.3061, Domain Loss: 3.2061, Class Loss: 0.1000
Epoch 7/50, Loss: 3.8174, Domain Loss: 3.6675, Class Loss: 0.1499
Epoch 8/50, Loss: 6.6260, Domain Loss: 6.4633, Class Loss: 0.1628
Epoch 9/50, Loss: 4.7993, Domain Loss: 4.5703, Class Loss: 0.2290
Epoch 10/50, Loss: 6.5705, Domain Loss: 5.9028, Class Loss: 0.6677
Epoch 11/50, Loss: 6.1700, Domain Loss: 5.5794, Class Loss: 0.5906
Epoch 12/50, Loss: 6.9463, Domain Loss: 5.7597, Class Loss: 1.1866
Epoch 13/50, Loss: 5.2813, Domain Loss: 4.2534, Class Loss: 1.0279
Epoch 14/50, Loss: 2.9102, Domain Loss: 2.1769, Class Loss: 0.7333
Epoch 15/50, Loss: 2.3485, Domain Loss: 1.7786, Class Loss: 0.5699
Epoch 16/50, Loss: 2.4519, Domain Loss: 2.0052, Class Loss: 0.4467
Epoch 17/50, Loss: 7.1189, Domain Loss: 5.2902, Class Loss: 1.8287
Epoch 18/50, Loss: 4.6610, Domain Loss: 3.3059, Class Loss: 1.3550
Epoch 19/50, Loss: 3.7183, Domain Loss: 3.1542, Class Loss: 0.5641
Epoch 20/50, Loss: 2.5645, Domain Loss: 2.0113, Class Loss: 0.5532
Epoch 21/50, Loss: 2.2804, Domain Loss: 1.8233, Class Loss: 0.4570
Epoch 22/50, Loss: 2.2870, Domain Loss: 1.9800, Class Loss: 0.3070
Epoch 23/50, Loss: 1.9589, Domain Loss: 1.7285, Class Loss: 0.2304
Epoch 24/50, Loss: 1.8134, Domain Loss: 1.6342, Class Loss: 0.1792
Epoch 25/50, Loss: 1.6640, Domain Loss: 1.5200, Class Loss: 0.1440
Epoch 26/50, Loss: 1.6685, Domain Loss: 1.4909, Class Loss: 0.1777
Epoch 27/50, Loss: 1.5707, Domain Loss: 1.4393, Class Loss: 0.1314
Epoch 28/50, Loss: 1.5785, Domain Loss: 1.4630, Class Loss: 0.1155
Epoch 29/50, Loss: 1.5897, Domain Loss: 1.4653, Class Loss: 0.1244
Epoch 30/50, Loss: 1.5613, Domain Loss: 1.4586, Class Loss: 0.1027
Epoch 31/50, Loss: 1.5239, Domain Loss: 1.4357, Class Loss: 0.0882
Epoch 32/50, Loss: 1.4959, Domain Loss: 1.4211, Class Loss: 0.0748
Epoch 33/50, Loss: 1.4839, Domain Loss: 1.4128, Class Loss: 0.0711
Epoch 34/50, Loss: 1.4791, Domain Loss: 1.4116, Class Loss: 0.0676
Epoch 35/50, Loss: 1.4770, Domain Loss: 1.4252, Class Loss: 0.0518
Epoch 36/50, Loss: 1.4857, Domain Loss: 1.4328, Class Loss: 0.0529
Epoch 37/50, Loss: 1.4537, Domain Loss: 1.4161, Class Loss: 0.0376
Epoch 38/50, Loss: 1.4967, Domain Loss: 1.4244, Class Loss: 0.0723
Epoch 39/50, Loss: 1.4842, Domain Loss: 1.4303, Class Loss: 0.0539
Epoch 40/50, Loss: 1.4752, Domain Loss: 1.4331, Class Loss: 0.0421
Epoch 41/50, Loss: 1.4797, Domain Loss: 1.4221, Class Loss: 0.0576
Epoch 42/50, Loss: 1.4569, Domain Loss: 1.4112, Class Loss: 0.0457
Epoch 43/50, Loss: 1.4550, Domain Loss: 1.4111, Class Loss: 0.0438
Epoch 44/50, Loss: 1.4478, Domain Loss: 1.4146, Class Loss: 0.0331
Epoch 45/50, Loss: 1.4403, Domain Loss: 1.4089, Class Loss: 0.0313
Epoch 46/50, Loss: 1.4365, Domain Loss: 1.4036, Class Loss: 0.0329
Epoch 47/50, Loss: 1.4784, Domain Loss: 1.4295, Class Loss: 0.0489
Epoch 48/50, Loss: 1.4543, Domain Loss: 1.4139, Class Loss: 0.0404
Epoch 49/50, Loss: 1.4525, Domain Loss: 1.4097, Class Loss: 0.0428
Epoch 50/50, Loss: 1.4430, Domain Loss: 1.4097, Class Loss: 0.0333
99.76


Epoch 1/50, Loss: 2.4328, Domain Loss: 1.4147, Class Loss: 1.0181
Epoch 2/50, Loss: 1.7661, Domain Loss: 1.3919, Class Loss: 0.3742
Epoch 3/50, Loss: 1.5443, Domain Loss: 1.3856, Class Loss: 0.1587
Epoch 4/50, Loss: 1.4735, Domain Loss: 1.3812, Class Loss: 0.0923
Epoch 5/50, Loss: 1.4779, Domain Loss: 1.3827, Class Loss: 0.0951
Epoch 6/50, Loss: 1.4481, Domain Loss: 1.3822, Class Loss: 0.0660
Epoch 7/50, Loss: 1.5648, Domain Loss: 1.4937, Class Loss: 0.0711
Epoch 8/50, Loss: 3.4411, Domain Loss: 3.2549, Class Loss: 0.1863
Epoch 9/50, Loss: 5.4251, Domain Loss: 5.2411, Class Loss: 0.1841
Epoch 10/50, Loss: 2.9657, Domain Loss: 2.7956, Class Loss: 0.1702
Epoch 11/50, Loss: 2.0790, Domain Loss: 1.9097, Class Loss: 0.1694
Epoch 12/50, Loss: 1.8273, Domain Loss: 1.7346, Class Loss: 0.0927
Epoch 13/50, Loss: 6.1697, Domain Loss: 5.7815, Class Loss: 0.3881
Epoch 14/50, Loss: 8.3669, Domain Loss: 7.9879, Class Loss: 0.3790
Epoch 15/50, Loss: 6.9484, Domain Loss: 6.4751, Class Loss: 0.4734
Epoch 16/50, Loss: 2.9005, Domain Loss: 2.6482, Class Loss: 0.2523
Epoch 17/50, Loss: 1.9300, Domain Loss: 1.7497, Class Loss: 0.1803
Epoch 18/50, Loss: 1.6392, Domain Loss: 1.5415, Class Loss: 0.0977
Epoch 19/50, Loss: 1.5834, Domain Loss: 1.5054, Class Loss: 0.0780
Epoch 20/50, Loss: 2.1777, Domain Loss: 2.0821, Class Loss: 0.0955
Epoch 21/50, Loss: 4.4630, Domain Loss: 3.7844, Class Loss: 0.6787
Epoch 22/50, Loss: 2.9593, Domain Loss: 2.8429, Class Loss: 0.1165
Epoch 23/50, Loss: 1.7049, Domain Loss: 1.6423, Class Loss: 0.0626
Epoch 24/50, Loss: 1.4952, Domain Loss: 1.4400, Class Loss: 0.0552
Epoch 25/50, Loss: 1.4386, Domain Loss: 1.3882, Class Loss: 0.0504
Epoch 26/50, Loss: 1.4395, Domain Loss: 1.3844, Class Loss: 0.0551
Epoch 27/50, Loss: 1.3912, Domain Loss: 1.3711, Class Loss: 0.0201
Epoch 28/50, Loss: 1.4552, Domain Loss: 1.3871, Class Loss: 0.0681
Epoch 29/50, Loss: 1.4340, Domain Loss: 1.3865, Class Loss: 0.0474
Epoch 30/50, Loss: 1.4185, Domain Loss: 1.3912, Class Loss: 0.0273
Epoch 31/50, Loss: 1.3947, Domain Loss: 1.3788, Class Loss: 0.0159
Epoch 32/50, Loss: 1.4149, Domain Loss: 1.3952, Class Loss: 0.0197
Epoch 33/50, Loss: 1.4263, Domain Loss: 1.3957, Class Loss: 0.0306
Epoch 34/50, Loss: 1.4433, Domain Loss: 1.4021, Class Loss: 0.0412
Epoch 35/50, Loss: 1.4254, Domain Loss: 1.4013, Class Loss: 0.0240
Epoch 36/50, Loss: 1.4064, Domain Loss: 1.3876, Class Loss: 0.0189
Epoch 37/50, Loss: 1.4145, Domain Loss: 1.3927, Class Loss: 0.0218
Epoch 38/50, Loss: 1.4543, Domain Loss: 1.3928, Class Loss: 0.0615
Epoch 39/50, Loss: 1.4677, Domain Loss: 1.3924, Class Loss: 0.0753
Epoch 40/50, Loss: 1.4861, Domain Loss: 1.3943, Class Loss: 0.0918
Epoch 41/50, Loss: 1.4208, Domain Loss: 1.3810, Class Loss: 0.0399
Epoch 42/50, Loss: 1.4505, Domain Loss: 1.3942, Class Loss: 0.0563
Epoch 43/50, Loss: 1.4174, Domain Loss: 1.3822, Class Loss: 0.0352
Epoch 44/50, Loss: 1.4245, Domain Loss: 1.3886, Class Loss: 0.0359
Epoch 45/50, Loss: 1.4231, Domain Loss: 1.3887, Class Loss: 0.0344
Epoch 46/50, Loss: 1.4155, Domain Loss: 1.3879, Class Loss: 0.0277
Epoch 47/50, Loss: 1.4196, Domain Loss: 1.3898, Class Loss: 0.0298
Epoch 48/50, Loss: 1.4058, Domain Loss: 1.3843, Class Loss: 0.0215
Epoch 49/50, Loss: 1.4379, Domain Loss: 1.3970, Class Loss: 0.0409
Epoch 50/50, Loss: 1.4182, Domain Loss: 1.3877, Class Loss: 0.0305
99.82


Epoch 1/50, Loss: 2.3655, Domain Loss: 1.4021, Class Loss: 0.9635
Epoch 2/50, Loss: 1.6959, Domain Loss: 1.3861, Class Loss: 0.3098
Epoch 3/50, Loss: 1.4863, Domain Loss: 1.3825, Class Loss: 0.1038
Epoch 4/50, Loss: 1.4457, Domain Loss: 1.3795, Class Loss: 0.0663
Epoch 5/50, Loss: 1.4248, Domain Loss: 1.3719, Class Loss: 0.0529
Epoch 6/50, Loss: 1.4882, Domain Loss: 1.3907, Class Loss: 0.0975
Epoch 7/50, Loss: 2.0772, Domain Loss: 1.9567, Class Loss: 0.1205
Epoch 8/50, Loss: 3.7918, Domain Loss: 3.6509, Class Loss: 0.1409
Epoch 9/50, Loss: 6.4928, Domain Loss: 6.1503, Class Loss: 0.3425
Epoch 10/50, Loss: 5.8830, Domain Loss: 5.1800, Class Loss: 0.7029
Epoch 11/50, Loss: 1.9206, Domain Loss: 1.7240, Class Loss: 0.1966
Epoch 12/50, Loss: 1.6190, Domain Loss: 1.4880, Class Loss: 0.1309
Epoch 13/50, Loss: 1.5505, Domain Loss: 1.4289, Class Loss: 0.1217
Epoch 14/50, Loss: 1.4900, Domain Loss: 1.4115, Class Loss: 0.0785
Epoch 15/50, Loss: 1.4813, Domain Loss: 1.4141, Class Loss: 0.0671
Epoch 16/50, Loss: 1.5344, Domain Loss: 1.4828, Class Loss: 0.0516
Epoch 17/50, Loss: 3.5007, Domain Loss: 3.0929, Class Loss: 0.4078
Epoch 18/50, Loss: 3.3784, Domain Loss: 3.0780, Class Loss: 0.3005
Epoch 19/50, Loss: 4.7445, Domain Loss: 3.4535, Class Loss: 1.2911
Epoch 20/50, Loss: 2.7793, Domain Loss: 2.4914, Class Loss: 0.2879
Epoch 21/50, Loss: 1.8668, Domain Loss: 1.6650, Class Loss: 0.2018
Epoch 22/50, Loss: 1.6338, Domain Loss: 1.4891, Class Loss: 0.1448
Epoch 23/50, Loss: 1.5041, Domain Loss: 1.4014, Class Loss: 0.1027
Epoch 24/50, Loss: 1.5629, Domain Loss: 1.4672, Class Loss: 0.0958
Epoch 25/50, Loss: 1.7846, Domain Loss: 1.6527, Class Loss: 0.1319
Epoch 26/50, Loss: 1.5244, Domain Loss: 1.4370, Class Loss: 0.0874
Epoch 27/50, Loss: 1.4758, Domain Loss: 1.4205, Class Loss: 0.0553
Epoch 28/50, Loss: 1.4643, Domain Loss: 1.4308, Class Loss: 0.0334
Epoch 29/50, Loss: 1.4667, Domain Loss: 1.4005, Class Loss: 0.0662
Epoch 30/50, Loss: 1.4509, Domain Loss: 1.3991, Class Loss: 0.0519
Epoch 31/50, Loss: 1.4724, Domain Loss: 1.4256, Class Loss: 0.0468
Epoch 32/50, Loss: 1.4489, Domain Loss: 1.4202, Class Loss: 0.0287
Epoch 33/50, Loss: 1.4653, Domain Loss: 1.4408, Class Loss: 0.0246
Epoch 34/50, Loss: 1.4798, Domain Loss: 1.4356, Class Loss: 0.0441
Epoch 35/50, Loss: 1.4727, Domain Loss: 1.4370, Class Loss: 0.0356
Epoch 36/50, Loss: 1.4593, Domain Loss: 1.4211, Class Loss: 0.0381
Epoch 37/50, Loss: 1.6775, Domain Loss: 1.5531, Class Loss: 0.1244
Epoch 38/50, Loss: 1.9784, Domain Loss: 1.8109, Class Loss: 0.1675
Epoch 39/50, Loss: 1.5332, Domain Loss: 1.4667, Class Loss: 0.0665
Epoch 40/50, Loss: 1.5022, Domain Loss: 1.4488, Class Loss: 0.0534
Epoch 41/50, Loss: 1.4998, Domain Loss: 1.4475, Class Loss: 0.0523
Epoch 42/50, Loss: 1.4660, Domain Loss: 1.4180, Class Loss: 0.0479
Epoch 43/50, Loss: 1.4979, Domain Loss: 1.4475, Class Loss: 0.0504
Epoch 44/50, Loss: 1.4697, Domain Loss: 1.4262, Class Loss: 0.0435
Epoch 45/50, Loss: 1.4480, Domain Loss: 1.3968, Class Loss: 0.0512
Epoch 46/50, Loss: 1.4380, Domain Loss: 1.4032, Class Loss: 0.0349
Epoch 47/50, Loss: 1.4278, Domain Loss: 1.4037, Class Loss: 0.0241
Epoch 48/50, Loss: 1.4701, Domain Loss: 1.4252, Class Loss: 0.0449
Epoch 49/50, Loss: 1.5213, Domain Loss: 1.4928, Class Loss: 0.0285
Epoch 50/50, Loss: 2.3657, Domain Loss: 2.3101, Class Loss: 0.0556
64.63


Epoch 1/50, Loss: 2.3182, Domain Loss: 1.4093, Class Loss: 0.9088
Epoch 2/50, Loss: 1.6862, Domain Loss: 1.3861, Class Loss: 0.3001
Epoch 3/50, Loss: 1.4833, Domain Loss: 1.3792, Class Loss: 0.1041
Epoch 4/50, Loss: 1.4746, Domain Loss: 1.3849, Class Loss: 0.0897
Epoch 5/50, Loss: 1.5030, Domain Loss: 1.4032, Class Loss: 0.0998
Epoch 6/50, Loss: 1.7447, Domain Loss: 1.6614, Class Loss: 0.0834
Epoch 7/50, Loss: 5.1073, Domain Loss: 4.9930, Class Loss: 0.1143
Epoch 8/50, Loss: 5.4517, Domain Loss: 5.2723, Class Loss: 0.1794
Epoch 9/50, Loss: 2.5726, Domain Loss: 2.4272, Class Loss: 0.1454
Epoch 10/50, Loss: 3.8530, Domain Loss: 3.2694, Class Loss: 0.5836
Epoch 11/50, Loss: 5.9774, Domain Loss: 5.3956, Class Loss: 0.5818
Epoch 12/50, Loss: 10.1716, Domain Loss: 9.5864, Class Loss: 0.5852
Epoch 13/50, Loss: 4.2924, Domain Loss: 4.0373, Class Loss: 0.2552
Epoch 14/50, Loss: 2.5485, Domain Loss: 2.3778, Class Loss: 0.1707
Epoch 15/50, Loss: 5.5923, Domain Loss: 3.3001, Class Loss: 2.2921
Epoch 16/50, Loss: 6.5703, Domain Loss: 5.7391, Class Loss: 0.8312
Epoch 17/50, Loss: 3.9586, Domain Loss: 3.3096, Class Loss: 0.6490
Epoch 18/50, Loss: 2.3808, Domain Loss: 2.0185, Class Loss: 0.3623
Epoch 19/50, Loss: 1.7756, Domain Loss: 1.5295, Class Loss: 0.2460
Epoch 20/50, Loss: 1.6414, Domain Loss: 1.4714, Class Loss: 0.1700
Epoch 21/50, Loss: 1.5121, Domain Loss: 1.4038, Class Loss: 0.1084
Epoch 22/50, Loss: 1.4792, Domain Loss: 1.3929, Class Loss: 0.0863
Epoch 23/50, Loss: 1.4688, Domain Loss: 1.3874, Class Loss: 0.0814
Epoch 24/50, Loss: 1.4547, Domain Loss: 1.3903, Class Loss: 0.0644
Epoch 25/50, Loss: 1.4512, Domain Loss: 1.3800, Class Loss: 0.0712
Epoch 26/50, Loss: 1.4250, Domain Loss: 1.3740, Class Loss: 0.0510
Epoch 27/50, Loss: 1.4303, Domain Loss: 1.3788, Class Loss: 0.0515
Epoch 28/50, Loss: 1.4337, Domain Loss: 1.3876, Class Loss: 0.0461
Epoch 29/50, Loss: 1.4346, Domain Loss: 1.3938, Class Loss: 0.0408
Epoch 30/50, Loss: 1.4367, Domain Loss: 1.3927, Class Loss: 0.0439
Epoch 31/50, Loss: 1.4362, Domain Loss: 1.3844, Class Loss: 0.0518
Epoch 32/50, Loss: 1.4247, Domain Loss: 1.3819, Class Loss: 0.0428
Epoch 33/50, Loss: 1.4230, Domain Loss: 1.3968, Class Loss: 0.0262
Epoch 34/50, Loss: 1.4306, Domain Loss: 1.3939, Class Loss: 0.0367
Epoch 35/50, Loss: 1.4323, Domain Loss: 1.3887, Class Loss: 0.0436
Epoch 36/50, Loss: 1.4337, Domain Loss: 1.3942, Class Loss: 0.0395
Epoch 37/50, Loss: 1.4206, Domain Loss: 1.3939, Class Loss: 0.0267
Epoch 38/50, Loss: 1.4188, Domain Loss: 1.3933, Class Loss: 0.0254
Epoch 39/50, Loss: 1.4334, Domain Loss: 1.3986, Class Loss: 0.0348
Epoch 40/50, Loss: 1.4334, Domain Loss: 1.3991, Class Loss: 0.0342
Epoch 41/50, Loss: 1.4119, Domain Loss: 1.3931, Class Loss: 0.0188
Epoch 42/50, Loss: 1.4167, Domain Loss: 1.3922, Class Loss: 0.0244
Epoch 43/50, Loss: 1.4020, Domain Loss: 1.3856, Class Loss: 0.0164
Epoch 44/50, Loss: 1.4052, Domain Loss: 1.3861, Class Loss: 0.0191
Epoch 45/50, Loss: 1.4013, Domain Loss: 1.3870, Class Loss: 0.0143
Epoch 46/50, Loss: 1.4122, Domain Loss: 1.3952, Class Loss: 0.0170
Epoch 47/50, Loss: 1.4078, Domain Loss: 1.3900, Class Loss: 0.0179
Epoch 48/50, Loss: 1.4133, Domain Loss: 1.3890, Class Loss: 0.0243
Epoch 49/50, Loss: 1.4044, Domain Loss: 1.3930, Class Loss: 0.0114
Epoch 50/50, Loss: 1.4045, Domain Loss: 1.3916, Class Loss: 0.0130
99.82


Epoch 1/50, Loss: 2.3982, Domain Loss: 1.3956, Class Loss: 1.0026
Epoch 2/50, Loss: 1.8772, Domain Loss: 1.3783, Class Loss: 0.4989
Epoch 3/50, Loss: 1.5645, Domain Loss: 1.3698, Class Loss: 0.1948
Epoch 4/50, Loss: 1.5174, Domain Loss: 1.3781, Class Loss: 0.1392
Epoch 5/50, Loss: 1.4584, Domain Loss: 1.3813, Class Loss: 0.0771
Epoch 6/50, Loss: 2.1237, Domain Loss: 2.0663, Class Loss: 0.0575
Epoch 7/50, Loss: 5.9366, Domain Loss: 5.6999, Class Loss: 0.2367
Epoch 8/50, Loss: 6.6331, Domain Loss: 6.3399, Class Loss: 0.2932
Epoch 9/50, Loss: 2.8968, Domain Loss: 2.7427, Class Loss: 0.1541
Epoch 10/50, Loss: 2.0635, Domain Loss: 1.9571, Class Loss: 0.1064
Epoch 11/50, Loss: 4.1888, Domain Loss: 2.8327, Class Loss: 1.3561
Epoch 12/50, Loss: 2.5482, Domain Loss: 2.1046, Class Loss: 0.4436
Epoch 13/50, Loss: 1.9021, Domain Loss: 1.6340, Class Loss: 0.2681
Epoch 14/50, Loss: 1.9396, Domain Loss: 1.6398, Class Loss: 0.2998
Epoch 15/50, Loss: 1.6523, Domain Loss: 1.4962, Class Loss: 0.1560
Epoch 16/50, Loss: 1.5797, Domain Loss: 1.4696, Class Loss: 0.1100
Epoch 17/50, Loss: 1.5047, Domain Loss: 1.4446, Class Loss: 0.0600
Epoch 18/50, Loss: 1.6449, Domain Loss: 1.5453, Class Loss: 0.0997
Epoch 19/50, Loss: 1.5945, Domain Loss: 1.5285, Class Loss: 0.0660
Epoch 20/50, Loss: 1.8882, Domain Loss: 1.7993, Class Loss: 0.0890
Epoch 21/50, Loss: 9.5719, Domain Loss: 8.2630, Class Loss: 1.3090
Epoch 22/50, Loss: 7.0203, Domain Loss: 6.4943, Class Loss: 0.5259
Epoch 23/50, Loss: 3.7114, Domain Loss: 3.2877, Class Loss: 0.4237
Epoch 24/50, Loss: 2.0164, Domain Loss: 1.7120, Class Loss: 0.3045
Epoch 25/50, Loss: 1.7076, Domain Loss: 1.4971, Class Loss: 0.2105
Epoch 26/50, Loss: 1.5707, Domain Loss: 1.4355, Class Loss: 0.1352
Epoch 27/50, Loss: 1.4911, Domain Loss: 1.3950, Class Loss: 0.0961
Epoch 28/50, Loss: 1.4827, Domain Loss: 1.4048, Class Loss: 0.0780
Epoch 29/50, Loss: 1.4670, Domain Loss: 1.3916, Class Loss: 0.0754
Epoch 30/50, Loss: 1.4430, Domain Loss: 1.3869, Class Loss: 0.0561
Epoch 31/50, Loss: 1.4330, Domain Loss: 1.3795, Class Loss: 0.0535
Epoch 32/50, Loss: 1.4190, Domain Loss: 1.3870, Class Loss: 0.0320
Epoch 33/50, Loss: 1.4184, Domain Loss: 1.3833, Class Loss: 0.0351
Epoch 34/50, Loss: 1.4221, Domain Loss: 1.3909, Class Loss: 0.0312
Epoch 35/50, Loss: 1.4393, Domain Loss: 1.4048, Class Loss: 0.0346
Epoch 36/50, Loss: 1.4270, Domain Loss: 1.3926, Class Loss: 0.0344
Epoch 37/50, Loss: 1.4372, Domain Loss: 1.3951, Class Loss: 0.0420
Epoch 38/50, Loss: 1.4197, Domain Loss: 1.3864, Class Loss: 0.0333
Epoch 39/50, Loss: 1.4269, Domain Loss: 1.3990, Class Loss: 0.0279
Epoch 40/50, Loss: 1.4420, Domain Loss: 1.4155, Class Loss: 0.0265
Epoch 41/50, Loss: 1.4130, Domain Loss: 1.3984, Class Loss: 0.0146
Epoch 42/50, Loss: 1.4145, Domain Loss: 1.3970, Class Loss: 0.0174
Epoch 43/50, Loss: 1.4067, Domain Loss: 1.3916, Class Loss: 0.0151
Epoch 44/50, Loss: 1.4161, Domain Loss: 1.4042, Class Loss: 0.0119
Epoch 45/50, Loss: 1.4261, Domain Loss: 1.4152, Class Loss: 0.0109
Epoch 46/50, Loss: 1.4136, Domain Loss: 1.3912, Class Loss: 0.0224
Epoch 47/50, Loss: 1.4118, Domain Loss: 1.3890, Class Loss: 0.0228
Epoch 48/50, Loss: 1.4076, Domain Loss: 1.3958, Class Loss: 0.0118
Epoch 49/50, Loss: 1.4107, Domain Loss: 1.4007, Class Loss: 0.0100
Epoch 50/50, Loss: 1.4343, Domain Loss: 1.4180, Class Loss: 0.0163
99.64


Source performance:
91.12 94.01 91.05 89.70 
Target performance:
90.92 94.01 90.83 89.28 

Per-class target performance: 91.72 95.05 87.67 88.86 Deep CORAL
Deep CORAL Run 1/10
Epoch 1: Source Val Acc = 0.6493, Target Val Acc = 0.7290
Epoch 2: Source Val Acc = 0.9970, Target Val Acc = 0.9964
Epoch 3: Source Val Acc = 0.8076, Target Val Acc = 0.8213
Epoch 4: Source Val Acc = 0.9976, Target Val Acc = 0.9904
Epoch 5: Source Val Acc = 0.9868, Target Val Acc = 0.9946
Epoch 6: Source Val Acc = 0.9185, Target Val Acc = 0.8981
Epoch 7: Source Val Acc = 0.9994, Target Val Acc = 0.9952
Epoch 8: Source Val Acc = 0.9748, Target Val Acc = 0.9718
Epoch 9: Source Val Acc = 0.9928, Target Val Acc = 0.9790
Epoch 10: Source Val Acc = 0.7500, Target Val Acc = 0.7650
Epoch 11: Source Val Acc = 0.9886, Target Val Acc = 0.9910
Epoch 12: Source Val Acc = 0.8513, Target Val Acc = 0.8801
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.8513, Target Val Acc = 0.8801

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.7752, Target Val Acc = 0.7788
Epoch 2: Source Val Acc = 0.7086, Target Val Acc = 0.7494
Epoch 3: Source Val Acc = 0.6055, Target Val Acc = 0.6481
Epoch 4: Source Val Acc = 0.7398, Target Val Acc = 0.7554
Epoch 5: Source Val Acc = 0.9982, Target Val Acc = 0.9982
Epoch 6: Source Val Acc = 0.8807, Target Val Acc = 0.9107
Epoch 7: Source Val Acc = 0.8987, Target Val Acc = 0.9149
Epoch 8: Source Val Acc = 0.9994, Target Val Acc = 0.9964
Epoch 9: Source Val Acc = 0.8483, Target Val Acc = 0.7752
Epoch 10: Source Val Acc = 0.6157, Target Val Acc = 0.5156
Epoch 11: Source Val Acc = 0.9970, Target Val Acc = 0.9976
Epoch 12: Source Val Acc = 0.9970, Target Val Acc = 0.9982
Epoch 13: Source Val Acc = 0.9952, Target Val Acc = 0.9868
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9952, Target Val Acc = 0.9868

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.8885, Target Val Acc = 0.8879
Epoch 2: Source Val Acc = 0.9886, Target Val Acc = 0.9940
Epoch 3: Source Val Acc = 0.8867, Target Val Acc = 0.9125
Epoch 4: Source Val Acc = 0.9748, Target Val Acc = 0.9616
Epoch 5: Source Val Acc = 0.9976, Target Val Acc = 0.9970
Epoch 6: Source Val Acc = 0.8699, Target Val Acc = 0.8969
Epoch 7: Source Val Acc = 0.9922, Target Val Acc = 0.9946
Epoch 8: Source Val Acc = 0.9868, Target Val Acc = 0.9898
Epoch 9: Source Val Acc = 0.9928, Target Val Acc = 0.9928
Epoch 10: Source Val Acc = 0.8849, Target Val Acc = 0.9388
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.8849, Target Val Acc = 0.9388

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.7500, Target Val Acc = 0.7398
Epoch 2: Source Val Acc = 0.9958, Target Val Acc = 0.9940
Epoch 3: Source Val Acc = 0.7320, Target Val Acc = 0.6337
Epoch 4: Source Val Acc = 0.1391, Target Val Acc = 0.1751
Epoch 5: Source Val Acc = 0.9898, Target Val Acc = 0.9964
Epoch 6: Source Val Acc = 0.9856, Target Val Acc = 0.9892
Epoch 7: Source Val Acc = 0.9982, Target Val Acc = 0.9970
Epoch 8: Source Val Acc = 0.9754, Target Val Acc = 0.9838
Epoch 9: Source Val Acc = 0.9994, Target Val Acc = 0.9964
Epoch 10: Source Val Acc = 0.9994, Target Val Acc = 0.9988
Epoch 11: Source Val Acc = 1.0000, Target Val Acc = 0.9982
Epoch 12: Source Val Acc = 0.9335, Target Val Acc = 0.9430
Epoch 13: Source Val Acc = 1.0000, Target Val Acc = 0.9982
Epoch 14: Source Val Acc = 0.9994, Target Val Acc = 0.9964
Epoch 15: Source Val Acc = 0.9796, Target Val Acc = 0.9880
Epoch 16: Source Val Acc = 0.9976, Target Val Acc = 0.9982
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.9976, Target Val Acc = 0.9982

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.6888, Target Val Acc = 0.7410
Epoch 2: Source Val Acc = 0.9844, Target Val Acc = 0.9766
Epoch 3: Source Val Acc = 0.9478, Target Val Acc = 0.9161
Epoch 4: Source Val Acc = 0.9676, Target Val Acc = 0.9544
Epoch 5: Source Val Acc = 0.9580, Target Val Acc = 0.9742
Epoch 6: Source Val Acc = 0.9760, Target Val Acc = 0.9442
Epoch 7: Source Val Acc = 1.0000, Target Val Acc = 0.9964
Epoch 8: Source Val Acc = 1.0000, Target Val Acc = 0.9976
Epoch 9: Source Val Acc = 0.9628, Target Val Acc = 0.9832
Epoch 10: Source Val Acc = 0.9994, Target Val Acc = 0.9976
Epoch 11: Source Val Acc = 0.8591, Target Val Acc = 0.8873
Epoch 12: Source Val Acc = 0.9982, Target Val Acc = 0.9976
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.9982, Target Val Acc = 0.9976

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.2602, Target Val Acc = 0.2620
Epoch 2: Source Val Acc = 0.7596, Target Val Acc = 0.7386
Epoch 3: Source Val Acc = 0.4760, Target Val Acc = 0.4958
Epoch 4: Source Val Acc = 0.9466, Target Val Acc = 0.9287
Epoch 5: Source Val Acc = 0.6133, Target Val Acc = 0.6169
Epoch 6: Source Val Acc = 0.9898, Target Val Acc = 0.9940
Epoch 7: Source Val Acc = 0.9958, Target Val Acc = 0.9958
Epoch 8: Source Val Acc = 0.9904, Target Val Acc = 0.9958
Epoch 9: Source Val Acc = 0.9994, Target Val Acc = 0.9982
Epoch 10: Source Val Acc = 0.9988, Target Val Acc = 0.9976
Epoch 11: Source Val Acc = 0.9982, Target Val Acc = 0.9946
Epoch 12: Source Val Acc = 0.9994, Target Val Acc = 0.9976
Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 0.9982
Epoch 14: Source Val Acc = 0.8957, Target Val Acc = 0.9646
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 0.8957, Target Val Acc = 0.9646

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.2764, Target Val Acc = 0.2776
Epoch 2: Source Val Acc = 0.5024, Target Val Acc = 0.5228
Epoch 3: Source Val Acc = 0.9946, Target Val Acc = 0.9970
Epoch 4: Source Val Acc = 0.7038, Target Val Acc = 0.6301
Epoch 5: Source Val Acc = 0.6811, Target Val Acc = 0.7206
Epoch 6: Source Val Acc = 0.9934, Target Val Acc = 0.9856
Epoch 7: Source Val Acc = 0.6559, Target Val Acc = 0.6589
Epoch 8: Source Val Acc = 0.9946, Target Val Acc = 0.9964
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.9946, Target Val Acc = 0.9964

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.7872, Target Val Acc = 0.7734
Epoch 2: Source Val Acc = 0.9568, Target Val Acc = 0.9706
Epoch 3: Source Val Acc = 0.7560, Target Val Acc = 0.7380
Epoch 4: Source Val Acc = 0.9910, Target Val Acc = 0.9676
Epoch 5: Source Val Acc = 0.9814, Target Val Acc = 0.9568
Epoch 6: Source Val Acc = 0.7212, Target Val Acc = 0.7104
Epoch 7: Source Val Acc = 0.9886, Target Val Acc = 0.9934
Epoch 8: Source Val Acc = 0.9976, Target Val Acc = 0.9976
Epoch 9: Source Val Acc = 0.5372, Target Val Acc = 0.5090
Epoch 10: Source Val Acc = 0.8969, Target Val Acc = 0.9221
Epoch 11: Source Val Acc = 0.9850, Target Val Acc = 0.9898
Epoch 12: Source Val Acc = 0.3040, Target Val Acc = 0.3165
Epoch 13: Source Val Acc = 0.7764, Target Val Acc = 0.8082
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.7764, Target Val Acc = 0.8082

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.8165, Target Val Acc = 0.8435
Epoch 2: Source Val Acc = 0.9772, Target Val Acc = 0.9880
Epoch 3: Source Val Acc = 0.5114, Target Val Acc = 0.5402
Epoch 4: Source Val Acc = 0.9922, Target Val Acc = 0.9922
Epoch 5: Source Val Acc = 0.7488, Target Val Acc = 0.7218
Epoch 6: Source Val Acc = 0.9880, Target Val Acc = 0.9910
Epoch 7: Source Val Acc = 0.9964, Target Val Acc = 0.9964
Epoch 8: Source Val Acc = 0.5192, Target Val Acc = 0.5282
Epoch 9: Source Val Acc = 0.6181, Target Val Acc = 0.5929
Epoch 10: Source Val Acc = 0.9958, Target Val Acc = 0.9958
Epoch 11: Source Val Acc = 0.9604, Target Val Acc = 0.9400
Epoch 12: Source Val Acc = 0.9958, Target Val Acc = 0.9916
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 0.9958, Target Val Acc = 0.9916

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.9634, Target Val Acc = 0.9628
Epoch 2: Source Val Acc = 0.7512, Target Val Acc = 0.7248
Epoch 3: Source Val Acc = 0.9952, Target Val Acc = 0.9964
Epoch 4: Source Val Acc = 0.7362, Target Val Acc = 0.7554
Epoch 5: Source Val Acc = 0.9856, Target Val Acc = 0.9862
Epoch 6: Source Val Acc = 0.9862, Target Val Acc = 0.9778
Epoch 7: Source Val Acc = 0.9946, Target Val Acc = 0.9868
Epoch 8: Source Val Acc = 0.9988, Target Val Acc = 0.9988
Epoch 9: Source Val Acc = 0.9844, Target Val Acc = 0.9724
Epoch 10: Source Val Acc = 0.9988, Target Val Acc = 0.9988
Epoch 11: Source Val Acc = 0.9784, Target Val Acc = 0.9886
Epoch 12: Source Val Acc = 0.9940, Target Val Acc = 0.9928
Epoch 13: Source Val Acc = 0.9808, Target Val Acc = 0.9874
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.9808, Target Val Acc = 0.9874

Deep CORAL: Average Source Val Acc = 0.9371, Average Target Val Acc = 0.9550
STAR

Run 1/10
Epoch [1/50], Class Loss: 2.1150, Discrepancy Loss: 0.1399
Epoch [2/50], Class Loss: 0.7434, Discrepancy Loss: 0.0765
Epoch [3/50], Class Loss: 0.1755, Discrepancy Loss: 0.0209
Epoch [4/50], Class Loss: 0.0593, Discrepancy Loss: 0.0105
Epoch [5/50], Class Loss: 0.1050, Discrepancy Loss: 0.0090
Epoch [6/50], Class Loss: 0.0484, Discrepancy Loss: 0.0074
Epoch [7/50], Class Loss: 0.0644, Discrepancy Loss: 0.0080
Epoch [8/50], Class Loss: 0.0246, Discrepancy Loss: 0.0044
Epoch [9/50], Class Loss: 0.0193, Discrepancy Loss: 0.0043
Epoch [10/50], Class Loss: 0.0131, Discrepancy Loss: 0.0025
Epoch [11/50], Class Loss: 0.0162, Discrepancy Loss: 0.0035
Epoch [12/50], Class Loss: 0.0142, Discrepancy Loss: 0.0027
Epoch [13/50], Class Loss: 0.0046, Discrepancy Loss: 0.0022
Epoch [14/50], Class Loss: 0.0063, Discrepancy Loss: 0.0022
Epoch [15/50], Class Loss: 0.0059, Discrepancy Loss: 0.0021
Epoch [16/50], Class Loss: 0.0074, Discrepancy Loss: 0.0026
Epoch [17/50], Class Loss: 0.0041, Discrepancy Loss: 0.0018
Epoch [18/50], Class Loss: 0.0100, Discrepancy Loss: 0.0016
Epoch [19/50], Class Loss: 0.0092, Discrepancy Loss: 0.0038
Epoch [20/50], Class Loss: 0.0052, Discrepancy Loss: 0.0016
Epoch [21/50], Class Loss: 0.0033, Discrepancy Loss: 0.0016
Epoch [22/50], Class Loss: 0.0041, Discrepancy Loss: 0.0016
Epoch [23/50], Class Loss: 0.0031, Discrepancy Loss: 0.0020
Epoch [24/50], Class Loss: 0.0016, Discrepancy Loss: 0.0020
Epoch [25/50], Class Loss: 0.0051, Discrepancy Loss: 0.0020
Epoch [26/50], Class Loss: 0.0045, Discrepancy Loss: 0.0022
Epoch [27/50], Class Loss: 0.0046, Discrepancy Loss: 0.0021
Epoch [28/50], Class Loss: 0.0115, Discrepancy Loss: 0.0019
Epoch [29/50], Class Loss: 0.0034, Discrepancy Loss: 0.0014
Epoch [30/50], Class Loss: 0.0039, Discrepancy Loss: 0.0027
Epoch [31/50], Class Loss: 0.0039, Discrepancy Loss: 0.0012
Epoch [32/50], Class Loss: 0.0101, Discrepancy Loss: 0.0020
Epoch [33/50], Class Loss: 0.0035, Discrepancy Loss: 0.0036
Epoch [34/50], Class Loss: 0.0037, Discrepancy Loss: 0.0018
Epoch [35/50], Class Loss: 0.0062, Discrepancy Loss: 0.0020
Epoch [36/50], Class Loss: 0.0028, Discrepancy Loss: 0.0016
Epoch [37/50], Class Loss: 0.0070, Discrepancy Loss: 0.0016
Epoch [38/50], Class Loss: 0.0769, Discrepancy Loss: 0.0026
Epoch [39/50], Class Loss: 0.0026, Discrepancy Loss: 0.0017
Epoch [40/50], Class Loss: 0.0041, Discrepancy Loss: 0.0014
Epoch [41/50], Class Loss: 0.0030, Discrepancy Loss: 0.0015
Epoch [42/50], Class Loss: 0.0047, Discrepancy Loss: 0.0024
Epoch [43/50], Class Loss: 0.0041, Discrepancy Loss: 0.0019
Epoch [44/50], Class Loss: 0.0061, Discrepancy Loss: 0.0015
Epoch [45/50], Class Loss: 0.0065, Discrepancy Loss: 0.0016
Epoch [46/50], Class Loss: 0.0103, Discrepancy Loss: 0.0014
Epoch [47/50], Class Loss: 0.0029, Discrepancy Loss: 0.0015
Epoch [48/50], Class Loss: 0.0029, Discrepancy Loss: 0.0020
Epoch [49/50], Class Loss: 0.0089, Discrepancy Loss: 0.0021
Epoch [50/50], Class Loss: 0.0040, Discrepancy Loss: 0.0021
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 2/10
Epoch [1/50], Class Loss: 1.4677, Discrepancy Loss: 0.1053
Epoch [2/50], Class Loss: 0.2951, Discrepancy Loss: 0.0287
Epoch [3/50], Class Loss: 0.1391, Discrepancy Loss: 0.0121
Epoch [4/50], Class Loss: 0.1018, Discrepancy Loss: 0.0110
Epoch [5/50], Class Loss: 0.0234, Discrepancy Loss: 0.0048
Epoch [6/50], Class Loss: 0.1564, Discrepancy Loss: 0.0155
Epoch [7/50], Class Loss: 0.0493, Discrepancy Loss: 0.0082
Epoch [8/50], Class Loss: 0.0301, Discrepancy Loss: 0.0065
Epoch [9/50], Class Loss: 0.0266, Discrepancy Loss: 0.0058
Epoch [10/50], Class Loss: 0.0799, Discrepancy Loss: 0.0043
Epoch [11/50], Class Loss: 0.0355, Discrepancy Loss: 0.0172
Epoch [12/50], Class Loss: 0.0121, Discrepancy Loss: 0.0048
Epoch [13/50], Class Loss: 0.0107, Discrepancy Loss: 0.0020
Epoch [14/50], Class Loss: 0.0202, Discrepancy Loss: 0.0037
Epoch [15/50], Class Loss: 0.0141, Discrepancy Loss: 0.0031
Epoch [16/50], Class Loss: 0.0051, Discrepancy Loss: 0.0022
Epoch [17/50], Class Loss: 0.0046, Discrepancy Loss: 0.0013
Epoch [18/50], Class Loss: 0.0105, Discrepancy Loss: 0.0026
Epoch [19/50], Class Loss: 0.0084, Discrepancy Loss: 0.0022
Epoch [20/50], Class Loss: 0.0064, Discrepancy Loss: 0.0017
Epoch [21/50], Class Loss: 0.0046, Discrepancy Loss: 0.0017
Epoch [22/50], Class Loss: 0.0038, Discrepancy Loss: 0.0012
Epoch [23/50], Class Loss: 0.0612, Discrepancy Loss: 0.0014
Epoch [24/50], Class Loss: 0.0125, Discrepancy Loss: 0.0022
Epoch [25/50], Class Loss: 0.0071, Discrepancy Loss: 0.0016
Epoch [26/50], Class Loss: 0.1287, Discrepancy Loss: 0.0012
Epoch [27/50], Class Loss: 0.0236, Discrepancy Loss: 0.0019
Epoch [28/50], Class Loss: 0.0063, Discrepancy Loss: 0.0023
Epoch [29/50], Class Loss: 0.0032, Discrepancy Loss: 0.0015
Epoch [30/50], Class Loss: 0.0600, Discrepancy Loss: 0.0022
Epoch [31/50], Class Loss: 0.0059, Discrepancy Loss: 0.0012
Epoch [32/50], Class Loss: 0.0069, Discrepancy Loss: 0.0021
Epoch [33/50], Class Loss: 0.0110, Discrepancy Loss: 0.0009
Epoch [34/50], Class Loss: 0.0021, Discrepancy Loss: 0.0024
Epoch [35/50], Class Loss: 0.0037, Discrepancy Loss: 0.0010
Epoch [36/50], Class Loss: 0.0048, Discrepancy Loss: 0.0015
Epoch [37/50], Class Loss: 0.0547, Discrepancy Loss: 0.0013
Epoch [38/50], Class Loss: 0.0068, Discrepancy Loss: 0.0016
Epoch [39/50], Class Loss: 0.0061, Discrepancy Loss: 0.0014
Epoch [40/50], Class Loss: 0.0116, Discrepancy Loss: 0.0014
Epoch [41/50], Class Loss: 0.0046, Discrepancy Loss: 0.0012
Epoch [42/50], Class Loss: 0.0035, Discrepancy Loss: 0.0018
Epoch [43/50], Class Loss: 0.0043, Discrepancy Loss: 0.0018
Epoch [44/50], Class Loss: 0.0049, Discrepancy Loss: 0.0021
Epoch [45/50], Class Loss: 0.0473, Discrepancy Loss: 0.0012
Epoch [46/50], Class Loss: 0.0068, Discrepancy Loss: 0.0018
Epoch [47/50], Class Loss: 0.0038, Discrepancy Loss: 0.0016
Epoch [48/50], Class Loss: 0.1430, Discrepancy Loss: 0.0022
Epoch [49/50], Class Loss: 0.0041, Discrepancy Loss: 0.0016
Epoch [50/50], Class Loss: 0.0053, Discrepancy Loss: 0.0014
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 3/10
Epoch [1/50], Class Loss: 1.4766, Discrepancy Loss: 0.0912
Epoch [2/50], Class Loss: 0.5571, Discrepancy Loss: 0.0404
Epoch [3/50], Class Loss: 0.1467, Discrepancy Loss: 0.0086
Epoch [4/50], Class Loss: 0.1174, Discrepancy Loss: 0.0133
Epoch [5/50], Class Loss: 0.2148, Discrepancy Loss: 0.0076
Epoch [6/50], Class Loss: 0.1314, Discrepancy Loss: 0.0092
Epoch [7/50], Class Loss: 0.0271, Discrepancy Loss: 0.0046
Epoch [8/50], Class Loss: 0.0424, Discrepancy Loss: 0.0061
Epoch [9/50], Class Loss: 0.0130, Discrepancy Loss: 0.0042
Epoch [10/50], Class Loss: 0.0073, Discrepancy Loss: 0.0029
Epoch [11/50], Class Loss: 0.0348, Discrepancy Loss: 0.0032
Epoch [12/50], Class Loss: 0.0287, Discrepancy Loss: 0.0071
Epoch [13/50], Class Loss: 0.0081, Discrepancy Loss: 0.0025
Epoch [14/50], Class Loss: 0.0069, Discrepancy Loss: 0.0026
Epoch [15/50], Class Loss: 0.0322, Discrepancy Loss: 0.0030
Epoch [16/50], Class Loss: 0.0102, Discrepancy Loss: 0.0049
Epoch [17/50], Class Loss: 0.0054, Discrepancy Loss: 0.0045
Epoch [18/50], Class Loss: 0.0092, Discrepancy Loss: 0.0067
Epoch [19/50], Class Loss: 0.0076, Discrepancy Loss: 0.0045
Epoch [20/50], Class Loss: 0.0066, Discrepancy Loss: 0.0023
Epoch [21/50], Class Loss: 0.0048, Discrepancy Loss: 0.0029
Epoch [22/50], Class Loss: 0.0045, Discrepancy Loss: 0.0031
Epoch [23/50], Class Loss: 0.0110, Discrepancy Loss: 0.0026
Epoch [24/50], Class Loss: 0.0036, Discrepancy Loss: 0.0049
Epoch [25/50], Class Loss: 0.0225, Discrepancy Loss: 0.0018
Epoch [26/50], Class Loss: 0.0999, Discrepancy Loss: 0.0027
Epoch [27/50], Class Loss: 0.0087, Discrepancy Loss: 0.0028
Epoch [28/50], Class Loss: 0.0067, Discrepancy Loss: 0.0013
Epoch [29/50], Class Loss: 0.0049, Discrepancy Loss: 0.0018
Epoch [30/50], Class Loss: 0.0056, Discrepancy Loss: 0.0015
Epoch [31/50], Class Loss: 0.0063, Discrepancy Loss: 0.0016
Epoch [32/50], Class Loss: 0.0566, Discrepancy Loss: 0.0014
Epoch [33/50], Class Loss: 0.0045, Discrepancy Loss: 0.0036
Epoch [34/50], Class Loss: 0.0051, Discrepancy Loss: 0.0014
Epoch [35/50], Class Loss: 0.0066, Discrepancy Loss: 0.0019
Epoch [36/50], Class Loss: 0.0064, Discrepancy Loss: 0.0017
Epoch [37/50], Class Loss: 0.1277, Discrepancy Loss: 0.0010
Epoch [38/50], Class Loss: 0.0087, Discrepancy Loss: 0.0027
Epoch [39/50], Class Loss: 0.0029, Discrepancy Loss: 0.0016
Epoch [40/50], Class Loss: 0.0090, Discrepancy Loss: 0.0023
Epoch [41/50], Class Loss: 0.0035, Discrepancy Loss: 0.0025
Epoch [42/50], Class Loss: 0.0071, Discrepancy Loss: 0.0022
Epoch [43/50], Class Loss: 0.0016, Discrepancy Loss: 0.0021
Epoch [44/50], Class Loss: 0.0051, Discrepancy Loss: 0.0027
Epoch [45/50], Class Loss: 0.0020, Discrepancy Loss: 0.0028
Epoch [46/50], Class Loss: 0.0116, Discrepancy Loss: 0.0014
Epoch [47/50], Class Loss: 0.0045, Discrepancy Loss: 0.0023
Epoch [48/50], Class Loss: 0.0144, Discrepancy Loss: 0.0018
Epoch [49/50], Class Loss: 0.0029, Discrepancy Loss: 0.0017
Epoch [50/50], Class Loss: 0.0042, Discrepancy Loss: 0.0016
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 4/10
Epoch [1/50], Class Loss: 1.5784, Discrepancy Loss: 0.1187
Epoch [2/50], Class Loss: 0.4455, Discrepancy Loss: 0.0456
Epoch [3/50], Class Loss: 0.1053, Discrepancy Loss: 0.0101
Epoch [4/50], Class Loss: 0.0575, Discrepancy Loss: 0.0077
Epoch [5/50], Class Loss: 0.0422, Discrepancy Loss: 0.0065
Epoch [6/50], Class Loss: 0.1226, Discrepancy Loss: 0.0044
Epoch [7/50], Class Loss: 0.1037, Discrepancy Loss: 0.0181
Epoch [8/50], Class Loss: 0.0396, Discrepancy Loss: 0.0114
Epoch [9/50], Class Loss: 0.0503, Discrepancy Loss: 0.0065
Epoch [10/50], Class Loss: 0.1772, Discrepancy Loss: 0.0048
Epoch [11/50], Class Loss: 0.0772, Discrepancy Loss: 0.0090
Epoch [12/50], Class Loss: 0.0317, Discrepancy Loss: 0.0079
Epoch [13/50], Class Loss: 0.0145, Discrepancy Loss: 0.0041
Epoch [14/50], Class Loss: 0.0433, Discrepancy Loss: 0.0073
Epoch [15/50], Class Loss: 0.0181, Discrepancy Loss: 0.0039
Epoch [16/50], Class Loss: 0.0285, Discrepancy Loss: 0.0027
Epoch [17/50], Class Loss: 0.0288, Discrepancy Loss: 0.0060
Epoch [18/50], Class Loss: 0.1109, Discrepancy Loss: 0.0055
Epoch [19/50], Class Loss: 0.0544, Discrepancy Loss: 0.0049
Epoch [20/50], Class Loss: 0.0246, Discrepancy Loss: 0.0083
Epoch [21/50], Class Loss: 0.0097, Discrepancy Loss: 0.0042
Epoch [22/50], Class Loss: 0.0114, Discrepancy Loss: 0.0034
Epoch [23/50], Class Loss: 0.0381, Discrepancy Loss: 0.0024
Epoch [24/50], Class Loss: 0.0322, Discrepancy Loss: 0.0046
Epoch [25/50], Class Loss: 0.0849, Discrepancy Loss: 0.0034
Epoch [26/50], Class Loss: 0.0061, Discrepancy Loss: 0.0028
Epoch [27/50], Class Loss: 0.0090, Discrepancy Loss: 0.0032
Epoch [28/50], Class Loss: 0.0605, Discrepancy Loss: 0.0030
Epoch [29/50], Class Loss: 0.0074, Discrepancy Loss: 0.0037
Epoch [30/50], Class Loss: 0.0076, Discrepancy Loss: 0.0045
Epoch [31/50], Class Loss: 0.0937, Discrepancy Loss: 0.0033
Epoch [32/50], Class Loss: 0.0124, Discrepancy Loss: 0.0030
Epoch [33/50], Class Loss: 0.0067, Discrepancy Loss: 0.0039
Epoch [34/50], Class Loss: 0.0083, Discrepancy Loss: 0.0036
Epoch [35/50], Class Loss: 0.0074, Discrepancy Loss: 0.0038
Epoch [36/50], Class Loss: 0.0064, Discrepancy Loss: 0.0021
Epoch [37/50], Class Loss: 0.0079, Discrepancy Loss: 0.0067
Epoch [38/50], Class Loss: 0.0184, Discrepancy Loss: 0.0027
Epoch [39/50], Class Loss: 0.0138, Discrepancy Loss: 0.0042
Epoch [40/50], Class Loss: 0.0221, Discrepancy Loss: 0.0032
Epoch [41/50], Class Loss: 0.0133, Discrepancy Loss: 0.0040
Epoch [42/50], Class Loss: 0.0063, Discrepancy Loss: 0.0038
Epoch [43/50], Class Loss: 0.0273, Discrepancy Loss: 0.0023
Epoch [44/50], Class Loss: 0.0157, Discrepancy Loss: 0.0038
Epoch [45/50], Class Loss: 0.0946, Discrepancy Loss: 0.0022
Epoch [46/50], Class Loss: 0.0054, Discrepancy Loss: 0.0038
Epoch [47/50], Class Loss: 0.0065, Discrepancy Loss: 0.0023
Epoch [48/50], Class Loss: 0.0054, Discrepancy Loss: 0.0022
Epoch [49/50], Class Loss: 0.3013, Discrepancy Loss: 0.0024
Epoch [50/50], Class Loss: 0.0162, Discrepancy Loss: 0.0058
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.76%, F1 Score: 99.76%

Run 5/10
Epoch [1/50], Class Loss: 1.6481, Discrepancy Loss: 0.1084
Epoch [2/50], Class Loss: 0.3785, Discrepancy Loss: 0.0312
Epoch [3/50], Class Loss: 0.1270, Discrepancy Loss: 0.0180
Epoch [4/50], Class Loss: 0.1983, Discrepancy Loss: 0.0203
Epoch [5/50], Class Loss: 0.0439, Discrepancy Loss: 0.0076
Epoch [6/50], Class Loss: 0.0354, Discrepancy Loss: 0.0052
Epoch [7/50], Class Loss: 0.0567, Discrepancy Loss: 0.0103
Epoch [8/50], Class Loss: 0.1473, Discrepancy Loss: 0.0169
Epoch [9/50], Class Loss: 0.0669, Discrepancy Loss: 0.0089
Epoch [10/50], Class Loss: 0.0321, Discrepancy Loss: 0.0064
Epoch [11/50], Class Loss: 0.0152, Discrepancy Loss: 0.0034
Epoch [12/50], Class Loss: 0.0263, Discrepancy Loss: 0.0047
Epoch [13/50], Class Loss: 0.0335, Discrepancy Loss: 0.0035
Epoch [14/50], Class Loss: 0.0218, Discrepancy Loss: 0.0053
Epoch [15/50], Class Loss: 0.1096, Discrepancy Loss: 0.0034
Epoch [16/50], Class Loss: 0.0166, Discrepancy Loss: 0.0028
Epoch [17/50], Class Loss: 0.0114, Discrepancy Loss: 0.0032
Epoch [18/50], Class Loss: 0.0133, Discrepancy Loss: 0.0051
Epoch [19/50], Class Loss: 0.0087, Discrepancy Loss: 0.0023
Epoch [20/50], Class Loss: 0.0091, Discrepancy Loss: 0.0042
Epoch [21/50], Class Loss: 0.0187, Discrepancy Loss: 0.0022
Epoch [22/50], Class Loss: 0.0113, Discrepancy Loss: 0.0025
Epoch [23/50], Class Loss: 0.0108, Discrepancy Loss: 0.0029
Epoch [24/50], Class Loss: 0.0167, Discrepancy Loss: 0.0024
Epoch [25/50], Class Loss: 0.1975, Discrepancy Loss: 0.0028
Epoch [26/50], Class Loss: 0.0093, Discrepancy Loss: 0.0028
Epoch [27/50], Class Loss: 0.0168, Discrepancy Loss: 0.0027
Epoch [28/50], Class Loss: 0.0285, Discrepancy Loss: 0.0020
Epoch [29/50], Class Loss: 0.0194, Discrepancy Loss: 0.0023
Epoch [30/50], Class Loss: 0.0215, Discrepancy Loss: 0.0040
Epoch [31/50], Class Loss: 0.0135, Discrepancy Loss: 0.0029
Epoch [32/50], Class Loss: 0.0350, Discrepancy Loss: 0.0024
Epoch [33/50], Class Loss: 0.0133, Discrepancy Loss: 0.0028
Epoch [34/50], Class Loss: 0.0079, Discrepancy Loss: 0.0016
Epoch [35/50], Class Loss: 0.0513, Discrepancy Loss: 0.0021
Epoch [36/50], Class Loss: 0.0113, Discrepancy Loss: 0.0025
Epoch [37/50], Class Loss: 0.0119, Discrepancy Loss: 0.0030
Epoch [38/50], Class Loss: 0.0697, Discrepancy Loss: 0.0031
Epoch [39/50], Class Loss: 0.0346, Discrepancy Loss: 0.0047
Epoch [40/50], Class Loss: 0.0092, Discrepancy Loss: 0.0038
Epoch [41/50], Class Loss: 0.0204, Discrepancy Loss: 0.0035
Epoch [42/50], Class Loss: 0.0080, Discrepancy Loss: 0.0026
Epoch [43/50], Class Loss: 0.0458, Discrepancy Loss: 0.0039
Epoch [44/50], Class Loss: 0.0104, Discrepancy Loss: 0.0028
Epoch [45/50], Class Loss: 0.0119, Discrepancy Loss: 0.0041
Epoch [46/50], Class Loss: 0.0119, Discrepancy Loss: 0.0028
Epoch [47/50], Class Loss: 0.0081, Discrepancy Loss: 0.0030
Epoch [48/50], Class Loss: 0.0070, Discrepancy Loss: 0.0026
Epoch [49/50], Class Loss: 0.0154, Discrepancy Loss: 0.0034
Epoch [50/50], Class Loss: 0.0255, Discrepancy Loss: 0.0020
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.81%, Recall: 99.82%, F1 Score: 99.82%

Run 6/10
Epoch [1/50], Class Loss: 1.8565, Discrepancy Loss: 0.1197
Epoch [2/50], Class Loss: 0.4765, Discrepancy Loss: 0.0321
Epoch [3/50], Class Loss: 0.1551, Discrepancy Loss: 0.0236
Epoch [4/50], Class Loss: 0.0687, Discrepancy Loss: 0.0110
Epoch [5/50], Class Loss: 0.1296, Discrepancy Loss: 0.0109
Epoch [6/50], Class Loss: 0.0708, Discrepancy Loss: 0.0068
Epoch [7/50], Class Loss: 0.0273, Discrepancy Loss: 0.0042
Epoch [8/50], Class Loss: 0.0900, Discrepancy Loss: 0.0075
Epoch [9/50], Class Loss: 0.0705, Discrepancy Loss: 0.0083
Epoch [10/50], Class Loss: 0.0695, Discrepancy Loss: 0.0046
Epoch [11/50], Class Loss: 0.1802, Discrepancy Loss: 0.0054
Epoch [12/50], Class Loss: 0.0218, Discrepancy Loss: 0.0036
Epoch [13/50], Class Loss: 0.0382, Discrepancy Loss: 0.0032
Epoch [14/50], Class Loss: 0.0368, Discrepancy Loss: 0.0048
Epoch [15/50], Class Loss: 0.0110, Discrepancy Loss: 0.0032
Epoch [16/50], Class Loss: 0.0091, Discrepancy Loss: 0.0035
Epoch [17/50], Class Loss: 0.0168, Discrepancy Loss: 0.0043
Epoch [18/50], Class Loss: 0.0109, Discrepancy Loss: 0.0042
Epoch [19/50], Class Loss: 0.0289, Discrepancy Loss: 0.0026
Epoch [20/50], Class Loss: 0.0242, Discrepancy Loss: 0.0028
Epoch [21/50], Class Loss: 0.0269, Discrepancy Loss: 0.0034
Epoch [22/50], Class Loss: 0.0211, Discrepancy Loss: 0.0018
Epoch [23/50], Class Loss: 0.0926, Discrepancy Loss: 0.0025
Epoch [24/50], Class Loss: 0.0087, Discrepancy Loss: 0.0019
Epoch [25/50], Class Loss: 0.0109, Discrepancy Loss: 0.0021
Epoch [26/50], Class Loss: 0.0174, Discrepancy Loss: 0.0023
Epoch [27/50], Class Loss: 0.0962, Discrepancy Loss: 0.0018
Epoch [28/50], Class Loss: 0.0101, Discrepancy Loss: 0.0034
Epoch [29/50], Class Loss: 0.0794, Discrepancy Loss: 0.0024
Epoch [30/50], Class Loss: 0.0167, Discrepancy Loss: 0.0024
Epoch [31/50], Class Loss: 0.0104, Discrepancy Loss: 0.0023
Epoch [32/50], Class Loss: 0.0173, Discrepancy Loss: 0.0020
Epoch [33/50], Class Loss: 0.0101, Discrepancy Loss: 0.0022
Epoch [34/50], Class Loss: 0.0104, Discrepancy Loss: 0.0024
Epoch [35/50], Class Loss: 0.0626, Discrepancy Loss: 0.0025
Epoch [36/50], Class Loss: 0.0105, Discrepancy Loss: 0.0021
Epoch [37/50], Class Loss: 0.0089, Discrepancy Loss: 0.0036
Epoch [38/50], Class Loss: 0.0469, Discrepancy Loss: 0.0019
Epoch [39/50], Class Loss: 0.0069, Discrepancy Loss: 0.0046
Epoch [40/50], Class Loss: 0.0160, Discrepancy Loss: 0.0019
Epoch [41/50], Class Loss: 0.0081, Discrepancy Loss: 0.0015
Epoch [42/50], Class Loss: 0.0325, Discrepancy Loss: 0.0032
Epoch [43/50], Class Loss: 0.0080, Discrepancy Loss: 0.0025
Epoch [44/50], Class Loss: 0.1215, Discrepancy Loss: 0.0021
Epoch [45/50], Class Loss: 0.0117, Discrepancy Loss: 0.0024
Epoch [46/50], Class Loss: 0.0092, Discrepancy Loss: 0.0032
Epoch [47/50], Class Loss: 0.0113, Discrepancy Loss: 0.0026
Epoch [48/50], Class Loss: 0.0088, Discrepancy Loss: 0.0018
Epoch [49/50], Class Loss: 0.1360, Discrepancy Loss: 0.0028
Epoch [50/50], Class Loss: 0.0256, Discrepancy Loss: 0.0022
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 7/10
Epoch [1/50], Class Loss: 1.7823, Discrepancy Loss: 0.1114
Epoch [2/50], Class Loss: 0.4016, Discrepancy Loss: 0.0359
Epoch [3/50], Class Loss: 0.0828, Discrepancy Loss: 0.0103
Epoch [4/50], Class Loss: 0.0723, Discrepancy Loss: 0.0069
Epoch [5/50], Class Loss: 0.0428, Discrepancy Loss: 0.0064
Epoch [6/50], Class Loss: 0.0205, Discrepancy Loss: 0.0033
Epoch [7/50], Class Loss: 0.0195, Discrepancy Loss: 0.0043
Epoch [8/50], Class Loss: 0.0216, Discrepancy Loss: 0.0042
Epoch [9/50], Class Loss: 0.0215, Discrepancy Loss: 0.0036
Epoch [10/50], Class Loss: 0.0313, Discrepancy Loss: 0.0055
Epoch [11/50], Class Loss: 0.0144, Discrepancy Loss: 0.0029
Epoch [12/50], Class Loss: 0.1681, Discrepancy Loss: 0.0031
Epoch [13/50], Class Loss: 0.0436, Discrepancy Loss: 0.0114
Epoch [14/50], Class Loss: 0.0139, Discrepancy Loss: 0.0029
Epoch [15/50], Class Loss: 0.0051, Discrepancy Loss: 0.0044
Epoch [16/50], Class Loss: 0.0038, Discrepancy Loss: 0.0028
Epoch [17/50], Class Loss: 0.0052, Discrepancy Loss: 0.0017
Epoch [18/50], Class Loss: 0.1433, Discrepancy Loss: 0.0026
Epoch [19/50], Class Loss: 0.0286, Discrepancy Loss: 0.0051
Epoch [20/50], Class Loss: 0.0161, Discrepancy Loss: 0.0033
Epoch [21/50], Class Loss: 0.0037, Discrepancy Loss: 0.0018
Epoch [22/50], Class Loss: 0.0028, Discrepancy Loss: 0.0019
Epoch [23/50], Class Loss: 0.0052, Discrepancy Loss: 0.0016
Epoch [24/50], Class Loss: 0.0051, Discrepancy Loss: 0.0019
Epoch [25/50], Class Loss: 0.0037, Discrepancy Loss: 0.0021
Epoch [26/50], Class Loss: 0.1470, Discrepancy Loss: 0.0018
Epoch [27/50], Class Loss: 0.0131, Discrepancy Loss: 0.0010
Epoch [28/50], Class Loss: 0.0046, Discrepancy Loss: 0.0020
Epoch [29/50], Class Loss: 0.0027, Discrepancy Loss: 0.0021
Epoch [30/50], Class Loss: 0.0039, Discrepancy Loss: 0.0014
Epoch [31/50], Class Loss: 0.0034, Discrepancy Loss: 0.0020
Epoch [32/50], Class Loss: 0.0065, Discrepancy Loss: 0.0017
Epoch [33/50], Class Loss: 0.0339, Discrepancy Loss: 0.0020
Epoch [34/50], Class Loss: 0.0022, Discrepancy Loss: 0.0019
Epoch [35/50], Class Loss: 0.0046, Discrepancy Loss: 0.0025
Epoch [36/50], Class Loss: 0.0237, Discrepancy Loss: 0.0031
Epoch [37/50], Class Loss: 0.0043, Discrepancy Loss: 0.0034
Epoch [38/50], Class Loss: 0.0045, Discrepancy Loss: 0.0016
Epoch [39/50], Class Loss: 0.0035, Discrepancy Loss: 0.0018
Epoch [40/50], Class Loss: 0.0060, Discrepancy Loss: 0.0023
Epoch [41/50], Class Loss: 0.0089, Discrepancy Loss: 0.0014
Epoch [42/50], Class Loss: 0.0054, Discrepancy Loss: 0.0014
Epoch [43/50], Class Loss: 0.0413, Discrepancy Loss: 0.0018
Epoch [44/50], Class Loss: 0.0057, Discrepancy Loss: 0.0011
Epoch [45/50], Class Loss: 0.0032, Discrepancy Loss: 0.0019
Epoch [46/50], Class Loss: 0.0043, Discrepancy Loss: 0.0022
Epoch [47/50], Class Loss: 0.0040, Discrepancy Loss: 0.0023
Epoch [48/50], Class Loss: 0.0056, Discrepancy Loss: 0.0022
Epoch [49/50], Class Loss: 0.0092, Discrepancy Loss: 0.0015
Epoch [50/50], Class Loss: 0.0036, Discrepancy Loss: 0.0027
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.64%, Precision: 99.63%, Recall: 99.63%, F1 Score: 99.63%

Run 8/10
Epoch [1/50], Class Loss: 1.7855, Discrepancy Loss: 0.1367
Epoch [2/50], Class Loss: 0.4541, Discrepancy Loss: 0.0538
Epoch [3/50], Class Loss: 0.2321, Discrepancy Loss: 0.0255
Epoch [4/50], Class Loss: 0.1416, Discrepancy Loss: 0.0216
Epoch [5/50], Class Loss: 0.0798, Discrepancy Loss: 0.0169
Epoch [6/50], Class Loss: 0.0880, Discrepancy Loss: 0.0097
Epoch [7/50], Class Loss: 0.0570, Discrepancy Loss: 0.0059
Epoch [8/50], Class Loss: 0.0181, Discrepancy Loss: 0.0056
Epoch [9/50], Class Loss: 0.0182, Discrepancy Loss: 0.0036
Epoch [10/50], Class Loss: 0.0323, Discrepancy Loss: 0.0065
Epoch [11/50], Class Loss: 0.0446, Discrepancy Loss: 0.0033
Epoch [12/50], Class Loss: 0.0328, Discrepancy Loss: 0.0026
Epoch [13/50], Class Loss: 0.0109, Discrepancy Loss: 0.0027
Epoch [14/50], Class Loss: 0.0118, Discrepancy Loss: 0.0024
Epoch [15/50], Class Loss: 0.0100, Discrepancy Loss: 0.0027
Epoch [16/50], Class Loss: 0.0098, Discrepancy Loss: 0.0023
Epoch [17/50], Class Loss: 0.0094, Discrepancy Loss: 0.0026
Epoch [18/50], Class Loss: 0.0069, Discrepancy Loss: 0.0023
Epoch [19/50], Class Loss: 0.0069, Discrepancy Loss: 0.0020
Epoch [20/50], Class Loss: 0.0058, Discrepancy Loss: 0.0018
Epoch [21/50], Class Loss: 0.0274, Discrepancy Loss: 0.0015
Epoch [22/50], Class Loss: 0.0242, Discrepancy Loss: 0.0050
Epoch [23/50], Class Loss: 0.0088, Discrepancy Loss: 0.0017
Epoch [24/50], Class Loss: 0.0142, Discrepancy Loss: 0.0024
Epoch [25/50], Class Loss: 0.0171, Discrepancy Loss: 0.0017
Epoch [26/50], Class Loss: 0.0145, Discrepancy Loss: 0.0016
Epoch [27/50], Class Loss: 0.0087, Discrepancy Loss: 0.0019
Epoch [28/50], Class Loss: 0.0054, Discrepancy Loss: 0.0023
Epoch [29/50], Class Loss: 0.0038, Discrepancy Loss: 0.0024
Epoch [30/50], Class Loss: 0.0050, Discrepancy Loss: 0.0021
Epoch [31/50], Class Loss: 0.0109, Discrepancy Loss: 0.0017
Epoch [32/50], Class Loss: 0.0235, Discrepancy Loss: 0.0016
Epoch [33/50], Class Loss: 0.0054, Discrepancy Loss: 0.0020
Epoch [34/50], Class Loss: 0.0069, Discrepancy Loss: 0.0030
Epoch [35/50], Class Loss: 0.0044, Discrepancy Loss: 0.0024
Epoch [36/50], Class Loss: 0.0049, Discrepancy Loss: 0.0017
Epoch [37/50], Class Loss: 0.1258, Discrepancy Loss: 0.0024
Epoch [38/50], Class Loss: 0.0056, Discrepancy Loss: 0.0022
Epoch [39/50], Class Loss: 0.0058, Discrepancy Loss: 0.0019
Epoch [40/50], Class Loss: 0.1874, Discrepancy Loss: 0.0019
Epoch [41/50], Class Loss: 0.0033, Discrepancy Loss: 0.0020
Epoch [42/50], Class Loss: 0.0056, Discrepancy Loss: 0.0030
Epoch [43/50], Class Loss: 0.0039, Discrepancy Loss: 0.0019
Epoch [44/50], Class Loss: 0.0053, Discrepancy Loss: 0.0013
Epoch [45/50], Class Loss: 0.0031, Discrepancy Loss: 0.0015
Epoch [46/50], Class Loss: 0.0041, Discrepancy Loss: 0.0030
Epoch [47/50], Class Loss: 0.0052, Discrepancy Loss: 0.0021
Epoch [48/50], Class Loss: 0.0052, Discrepancy Loss: 0.0014
Epoch [49/50], Class Loss: 0.0051, Discrepancy Loss: 0.0014
Epoch [50/50], Class Loss: 0.0148, Discrepancy Loss: 0.0015
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.81%, F1 Score: 99.81%

Run 9/10
Epoch [1/50], Class Loss: 1.6475, Discrepancy Loss: 0.1171
Epoch [2/50], Class Loss: 0.3921, Discrepancy Loss: 0.0308
Epoch [3/50], Class Loss: 0.2046, Discrepancy Loss: 0.0230
Epoch [4/50], Class Loss: 0.1415, Discrepancy Loss: 0.0126
Epoch [5/50], Class Loss: 0.0623, Discrepancy Loss: 0.0093
Epoch [6/50], Class Loss: 0.3127, Discrepancy Loss: 0.0309
Epoch [7/50], Class Loss: 0.2625, Discrepancy Loss: 0.0138
Epoch [8/50], Class Loss: 0.1995, Discrepancy Loss: 0.0084
Epoch [9/50], Class Loss: 0.1642, Discrepancy Loss: 0.0223
Epoch [10/50], Class Loss: 0.2040, Discrepancy Loss: 0.0063
Epoch [11/50], Class Loss: 0.0588, Discrepancy Loss: 0.0102
Epoch [12/50], Class Loss: 0.0377, Discrepancy Loss: 0.0066
Epoch [13/50], Class Loss: 0.0657, Discrepancy Loss: 0.0065
Epoch [14/50], Class Loss: 0.0322, Discrepancy Loss: 0.0074
Epoch [15/50], Class Loss: 0.0459, Discrepancy Loss: 0.0089
Epoch [16/50], Class Loss: 0.0304, Discrepancy Loss: 0.0037
Epoch [17/50], Class Loss: 0.0308, Discrepancy Loss: 0.0069
Epoch [18/50], Class Loss: 0.0501, Discrepancy Loss: 0.0084
Epoch [19/50], Class Loss: 0.0272, Discrepancy Loss: 0.0044
Epoch [20/50], Class Loss: 0.0240, Discrepancy Loss: 0.0047
Epoch [21/50], Class Loss: 0.0451, Discrepancy Loss: 0.0037
Epoch [22/50], Class Loss: 0.0261, Discrepancy Loss: 0.0033
Epoch [23/50], Class Loss: 0.0216, Discrepancy Loss: 0.0041
Epoch [24/50], Class Loss: 0.0471, Discrepancy Loss: 0.0035
Epoch [25/50], Class Loss: 0.0200, Discrepancy Loss: 0.0065
Epoch [26/50], Class Loss: 0.0992, Discrepancy Loss: 0.0040
Epoch [27/50], Class Loss: 0.0161, Discrepancy Loss: 0.0038
Epoch [28/50], Class Loss: 0.0167, Discrepancy Loss: 0.0065
Epoch [29/50], Class Loss: 0.0351, Discrepancy Loss: 0.0032
Epoch [30/50], Class Loss: 0.0189, Discrepancy Loss: 0.0040
Epoch [31/50], Class Loss: 0.0308, Discrepancy Loss: 0.0047
Epoch [32/50], Class Loss: 0.0183, Discrepancy Loss: 0.0068
Epoch [33/50], Class Loss: 0.0137, Discrepancy Loss: 0.0053
Epoch [34/50], Class Loss: 0.0290, Discrepancy Loss: 0.0046
Epoch [35/50], Class Loss: 0.0349, Discrepancy Loss: 0.0031
Epoch [36/50], Class Loss: 0.0423, Discrepancy Loss: 0.0037
Epoch [37/50], Class Loss: 0.0140, Discrepancy Loss: 0.0044
Epoch [38/50], Class Loss: 0.0244, Discrepancy Loss: 0.0036
Epoch [39/50], Class Loss: 0.0180, Discrepancy Loss: 0.0041
Epoch [40/50], Class Loss: 0.0169, Discrepancy Loss: 0.0053
Epoch [41/50], Class Loss: 0.0247, Discrepancy Loss: 0.0039
Epoch [42/50], Class Loss: 0.0413, Discrepancy Loss: 0.0049
Epoch [43/50], Class Loss: 0.0124, Discrepancy Loss: 0.0037
Epoch [44/50], Class Loss: 0.0258, Discrepancy Loss: 0.0042
Epoch [45/50], Class Loss: 0.0297, Discrepancy Loss: 0.0045
Epoch [46/50], Class Loss: 0.0245, Discrepancy Loss: 0.0039
Epoch [47/50], Class Loss: 0.0211, Discrepancy Loss: 0.0041
Epoch [48/50], Class Loss: 0.1349, Discrepancy Loss: 0.0043
Epoch [49/50], Class Loss: 0.0120, Discrepancy Loss: 0.0039
Epoch [50/50], Class Loss: 0.0610, Discrepancy Loss: 0.0052
Source Domain Performance - Accuracy: 99.70%, Precision: 99.69%, Recall: 99.70%, F1 Score: 99.69%
Target Domain Performance - Accuracy: 99.70%, Precision: 99.70%, Recall: 99.70%, F1 Score: 99.70%

Run 10/10
Epoch [1/50], Class Loss: 1.8914, Discrepancy Loss: 0.0917
Epoch [2/50], Class Loss: 0.3038, Discrepancy Loss: 0.0214
Epoch [3/50], Class Loss: 0.0553, Discrepancy Loss: 0.0072
Epoch [4/50], Class Loss: 0.0774, Discrepancy Loss: 0.0127
Epoch [5/50], Class Loss: 0.0606, Discrepancy Loss: 0.0079
Epoch [6/50], Class Loss: 0.0281, Discrepancy Loss: 0.0074
Epoch [7/50], Class Loss: 0.0309, Discrepancy Loss: 0.0061
Epoch [8/50], Class Loss: 0.0371, Discrepancy Loss: 0.0052
Epoch [9/50], Class Loss: 0.0204, Discrepancy Loss: 0.0041
Epoch [10/50], Class Loss: 0.0608, Discrepancy Loss: 0.0072
Epoch [11/50], Class Loss: 0.0682, Discrepancy Loss: 0.0099
Epoch [12/50], Class Loss: 0.0157, Discrepancy Loss: 0.0034
Epoch [13/50], Class Loss: 0.0485, Discrepancy Loss: 0.0033
Epoch [14/50], Class Loss: 0.0167, Discrepancy Loss: 0.0043
Epoch [15/50], Class Loss: 0.1335, Discrepancy Loss: 0.0033
Epoch [16/50], Class Loss: 0.0301, Discrepancy Loss: 0.0063
Epoch [17/50], Class Loss: 0.0078, Discrepancy Loss: 0.0028
Epoch [18/50], Class Loss: 0.0067, Discrepancy Loss: 0.0029
Epoch [19/50], Class Loss: 0.0042, Discrepancy Loss: 0.0019
Epoch [20/50], Class Loss: 0.0057, Discrepancy Loss: 0.0021
Epoch [21/50], Class Loss: 0.1386, Discrepancy Loss: 0.0018
Epoch [22/50], Class Loss: 0.0896, Discrepancy Loss: 0.0037
Epoch [23/50], Class Loss: 0.1144, Discrepancy Loss: 0.0021
Epoch [24/50], Class Loss: 0.0037, Discrepancy Loss: 0.0017
Epoch [25/50], Class Loss: 0.0039, Discrepancy Loss: 0.0013
Epoch [26/50], Class Loss: 0.0141, Discrepancy Loss: 0.0021
Epoch [27/50], Class Loss: 0.0033, Discrepancy Loss: 0.0018
Epoch [28/50], Class Loss: 0.0121, Discrepancy Loss: 0.0020
Epoch [29/50], Class Loss: 0.0151, Discrepancy Loss: 0.0028
Epoch [30/50], Class Loss: 0.0031, Discrepancy Loss: 0.0026
Epoch [31/50], Class Loss: 0.0040, Discrepancy Loss: 0.0034
Epoch [32/50], Class Loss: 0.0035, Discrepancy Loss: 0.0014
Epoch [33/50], Class Loss: 0.0057, Discrepancy Loss: 0.0015
Epoch [34/50], Class Loss: 0.0041, Discrepancy Loss: 0.0013
Epoch [35/50], Class Loss: 0.0989, Discrepancy Loss: 0.0016
Epoch [36/50], Class Loss: 0.0929, Discrepancy Loss: 0.0020
Epoch [37/50], Class Loss: 0.0075, Discrepancy Loss: 0.0027
Epoch [38/50], Class Loss: 0.0138, Discrepancy Loss: 0.0026
Epoch [39/50], Class Loss: 0.0037, Discrepancy Loss: 0.0015
Epoch [40/50], Class Loss: 0.1454, Discrepancy Loss: 0.0017
Epoch [41/50], Class Loss: 0.0059, Discrepancy Loss: 0.0012
Epoch [42/50], Class Loss: 0.0031, Discrepancy Loss: 0.0017
Epoch [43/50], Class Loss: 0.0040, Discrepancy Loss: 0.0018
Epoch [44/50], Class Loss: 0.0106, Discrepancy Loss: 0.0013
Epoch [45/50], Class Loss: 0.0038, Discrepancy Loss: 0.0013
Epoch [46/50], Class Loss: 0.0037, Discrepancy Loss: 0.0026
Epoch [47/50], Class Loss: 0.0031, Discrepancy Loss: 0.0022
Epoch [48/50], Class Loss: 0.0040, Discrepancy Loss: 0.0017
Epoch [49/50], Class Loss: 0.0070, Discrepancy Loss: 0.0023
Epoch [50/50], Class Loss: 0.0431, Discrepancy Loss: 0.0053
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Source performance: 99.93% 99.93% 99.93% 99.93%
Target performance: 99.83% 99.83% 99.83% 99.83%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 99.75%
16qam: 99.57%
8apsk: 100.00%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.4732, Discrepancy Loss: 0.0167
Validation Loss: 0.3841
Epoch [2/50], Class Loss: 0.1128, Discrepancy Loss: 0.0059
Validation Loss: 1.6215
Epoch [3/50], Class Loss: 0.1308, Discrepancy Loss: 0.0033
Validation Loss: 20.5677
Epoch [4/50], Class Loss: 0.0332, Discrepancy Loss: 0.0015
Validation Loss: 0.0147
Epoch [5/50], Class Loss: 0.0258, Discrepancy Loss: 0.0020
Validation Loss: 1.3723
Epoch [6/50], Class Loss: 0.0570, Discrepancy Loss: 0.0033
Validation Loss: 1.2209
Epoch [7/50], Class Loss: 0.0715, Discrepancy Loss: 0.0021
Validation Loss: 0.1000
Epoch [8/50], Class Loss: 0.0315, Discrepancy Loss: 0.0027
Validation Loss: 1.0479
Epoch [9/50], Class Loss: 0.0222, Discrepancy Loss: 0.0020
Validation Loss: 4.2161
Early stopping!
Source Domain Performance - Accuracy: 75.72%, Precision: 76.45%, Recall: 75.35%, F1 Score: 71.02%
Target Domain Performance - Accuracy: 70.32%, Precision: 69.89%, Recall: 69.78%, F1 Score: 66.67%

Run 2/10
Epoch [1/50], Class Loss: 0.5794, Discrepancy Loss: 0.0262
Validation Loss: 0.4295
Epoch [2/50], Class Loss: 0.0752, Discrepancy Loss: 0.0061
Validation Loss: 3.5477
Epoch [3/50], Class Loss: 0.0495, Discrepancy Loss: 0.0043
Validation Loss: 0.0238
Epoch [4/50], Class Loss: 0.0328, Discrepancy Loss: 0.0047
Validation Loss: 0.0249
Epoch [5/50], Class Loss: 0.0351, Discrepancy Loss: 0.0020
Validation Loss: 0.0219
Epoch [6/50], Class Loss: 0.0552, Discrepancy Loss: 0.0021
Validation Loss: 3.4702
Epoch [7/50], Class Loss: 0.0503, Discrepancy Loss: 0.0027
Validation Loss: 0.0142
Epoch [8/50], Class Loss: 0.0204, Discrepancy Loss: 0.0015
Validation Loss: 0.2796
Epoch [9/50], Class Loss: 0.0203, Discrepancy Loss: 0.0030
Validation Loss: 0.0429
Epoch [10/50], Class Loss: 0.0323, Discrepancy Loss: 0.0021
Validation Loss: 0.2781
Epoch [11/50], Class Loss: 0.0044, Discrepancy Loss: 0.0002
Validation Loss: 0.0001
Epoch [12/50], Class Loss: 0.0031, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [13/50], Class Loss: 0.0018, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [14/50], Class Loss: 0.0006, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [15/50], Class Loss: 0.0008, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [16/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0056, Discrepancy Loss: 0.0001
Validation Loss: 0.0003
Epoch [18/50], Class Loss: 0.0057, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [19/50], Class Loss: 0.0088, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [20/50], Class Loss: 0.0013, Discrepancy Loss: 0.0001
Validation Loss: 0.0000
Epoch [21/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [22/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [23/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [24/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0000
Epoch [25/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0000
Epoch [26/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0000
Epoch [27/50], Class Loss: 0.0007, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [28/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0000
Epoch [29/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [30/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0000
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 3/10
Epoch [1/50], Class Loss: 0.5580, Discrepancy Loss: 0.0195
Validation Loss: 2.3259
Epoch [2/50], Class Loss: 0.0806, Discrepancy Loss: 0.0040
Validation Loss: 1.1835
Epoch [3/50], Class Loss: 0.0820, Discrepancy Loss: 0.0026
Validation Loss: 0.1397
Epoch [4/50], Class Loss: 0.0418, Discrepancy Loss: 0.0023
Validation Loss: 0.2901
Epoch [5/50], Class Loss: 0.0870, Discrepancy Loss: 0.0034
Validation Loss: 0.5968
Epoch [6/50], Class Loss: 0.1247, Discrepancy Loss: 0.0052
Validation Loss: 0.4085
Epoch [7/50], Class Loss: 0.0789, Discrepancy Loss: 0.0066
Validation Loss: 0.0704
Epoch [8/50], Class Loss: 0.0168, Discrepancy Loss: 0.0011
Validation Loss: 0.7379
Epoch [9/50], Class Loss: 0.0332, Discrepancy Loss: 0.0016
Validation Loss: 2.5417
Epoch [10/50], Class Loss: 0.0463, Discrepancy Loss: 0.0023
Validation Loss: 0.0657
Epoch [11/50], Class Loss: 0.0043, Discrepancy Loss: 0.0002
Validation Loss: 0.0026
Epoch [12/50], Class Loss: 0.0013, Discrepancy Loss: 0.0001
Validation Loss: 0.0006
Epoch [13/50], Class Loss: 0.0212, Discrepancy Loss: 0.0001
Validation Loss: 0.0015
Epoch [14/50], Class Loss: 0.0027, Discrepancy Loss: 0.0001
Validation Loss: 0.0007
Epoch [15/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Validation Loss: 0.0010
Epoch [16/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Validation Loss: 0.0003
Epoch [17/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0005
Epoch [18/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [19/50], Class Loss: 0.0006, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [20/50], Class Loss: 0.0011, Discrepancy Loss: 0.0002
Validation Loss: 0.0021
Epoch [21/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Validation Loss: 0.0007
Epoch [22/50], Class Loss: 0.0004, Discrepancy Loss: 0.0001
Validation Loss: 0.0009
Epoch [23/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0003
Epoch [24/50], Class Loss: 0.0014, Discrepancy Loss: 0.0001
Validation Loss: 0.0008
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 4/10
Epoch [1/50], Class Loss: 0.4995, Discrepancy Loss: 0.0282
Validation Loss: 1.9986
Epoch [2/50], Class Loss: 0.0715, Discrepancy Loss: 0.0047
Validation Loss: 1.6942
Epoch [3/50], Class Loss: 0.0653, Discrepancy Loss: 0.0019
Validation Loss: 2.2273
Epoch [4/50], Class Loss: 0.0372, Discrepancy Loss: 0.0020
Validation Loss: 0.1287
Epoch [5/50], Class Loss: 0.0518, Discrepancy Loss: 0.0029
Validation Loss: 0.1845
Epoch [6/50], Class Loss: 0.0432, Discrepancy Loss: 0.0027
Validation Loss: 0.5511
Epoch [7/50], Class Loss: 0.0327, Discrepancy Loss: 0.0020
Validation Loss: 0.0331
Epoch [8/50], Class Loss: 0.0188, Discrepancy Loss: 0.0005
Validation Loss: 1.3930
Epoch [9/50], Class Loss: 0.0309, Discrepancy Loss: 0.0022
Validation Loss: 3.2742
Epoch [10/50], Class Loss: 0.0357, Discrepancy Loss: 0.0009
Validation Loss: 0.0311
Epoch [11/50], Class Loss: 0.0079, Discrepancy Loss: 0.0005
Validation Loss: 0.0003
Epoch [12/50], Class Loss: 0.0059, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [13/50], Class Loss: 0.0007, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [14/50], Class Loss: 0.0006, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [15/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [16/50], Class Loss: 0.0010, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0010, Discrepancy Loss: 0.0002
Validation Loss: 0.0001
Epoch [18/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [19/50], Class Loss: 0.0015, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [20/50], Class Loss: 0.0475, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [21/50], Class Loss: 0.0005, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [22/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [23/50], Class Loss: 0.0509, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [24/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [25/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [26/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [27/50], Class Loss: 0.0057, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 5/10
Epoch [1/50], Class Loss: 0.4824, Discrepancy Loss: 0.0209
Validation Loss: 4.1655
Epoch [2/50], Class Loss: 0.1061, Discrepancy Loss: 0.0060
Validation Loss: 0.0744
Epoch [3/50], Class Loss: 0.0481, Discrepancy Loss: 0.0017
Validation Loss: 0.1523
Epoch [4/50], Class Loss: 0.0370, Discrepancy Loss: 0.0020
Validation Loss: 0.9358
Epoch [5/50], Class Loss: 0.0744, Discrepancy Loss: 0.0023
Validation Loss: 0.0960
Epoch [6/50], Class Loss: 0.0410, Discrepancy Loss: 0.0033
Validation Loss: 0.2165
Epoch [7/50], Class Loss: 0.0273, Discrepancy Loss: 0.0032
Validation Loss: 0.0425
Epoch [8/50], Class Loss: 0.0320, Discrepancy Loss: 0.0027
Validation Loss: 0.0976
Epoch [9/50], Class Loss: 0.0312, Discrepancy Loss: 0.0016
Validation Loss: 0.0796
Epoch [10/50], Class Loss: 0.0177, Discrepancy Loss: 0.0015
Validation Loss: 0.0326
Epoch [11/50], Class Loss: 0.0098, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [12/50], Class Loss: 0.0034, Discrepancy Loss: 0.0001
Validation Loss: 0.0058
Epoch [13/50], Class Loss: 0.0013, Discrepancy Loss: 0.0001
Validation Loss: 0.0000
Epoch [14/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Validation Loss: 0.0000
Epoch [15/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0000
Epoch [16/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0000
Epoch [17/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [18/50], Class Loss: 0.0001, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [19/50], Class Loss: 0.0001, Discrepancy Loss: 0.0001
Validation Loss: 0.0000
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.81%, Recall: 99.83%, F1 Score: 99.82%

Run 6/10
Epoch [1/50], Class Loss: 0.6019, Discrepancy Loss: 0.0214
Validation Loss: 13.9253
Epoch [2/50], Class Loss: 0.0573, Discrepancy Loss: 0.0027
Validation Loss: 0.0557
Epoch [3/50], Class Loss: 0.0680, Discrepancy Loss: 0.0023
Validation Loss: 0.0449
Epoch [4/50], Class Loss: 0.0520, Discrepancy Loss: 0.0014
Validation Loss: 0.0590
Epoch [5/50], Class Loss: 0.0192, Discrepancy Loss: 0.0019
Validation Loss: 2.4338
Epoch [6/50], Class Loss: 0.0297, Discrepancy Loss: 0.0024
Validation Loss: 5.7119
Epoch [7/50], Class Loss: 0.0327, Discrepancy Loss: 0.0008
Validation Loss: 0.0161
Epoch [8/50], Class Loss: 0.0204, Discrepancy Loss: 0.0005
Validation Loss: 3.2405
Epoch [9/50], Class Loss: 0.0153, Discrepancy Loss: 0.0008
Validation Loss: 0.2434
Epoch [10/50], Class Loss: 0.0231, Discrepancy Loss: 0.0013
Validation Loss: 0.0463
Epoch [11/50], Class Loss: 0.0222, Discrepancy Loss: 0.0010
Validation Loss: 0.0006
Epoch [12/50], Class Loss: 0.0042, Discrepancy Loss: 0.0004
Validation Loss: 0.0005
Epoch [13/50], Class Loss: 0.0008, Discrepancy Loss: 0.0002
Validation Loss: 0.0002
Epoch [14/50], Class Loss: 0.0011, Discrepancy Loss: 0.0002
Validation Loss: 0.0001
Epoch [15/50], Class Loss: 0.0004, Discrepancy Loss: 0.0002
Validation Loss: 0.0001
Epoch [16/50], Class Loss: 0.0004, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0009, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [18/50], Class Loss: 0.0019, Discrepancy Loss: 0.0001
Validation Loss: 0.0658
Epoch [19/50], Class Loss: 0.0011, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [20/50], Class Loss: 0.0058, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [21/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [22/50], Class Loss: 0.0307, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [23/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [24/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [25/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [26/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 7/10
Epoch [1/50], Class Loss: 0.5480, Discrepancy Loss: 0.0187
Validation Loss: 6.4556
Epoch [2/50], Class Loss: 0.0718, Discrepancy Loss: 0.0044
Validation Loss: 2.7398
Epoch [3/50], Class Loss: 0.0425, Discrepancy Loss: 0.0035
Validation Loss: 0.0822
Epoch [4/50], Class Loss: 0.0465, Discrepancy Loss: 0.0015
Validation Loss: 0.1241
Epoch [5/50], Class Loss: 0.0518, Discrepancy Loss: 0.0015
Validation Loss: 6.0289
Epoch [6/50], Class Loss: 0.0446, Discrepancy Loss: 0.0025
Validation Loss: 0.1500
Epoch [7/50], Class Loss: 0.0311, Discrepancy Loss: 0.0012
Validation Loss: 0.4422
Epoch [8/50], Class Loss: 0.0373, Discrepancy Loss: 0.0024
Validation Loss: 0.0012
Epoch [9/50], Class Loss: 0.0525, Discrepancy Loss: 0.0017
Validation Loss: 3.4328
Epoch [10/50], Class Loss: 0.0314, Discrepancy Loss: 0.0022
Validation Loss: 0.0471
Epoch [11/50], Class Loss: 0.0020, Discrepancy Loss: 0.0007
Validation Loss: 0.0009
Epoch [12/50], Class Loss: 0.0007, Discrepancy Loss: 0.0004
Validation Loss: 0.0004
Epoch [13/50], Class Loss: 0.0005, Discrepancy Loss: 0.0003
Validation Loss: 0.0003
Epoch [14/50], Class Loss: 0.0016, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [15/50], Class Loss: 0.0009, Discrepancy Loss: 0.0002
Validation Loss: 0.0004
Epoch [16/50], Class Loss: 0.0004, Discrepancy Loss: 0.0002
Validation Loss: 0.0003
Epoch [17/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [18/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [19/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [20/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [21/50], Class Loss: 0.0023, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [22/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [23/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [24/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [25/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [26/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.81%, Recall: 99.82%, F1 Score: 99.82%

Run 8/10
Epoch [1/50], Class Loss: 0.6063, Discrepancy Loss: 0.0231
Validation Loss: 0.2975
Epoch [2/50], Class Loss: 0.0883, Discrepancy Loss: 0.0057
Validation Loss: 4.5745
Epoch [3/50], Class Loss: 0.0737, Discrepancy Loss: 0.0022
Validation Loss: 0.3053
Epoch [4/50], Class Loss: 0.0467, Discrepancy Loss: 0.0032
Validation Loss: 0.9127
Epoch [5/50], Class Loss: 0.0387, Discrepancy Loss: 0.0033
Validation Loss: 0.1007
Epoch [6/50], Class Loss: 0.0481, Discrepancy Loss: 0.0016
Validation Loss: 0.3264
Epoch [7/50], Class Loss: 0.0624, Discrepancy Loss: 0.0027
Validation Loss: 8.7539
Epoch [8/50], Class Loss: 0.0485, Discrepancy Loss: 0.0036
Validation Loss: 0.0454
Epoch [9/50], Class Loss: 0.0532, Discrepancy Loss: 0.0032
Validation Loss: 0.4204
Epoch [10/50], Class Loss: 0.0403, Discrepancy Loss: 0.0014
Validation Loss: 1.6988
Epoch [11/50], Class Loss: 0.0050, Discrepancy Loss: 0.0003
Validation Loss: 0.0006
Epoch [12/50], Class Loss: 0.0009, Discrepancy Loss: 0.0002
Validation Loss: 0.0008
Epoch [13/50], Class Loss: 0.0084, Discrepancy Loss: 0.0003
Validation Loss: 0.0017
Epoch [14/50], Class Loss: 0.0008, Discrepancy Loss: 0.0002
Validation Loss: 0.0010
Epoch [15/50], Class Loss: 0.0007, Discrepancy Loss: 0.0002
Validation Loss: 0.0006
Epoch [16/50], Class Loss: 0.0008, Discrepancy Loss: 0.0002
Validation Loss: 0.0005
Epoch [17/50], Class Loss: 0.0010, Discrepancy Loss: 0.0004
Validation Loss: 0.0012
Epoch [18/50], Class Loss: 0.0003, Discrepancy Loss: 0.0005
Validation Loss: 0.0019
Epoch [19/50], Class Loss: 0.0002, Discrepancy Loss: 0.0002
Validation Loss: 0.0004
Epoch [20/50], Class Loss: 0.0004, Discrepancy Loss: 0.0001
Validation Loss: 0.0006
Epoch [21/50], Class Loss: 0.0005, Discrepancy Loss: 0.0002
Validation Loss: 0.0003
Epoch [22/50], Class Loss: 0.0007, Discrepancy Loss: 0.0002
Validation Loss: 0.0003
Epoch [23/50], Class Loss: 0.0007, Discrepancy Loss: 0.0001
Validation Loss: 0.0003
Epoch [24/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [25/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0003
Epoch [26/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [27/50], Class Loss: 0.0007, Discrepancy Loss: 0.0001
Validation Loss: 0.0005
Epoch [28/50], Class Loss: 0.0010, Discrepancy Loss: 0.0001
Validation Loss: 0.0006
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 9/10
Epoch [1/50], Class Loss: 0.5183, Discrepancy Loss: 0.0177
Validation Loss: 17.3382
Epoch [2/50], Class Loss: 0.1319, Discrepancy Loss: 0.0035
Validation Loss: 0.0316
Epoch [3/50], Class Loss: 0.0578, Discrepancy Loss: 0.0034
Validation Loss: 2.1682
Epoch [4/50], Class Loss: 0.0752, Discrepancy Loss: 0.0021
Validation Loss: 0.0040
Epoch [5/50], Class Loss: 0.0444, Discrepancy Loss: 0.0033
Validation Loss: 16.4802
Epoch [6/50], Class Loss: 0.0466, Discrepancy Loss: 0.0040
Validation Loss: 6.6011
Epoch [7/50], Class Loss: 0.0416, Discrepancy Loss: 0.0046
Validation Loss: 0.0204
Epoch [8/50], Class Loss: 0.0257, Discrepancy Loss: 0.0033
Validation Loss: 0.0900
Epoch [9/50], Class Loss: 0.0252, Discrepancy Loss: 0.0009
Validation Loss: 0.0408
Early stopping!
Source Domain Performance - Accuracy: 99.46%, Precision: 99.47%, Recall: 99.46%, F1 Score: 99.46%
Target Domain Performance - Accuracy: 97.18%, Precision: 97.20%, Recall: 97.23%, F1 Score: 97.15%

Run 10/10
Epoch [1/50], Class Loss: 0.5409, Discrepancy Loss: 0.0197
Validation Loss: 12.5807
Epoch [2/50], Class Loss: 0.0698, Discrepancy Loss: 0.0034
Validation Loss: 0.5913
Epoch [3/50], Class Loss: 0.0514, Discrepancy Loss: 0.0016
Validation Loss: 2.7590
Epoch [4/50], Class Loss: 0.0384, Discrepancy Loss: 0.0032
Validation Loss: 0.1315
Epoch [5/50], Class Loss: 0.0432, Discrepancy Loss: 0.0029
Validation Loss: 0.0289
Epoch [6/50], Class Loss: 0.1057, Discrepancy Loss: 0.0021
Validation Loss: 2.1984
Epoch [7/50], Class Loss: 0.0315, Discrepancy Loss: 0.0015
Validation Loss: 0.2642
Epoch [8/50], Class Loss: 0.0262, Discrepancy Loss: 0.0023
Validation Loss: 0.0013
Epoch [9/50], Class Loss: 0.0362, Discrepancy Loss: 0.0031
Validation Loss: 0.1348
Epoch [10/50], Class Loss: 0.0166, Discrepancy Loss: 0.0011
Validation Loss: 0.1893
Epoch [11/50], Class Loss: 0.0082, Discrepancy Loss: 0.0002
Validation Loss: 0.0008
Epoch [12/50], Class Loss: 0.0013, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [13/50], Class Loss: 0.0010, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [14/50], Class Loss: 0.0010, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [15/50], Class Loss: 0.0024, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [16/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [18/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [19/50], Class Loss: 0.0030, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [20/50], Class Loss: 0.0010, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [21/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [22/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [23/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [24/50], Class Loss: 0.0021, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.81%, Recall: 99.82%, F1 Score: 99.82%

Source performance: 97.52% 97.59% 97.48% 97.05%
Target performance: 96.65% 96.60% 96.60% 96.28%

Per-Class Accuracy on Target Domain:
bpsk: 99.95%
qpsk: 95.58%
16qam: 90.88%
8apsk: 100.00%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.4094, JMMD Loss: 0.0399
Validation Loss: 3.0226
Epoch [2/50], Class Loss: 0.0459, JMMD Loss: 0.0374
Validation Loss: 4.9561
Epoch [3/50], Class Loss: 0.0883, JMMD Loss: 0.0354
Validation Loss: 0.3449
Epoch [4/50], Class Loss: 0.0844, JMMD Loss: 0.0360
Validation Loss: 0.0733
Epoch [5/50], Class Loss: 0.0594, JMMD Loss: 0.0456
Validation Loss: 2.2315
Epoch [6/50], Class Loss: 0.1223, JMMD Loss: 0.0336
Validation Loss: 0.4301
Epoch [7/50], Class Loss: 0.1645, JMMD Loss: 0.0481
Validation Loss: 0.5944
Epoch [8/50], Class Loss: 0.1335, JMMD Loss: 0.0459
Validation Loss: 0.0076
Epoch [9/50], Class Loss: 0.0148, JMMD Loss: 0.0385
Validation Loss: 0.1680
Epoch [10/50], Class Loss: 0.0146, JMMD Loss: 0.0412
Validation Loss: 0.0367
Epoch [11/50], Class Loss: 0.0117, JMMD Loss: 0.0377
Validation Loss: 0.0023
Epoch [12/50], Class Loss: 0.0082, JMMD Loss: 0.0631
Validation Loss: 0.0023
Epoch [13/50], Class Loss: 0.0061, JMMD Loss: 0.0343
Validation Loss: 0.0019
Epoch [14/50], Class Loss: 0.0167, JMMD Loss: 0.0377
Validation Loss: 0.0019
Epoch [15/50], Class Loss: 0.0097, JMMD Loss: 0.0383
Validation Loss: 0.0030
Epoch [16/50], Class Loss: 0.0068, JMMD Loss: 0.0511
Validation Loss: 0.0060
Epoch [17/50], Class Loss: 0.0064, JMMD Loss: 0.0372
Validation Loss: 0.0017
Epoch [18/50], Class Loss: 0.0060, JMMD Loss: 0.0474
Validation Loss: 0.0020
Epoch [19/50], Class Loss: 0.0580, JMMD Loss: 0.0427
Validation Loss: 0.0016
Epoch [20/50], Class Loss: 0.0193, JMMD Loss: 0.0405
Validation Loss: 0.0033
Epoch [21/50], Class Loss: 0.0089, JMMD Loss: 0.0358
Validation Loss: 0.0031
Epoch [22/50], Class Loss: 0.0108, JMMD Loss: 0.0430
Validation Loss: 0.0033
Epoch [23/50], Class Loss: 0.0084, JMMD Loss: 0.0360
Validation Loss: 0.0025
Epoch [24/50], Class Loss: 0.0077, JMMD Loss: 0.0401
Validation Loss: 0.0022
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 2/10
Epoch [1/50], Class Loss: 0.4074, JMMD Loss: 0.0358
Validation Loss: 0.6461
Epoch [2/50], Class Loss: 0.0676, JMMD Loss: 0.0369
Validation Loss: 5.5386
Epoch [3/50], Class Loss: 0.1638, JMMD Loss: 0.0428
Validation Loss: 0.0855
Epoch [4/50], Class Loss: 0.1406, JMMD Loss: 0.0306
Validation Loss: 1.3674
Epoch [5/50], Class Loss: 0.0658, JMMD Loss: 0.0362
Validation Loss: 0.9933
Epoch [6/50], Class Loss: 0.0592, JMMD Loss: 0.0395
Validation Loss: 0.4398
Epoch [7/50], Class Loss: 0.0570, JMMD Loss: 0.0331
Validation Loss: 0.0055
Epoch [8/50], Class Loss: 0.0111, JMMD Loss: 0.0382
Validation Loss: 0.0032
Epoch [9/50], Class Loss: 0.0097, JMMD Loss: 0.0296
Validation Loss: 0.0484
Epoch [10/50], Class Loss: 0.0116, JMMD Loss: 0.0389
Validation Loss: 0.4014
Epoch [11/50], Class Loss: 0.0110, JMMD Loss: 0.0356
Validation Loss: 0.0011
Epoch [12/50], Class Loss: 0.1065, JMMD Loss: 0.0383
Validation Loss: 0.0025
Epoch [13/50], Class Loss: 0.0229, JMMD Loss: 0.0342
Validation Loss: 0.0023
Epoch [14/50], Class Loss: 0.0068, JMMD Loss: 0.0394
Validation Loss: 0.0013
Epoch [15/50], Class Loss: 0.0041, JMMD Loss: 0.0419
Validation Loss: 0.0019
Epoch [16/50], Class Loss: 0.0403, JMMD Loss: 0.0501
Validation Loss: 0.0023
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.81%, Recall: 99.82%, F1 Score: 99.82%

Run 3/10
Epoch [1/50], Class Loss: 0.3228, JMMD Loss: 0.0402
Validation Loss: 0.0053
Epoch [2/50], Class Loss: 0.0537, JMMD Loss: 0.0318
Validation Loss: 0.0399
Epoch [3/50], Class Loss: 0.0167, JMMD Loss: 0.0392
Validation Loss: 0.0193
Epoch [4/50], Class Loss: 0.0746, JMMD Loss: 0.0338
Validation Loss: 8.2541
Epoch [5/50], Class Loss: 0.1132, JMMD Loss: 0.0334
Validation Loss: 0.9752
Epoch [6/50], Class Loss: 0.0221, JMMD Loss: 0.0352
Validation Loss: 0.0043
Epoch [7/50], Class Loss: 0.0135, JMMD Loss: 0.0428
Validation Loss: 0.0098
Epoch [8/50], Class Loss: 0.0313, JMMD Loss: 0.0407
Validation Loss: 1.5123
Epoch [9/50], Class Loss: 0.1742, JMMD Loss: 0.0379
Validation Loss: 0.3441
Epoch [10/50], Class Loss: 0.0581, JMMD Loss: 0.0419
Validation Loss: 0.0396
Epoch [11/50], Class Loss: 0.0468, JMMD Loss: 0.0293
Validation Loss: 0.0046
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.76%, F1 Score: 99.76%

Run 4/10
Epoch [1/50], Class Loss: 0.4045, JMMD Loss: 0.0369
Validation Loss: 0.0187
Epoch [2/50], Class Loss: 0.0984, JMMD Loss: 0.0341
Validation Loss: 0.0975
Epoch [3/50], Class Loss: 0.0218, JMMD Loss: 0.0342
Validation Loss: 0.0479
Epoch [4/50], Class Loss: 0.0163, JMMD Loss: 0.0271
Validation Loss: 0.3754
Epoch [5/50], Class Loss: 0.0193, JMMD Loss: 0.0466
Validation Loss: 0.0169
Epoch [6/50], Class Loss: 0.0136, JMMD Loss: 0.0295
Validation Loss: 1.2783
Epoch [7/50], Class Loss: 0.0609, JMMD Loss: 0.0383
Validation Loss: 0.0916
Epoch [8/50], Class Loss: 0.0403, JMMD Loss: 0.0333
Validation Loss: 0.4542
Epoch [9/50], Class Loss: 0.0116, JMMD Loss: 0.0365
Validation Loss: 0.3067
Epoch [10/50], Class Loss: 0.0497, JMMD Loss: 0.0383
Validation Loss: 2.2363
Early stopping!
Source Domain Performance - Accuracy: 73.56%, Precision: 61.60%, Recall: 74.57%, F1 Score: 65.93%
Target Domain Performance - Accuracy: 72.96%, Precision: 60.71%, Recall: 73.94%, F1 Score: 65.22%

Run 5/10
Epoch [1/50], Class Loss: 0.2791, JMMD Loss: 0.0455
Validation Loss: 2.4041
Epoch [2/50], Class Loss: 0.1465, JMMD Loss: 0.0368
Validation Loss: 0.4750
Epoch [3/50], Class Loss: 0.0490, JMMD Loss: 0.0346
Validation Loss: 0.2423
Epoch [4/50], Class Loss: 0.0194, JMMD Loss: 0.0414
Validation Loss: 0.0756
Epoch [5/50], Class Loss: 0.0593, JMMD Loss: 0.0379
Validation Loss: 0.0305
Epoch [6/50], Class Loss: 0.0590, JMMD Loss: 0.0397
Validation Loss: 0.1541
Epoch [7/50], Class Loss: 0.0131, JMMD Loss: 0.0452
Validation Loss: 0.0015
Epoch [8/50], Class Loss: 0.0041, JMMD Loss: 0.0329
Validation Loss: 0.0015
Epoch [9/50], Class Loss: 0.0084, JMMD Loss: 0.0346
Validation Loss: 0.0025
Epoch [10/50], Class Loss: 0.0510, JMMD Loss: 0.0365
Validation Loss: 0.0036
Epoch [11/50], Class Loss: 0.0089, JMMD Loss: 0.0505
Validation Loss: 0.0013
Epoch [12/50], Class Loss: 0.0501, JMMD Loss: 0.0460
Validation Loss: 0.0028
Epoch [13/50], Class Loss: 0.0891, JMMD Loss: 0.0517
Validation Loss: 0.0024
Epoch [14/50], Class Loss: 0.0173, JMMD Loss: 0.0357
Validation Loss: 0.0017
Epoch [15/50], Class Loss: 0.0127, JMMD Loss: 0.0412
Validation Loss: 0.0014
Epoch [16/50], Class Loss: 0.0225, JMMD Loss: 0.0481
Validation Loss: 0.0030
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 6/10
Epoch [1/50], Class Loss: 0.2981, JMMD Loss: 0.0358
Validation Loss: 2.1858
Epoch [2/50], Class Loss: 0.0500, JMMD Loss: 0.0421
Validation Loss: 0.0335
Epoch [3/50], Class Loss: 0.0435, JMMD Loss: 0.0342
Validation Loss: 0.0092
Epoch [4/50], Class Loss: 0.0840, JMMD Loss: 0.0335
Validation Loss: 0.0051
Epoch [5/50], Class Loss: 0.0141, JMMD Loss: 0.0417
Validation Loss: 0.0461
Epoch [6/50], Class Loss: 0.0193, JMMD Loss: 0.0412
Validation Loss: 0.0597
Epoch [7/50], Class Loss: 0.0265, JMMD Loss: 0.0308
Validation Loss: 1.0336
Epoch [8/50], Class Loss: 0.0638, JMMD Loss: 0.0401
Validation Loss: 0.3417
Epoch [9/50], Class Loss: 0.0199, JMMD Loss: 0.0408
Validation Loss: 0.7189
Early stopping!
Source Domain Performance - Accuracy: 65.71%, Precision: 81.06%, Recall: 64.68%, F1 Score: 60.21%
Target Domain Performance - Accuracy: 68.35%, Precision: 82.83%, Recall: 68.77%, F1 Score: 63.78%

Run 7/10
Epoch [1/50], Class Loss: 0.2533, JMMD Loss: 0.0503
Validation Loss: 0.0565
Epoch [2/50], Class Loss: 0.0690, JMMD Loss: 0.0356
Validation Loss: 0.9132
Epoch [3/50], Class Loss: 0.0891, JMMD Loss: 0.0314
Validation Loss: 0.2065
Epoch [4/50], Class Loss: 0.0858, JMMD Loss: 0.0380
Validation Loss: 2.0313
Epoch [5/50], Class Loss: 0.0456, JMMD Loss: 0.0381
Validation Loss: 0.0101
Epoch [6/50], Class Loss: 0.0157, JMMD Loss: 0.0443
Validation Loss: 0.0262
Epoch [7/50], Class Loss: 0.0515, JMMD Loss: 0.0361
Validation Loss: 0.0024
Epoch [8/50], Class Loss: 0.0130, JMMD Loss: 0.0352
Validation Loss: 0.2389
Epoch [9/50], Class Loss: 0.0076, JMMD Loss: 0.0385
Validation Loss: 0.0168
Epoch [10/50], Class Loss: 0.0155, JMMD Loss: 0.0470
Validation Loss: 0.0021
Epoch [11/50], Class Loss: 0.0175, JMMD Loss: 0.0344
Validation Loss: 0.0007
Epoch [12/50], Class Loss: 0.0021, JMMD Loss: 0.0376
Validation Loss: 0.0006
Epoch [13/50], Class Loss: 0.0020, JMMD Loss: 0.0308
Validation Loss: 0.0017
Epoch [14/50], Class Loss: 0.0018, JMMD Loss: 0.0376
Validation Loss: 0.0007
Epoch [15/50], Class Loss: 0.0041, JMMD Loss: 0.0316
Validation Loss: 0.0004
Epoch [16/50], Class Loss: 0.0080, JMMD Loss: 0.0289
Validation Loss: 0.0006
Epoch [17/50], Class Loss: 0.0103, JMMD Loss: 0.0368
Validation Loss: 0.0004
Epoch [18/50], Class Loss: 0.0090, JMMD Loss: 0.0399
Validation Loss: 0.0006
Epoch [19/50], Class Loss: 0.0038, JMMD Loss: 0.0313
Validation Loss: 0.0006
Epoch [20/50], Class Loss: 0.0193, JMMD Loss: 0.0422
Validation Loss: 0.0008
Epoch [21/50], Class Loss: 0.0025, JMMD Loss: 0.0275
Validation Loss: 0.0011
Epoch [22/50], Class Loss: 0.0019, JMMD Loss: 0.0330
Validation Loss: 0.0013
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 8/10
Epoch [1/50], Class Loss: 0.3304, JMMD Loss: 0.0489
Validation Loss: 0.0627
Epoch [2/50], Class Loss: 0.0695, JMMD Loss: 0.0299
Validation Loss: 0.0225
Epoch [3/50], Class Loss: 0.0714, JMMD Loss: 0.0324
Validation Loss: 0.0092
Epoch [4/50], Class Loss: 0.0251, JMMD Loss: 0.0356
Validation Loss: 0.0088
Epoch [5/50], Class Loss: 0.1161, JMMD Loss: 0.0359
Validation Loss: 0.0807
Epoch [6/50], Class Loss: 0.1034, JMMD Loss: 0.0449
Validation Loss: 0.0124
Epoch [7/50], Class Loss: 0.0285, JMMD Loss: 0.0380
Validation Loss: 0.0301
Epoch [8/50], Class Loss: 0.0267, JMMD Loss: 0.0397
Validation Loss: 0.0098
Epoch [9/50], Class Loss: 0.0074, JMMD Loss: 0.0431
Validation Loss: 0.0010
Epoch [10/50], Class Loss: 0.0064, JMMD Loss: 0.0361
Validation Loss: 0.0054
Epoch [11/50], Class Loss: 0.0266, JMMD Loss: 0.0466
Validation Loss: 0.0010
Epoch [12/50], Class Loss: 0.0080, JMMD Loss: 0.0427
Validation Loss: 0.0008
Epoch [13/50], Class Loss: 0.0171, JMMD Loss: 0.0429
Validation Loss: 0.0015
Epoch [14/50], Class Loss: 0.0092, JMMD Loss: 0.0477
Validation Loss: 0.0006
Epoch [15/50], Class Loss: 0.0042, JMMD Loss: 0.0474
Validation Loss: 0.0009
Epoch [16/50], Class Loss: 0.0029, JMMD Loss: 0.0366
Validation Loss: 0.0008
Epoch [17/50], Class Loss: 0.0127, JMMD Loss: 0.0445
Validation Loss: 0.0023
Epoch [18/50], Class Loss: 0.0127, JMMD Loss: 0.0475
Validation Loss: 0.0014
Epoch [19/50], Class Loss: 0.0029, JMMD Loss: 0.0473
Validation Loss: 0.0008
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 9/10
Epoch [1/50], Class Loss: 0.2963, JMMD Loss: 0.0351
Validation Loss: 2.3502
Epoch [2/50], Class Loss: 0.0634, JMMD Loss: 0.0351
Validation Loss: 2.1547
Epoch [3/50], Class Loss: 0.0487, JMMD Loss: 0.0267
Validation Loss: 0.0151
Epoch [4/50], Class Loss: 0.0172, JMMD Loss: 0.0319
Validation Loss: 0.3540
Epoch [5/50], Class Loss: 0.0205, JMMD Loss: 0.0399
Validation Loss: 0.2213
Epoch [6/50], Class Loss: 0.0701, JMMD Loss: 0.0494
Validation Loss: 1.1759
Epoch [7/50], Class Loss: 0.0796, JMMD Loss: 0.0335
Validation Loss: 0.0169
Epoch [8/50], Class Loss: 0.0112, JMMD Loss: 0.0455
Validation Loss: 0.1696
Early stopping!
Source Domain Performance - Accuracy: 93.29%, Precision: 94.55%, Recall: 92.96%, F1 Score: 92.92%
Target Domain Performance - Accuracy: 94.24%, Precision: 95.14%, Recall: 94.41%, F1 Score: 94.19%

Run 10/10
Epoch [1/50], Class Loss: 0.6037, JMMD Loss: 0.0281
Validation Loss: 0.1560
Epoch [2/50], Class Loss: 0.2055, JMMD Loss: 0.0346
Validation Loss: 0.0611
Epoch [3/50], Class Loss: 0.0486, JMMD Loss: 0.0449
Validation Loss: 2.4416
Epoch [4/50], Class Loss: 0.0309, JMMD Loss: 0.0406
Validation Loss: 0.0261
Epoch [5/50], Class Loss: 0.0488, JMMD Loss: 0.0440
Validation Loss: 0.3468
Epoch [6/50], Class Loss: 0.0641, JMMD Loss: 0.0329
Validation Loss: 0.0084
Epoch [7/50], Class Loss: 0.0422, JMMD Loss: 0.0391
Validation Loss: 0.1034
Epoch [8/50], Class Loss: 0.0816, JMMD Loss: 0.0402
Validation Loss: 0.1548
Epoch [9/50], Class Loss: 0.0457, JMMD Loss: 0.0434
Validation Loss: 0.0195
Epoch [10/50], Class Loss: 0.0161, JMMD Loss: 0.0395
Validation Loss: 0.0024
Epoch [11/50], Class Loss: 0.0083, JMMD Loss: 0.0433
Validation Loss: 0.0021
Epoch [12/50], Class Loss: 0.0582, JMMD Loss: 0.0443
Validation Loss: 0.0013
Epoch [13/50], Class Loss: 0.0154, JMMD Loss: 0.0358
Validation Loss: 0.0034
Epoch [14/50], Class Loss: 0.0085, JMMD Loss: 0.0403
Validation Loss: 0.0028
Epoch [15/50], Class Loss: 0.0060, JMMD Loss: 0.0390
Validation Loss: 0.0030
Epoch [16/50], Class Loss: 0.0046, JMMD Loss: 0.0382
Validation Loss: 0.0024
Epoch [17/50], Class Loss: 0.0039, JMMD Loss: 0.0526
Validation Loss: 0.0012
Epoch [18/50], Class Loss: 0.0030, JMMD Loss: 0.0514
Validation Loss: 0.0013
Epoch [19/50], Class Loss: 0.0046, JMMD Loss: 0.0548
Validation Loss: 0.0018
Epoch [20/50], Class Loss: 0.0040, JMMD Loss: 0.0443
Validation Loss: 0.0010
Epoch [21/50], Class Loss: 0.0314, JMMD Loss: 0.0381
Validation Loss: 0.0038
Epoch [22/50], Class Loss: 0.0037, JMMD Loss: 0.0405
Validation Loss: 0.0018
Epoch [23/50], Class Loss: 0.0023, JMMD Loss: 0.0383
Validation Loss: 0.0010
Epoch [24/50], Class Loss: 0.0023, JMMD Loss: 0.0359
Validation Loss: 0.0019
Epoch [25/50], Class Loss: 0.0040, JMMD Loss: 0.0435
Validation Loss: 0.0020
Epoch [26/50], Class Loss: 0.0294, JMMD Loss: 0.0386
Validation Loss: 0.0020
Epoch [27/50], Class Loss: 0.0024, JMMD Loss: 0.0377
Validation Loss: 0.0009
Epoch [28/50], Class Loss: 0.0062, JMMD Loss: 0.0479
Validation Loss: 0.0007
Epoch [29/50], Class Loss: 0.0030, JMMD Loss: 0.0456
Validation Loss: 0.0008
Epoch [30/50], Class Loss: 0.0389, JMMD Loss: 0.0439
Validation Loss: 0.0011
Epoch [31/50], Class Loss: 0.0099, JMMD Loss: 0.0507
Validation Loss: 0.0006
Epoch [32/50], Class Loss: 0.0028, JMMD Loss: 0.0395
Validation Loss: 0.0007
Epoch [33/50], Class Loss: 0.0034, JMMD Loss: 0.0430
Validation Loss: 0.0007
Epoch [34/50], Class Loss: 0.0117, JMMD Loss: 0.0321
Validation Loss: 0.0022
Epoch [35/50], Class Loss: 0.0034, JMMD Loss: 0.0325
Validation Loss: 0.0009
Epoch [36/50], Class Loss: 0.0034, JMMD Loss: 0.0528
Validation Loss: 0.0009
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Source performance: 93.24% 93.71% 93.21% 91.89%
Target performance: 93.50% 93.81% 93.65% 92.26%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 88.56%
  Class 1: 89.91%
  Class 2: 98.17%
  Class 3: 97.97%
Base

Run 1/10
Epoch 1/50, Train Loss: 0.3369, Train Acc: 0.8946, Val Loss: 1.2506, Val Acc: 0.6385
Epoch 2/50, Train Loss: 0.0326, Train Acc: 0.9909, Val Loss: 0.0542, Val Acc: 0.9850
Epoch 3/50, Train Loss: 0.0182, Train Acc: 0.9958, Val Loss: 0.1587, Val Acc: 0.9323
Epoch 4/50, Train Loss: 0.0193, Train Acc: 0.9952, Val Loss: 0.2149, Val Acc: 0.9394
Epoch 5/50, Train Loss: 0.0196, Train Acc: 0.9939, Val Loss: 3.1051, Val Acc: 0.5324
Epoch 6/50, Train Loss: 0.0248, Train Acc: 0.9924, Val Loss: 0.0644, Val Acc: 0.9742
Epoch 7/50, Train Loss: 0.0126, Train Acc: 0.9972, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 8/50, Train Loss: 0.0089, Train Acc: 0.9973, Val Loss: 0.0182, Val Acc: 0.9922
Epoch 9/50, Train Loss: 0.0221, Train Acc: 0.9930, Val Loss: 0.0548, Val Acc: 0.9784
Epoch 10/50, Train Loss: 0.0185, Train Acc: 0.9946, Val Loss: 0.2677, Val Acc: 0.8981
Epoch 11/50, Train Loss: 0.0073, Train Acc: 0.9984, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0032, Train Acc: 0.9997, Val Loss: 0.0015, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0027, Train Acc: 0.9996, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0025, Train Acc: 0.9993, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0022, Train Acc: 0.9996, Val Loss: 0.0025, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0021, Train Acc: 0.9996, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0020, Train Acc: 0.9994, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0016, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0020, Train Acc: 0.9996, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 32/50, Train Loss: 0.0014, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 33/50, Train Loss: 0.0020, Train Acc: 0.9993, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 2/10
Epoch 1/50, Train Loss: 0.2952, Train Acc: 0.9036, Val Loss: 1.6570, Val Acc: 0.6337
Epoch 2/50, Train Loss: 0.0350, Train Acc: 0.9900, Val Loss: 1.1095, Val Acc: 0.6373
Epoch 3/50, Train Loss: 0.0191, Train Acc: 0.9957, Val Loss: 0.0305, Val Acc: 0.9898
Epoch 4/50, Train Loss: 0.0226, Train Acc: 0.9939, Val Loss: 0.0678, Val Acc: 0.9868
Epoch 5/50, Train Loss: 0.0144, Train Acc: 0.9960, Val Loss: 0.0138, Val Acc: 0.9976
Epoch 6/50, Train Loss: 0.0148, Train Acc: 0.9957, Val Loss: 0.0469, Val Acc: 0.9850
Epoch 7/50, Train Loss: 0.0238, Train Acc: 0.9916, Val Loss: 0.1244, Val Acc: 0.9460
Epoch 8/50, Train Loss: 0.0137, Train Acc: 0.9961, Val Loss: 0.0022, Val Acc: 0.9994
Epoch 9/50, Train Loss: 0.0074, Train Acc: 0.9990, Val Loss: 0.2478, Val Acc: 0.8717
Epoch 10/50, Train Loss: 0.0097, Train Acc: 0.9969, Val Loss: 0.1173, Val Acc: 0.9592
Epoch 11/50, Train Loss: 0.0048, Train Acc: 0.9988, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0018, Train Acc: 0.9997, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0020, Train Acc: 0.9997, Val Loss: 0.0022, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0017, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0034, Train Acc: 0.9993, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0016, Train Acc: 0.9996, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Early stopping!

Run 3/10
Epoch 1/50, Train Loss: 0.3774, Train Acc: 0.8790, Val Loss: 2.4390, Val Acc: 0.5054
Epoch 2/50, Train Loss: 0.0460, Train Acc: 0.9868, Val Loss: 2.1817, Val Acc: 0.5510
Epoch 3/50, Train Loss: 0.0787, Train Acc: 0.9828, Val Loss: 0.1322, Val Acc: 0.9466
Epoch 4/50, Train Loss: 0.0180, Train Acc: 0.9955, Val Loss: 0.0791, Val Acc: 0.9748
Epoch 5/50, Train Loss: 0.0306, Train Acc: 0.9918, Val Loss: 0.0147, Val Acc: 0.9958
Epoch 6/50, Train Loss: 0.0181, Train Acc: 0.9948, Val Loss: 1.8261, Val Acc: 0.7170
Epoch 7/50, Train Loss: 0.0107, Train Acc: 0.9975, Val Loss: 0.4390, Val Acc: 0.8423
Epoch 8/50, Train Loss: 0.0186, Train Acc: 0.9951, Val Loss: 0.0145, Val Acc: 0.9970
Epoch 9/50, Train Loss: 0.0646, Train Acc: 0.9855, Val Loss: 0.0962, Val Acc: 0.9634
Epoch 10/50, Train Loss: 0.0134, Train Acc: 0.9963, Val Loss: 0.5954, Val Acc: 0.7938
Epoch 11/50, Train Loss: 0.0077, Train Acc: 0.9976, Val Loss: 0.0015, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0042, Train Acc: 0.9990, Val Loss: 0.0024, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0021, Train Acc: 0.9996, Val Loss: 0.0018, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0020, Train Acc: 0.9996, Val Loss: 0.0016, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0034, Train Acc: 0.9993, Val Loss: 0.0012, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0062, Train Acc: 0.9990, Val Loss: 0.0028, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0030, Train Acc: 0.9997, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0058, Train Acc: 0.9984, Val Loss: 0.0051, Val Acc: 0.9976
Epoch 19/50, Train Loss: 0.0034, Train Acc: 0.9996, Val Loss: 0.0018, Val Acc: 0.9988
Epoch 20/50, Train Loss: 0.0034, Train Acc: 0.9994, Val Loss: 0.0017, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0076, Train Acc: 0.9990, Val Loss: 0.0056, Val Acc: 0.9982
Epoch 22/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0014, Val Acc: 0.9994
Early stopping!

Run 4/10
Epoch 1/50, Train Loss: 0.3539, Train Acc: 0.8808, Val Loss: 2.8873, Val Acc: 0.5102
Epoch 2/50, Train Loss: 0.0416, Train Acc: 0.9882, Val Loss: 0.5310, Val Acc: 0.8195
Epoch 3/50, Train Loss: 0.0323, Train Acc: 0.9912, Val Loss: 3.1772, Val Acc: 0.5318
Epoch 4/50, Train Loss: 0.0248, Train Acc: 0.9942, Val Loss: 1.5143, Val Acc: 0.7590
Epoch 5/50, Train Loss: 0.0195, Train Acc: 0.9948, Val Loss: 0.1017, Val Acc: 0.9658
Epoch 6/50, Train Loss: 0.0123, Train Acc: 0.9961, Val Loss: 0.0155, Val Acc: 0.9946
Epoch 7/50, Train Loss: 0.0081, Train Acc: 0.9978, Val Loss: 0.0074, Val Acc: 0.9976
Epoch 8/50, Train Loss: 0.0079, Train Acc: 0.9981, Val Loss: 0.2087, Val Acc: 0.9359
Epoch 9/50, Train Loss: 0.0092, Train Acc: 0.9981, Val Loss: 0.0469, Val Acc: 0.9856
Epoch 10/50, Train Loss: 0.0136, Train Acc: 0.9964, Val Loss: 3.4375, Val Acc: 0.4904
Epoch 11/50, Train Loss: 0.0212, Train Acc: 0.9951, Val Loss: 0.0014, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0029, Train Acc: 0.9993, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0046, Train Acc: 0.9996, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0020, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0018, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0013, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0030, Train Acc: 0.9993, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0028, Train Acc: 0.9994, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0010, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0024, Train Acc: 0.9994, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 32/50, Train Loss: 0.0023, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 33/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Early stopping!

Run 5/10
Epoch 1/50, Train Loss: 0.3188, Train Acc: 0.8880, Val Loss: 0.3326, Val Acc: 0.8807
Epoch 2/50, Train Loss: 0.0274, Train Acc: 0.9936, Val Loss: 0.4611, Val Acc: 0.8351
Epoch 3/50, Train Loss: 0.0247, Train Acc: 0.9930, Val Loss: 0.0210, Val Acc: 0.9934
Epoch 4/50, Train Loss: 0.0156, Train Acc: 0.9957, Val Loss: 0.0581, Val Acc: 0.9820
Epoch 5/50, Train Loss: 0.0194, Train Acc: 0.9937, Val Loss: 0.4240, Val Acc: 0.8423
Epoch 6/50, Train Loss: 0.0202, Train Acc: 0.9939, Val Loss: 2.5054, Val Acc: 0.5923
Epoch 7/50, Train Loss: 0.0175, Train Acc: 0.9949, Val Loss: 0.7334, Val Acc: 0.8064
Epoch 8/50, Train Loss: 0.0213, Train Acc: 0.9937, Val Loss: 0.0141, Val Acc: 0.9952
Epoch 9/50, Train Loss: 0.0087, Train Acc: 0.9981, Val Loss: 0.0284, Val Acc: 0.9898
Epoch 10/50, Train Loss: 0.0031, Train Acc: 0.9993, Val Loss: 0.2438, Val Acc: 0.9275
Epoch 11/50, Train Loss: 0.0038, Train Acc: 0.9993, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0021, Train Acc: 0.9997, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0062, Train Acc: 0.9994, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0020, Train Acc: 0.9997, Val Loss: 0.0019, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0040, Train Acc: 0.9993, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0016, Train Acc: 0.9994, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0013, Train Acc: 0.9996, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0041, Train Acc: 0.9988, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0015, Train Acc: 0.9994, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 6/10
Epoch 1/50, Train Loss: 0.3472, Train Acc: 0.8833, Val Loss: 0.6400, Val Acc: 0.7662
Epoch 2/50, Train Loss: 0.0333, Train Acc: 0.9916, Val Loss: 0.1047, Val Acc: 0.9592
Epoch 3/50, Train Loss: 0.0261, Train Acc: 0.9921, Val Loss: 0.0567, Val Acc: 0.9790
Epoch 4/50, Train Loss: 0.0169, Train Acc: 0.9957, Val Loss: 0.0314, Val Acc: 0.9940
Epoch 5/50, Train Loss: 0.0224, Train Acc: 0.9936, Val Loss: 0.4927, Val Acc: 0.8831
Epoch 6/50, Train Loss: 0.0154, Train Acc: 0.9955, Val Loss: 0.8120, Val Acc: 0.7500
Epoch 7/50, Train Loss: 0.0125, Train Acc: 0.9967, Val Loss: 0.0509, Val Acc: 0.9862
Epoch 8/50, Train Loss: 0.0069, Train Acc: 0.9975, Val Loss: 0.0160, Val Acc: 0.9958
Epoch 9/50, Train Loss: 0.0083, Train Acc: 0.9976, Val Loss: 0.0026, Val Acc: 0.9994
Epoch 10/50, Train Loss: 0.0026, Train Acc: 0.9994, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0036, Train Acc: 0.9988, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0319, Val Acc: 0.9910
Epoch 16/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0083, Train Acc: 0.9988, Val Loss: 0.0005, Val Acc: 1.0000
Early stopping!

Run 7/10
Epoch 1/50, Train Loss: 0.3657, Train Acc: 0.8695, Val Loss: 0.4986, Val Acc: 0.8267
Epoch 2/50, Train Loss: 0.0395, Train Acc: 0.9892, Val Loss: 1.0566, Val Acc: 0.7422
Epoch 3/50, Train Loss: 0.0405, Train Acc: 0.9877, Val Loss: 0.0517, Val Acc: 0.9892
Epoch 4/50, Train Loss: 0.0203, Train Acc: 0.9951, Val Loss: 0.6157, Val Acc: 0.8261
Epoch 5/50, Train Loss: 0.0078, Train Acc: 0.9985, Val Loss: 0.0501, Val Acc: 0.9802
Epoch 6/50, Train Loss: 0.0099, Train Acc: 0.9976, Val Loss: 0.0907, Val Acc: 0.9742
Epoch 7/50, Train Loss: 0.0199, Train Acc: 0.9942, Val Loss: 0.9015, Val Acc: 0.8034
Epoch 8/50, Train Loss: 0.0160, Train Acc: 0.9960, Val Loss: 1.1830, Val Acc: 0.7668
Epoch 9/50, Train Loss: 0.0068, Train Acc: 0.9987, Val Loss: 0.0629, Val Acc: 0.9802
Epoch 10/50, Train Loss: 0.0139, Train Acc: 0.9954, Val Loss: 1.0139, Val Acc: 0.7308
Early stopping!

Run 8/10
Epoch 1/50, Train Loss: 0.4335, Train Acc: 0.8544, Val Loss: 1.6336, Val Acc: 0.6865
Epoch 2/50, Train Loss: 0.0542, Train Acc: 0.9835, Val Loss: 0.0406, Val Acc: 0.9856
Epoch 3/50, Train Loss: 0.0360, Train Acc: 0.9915, Val Loss: 0.0366, Val Acc: 0.9916
Epoch 4/50, Train Loss: 0.0161, Train Acc: 0.9973, Val Loss: 0.0865, Val Acc: 0.9688
Epoch 5/50, Train Loss: 0.0233, Train Acc: 0.9936, Val Loss: 0.1023, Val Acc: 0.9622
Epoch 6/50, Train Loss: 0.0204, Train Acc: 0.9952, Val Loss: 0.0076, Val Acc: 1.0000
Epoch 7/50, Train Loss: 0.0164, Train Acc: 0.9954, Val Loss: 0.0081, Val Acc: 0.9994
Epoch 8/50, Train Loss: 0.0080, Train Acc: 0.9982, Val Loss: 0.0020, Val Acc: 0.9994
Epoch 9/50, Train Loss: 0.0087, Train Acc: 0.9981, Val Loss: 0.0053, Val Acc: 0.9976
Epoch 10/50, Train Loss: 0.0055, Train Acc: 0.9985, Val Loss: 0.0035, Val Acc: 0.9994
Epoch 11/50, Train Loss: 0.0025, Train Acc: 0.9996, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0021, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0023, Train Acc: 0.9996, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0016, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0013, Train Acc: 0.9996, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0011, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Early stopping!

Run 9/10
Epoch 1/50, Train Loss: 0.3595, Train Acc: 0.8844, Val Loss: 1.8027, Val Acc: 0.5833
Epoch 2/50, Train Loss: 0.0519, Train Acc: 0.9832, Val Loss: 0.0254, Val Acc: 0.9910
Epoch 3/50, Train Loss: 0.0175, Train Acc: 0.9966, Val Loss: 0.1104, Val Acc: 0.9610
Epoch 4/50, Train Loss: 0.0261, Train Acc: 0.9921, Val Loss: 1.7257, Val Acc: 0.6876
Epoch 5/50, Train Loss: 0.0284, Train Acc: 0.9915, Val Loss: 0.6846, Val Acc: 0.8249
Epoch 6/50, Train Loss: 0.0178, Train Acc: 0.9943, Val Loss: 1.4985, Val Acc: 0.7854
Epoch 7/50, Train Loss: 0.0126, Train Acc: 0.9967, Val Loss: 1.5783, Val Acc: 0.7740
Early stopping!

Run 10/10
Epoch 1/50, Train Loss: 0.4274, Train Acc: 0.8497, Val Loss: 2.4812, Val Acc: 0.5054
Epoch 2/50, Train Loss: 0.0596, Train Acc: 0.9819, Val Loss: 0.0813, Val Acc: 0.9748
Epoch 3/50, Train Loss: 0.0230, Train Acc: 0.9951, Val Loss: 0.0560, Val Acc: 0.9796
Epoch 4/50, Train Loss: 0.0284, Train Acc: 0.9906, Val Loss: 0.1538, Val Acc: 0.9478
Epoch 5/50, Train Loss: 0.0132, Train Acc: 0.9967, Val Loss: 0.5190, Val Acc: 0.8285
Epoch 6/50, Train Loss: 0.0112, Train Acc: 0.9973, Val Loss: 1.6448, Val Acc: 0.6918
Epoch 7/50, Train Loss: 0.0353, Train Acc: 0.9910, Val Loss: 0.0414, Val Acc: 0.9850
Epoch 8/50, Train Loss: 0.0193, Train Acc: 0.9951, Val Loss: 0.0015, Val Acc: 1.0000
Epoch 9/50, Train Loss: 0.0127, Train Acc: 0.9958, Val Loss: 0.0483, Val Acc: 0.9856
Epoch 10/50, Train Loss: 0.0098, Train Acc: 0.9976, Val Loss: 0.0797, Val Acc: 0.9634
Epoch 11/50, Train Loss: 0.0039, Train Acc: 0.9991, Val Loss: 0.0014, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0040, Train Acc: 0.9988, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0013, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0039, Train Acc: 0.9996, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0036, Train Acc: 0.9996, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0018, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0027, Train Acc: 0.9993, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0016, Train Acc: 0.9996, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0049, Train Acc: 0.9994, Val Loss: 0.0021, Val Acc: 0.9988
Epoch 26/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0029, Train Acc: 0.9994, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0014, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 32/50, Train Loss: 0.0032, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Early stopping!

Source performance: 95.04 97.27 95.15 94.04
Target performance: 95.23 97.45 95.25 94.16

bpsk: 91.10
qpsk: 99.11
16qam: 90.79
8apsk: 100.00
DANN
Epoch 1/50, Loss: 2.4409, Domain Loss: 1.4101, Class Loss: 1.0308
Epoch 2/50, Loss: 1.7661, Domain Loss: 1.3872, Class Loss: 0.3789
Epoch 3/50, Loss: 1.5370, Domain Loss: 1.3870, Class Loss: 0.1501
Epoch 4/50, Loss: 1.4549, Domain Loss: 1.3831, Class Loss: 0.0719
Epoch 5/50, Loss: 1.5014, Domain Loss: 1.3867, Class Loss: 0.1147
Epoch 6/50, Loss: 1.4968, Domain Loss: 1.3915, Class Loss: 0.1053
Epoch 7/50, Loss: 1.4344, Domain Loss: 1.3863, Class Loss: 0.0481
Epoch 8/50, Loss: 1.5287, Domain Loss: 1.3899, Class Loss: 0.1388
Epoch 9/50, Loss: 1.4769, Domain Loss: 1.3899, Class Loss: 0.0870
Epoch 10/50, Loss: 1.4521, Domain Loss: 1.3994, Class Loss: 0.0527
Epoch 11/50, Loss: 1.4624, Domain Loss: 1.4200, Class Loss: 0.0424
Epoch 12/50, Loss: 1.4270, Domain Loss: 1.4013, Class Loss: 0.0257
Epoch 13/50, Loss: 1.4685, Domain Loss: 1.4186, Class Loss: 0.0498
Epoch 14/50, Loss: 1.4318, Domain Loss: 1.4013, Class Loss: 0.0305
Epoch 15/50, Loss: 1.8420, Domain Loss: 1.7997, Class Loss: 0.0423
Epoch 16/50, Loss: 2.5834, Domain Loss: 2.2805, Class Loss: 0.3029
Epoch 17/50, Loss: 2.0671, Domain Loss: 1.9530, Class Loss: 0.1141
Epoch 18/50, Loss: 1.8314, Domain Loss: 1.6851, Class Loss: 0.1464
Epoch 19/50, Loss: 1.8886, Domain Loss: 1.7458, Class Loss: 0.1428
Epoch 20/50, Loss: 1.8969, Domain Loss: 1.7465, Class Loss: 0.1504
Epoch 21/50, Loss: 1.5617, Domain Loss: 1.4695, Class Loss: 0.0922
Epoch 22/50, Loss: 1.4988, Domain Loss: 1.4329, Class Loss: 0.0659
Epoch 23/50, Loss: 1.4767, Domain Loss: 1.4024, Class Loss: 0.0743
Epoch 24/50, Loss: 1.4631, Domain Loss: 1.4049, Class Loss: 0.0582
Epoch 25/50, Loss: 1.4609, Domain Loss: 1.3958, Class Loss: 0.0651
Epoch 26/50, Loss: 1.4674, Domain Loss: 1.4073, Class Loss: 0.0601
Epoch 27/50, Loss: 1.4275, Domain Loss: 1.3919, Class Loss: 0.0356
Epoch 28/50, Loss: 1.4239, Domain Loss: 1.3928, Class Loss: 0.0311
Epoch 29/50, Loss: 1.4115, Domain Loss: 1.3907, Class Loss: 0.0208
Epoch 30/50, Loss: 1.4085, Domain Loss: 1.3932, Class Loss: 0.0152
Epoch 31/50, Loss: 1.4299, Domain Loss: 1.4034, Class Loss: 0.0265
Epoch 32/50, Loss: 1.4118, Domain Loss: 1.3938, Class Loss: 0.0179
Epoch 33/50, Loss: 1.4153, Domain Loss: 1.3902, Class Loss: 0.0251
Epoch 34/50, Loss: 1.4190, Domain Loss: 1.3892, Class Loss: 0.0297
Epoch 35/50, Loss: 1.4177, Domain Loss: 1.3914, Class Loss: 0.0263
Epoch 36/50, Loss: 1.4072, Domain Loss: 1.3847, Class Loss: 0.0224
Epoch 37/50, Loss: 1.4116, Domain Loss: 1.3923, Class Loss: 0.0194
Epoch 38/50, Loss: 1.4211, Domain Loss: 1.3996, Class Loss: 0.0215
Epoch 39/50, Loss: 1.4233, Domain Loss: 1.3868, Class Loss: 0.0366
Epoch 40/50, Loss: 1.4011, Domain Loss: 1.3849, Class Loss: 0.0162
Epoch 41/50, Loss: 1.3946, Domain Loss: 1.3836, Class Loss: 0.0110
Epoch 42/50, Loss: 1.5226, Domain Loss: 1.3942, Class Loss: 0.1284
Epoch 43/50, Loss: 1.4464, Domain Loss: 1.3850, Class Loss: 0.0615
Epoch 44/50, Loss: 1.4212, Domain Loss: 1.3852, Class Loss: 0.0360
Epoch 45/50, Loss: 1.4281, Domain Loss: 1.3849, Class Loss: 0.0431
Epoch 46/50, Loss: 1.4029, Domain Loss: 1.3818, Class Loss: 0.0211
Epoch 47/50, Loss: 1.4152, Domain Loss: 1.3889, Class Loss: 0.0264
Epoch 48/50, Loss: 1.4043, Domain Loss: 1.3834, Class Loss: 0.0209
Epoch 49/50, Loss: 1.4048, Domain Loss: 1.3853, Class Loss: 0.0195
Epoch 50/50, Loss: 1.3978, Domain Loss: 1.3861, Class Loss: 0.0117
99.28


Epoch 1/50, Loss: 2.4367, Domain Loss: 1.4104, Class Loss: 1.0263
Epoch 2/50, Loss: 1.7798, Domain Loss: 1.3773, Class Loss: 0.4025
Epoch 3/50, Loss: 1.5282, Domain Loss: 1.3810, Class Loss: 0.1473
Epoch 4/50, Loss: 1.4800, Domain Loss: 1.3842, Class Loss: 0.0958
Epoch 5/50, Loss: 1.4378, Domain Loss: 1.3764, Class Loss: 0.0614
Epoch 6/50, Loss: 1.4445, Domain Loss: 1.3943, Class Loss: 0.0502
Epoch 7/50, Loss: 3.0283, Domain Loss: 2.9289, Class Loss: 0.0994
Epoch 8/50, Loss: 4.0270, Domain Loss: 3.8958, Class Loss: 0.1312
Epoch 9/50, Loss: 2.7838, Domain Loss: 2.6732, Class Loss: 0.1106
Epoch 10/50, Loss: 1.6918, Domain Loss: 1.6204, Class Loss: 0.0714
Epoch 11/50, Loss: 2.0717, Domain Loss: 1.7122, Class Loss: 0.3596
Epoch 12/50, Loss: 5.4339, Domain Loss: 3.8171, Class Loss: 1.6168
Epoch 13/50, Loss: 4.4684, Domain Loss: 3.5407, Class Loss: 0.9277
Epoch 14/50, Loss: 7.5199, Domain Loss: 6.8104, Class Loss: 0.7095
Epoch 15/50, Loss: 7.8262, Domain Loss: 7.1092, Class Loss: 0.7170
Epoch 16/50, Loss: 5.9465, Domain Loss: 5.0115, Class Loss: 0.9349
Epoch 17/50, Loss: 3.4323, Domain Loss: 2.7284, Class Loss: 0.7039
Epoch 18/50, Loss: 3.9321, Domain Loss: 3.3113, Class Loss: 0.6208
Epoch 19/50, Loss: 3.2292, Domain Loss: 2.7234, Class Loss: 0.5057
Epoch 20/50, Loss: 2.1979, Domain Loss: 1.8147, Class Loss: 0.3832
Epoch 21/50, Loss: 2.0851, Domain Loss: 1.6543, Class Loss: 0.4308
Epoch 22/50, Loss: 1.8778, Domain Loss: 1.6349, Class Loss: 0.2429
Epoch 23/50, Loss: 1.8068, Domain Loss: 1.6190, Class Loss: 0.1879
Epoch 24/50, Loss: 1.6622, Domain Loss: 1.5190, Class Loss: 0.1432
Epoch 25/50, Loss: 1.5810, Domain Loss: 1.4688, Class Loss: 0.1122
Epoch 26/50, Loss: 1.5422, Domain Loss: 1.4545, Class Loss: 0.0877
Epoch 27/50, Loss: 1.5121, Domain Loss: 1.4380, Class Loss: 0.0741
Epoch 28/50, Loss: 1.5081, Domain Loss: 1.4216, Class Loss: 0.0865
Epoch 29/50, Loss: 1.4717, Domain Loss: 1.4177, Class Loss: 0.0540
Epoch 30/50, Loss: 1.4694, Domain Loss: 1.4172, Class Loss: 0.0522
Epoch 31/50, Loss: 1.4489, Domain Loss: 1.4086, Class Loss: 0.0402
Epoch 32/50, Loss: 1.4661, Domain Loss: 1.4073, Class Loss: 0.0588
Epoch 33/50, Loss: 1.4546, Domain Loss: 1.3997, Class Loss: 0.0550
Epoch 34/50, Loss: 1.4557, Domain Loss: 1.3989, Class Loss: 0.0568
Epoch 35/50, Loss: 1.4530, Domain Loss: 1.3973, Class Loss: 0.0556
Epoch 36/50, Loss: 1.4667, Domain Loss: 1.3867, Class Loss: 0.0799
Epoch 37/50, Loss: 1.4351, Domain Loss: 1.3912, Class Loss: 0.0439
Epoch 38/50, Loss: 1.4264, Domain Loss: 1.3912, Class Loss: 0.0351
Epoch 39/50, Loss: 1.4309, Domain Loss: 1.3922, Class Loss: 0.0387
Epoch 40/50, Loss: 1.4237, Domain Loss: 1.3876, Class Loss: 0.0361
Epoch 41/50, Loss: 1.4583, Domain Loss: 1.3933, Class Loss: 0.0651
Epoch 42/50, Loss: 1.4264, Domain Loss: 1.3869, Class Loss: 0.0395
Epoch 43/50, Loss: 1.4384, Domain Loss: 1.3844, Class Loss: 0.0541
Epoch 44/50, Loss: 1.4381, Domain Loss: 1.3846, Class Loss: 0.0535
Epoch 45/50, Loss: 1.4178, Domain Loss: 1.3826, Class Loss: 0.0352
Epoch 46/50, Loss: 1.4162, Domain Loss: 1.3797, Class Loss: 0.0365
Epoch 47/50, Loss: 1.4215, Domain Loss: 1.3881, Class Loss: 0.0334
Epoch 48/50, Loss: 1.4214, Domain Loss: 1.3870, Class Loss: 0.0345
Epoch 49/50, Loss: 1.4141, Domain Loss: 1.3781, Class Loss: 0.0359
Epoch 50/50, Loss: 1.4147, Domain Loss: 1.3879, Class Loss: 0.0267
80.04


Epoch 1/50, Loss: 2.3901, Domain Loss: 1.3930, Class Loss: 0.9971
Epoch 2/50, Loss: 1.6928, Domain Loss: 1.3906, Class Loss: 0.3021
Epoch 3/50, Loss: 1.4900, Domain Loss: 1.3837, Class Loss: 0.1063
Epoch 4/50, Loss: 1.5267, Domain Loss: 1.3878, Class Loss: 0.1389
Epoch 5/50, Loss: 1.4465, Domain Loss: 1.3864, Class Loss: 0.0601
Epoch 6/50, Loss: 1.4237, Domain Loss: 1.3852, Class Loss: 0.0385
Epoch 7/50, Loss: 1.5766, Domain Loss: 1.5004, Class Loss: 0.0762
Epoch 8/50, Loss: 3.4007, Domain Loss: 3.1386, Class Loss: 0.2621
Epoch 9/50, Loss: 2.4008, Domain Loss: 2.2477, Class Loss: 0.1531
Epoch 10/50, Loss: 1.7352, Domain Loss: 1.6612, Class Loss: 0.0740
Epoch 11/50, Loss: 1.6999, Domain Loss: 1.6257, Class Loss: 0.0742
Epoch 12/50, Loss: 2.1085, Domain Loss: 1.9622, Class Loss: 0.1463
Epoch 13/50, Loss: 2.8847, Domain Loss: 2.6309, Class Loss: 0.2538
Epoch 14/50, Loss: 2.3194, Domain Loss: 2.1605, Class Loss: 0.1589
Epoch 15/50, Loss: 1.9905, Domain Loss: 1.8453, Class Loss: 0.1452
Epoch 16/50, Loss: 1.7130, Domain Loss: 1.5786, Class Loss: 0.1344
Epoch 17/50, Loss: 1.5159, Domain Loss: 1.4300, Class Loss: 0.0858
Epoch 18/50, Loss: 1.4675, Domain Loss: 1.4075, Class Loss: 0.0600
Epoch 19/50, Loss: 1.4905, Domain Loss: 1.4538, Class Loss: 0.0367
Epoch 20/50, Loss: 1.5652, Domain Loss: 1.5088, Class Loss: 0.0563
Epoch 21/50, Loss: 1.5304, Domain Loss: 1.4838, Class Loss: 0.0465
Epoch 22/50, Loss: 1.5687, Domain Loss: 1.5185, Class Loss: 0.0503
Epoch 23/50, Loss: 1.5160, Domain Loss: 1.4751, Class Loss: 0.0408
Epoch 24/50, Loss: 1.5120, Domain Loss: 1.4677, Class Loss: 0.0443
Epoch 25/50, Loss: 1.4810, Domain Loss: 1.4374, Class Loss: 0.0437
Epoch 26/50, Loss: 1.4740, Domain Loss: 1.4134, Class Loss: 0.0607
Epoch 27/50, Loss: 1.4620, Domain Loss: 1.4122, Class Loss: 0.0498
Epoch 28/50, Loss: 1.4666, Domain Loss: 1.4153, Class Loss: 0.0514
Epoch 29/50, Loss: 1.4534, Domain Loss: 1.4078, Class Loss: 0.0456
Epoch 30/50, Loss: 1.4963, Domain Loss: 1.4711, Class Loss: 0.0252
Epoch 31/50, Loss: 1.4963, Domain Loss: 1.4583, Class Loss: 0.0380
Epoch 32/50, Loss: 1.4243, Domain Loss: 1.3973, Class Loss: 0.0270
Epoch 33/50, Loss: 1.4180, Domain Loss: 1.4021, Class Loss: 0.0160
Epoch 34/50, Loss: 1.4317, Domain Loss: 1.4150, Class Loss: 0.0167
Epoch 35/50, Loss: 1.4517, Domain Loss: 1.4282, Class Loss: 0.0236
Epoch 36/50, Loss: 1.4469, Domain Loss: 1.4096, Class Loss: 0.0373
Epoch 37/50, Loss: 1.4417, Domain Loss: 1.4173, Class Loss: 0.0244
Epoch 38/50, Loss: 1.4218, Domain Loss: 1.4003, Class Loss: 0.0215
Epoch 39/50, Loss: 1.4247, Domain Loss: 1.3941, Class Loss: 0.0305
Epoch 40/50, Loss: 1.4281, Domain Loss: 1.4116, Class Loss: 0.0164
Epoch 41/50, Loss: 1.4069, Domain Loss: 1.4010, Class Loss: 0.0060
Epoch 42/50, Loss: 1.4048, Domain Loss: 1.4008, Class Loss: 0.0040
Epoch 43/50, Loss: 1.3987, Domain Loss: 1.3941, Class Loss: 0.0046
Epoch 44/50, Loss: 1.4116, Domain Loss: 1.3946, Class Loss: 0.0170
Epoch 45/50, Loss: 1.4092, Domain Loss: 1.3987, Class Loss: 0.0105
Epoch 46/50, Loss: 1.4047, Domain Loss: 1.3985, Class Loss: 0.0062
Epoch 47/50, Loss: 1.4065, Domain Loss: 1.4006, Class Loss: 0.0060
Epoch 48/50, Loss: 1.4043, Domain Loss: 1.3967, Class Loss: 0.0075
Epoch 49/50, Loss: 1.4276, Domain Loss: 1.3993, Class Loss: 0.0283
Epoch 50/50, Loss: 1.4130, Domain Loss: 1.4020, Class Loss: 0.0110
98.38


Epoch 1/50, Loss: 2.4693, Domain Loss: 1.4163, Class Loss: 1.0529
Epoch 2/50, Loss: 1.7702, Domain Loss: 1.3925, Class Loss: 0.3777
Epoch 3/50, Loss: 1.5357, Domain Loss: 1.3921, Class Loss: 0.1436
Epoch 4/50, Loss: 1.4699, Domain Loss: 1.3855, Class Loss: 0.0844
Epoch 5/50, Loss: 1.4559, Domain Loss: 1.3881, Class Loss: 0.0678
Epoch 6/50, Loss: 1.4468, Domain Loss: 1.3942, Class Loss: 0.0526
Epoch 7/50, Loss: 1.6808, Domain Loss: 1.6361, Class Loss: 0.0447
Epoch 8/50, Loss: 3.1427, Domain Loss: 3.0332, Class Loss: 0.1095
Epoch 9/50, Loss: 3.1065, Domain Loss: 3.0103, Class Loss: 0.0962
Epoch 10/50, Loss: 2.1151, Domain Loss: 1.7511, Class Loss: 0.3640
Epoch 11/50, Loss: 1.5744, Domain Loss: 1.4464, Class Loss: 0.1279
Epoch 12/50, Loss: 1.5075, Domain Loss: 1.4260, Class Loss: 0.0814
Epoch 13/50, Loss: 1.4783, Domain Loss: 1.4243, Class Loss: 0.0540
Epoch 14/50, Loss: 1.4989, Domain Loss: 1.4475, Class Loss: 0.0514
Epoch 15/50, Loss: 1.6618, Domain Loss: 1.5821, Class Loss: 0.0798
Epoch 16/50, Loss: 1.5374, Domain Loss: 1.4850, Class Loss: 0.0524
Epoch 17/50, Loss: 1.4830, Domain Loss: 1.4415, Class Loss: 0.0415
Epoch 18/50, Loss: 1.4299, Domain Loss: 1.3959, Class Loss: 0.0340
Epoch 19/50, Loss: 1.4336, Domain Loss: 1.3932, Class Loss: 0.0404
Epoch 20/50, Loss: 1.4516, Domain Loss: 1.4053, Class Loss: 0.0463
Epoch 21/50, Loss: 1.4546, Domain Loss: 1.4139, Class Loss: 0.0407
Epoch 22/50, Loss: 1.4302, Domain Loss: 1.4109, Class Loss: 0.0193
Epoch 23/50, Loss: 1.4579, Domain Loss: 1.4136, Class Loss: 0.0443
Epoch 24/50, Loss: 1.4452, Domain Loss: 1.4080, Class Loss: 0.0371
Epoch 25/50, Loss: 1.4615, Domain Loss: 1.4344, Class Loss: 0.0271
Epoch 26/50, Loss: 1.7116, Domain Loss: 1.6590, Class Loss: 0.0526
Epoch 27/50, Loss: 1.6512, Domain Loss: 1.5931, Class Loss: 0.0581
Epoch 28/50, Loss: 1.5185, Domain Loss: 1.4690, Class Loss: 0.0494
Epoch 29/50, Loss: 1.4822, Domain Loss: 1.4589, Class Loss: 0.0233
Epoch 30/50, Loss: 1.4118, Domain Loss: 1.3892, Class Loss: 0.0226
Epoch 31/50, Loss: 1.4672, Domain Loss: 1.4131, Class Loss: 0.0541
Epoch 32/50, Loss: 1.4357, Domain Loss: 1.4058, Class Loss: 0.0299
Epoch 33/50, Loss: 1.4238, Domain Loss: 1.3995, Class Loss: 0.0243
Epoch 34/50, Loss: 1.4143, Domain Loss: 1.3933, Class Loss: 0.0210
Epoch 35/50, Loss: 1.4370, Domain Loss: 1.3957, Class Loss: 0.0412
Epoch 36/50, Loss: 1.4338, Domain Loss: 1.3961, Class Loss: 0.0378
Epoch 37/50, Loss: 1.4325, Domain Loss: 1.4050, Class Loss: 0.0276
Epoch 38/50, Loss: 1.4138, Domain Loss: 1.3988, Class Loss: 0.0150
Epoch 39/50, Loss: 1.4226, Domain Loss: 1.4020, Class Loss: 0.0206
Epoch 40/50, Loss: 1.4176, Domain Loss: 1.3982, Class Loss: 0.0194
Epoch 41/50, Loss: 1.4115, Domain Loss: 1.3925, Class Loss: 0.0190
Epoch 42/50, Loss: 1.4095, Domain Loss: 1.3955, Class Loss: 0.0140
Epoch 43/50, Loss: 1.4035, Domain Loss: 1.3967, Class Loss: 0.0067
Epoch 44/50, Loss: 1.4059, Domain Loss: 1.3987, Class Loss: 0.0071
Epoch 45/50, Loss: 1.4033, Domain Loss: 1.3987, Class Loss: 0.0046
Epoch 46/50, Loss: 1.4168, Domain Loss: 1.4007, Class Loss: 0.0161
Epoch 47/50, Loss: 1.4026, Domain Loss: 1.3962, Class Loss: 0.0064
Epoch 48/50, Loss: 1.4057, Domain Loss: 1.3938, Class Loss: 0.0119
Epoch 49/50, Loss: 1.3998, Domain Loss: 1.3883, Class Loss: 0.0115
Epoch 50/50, Loss: 1.4183, Domain Loss: 1.3910, Class Loss: 0.0273
98.02


Epoch 1/50, Loss: 2.3577, Domain Loss: 1.4074, Class Loss: 0.9503
Epoch 2/50, Loss: 1.7469, Domain Loss: 1.3870, Class Loss: 0.3599
Epoch 3/50, Loss: 1.5568, Domain Loss: 1.3835, Class Loss: 0.1734
Epoch 4/50, Loss: 1.4754, Domain Loss: 1.3880, Class Loss: 0.0873
Epoch 5/50, Loss: 1.4283, Domain Loss: 1.3816, Class Loss: 0.0467
Epoch 6/50, Loss: 1.4169, Domain Loss: 1.3853, Class Loss: 0.0316
Epoch 7/50, Loss: 1.4161, Domain Loss: 1.3886, Class Loss: 0.0276
Epoch 8/50, Loss: 1.4352, Domain Loss: 1.3932, Class Loss: 0.0420
Epoch 9/50, Loss: 1.4260, Domain Loss: 1.3966, Class Loss: 0.0294
Epoch 10/50, Loss: 1.8061, Domain Loss: 1.7339, Class Loss: 0.0722
Epoch 11/50, Loss: 2.9594, Domain Loss: 2.7916, Class Loss: 0.1678
Epoch 12/50, Loss: 8.6337, Domain Loss: 8.2231, Class Loss: 0.4106
Epoch 13/50, Loss: 6.3857, Domain Loss: 6.0002, Class Loss: 0.3854
Epoch 14/50, Loss: 2.4183, Domain Loss: 2.2065, Class Loss: 0.2117
Epoch 15/50, Loss: 1.6366, Domain Loss: 1.5234, Class Loss: 0.1133
Epoch 16/50, Loss: 1.5081, Domain Loss: 1.4292, Class Loss: 0.0788
Epoch 17/50, Loss: 1.4564, Domain Loss: 1.4090, Class Loss: 0.0473
Epoch 18/50, Loss: 1.4400, Domain Loss: 1.4072, Class Loss: 0.0328
Epoch 19/50, Loss: 1.4728, Domain Loss: 1.4348, Class Loss: 0.0381
Epoch 20/50, Loss: 1.4789, Domain Loss: 1.4275, Class Loss: 0.0514
Epoch 21/50, Loss: 1.4831, Domain Loss: 1.4162, Class Loss: 0.0669
Epoch 22/50, Loss: 1.4139, Domain Loss: 1.3784, Class Loss: 0.0355
Epoch 23/50, Loss: 1.4301, Domain Loss: 1.4015, Class Loss: 0.0286
Epoch 24/50, Loss: 1.4149, Domain Loss: 1.3942, Class Loss: 0.0207
Epoch 25/50, Loss: 1.4285, Domain Loss: 1.3897, Class Loss: 0.0388
Epoch 26/50, Loss: 1.4337, Domain Loss: 1.4089, Class Loss: 0.0248
Epoch 27/50, Loss: 1.4581, Domain Loss: 1.4121, Class Loss: 0.0459
Epoch 28/50, Loss: 1.4447, Domain Loss: 1.4149, Class Loss: 0.0298
Epoch 29/50, Loss: 1.4883, Domain Loss: 1.4492, Class Loss: 0.0391
Epoch 30/50, Loss: 1.5342, Domain Loss: 1.3932, Class Loss: 0.1410
Epoch 31/50, Loss: 1.4346, Domain Loss: 1.3894, Class Loss: 0.0453
Epoch 32/50, Loss: 1.4479, Domain Loss: 1.3939, Class Loss: 0.0540
Epoch 33/50, Loss: 1.4251, Domain Loss: 1.3997, Class Loss: 0.0255
Epoch 34/50, Loss: 1.4143, Domain Loss: 1.3914, Class Loss: 0.0230
Epoch 35/50, Loss: 1.4207, Domain Loss: 1.4012, Class Loss: 0.0195
Epoch 36/50, Loss: 1.4256, Domain Loss: 1.4063, Class Loss: 0.0193
Epoch 37/50, Loss: 1.4325, Domain Loss: 1.4027, Class Loss: 0.0298
Epoch 38/50, Loss: 1.4297, Domain Loss: 1.4065, Class Loss: 0.0232
Epoch 39/50, Loss: 1.4177, Domain Loss: 1.4052, Class Loss: 0.0125
Epoch 40/50, Loss: 1.4152, Domain Loss: 1.3980, Class Loss: 0.0172
Epoch 41/50, Loss: 1.4181, Domain Loss: 1.3998, Class Loss: 0.0183
Epoch 42/50, Loss: 1.4172, Domain Loss: 1.3906, Class Loss: 0.0266
Epoch 43/50, Loss: 1.4326, Domain Loss: 1.4053, Class Loss: 0.0273
Epoch 44/50, Loss: 1.4384, Domain Loss: 1.3957, Class Loss: 0.0427
Epoch 45/50, Loss: 1.4091, Domain Loss: 1.3969, Class Loss: 0.0122
Epoch 46/50, Loss: 1.4149, Domain Loss: 1.3942, Class Loss: 0.0207
Epoch 47/50, Loss: 1.4202, Domain Loss: 1.4000, Class Loss: 0.0202
Epoch 48/50, Loss: 1.4332, Domain Loss: 1.3997, Class Loss: 0.0334
Epoch 49/50, Loss: 1.4107, Domain Loss: 1.3971, Class Loss: 0.0136
Epoch 50/50, Loss: 1.4301, Domain Loss: 1.4021, Class Loss: 0.0280
96.58


Epoch 1/50, Loss: 2.4188, Domain Loss: 1.4098, Class Loss: 1.0090
Epoch 2/50, Loss: 1.7590, Domain Loss: 1.3919, Class Loss: 0.3671
Epoch 3/50, Loss: 1.5210, Domain Loss: 1.3806, Class Loss: 0.1404
Epoch 4/50, Loss: 1.5123, Domain Loss: 1.3854, Class Loss: 0.1269
Epoch 5/50, Loss: 1.6593, Domain Loss: 1.3940, Class Loss: 0.2654
Epoch 6/50, Loss: 1.5115, Domain Loss: 1.3858, Class Loss: 0.1258
Epoch 7/50, Loss: 1.5403, Domain Loss: 1.3961, Class Loss: 0.1442
Epoch 8/50, Loss: 3.2491, Domain Loss: 3.1288, Class Loss: 0.1203
Epoch 9/50, Loss: 4.8011, Domain Loss: 4.6720, Class Loss: 0.1291
Epoch 10/50, Loss: 8.4466, Domain Loss: 8.2127, Class Loss: 0.2338
Epoch 11/50, Loss: 4.2306, Domain Loss: 3.8986, Class Loss: 0.3319
Epoch 12/50, Loss: 3.8231, Domain Loss: 3.6467, Class Loss: 0.1764
Epoch 13/50, Loss: 2.8087, Domain Loss: 2.6327, Class Loss: 0.1760
Epoch 14/50, Loss: 2.4785, Domain Loss: 2.2501, Class Loss: 0.2283
Epoch 15/50, Loss: 2.0267, Domain Loss: 1.8797, Class Loss: 0.1470
Epoch 16/50, Loss: 1.7178, Domain Loss: 1.6659, Class Loss: 0.0519
Epoch 17/50, Loss: 1.8199, Domain Loss: 1.7585, Class Loss: 0.0614
Epoch 18/50, Loss: 2.8542, Domain Loss: 2.4197, Class Loss: 0.4345
Epoch 19/50, Loss: 4.4766, Domain Loss: 3.2359, Class Loss: 1.2407
Epoch 20/50, Loss: 4.0857, Domain Loss: 3.1302, Class Loss: 0.9555
Epoch 21/50, Loss: 2.7404, Domain Loss: 2.2241, Class Loss: 0.5163
Epoch 22/50, Loss: 2.0354, Domain Loss: 1.7675, Class Loss: 0.2679
Epoch 23/50, Loss: 1.6979, Domain Loss: 1.5327, Class Loss: 0.1651
Epoch 24/50, Loss: 1.5954, Domain Loss: 1.4739, Class Loss: 0.1215
Epoch 25/50, Loss: 1.5081, Domain Loss: 1.4230, Class Loss: 0.0851
Epoch 26/50, Loss: 1.5024, Domain Loss: 1.4190, Class Loss: 0.0835
Epoch 27/50, Loss: 1.4729, Domain Loss: 1.4045, Class Loss: 0.0685
Epoch 28/50, Loss: 1.4563, Domain Loss: 1.3936, Class Loss: 0.0628
Epoch 29/50, Loss: 1.4497, Domain Loss: 1.3924, Class Loss: 0.0573
Epoch 30/50, Loss: 1.4452, Domain Loss: 1.4056, Class Loss: 0.0396
Epoch 31/50, Loss: 1.4577, Domain Loss: 1.4102, Class Loss: 0.0475
Epoch 32/50, Loss: 1.4395, Domain Loss: 1.3975, Class Loss: 0.0419
Epoch 33/50, Loss: 1.4479, Domain Loss: 1.4044, Class Loss: 0.0435
Epoch 34/50, Loss: 1.4368, Domain Loss: 1.4032, Class Loss: 0.0336
Epoch 35/50, Loss: 1.5037, Domain Loss: 1.4789, Class Loss: 0.0247
Epoch 36/50, Loss: 1.4096, Domain Loss: 1.3882, Class Loss: 0.0213
Epoch 37/50, Loss: 1.4033, Domain Loss: 1.3860, Class Loss: 0.0172
Epoch 38/50, Loss: 1.4290, Domain Loss: 1.4044, Class Loss: 0.0245
Epoch 39/50, Loss: 1.3984, Domain Loss: 1.3697, Class Loss: 0.0287
Epoch 40/50, Loss: 1.4365, Domain Loss: 1.4159, Class Loss: 0.0206
Epoch 41/50, Loss: 1.4980, Domain Loss: 1.4585, Class Loss: 0.0394
Epoch 42/50, Loss: 1.4439, Domain Loss: 1.4040, Class Loss: 0.0399
Epoch 43/50, Loss: 2.0788, Domain Loss: 1.9614, Class Loss: 0.1174
Epoch 44/50, Loss: 1.8801, Domain Loss: 1.8004, Class Loss: 0.0797
Epoch 45/50, Loss: 1.5371, Domain Loss: 1.4621, Class Loss: 0.0750
Epoch 46/50, Loss: 1.5043, Domain Loss: 1.4550, Class Loss: 0.0493
Epoch 47/50, Loss: 1.4740, Domain Loss: 1.4336, Class Loss: 0.0405
Epoch 48/50, Loss: 1.4424, Domain Loss: 1.3958, Class Loss: 0.0465
Epoch 49/50, Loss: 1.4570, Domain Loss: 1.4187, Class Loss: 0.0382
Epoch 50/50, Loss: 1.4642, Domain Loss: 1.4221, Class Loss: 0.0421
99.58


Epoch 1/50, Loss: 2.4465, Domain Loss: 1.4170, Class Loss: 1.0295
Epoch 2/50, Loss: 1.8816, Domain Loss: 1.3942, Class Loss: 0.4874
Epoch 3/50, Loss: 1.5379, Domain Loss: 1.3825, Class Loss: 0.1554
Epoch 4/50, Loss: 1.4802, Domain Loss: 1.3777, Class Loss: 0.1025
Epoch 5/50, Loss: 1.4873, Domain Loss: 1.3912, Class Loss: 0.0961
Epoch 6/50, Loss: 1.5887, Domain Loss: 1.5279, Class Loss: 0.0609
Epoch 7/50, Loss: 4.5422, Domain Loss: 4.3800, Class Loss: 0.1622
Epoch 8/50, Loss: 6.3343, Domain Loss: 6.1724, Class Loss: 0.1619
Epoch 9/50, Loss: 5.3016, Domain Loss: 5.0498, Class Loss: 0.2518
Epoch 10/50, Loss: 2.7557, Domain Loss: 2.4871, Class Loss: 0.2686
Epoch 11/50, Loss: 2.0953, Domain Loss: 1.9530, Class Loss: 0.1423
Epoch 12/50, Loss: 6.1292, Domain Loss: 4.7766, Class Loss: 1.3526
Epoch 13/50, Loss: 7.1726, Domain Loss: 6.4492, Class Loss: 0.7235
Epoch 14/50, Loss: 5.6171, Domain Loss: 5.0448, Class Loss: 0.5723
Epoch 15/50, Loss: 4.3265, Domain Loss: 3.7058, Class Loss: 0.6207
Epoch 16/50, Loss: 2.5753, Domain Loss: 2.0993, Class Loss: 0.4760
Epoch 17/50, Loss: 1.9796, Domain Loss: 1.5855, Class Loss: 0.3941
Epoch 18/50, Loss: 1.8229, Domain Loss: 1.5523, Class Loss: 0.2706
Epoch 19/50, Loss: 1.6359, Domain Loss: 1.4642, Class Loss: 0.1717
Epoch 20/50, Loss: 1.5825, Domain Loss: 1.4393, Class Loss: 0.1432
Epoch 21/50, Loss: 1.5144, Domain Loss: 1.4119, Class Loss: 0.1025
Epoch 22/50, Loss: 1.4912, Domain Loss: 1.4092, Class Loss: 0.0820
Epoch 23/50, Loss: 1.4789, Domain Loss: 1.3950, Class Loss: 0.0840
Epoch 24/50, Loss: 1.4707, Domain Loss: 1.3947, Class Loss: 0.0760
Epoch 25/50, Loss: 1.4628, Domain Loss: 1.3984, Class Loss: 0.0644
Epoch 26/50, Loss: 1.4666, Domain Loss: 1.4013, Class Loss: 0.0653
Epoch 27/50, Loss: 1.4580, Domain Loss: 1.3939, Class Loss: 0.0641
Epoch 28/50, Loss: 1.4495, Domain Loss: 1.4007, Class Loss: 0.0488
Epoch 29/50, Loss: 1.4553, Domain Loss: 1.4112, Class Loss: 0.0441
Epoch 30/50, Loss: 1.4886, Domain Loss: 1.4446, Class Loss: 0.0441
Epoch 31/50, Loss: 1.5322, Domain Loss: 1.4730, Class Loss: 0.0592
Epoch 32/50, Loss: 1.5052, Domain Loss: 1.4483, Class Loss: 0.0569
Epoch 33/50, Loss: 1.6097, Domain Loss: 1.5545, Class Loss: 0.0552
Epoch 34/50, Loss: 1.5973, Domain Loss: 1.5514, Class Loss: 0.0459
Epoch 35/50, Loss: 1.9879, Domain Loss: 1.8456, Class Loss: 0.1423
Epoch 36/50, Loss: 1.9718, Domain Loss: 1.8223, Class Loss: 0.1494
Epoch 37/50, Loss: 1.6258, Domain Loss: 1.5541, Class Loss: 0.0717
Epoch 38/50, Loss: 1.5024, Domain Loss: 1.4556, Class Loss: 0.0468
Epoch 39/50, Loss: 1.6088, Domain Loss: 1.4981, Class Loss: 0.1107
Epoch 40/50, Loss: 1.5454, Domain Loss: 1.4718, Class Loss: 0.0735
Epoch 41/50, Loss: 2.3982, Domain Loss: 2.1653, Class Loss: 0.2329
Epoch 42/50, Loss: 2.6733, Domain Loss: 2.2931, Class Loss: 0.3802
Epoch 43/50, Loss: 1.7050, Domain Loss: 1.5592, Class Loss: 0.1458
Epoch 44/50, Loss: 1.5964, Domain Loss: 1.5267, Class Loss: 0.0697
Epoch 45/50, Loss: 1.4945, Domain Loss: 1.4277, Class Loss: 0.0668
Epoch 46/50, Loss: 1.4569, Domain Loss: 1.4061, Class Loss: 0.0508
Epoch 47/50, Loss: 1.4391, Domain Loss: 1.3998, Class Loss: 0.0393
Epoch 48/50, Loss: 1.4342, Domain Loss: 1.3883, Class Loss: 0.0459
Epoch 49/50, Loss: 1.4114, Domain Loss: 1.3785, Class Loss: 0.0329
Epoch 50/50, Loss: 1.4337, Domain Loss: 1.3927, Class Loss: 0.0410
98.98


Epoch 1/50, Loss: 2.3874, Domain Loss: 1.4021, Class Loss: 0.9853
Epoch 2/50, Loss: 1.8555, Domain Loss: 1.3851, Class Loss: 0.4704
Epoch 3/50, Loss: 1.6060, Domain Loss: 1.3853, Class Loss: 0.2208
Epoch 4/50, Loss: 1.5319, Domain Loss: 1.3921, Class Loss: 0.1398
Epoch 5/50, Loss: 1.4650, Domain Loss: 1.3883, Class Loss: 0.0767
Epoch 6/50, Loss: 1.4301, Domain Loss: 1.3849, Class Loss: 0.0452
Epoch 7/50, Loss: 1.4464, Domain Loss: 1.3860, Class Loss: 0.0604
Epoch 8/50, Loss: 1.4385, Domain Loss: 1.3928, Class Loss: 0.0458
Epoch 9/50, Loss: 2.2226, Domain Loss: 2.1160, Class Loss: 0.1066
Epoch 10/50, Loss: 2.7190, Domain Loss: 2.5254, Class Loss: 0.1936
Epoch 11/50, Loss: 2.8124, Domain Loss: 2.6020, Class Loss: 0.2104
Epoch 12/50, Loss: 15.6561, Domain Loss: 14.7779, Class Loss: 0.8782
Epoch 13/50, Loss: 10.2299, Domain Loss: 9.8636, Class Loss: 0.3663
Epoch 14/50, Loss: 5.4318, Domain Loss: 5.1351, Class Loss: 0.2967
Epoch 15/50, Loss: 2.5600, Domain Loss: 2.3732, Class Loss: 0.1868
Epoch 16/50, Loss: 7.8147, Domain Loss: 5.4437, Class Loss: 2.3710
Epoch 17/50, Loss: 5.6666, Domain Loss: 4.9177, Class Loss: 0.7488
Epoch 18/50, Loss: 5.3388, Domain Loss: 4.3085, Class Loss: 1.0303
Epoch 19/50, Loss: 3.0454, Domain Loss: 2.4569, Class Loss: 0.5885
Epoch 20/50, Loss: 2.2639, Domain Loss: 1.8187, Class Loss: 0.4452
Epoch 21/50, Loss: 2.2950, Domain Loss: 1.8705, Class Loss: 0.4246
Epoch 22/50, Loss: 6.1318, Domain Loss: 5.3513, Class Loss: 0.7804
Epoch 23/50, Loss: 6.4199, Domain Loss: 5.7688, Class Loss: 0.6512
Epoch 24/50, Loss: 4.8186, Domain Loss: 4.0012, Class Loss: 0.8173
Epoch 25/50, Loss: 2.7244, Domain Loss: 2.0569, Class Loss: 0.6675
Epoch 26/50, Loss: 1.8676, Domain Loss: 1.4790, Class Loss: 0.3886
Epoch 27/50, Loss: 1.9040, Domain Loss: 1.4811, Class Loss: 0.4230
Epoch 28/50, Loss: 1.7115, Domain Loss: 1.3958, Class Loss: 0.3157
Epoch 29/50, Loss: 1.6530, Domain Loss: 1.4026, Class Loss: 0.2504
Epoch 30/50, Loss: 1.6233, Domain Loss: 1.4090, Class Loss: 0.2144
Epoch 31/50, Loss: 1.5919, Domain Loss: 1.4435, Class Loss: 0.1483
Epoch 32/50, Loss: 1.6051, Domain Loss: 1.4266, Class Loss: 0.1785
Epoch 33/50, Loss: 1.5510, Domain Loss: 1.4444, Class Loss: 0.1066
Epoch 34/50, Loss: 1.5501, Domain Loss: 1.4460, Class Loss: 0.1040
Epoch 35/50, Loss: 1.4809, Domain Loss: 1.4128, Class Loss: 0.0681
Epoch 36/50, Loss: 1.4501, Domain Loss: 1.3969, Class Loss: 0.0532
Epoch 37/50, Loss: 1.4690, Domain Loss: 1.4143, Class Loss: 0.0547
Epoch 38/50, Loss: 1.4716, Domain Loss: 1.4150, Class Loss: 0.0566
Epoch 39/50, Loss: 1.4288, Domain Loss: 1.3820, Class Loss: 0.0468
Epoch 40/50, Loss: 1.4331, Domain Loss: 1.3927, Class Loss: 0.0404
Epoch 41/50, Loss: 1.4392, Domain Loss: 1.3936, Class Loss: 0.0456
Epoch 42/50, Loss: 1.4058, Domain Loss: 1.3755, Class Loss: 0.0302
Epoch 43/50, Loss: 1.4273, Domain Loss: 1.3825, Class Loss: 0.0447
Epoch 44/50, Loss: 1.4141, Domain Loss: 1.3777, Class Loss: 0.0364
Epoch 45/50, Loss: 1.4071, Domain Loss: 1.3826, Class Loss: 0.0245
Epoch 46/50, Loss: 1.4040, Domain Loss: 1.3688, Class Loss: 0.0352
Epoch 47/50, Loss: 1.4099, Domain Loss: 1.3809, Class Loss: 0.0290
Epoch 48/50, Loss: 1.4164, Domain Loss: 1.3764, Class Loss: 0.0400
Epoch 49/50, Loss: 1.4018, Domain Loss: 1.3659, Class Loss: 0.0358
Epoch 50/50, Loss: 1.4052, Domain Loss: 1.3794, Class Loss: 0.0258
99.70


Epoch 1/50, Loss: 2.3804, Domain Loss: 1.4014, Class Loss: 0.9790
Epoch 2/50, Loss: 1.6942, Domain Loss: 1.3877, Class Loss: 0.3064
Epoch 3/50, Loss: 1.5789, Domain Loss: 1.3908, Class Loss: 0.1881
Epoch 4/50, Loss: 1.4802, Domain Loss: 1.3873, Class Loss: 0.0929
Epoch 5/50, Loss: 1.4505, Domain Loss: 1.3930, Class Loss: 0.0575
Epoch 6/50, Loss: 1.4716, Domain Loss: 1.4116, Class Loss: 0.0600
Epoch 7/50, Loss: 1.9457, Domain Loss: 1.8204, Class Loss: 0.1254
Epoch 8/50, Loss: 2.5496, Domain Loss: 2.3749, Class Loss: 0.1746
Epoch 9/50, Loss: 1.7230, Domain Loss: 1.6037, Class Loss: 0.1193
Epoch 10/50, Loss: 1.6803, Domain Loss: 1.5693, Class Loss: 0.1110
Epoch 11/50, Loss: 1.6176, Domain Loss: 1.5323, Class Loss: 0.0853
Epoch 12/50, Loss: 2.8012, Domain Loss: 2.6302, Class Loss: 0.1710
Epoch 13/50, Loss: 7.0105, Domain Loss: 6.0856, Class Loss: 0.9250
Epoch 14/50, Loss: 5.8212, Domain Loss: 5.1831, Class Loss: 0.6381
Epoch 15/50, Loss: 6.9425, Domain Loss: 5.2364, Class Loss: 1.7061
Epoch 16/50, Loss: 8.1988, Domain Loss: 7.0719, Class Loss: 1.1269
Epoch 17/50, Loss: 9.0031, Domain Loss: 8.0909, Class Loss: 0.9122
Epoch 18/50, Loss: 3.8707, Domain Loss: 3.2553, Class Loss: 0.6154
Epoch 19/50, Loss: 2.3847, Domain Loss: 2.0193, Class Loss: 0.3654
Epoch 20/50, Loss: 2.3971, Domain Loss: 2.0942, Class Loss: 0.3030
Epoch 21/50, Loss: 2.2814, Domain Loss: 1.9017, Class Loss: 0.3797
Epoch 22/50, Loss: 1.9467, Domain Loss: 1.7036, Class Loss: 0.2432
Epoch 23/50, Loss: 1.8329, Domain Loss: 1.6266, Class Loss: 0.2063
Epoch 24/50, Loss: 1.7390, Domain Loss: 1.5458, Class Loss: 0.1932
Epoch 25/50, Loss: 1.6353, Domain Loss: 1.4572, Class Loss: 0.1782
Epoch 26/50, Loss: 1.5666, Domain Loss: 1.4339, Class Loss: 0.1327
Epoch 27/50, Loss: 1.4973, Domain Loss: 1.3965, Class Loss: 0.1008
Epoch 28/50, Loss: 1.4595, Domain Loss: 1.3946, Class Loss: 0.0650
Epoch 29/50, Loss: 1.4555, Domain Loss: 1.3886, Class Loss: 0.0670
Epoch 30/50, Loss: 1.4628, Domain Loss: 1.3970, Class Loss: 0.0658
Epoch 31/50, Loss: 1.4514, Domain Loss: 1.4014, Class Loss: 0.0500
Epoch 32/50, Loss: 1.4931, Domain Loss: 1.4238, Class Loss: 0.0693
Epoch 33/50, Loss: 1.4980, Domain Loss: 1.4491, Class Loss: 0.0490
Epoch 34/50, Loss: 1.4731, Domain Loss: 1.4349, Class Loss: 0.0382
Epoch 35/50, Loss: 1.4536, Domain Loss: 1.4062, Class Loss: 0.0474
Epoch 36/50, Loss: 1.4527, Domain Loss: 1.4086, Class Loss: 0.0441
Epoch 37/50, Loss: 1.4412, Domain Loss: 1.4013, Class Loss: 0.0399
Epoch 38/50, Loss: 1.4452, Domain Loss: 1.4042, Class Loss: 0.0410
Epoch 39/50, Loss: 1.4548, Domain Loss: 1.4073, Class Loss: 0.0475
Epoch 40/50, Loss: 1.4736, Domain Loss: 1.4216, Class Loss: 0.0520
Epoch 41/50, Loss: 1.4406, Domain Loss: 1.4029, Class Loss: 0.0377
Epoch 42/50, Loss: 1.4656, Domain Loss: 1.4251, Class Loss: 0.0405
Epoch 43/50, Loss: 1.5038, Domain Loss: 1.4500, Class Loss: 0.0538
Epoch 44/50, Loss: 1.4868, Domain Loss: 1.4344, Class Loss: 0.0525
Epoch 45/50, Loss: 1.4390, Domain Loss: 1.4034, Class Loss: 0.0356
Epoch 46/50, Loss: 1.4973, Domain Loss: 1.4459, Class Loss: 0.0514
Epoch 47/50, Loss: 1.5477, Domain Loss: 1.5069, Class Loss: 0.0407
Epoch 48/50, Loss: 1.4374, Domain Loss: 1.4098, Class Loss: 0.0277
Epoch 49/50, Loss: 1.4686, Domain Loss: 1.4356, Class Loss: 0.0330
Epoch 50/50, Loss: 1.4886, Domain Loss: 1.4475, Class Loss: 0.0411
94.18


Epoch 1/50, Loss: 2.3165, Domain Loss: 1.3844, Class Loss: 0.9320
Epoch 2/50, Loss: 1.7465, Domain Loss: 1.4075, Class Loss: 0.3389
Epoch 3/50, Loss: 1.5215, Domain Loss: 1.3829, Class Loss: 0.1386
Epoch 4/50, Loss: 1.4472, Domain Loss: 1.3872, Class Loss: 0.0600
Epoch 5/50, Loss: 2.2265, Domain Loss: 2.1477, Class Loss: 0.0787
Epoch 6/50, Loss: 7.8018, Domain Loss: 7.5253, Class Loss: 0.2764
Epoch 7/50, Loss: 5.6356, Domain Loss: 5.4264, Class Loss: 0.2092
Epoch 8/50, Loss: 7.3439, Domain Loss: 6.9580, Class Loss: 0.3859
Epoch 9/50, Loss: 10.1527, Domain Loss: 9.0857, Class Loss: 1.0670
Epoch 10/50, Loss: 14.8900, Domain Loss: 14.2963, Class Loss: 0.5936
Epoch 11/50, Loss: 8.6363, Domain Loss: 8.2404, Class Loss: 0.3960
Epoch 12/50, Loss: 3.4002, Domain Loss: 3.1607, Class Loss: 0.2395
Epoch 13/50, Loss: 2.1170, Domain Loss: 1.9660, Class Loss: 0.1510
Epoch 14/50, Loss: 1.7854, Domain Loss: 1.6499, Class Loss: 0.1355
Epoch 15/50, Loss: 1.6197, Domain Loss: 1.5158, Class Loss: 0.1039
Epoch 16/50, Loss: 1.5155, Domain Loss: 1.4206, Class Loss: 0.0949
Epoch 17/50, Loss: 1.5019, Domain Loss: 1.4184, Class Loss: 0.0835
Epoch 18/50, Loss: 1.4720, Domain Loss: 1.4056, Class Loss: 0.0663
Epoch 19/50, Loss: 1.4870, Domain Loss: 1.4302, Class Loss: 0.0568
Epoch 20/50, Loss: 1.4830, Domain Loss: 1.4275, Class Loss: 0.0555
Epoch 21/50, Loss: 1.4580, Domain Loss: 1.4278, Class Loss: 0.0301
Epoch 22/50, Loss: 1.4379, Domain Loss: 1.4096, Class Loss: 0.0283
Epoch 23/50, Loss: 1.4424, Domain Loss: 1.4016, Class Loss: 0.0408
Epoch 24/50, Loss: 1.4418, Domain Loss: 1.4170, Class Loss: 0.0248
Epoch 25/50, Loss: 1.4705, Domain Loss: 1.4319, Class Loss: 0.0386
Epoch 26/50, Loss: 1.5874, Domain Loss: 1.4504, Class Loss: 0.1370
Epoch 27/50, Loss: 1.5431, Domain Loss: 1.4536, Class Loss: 0.0895
Epoch 28/50, Loss: 1.5085, Domain Loss: 1.4292, Class Loss: 0.0793
Epoch 29/50, Loss: 1.4641, Domain Loss: 1.4111, Class Loss: 0.0530
Epoch 30/50, Loss: 1.4998, Domain Loss: 1.4292, Class Loss: 0.0707
Epoch 31/50, Loss: 1.4638, Domain Loss: 1.4084, Class Loss: 0.0554
Epoch 32/50, Loss: 1.4780, Domain Loss: 1.4347, Class Loss: 0.0433
Epoch 33/50, Loss: 1.4701, Domain Loss: 1.4364, Class Loss: 0.0337
Epoch 34/50, Loss: 1.4816, Domain Loss: 1.4457, Class Loss: 0.0358
Epoch 35/50, Loss: 1.4991, Domain Loss: 1.4705, Class Loss: 0.0286
Epoch 36/50, Loss: 1.5108, Domain Loss: 1.4655, Class Loss: 0.0453
Epoch 37/50, Loss: 1.5025, Domain Loss: 1.4742, Class Loss: 0.0283
Epoch 38/50, Loss: 1.5032, Domain Loss: 1.4754, Class Loss: 0.0277
Epoch 39/50, Loss: 1.5319, Domain Loss: 1.4875, Class Loss: 0.0445
Epoch 40/50, Loss: 1.6672, Domain Loss: 1.5961, Class Loss: 0.0711
Epoch 41/50, Loss: 1.7892, Domain Loss: 1.6981, Class Loss: 0.0911
Epoch 42/50, Loss: 1.8269, Domain Loss: 1.7442, Class Loss: 0.0828
Epoch 43/50, Loss: 1.5105, Domain Loss: 1.4486, Class Loss: 0.0619
Epoch 44/50, Loss: 1.4759, Domain Loss: 1.4287, Class Loss: 0.0472
Epoch 45/50, Loss: 1.4369, Domain Loss: 1.4140, Class Loss: 0.0229
Epoch 46/50, Loss: 1.4309, Domain Loss: 1.4012, Class Loss: 0.0297
Epoch 47/50, Loss: 1.4234, Domain Loss: 1.3961, Class Loss: 0.0274
Epoch 48/50, Loss: 1.4512, Domain Loss: 1.4042, Class Loss: 0.0470
Epoch 49/50, Loss: 1.4803, Domain Loss: 1.4154, Class Loss: 0.0649
Epoch 50/50, Loss: 1.4441, Domain Loss: 1.4023, Class Loss: 0.0418
99.34


Source performance:
96.21 97.01 96.20 95.84 
Target performance:
96.41 97.21 96.43 96.08 

Per-class target performance: 100.00 99.18 87.86 98.68 Deep CORAL
Deep CORAL Run 1/10
Epoch 1: Source Val Acc = 0.7182, Target Val Acc = 0.7332
Epoch 2: Source Val Acc = 0.8927, Target Val Acc = 0.8849
Epoch 3: Source Val Acc = 0.5372, Target Val Acc = 0.5348
Epoch 4: Source Val Acc = 0.8195, Target Val Acc = 0.8243
Epoch 5: Source Val Acc = 0.9550, Target Val Acc = 0.9532
Epoch 6: Source Val Acc = 0.9514, Target Val Acc = 0.9622
Epoch 7: Source Val Acc = 0.9988, Target Val Acc = 0.9970
Epoch 8: Source Val Acc = 0.9922, Target Val Acc = 0.9922
Epoch 9: Source Val Acc = 0.9760, Target Val Acc = 0.9874
Epoch 10: Source Val Acc = 0.9664, Target Val Acc = 0.9700
Epoch 11: Source Val Acc = 0.9916, Target Val Acc = 0.9922
Epoch 12: Source Val Acc = 0.9742, Target Val Acc = 0.9808
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.9742, Target Val Acc = 0.9808

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.5839, Target Val Acc = 0.5659
Epoch 2: Source Val Acc = 0.8225, Target Val Acc = 0.8297
Epoch 3: Source Val Acc = 0.6619, Target Val Acc = 0.6769
Epoch 4: Source Val Acc = 0.9982, Target Val Acc = 0.9976
Epoch 5: Source Val Acc = 0.9964, Target Val Acc = 0.9970
Epoch 6: Source Val Acc = 0.8783, Target Val Acc = 0.8657
Epoch 7: Source Val Acc = 0.7440, Target Val Acc = 0.7410
Epoch 8: Source Val Acc = 0.9898, Target Val Acc = 0.9928
Epoch 9: Source Val Acc = 0.9850, Target Val Acc = 0.9850
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9850, Target Val Acc = 0.9850

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.9281, Target Val Acc = 0.9305
Epoch 2: Source Val Acc = 0.9988, Target Val Acc = 0.9970
Epoch 3: Source Val Acc = 0.8441, Target Val Acc = 0.8585
Epoch 4: Source Val Acc = 0.6954, Target Val Acc = 0.7014
Epoch 5: Source Val Acc = 0.9910, Target Val Acc = 0.9898
Epoch 6: Source Val Acc = 0.9478, Target Val Acc = 0.9634
Epoch 7: Source Val Acc = 0.8094, Target Val Acc = 0.8231
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.8094, Target Val Acc = 0.8231

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.7512, Target Val Acc = 0.7464
Epoch 2: Source Val Acc = 0.8393, Target Val Acc = 0.8585
Epoch 3: Source Val Acc = 0.9784, Target Val Acc = 0.9724
Epoch 4: Source Val Acc = 0.9251, Target Val Acc = 0.9203
Epoch 5: Source Val Acc = 0.9670, Target Val Acc = 0.9712
Epoch 6: Source Val Acc = 0.9766, Target Val Acc = 0.9820
Epoch 7: Source Val Acc = 0.9988, Target Val Acc = 0.9994
Epoch 8: Source Val Acc = 0.9988, Target Val Acc = 0.9982
Epoch 9: Source Val Acc = 0.9838, Target Val Acc = 0.9838
Epoch 10: Source Val Acc = 0.8807, Target Val Acc = 0.8915
Epoch 11: Source Val Acc = 0.9862, Target Val Acc = 0.9850
Epoch 12: Source Val Acc = 0.9928, Target Val Acc = 0.9976
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.9928, Target Val Acc = 0.9976

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.7962, Target Val Acc = 0.7980
Epoch 2: Source Val Acc = 0.7356, Target Val Acc = 0.7428
Epoch 3: Source Val Acc = 0.9952, Target Val Acc = 0.9958
Epoch 4: Source Val Acc = 0.7548, Target Val Acc = 0.7686
Epoch 5: Source Val Acc = 0.9826, Target Val Acc = 0.9814
Epoch 6: Source Val Acc = 0.5480, Target Val Acc = 0.5486
Epoch 7: Source Val Acc = 0.9838, Target Val Acc = 0.9898
Epoch 8: Source Val Acc = 0.7356, Target Val Acc = 0.7470
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.7356, Target Val Acc = 0.7470

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.9676, Target Val Acc = 0.9694
Epoch 2: Source Val Acc = 0.5635, Target Val Acc = 0.5624
Epoch 3: Source Val Acc = 0.7452, Target Val Acc = 0.7650
Epoch 4: Source Val Acc = 0.9802, Target Val Acc = 0.9802
Epoch 5: Source Val Acc = 0.9952, Target Val Acc = 0.9982
Epoch 6: Source Val Acc = 0.8447, Target Val Acc = 0.8465
Epoch 7: Source Val Acc = 0.9886, Target Val Acc = 0.9928
Epoch 8: Source Val Acc = 0.9988, Target Val Acc = 1.0000
Epoch 9: Source Val Acc = 0.9952, Target Val Acc = 0.9940
Epoch 10: Source Val Acc = 0.9910, Target Val Acc = 0.9916
Epoch 11: Source Val Acc = 0.9868, Target Val Acc = 0.9856
Epoch 12: Source Val Acc = 0.7350, Target Val Acc = 0.7482
Epoch 13: Source Val Acc = 0.9868, Target Val Acc = 0.9862
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 0.9868, Target Val Acc = 0.9862

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.7506, Target Val Acc = 0.7482
Epoch 2: Source Val Acc = 0.5965, Target Val Acc = 0.5995
Epoch 3: Source Val Acc = 0.5378, Target Val Acc = 0.5330
Epoch 4: Source Val Acc = 0.9784, Target Val Acc = 0.9760
Epoch 5: Source Val Acc = 0.9958, Target Val Acc = 0.9964
Epoch 6: Source Val Acc = 1.0000, Target Val Acc = 0.9994
Epoch 7: Source Val Acc = 0.9970, Target Val Acc = 0.9982
Epoch 8: Source Val Acc = 0.7356, Target Val Acc = 0.7446
Epoch 9: Source Val Acc = 0.7188, Target Val Acc = 0.7338
Epoch 10: Source Val Acc = 0.8273, Target Val Acc = 0.8441
Epoch 11: Source Val Acc = 0.9922, Target Val Acc = 0.9952
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.9922, Target Val Acc = 0.9952

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.6595, Target Val Acc = 0.6691
Epoch 2: Source Val Acc = 0.8663, Target Val Acc = 0.8747
Epoch 3: Source Val Acc = 0.9964, Target Val Acc = 0.9970
Epoch 4: Source Val Acc = 0.8309, Target Val Acc = 0.8345
Epoch 5: Source Val Acc = 0.9832, Target Val Acc = 0.9856
Epoch 6: Source Val Acc = 0.9574, Target Val Acc = 0.9604
Epoch 7: Source Val Acc = 0.9928, Target Val Acc = 0.9976
Epoch 8: Source Val Acc = 0.6301, Target Val Acc = 0.6439
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.6301, Target Val Acc = 0.6439

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.5707, Target Val Acc = 0.5857
Epoch 2: Source Val Acc = 0.9658, Target Val Acc = 0.9640
Epoch 3: Source Val Acc = 0.9874, Target Val Acc = 0.9832
Epoch 4: Source Val Acc = 0.9664, Target Val Acc = 0.9760
Epoch 5: Source Val Acc = 0.9191, Target Val Acc = 0.9275
Epoch 6: Source Val Acc = 0.8279, Target Val Acc = 0.8417
Epoch 7: Source Val Acc = 0.9868, Target Val Acc = 0.9910
Epoch 8: Source Val Acc = 0.9436, Target Val Acc = 0.9484
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 0.9436, Target Val Acc = 0.9484

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.6841, Target Val Acc = 0.6972
Epoch 2: Source Val Acc = 0.7362, Target Val Acc = 0.7464
Epoch 3: Source Val Acc = 0.7476, Target Val Acc = 0.7410
Epoch 4: Source Val Acc = 0.8351, Target Val Acc = 0.8405
Epoch 5: Source Val Acc = 0.9059, Target Val Acc = 0.9077
Epoch 6: Source Val Acc = 0.9904, Target Val Acc = 0.9868
Epoch 7: Source Val Acc = 0.9928, Target Val Acc = 0.9904
Epoch 8: Source Val Acc = 0.9970, Target Val Acc = 0.9988
Epoch 9: Source Val Acc = 0.6769, Target Val Acc = 0.6709
Epoch 10: Source Val Acc = 0.9394, Target Val Acc = 0.9436
Epoch 11: Source Val Acc = 0.7272, Target Val Acc = 0.7404
Epoch 12: Source Val Acc = 0.9958, Target Val Acc = 0.9976
Epoch 13: Source Val Acc = 0.9964, Target Val Acc = 0.9988
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.9964, Target Val Acc = 0.9988

Deep CORAL: Average Source Val Acc = 0.9046, Average Target Val Acc = 0.9106
STAR

Run 1/10
Epoch [1/50], Class Loss: 1.8888, Discrepancy Loss: 0.1397
Epoch [2/50], Class Loss: 0.4296, Discrepancy Loss: 0.0412
Epoch [3/50], Class Loss: 0.2240, Discrepancy Loss: 0.0175
Epoch [4/50], Class Loss: 0.1451, Discrepancy Loss: 0.0217
Epoch [5/50], Class Loss: 0.0565, Discrepancy Loss: 0.0092
Epoch [6/50], Class Loss: 0.2893, Discrepancy Loss: 0.0198
Epoch [7/50], Class Loss: 0.0621, Discrepancy Loss: 0.0144
Epoch [8/50], Class Loss: 0.0666, Discrepancy Loss: 0.0041
Epoch [9/50], Class Loss: 0.0535, Discrepancy Loss: 0.0064
Epoch [10/50], Class Loss: 0.0388, Discrepancy Loss: 0.0046
Epoch [11/50], Class Loss: 0.0111, Discrepancy Loss: 0.0045
Epoch [12/50], Class Loss: 0.0146, Discrepancy Loss: 0.0024
Epoch [13/50], Class Loss: 0.0115, Discrepancy Loss: 0.0028
Epoch [14/50], Class Loss: 0.0073, Discrepancy Loss: 0.0024
Epoch [15/50], Class Loss: 0.0074, Discrepancy Loss: 0.0030
Epoch [16/50], Class Loss: 0.0929, Discrepancy Loss: 0.0021
Epoch [17/50], Class Loss: 0.0486, Discrepancy Loss: 0.0084
Epoch [18/50], Class Loss: 0.0128, Discrepancy Loss: 0.0029
Epoch [19/50], Class Loss: 0.0082, Discrepancy Loss: 0.0028
Epoch [20/50], Class Loss: 0.0113, Discrepancy Loss: 0.0030
Epoch [21/50], Class Loss: 0.0632, Discrepancy Loss: 0.0030
Epoch [22/50], Class Loss: 0.0132, Discrepancy Loss: 0.0013
Epoch [23/50], Class Loss: 0.0066, Discrepancy Loss: 0.0030
Epoch [24/50], Class Loss: 0.0072, Discrepancy Loss: 0.0020
Epoch [25/50], Class Loss: 0.1073, Discrepancy Loss: 0.0012
Epoch [26/50], Class Loss: 0.0076, Discrepancy Loss: 0.0015
Epoch [27/50], Class Loss: 0.0301, Discrepancy Loss: 0.0011
Epoch [28/50], Class Loss: 0.0075, Discrepancy Loss: 0.0017
Epoch [29/50], Class Loss: 0.0068, Discrepancy Loss: 0.0017
Epoch [30/50], Class Loss: 0.0699, Discrepancy Loss: 0.0024
Epoch [31/50], Class Loss: 0.0061, Discrepancy Loss: 0.0026
Epoch [32/50], Class Loss: 0.0067, Discrepancy Loss: 0.0014
Epoch [33/50], Class Loss: 0.0090, Discrepancy Loss: 0.0027
Epoch [34/50], Class Loss: 0.0336, Discrepancy Loss: 0.0019
Epoch [35/50], Class Loss: 0.0122, Discrepancy Loss: 0.0044
Epoch [36/50], Class Loss: 0.0061, Discrepancy Loss: 0.0030
Epoch [37/50], Class Loss: 0.1413, Discrepancy Loss: 0.0020
Epoch [38/50], Class Loss: 0.0098, Discrepancy Loss: 0.0015
Epoch [39/50], Class Loss: 0.0152, Discrepancy Loss: 0.0022
Epoch [40/50], Class Loss: 0.0052, Discrepancy Loss: 0.0017
Epoch [41/50], Class Loss: 0.0058, Discrepancy Loss: 0.0019
Epoch [42/50], Class Loss: 0.0303, Discrepancy Loss: 0.0019
Epoch [43/50], Class Loss: 0.1619, Discrepancy Loss: 0.0020
Epoch [44/50], Class Loss: 0.0058, Discrepancy Loss: 0.0020
Epoch [45/50], Class Loss: 0.0104, Discrepancy Loss: 0.0016
Epoch [46/50], Class Loss: 0.1215, Discrepancy Loss: 0.0021
Epoch [47/50], Class Loss: 0.0103, Discrepancy Loss: 0.0020
Epoch [48/50], Class Loss: 0.0105, Discrepancy Loss: 0.0013
Epoch [49/50], Class Loss: 0.0097, Discrepancy Loss: 0.0026
Epoch [50/50], Class Loss: 0.0086, Discrepancy Loss: 0.0020
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 2/10
Epoch [1/50], Class Loss: 1.8750, Discrepancy Loss: 0.1243
Epoch [2/50], Class Loss: 0.5149, Discrepancy Loss: 0.0580
Epoch [3/50], Class Loss: 0.2093, Discrepancy Loss: 0.0221
Epoch [4/50], Class Loss: 0.0683, Discrepancy Loss: 0.0093
Epoch [5/50], Class Loss: 0.1401, Discrepancy Loss: 0.0143
Epoch [6/50], Class Loss: 0.0280, Discrepancy Loss: 0.0079
Epoch [7/50], Class Loss: 0.0491, Discrepancy Loss: 0.0075
Epoch [8/50], Class Loss: 0.0222, Discrepancy Loss: 0.0037
Epoch [9/50], Class Loss: 0.0606, Discrepancy Loss: 0.0060
Epoch [10/50], Class Loss: 0.0240, Discrepancy Loss: 0.0056
Epoch [11/50], Class Loss: 0.0177, Discrepancy Loss: 0.0023
Epoch [12/50], Class Loss: 0.0149, Discrepancy Loss: 0.0017
Epoch [13/50], Class Loss: 0.1059, Discrepancy Loss: 0.0027
Epoch [14/50], Class Loss: 0.0347, Discrepancy Loss: 0.0049
Epoch [15/50], Class Loss: 0.0262, Discrepancy Loss: 0.0025
Epoch [16/50], Class Loss: 0.0156, Discrepancy Loss: 0.0031
Epoch [17/50], Class Loss: 0.1128, Discrepancy Loss: 0.0031
Epoch [18/50], Class Loss: 0.0150, Discrepancy Loss: 0.0036
Epoch [19/50], Class Loss: 0.0096, Discrepancy Loss: 0.0028
Epoch [20/50], Class Loss: 0.0071, Discrepancy Loss: 0.0042
Epoch [21/50], Class Loss: 0.0049, Discrepancy Loss: 0.0029
Epoch [22/50], Class Loss: 0.0274, Discrepancy Loss: 0.0022
Epoch [23/50], Class Loss: 0.0128, Discrepancy Loss: 0.0051
Epoch [24/50], Class Loss: 0.0125, Discrepancy Loss: 0.0019
Epoch [25/50], Class Loss: 0.0071, Discrepancy Loss: 0.0058
Epoch [26/50], Class Loss: 0.0551, Discrepancy Loss: 0.0021
Epoch [27/50], Class Loss: 0.0140, Discrepancy Loss: 0.0026
Epoch [28/50], Class Loss: 0.0123, Discrepancy Loss: 0.0027
Epoch [29/50], Class Loss: 0.0094, Discrepancy Loss: 0.0017
Epoch [30/50], Class Loss: 0.0069, Discrepancy Loss: 0.0018
Epoch [31/50], Class Loss: 0.0094, Discrepancy Loss: 0.0014
Epoch [32/50], Class Loss: 0.0045, Discrepancy Loss: 0.0025
Epoch [33/50], Class Loss: 0.0035, Discrepancy Loss: 0.0016
Epoch [34/50], Class Loss: 0.0078, Discrepancy Loss: 0.0032
Epoch [35/50], Class Loss: 0.0042, Discrepancy Loss: 0.0037
Epoch [36/50], Class Loss: 0.0040, Discrepancy Loss: 0.0035
Epoch [37/50], Class Loss: 0.0050, Discrepancy Loss: 0.0015
Epoch [38/50], Class Loss: 0.0080, Discrepancy Loss: 0.0018
Epoch [39/50], Class Loss: 0.0191, Discrepancy Loss: 0.0014
Epoch [40/50], Class Loss: 0.0054, Discrepancy Loss: 0.0023
Epoch [41/50], Class Loss: 0.0060, Discrepancy Loss: 0.0016
Epoch [42/50], Class Loss: 0.0062, Discrepancy Loss: 0.0045
Epoch [43/50], Class Loss: 0.0033, Discrepancy Loss: 0.0013
Epoch [44/50], Class Loss: 0.0086, Discrepancy Loss: 0.0016
Epoch [45/50], Class Loss: 0.0058, Discrepancy Loss: 0.0020
Epoch [46/50], Class Loss: 0.0077, Discrepancy Loss: 0.0025
Epoch [47/50], Class Loss: 0.0049, Discrepancy Loss: 0.0013
Epoch [48/50], Class Loss: 0.0188, Discrepancy Loss: 0.0013
Epoch [49/50], Class Loss: 0.0043, Discrepancy Loss: 0.0019
Epoch [50/50], Class Loss: 0.0089, Discrepancy Loss: 0.0013
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 3/10
Epoch [1/50], Class Loss: 2.2884, Discrepancy Loss: 0.1132
Epoch [2/50], Class Loss: 0.5914, Discrepancy Loss: 0.0408
Epoch [3/50], Class Loss: 0.2661, Discrepancy Loss: 0.0205
Epoch [4/50], Class Loss: 0.0478, Discrepancy Loss: 0.0049
Epoch [5/50], Class Loss: 0.0663, Discrepancy Loss: 0.0081
Epoch [6/50], Class Loss: 0.1701, Discrepancy Loss: 0.0093
Epoch [7/50], Class Loss: 0.1385, Discrepancy Loss: 0.0122
Epoch [8/50], Class Loss: 0.0627, Discrepancy Loss: 0.0101
Epoch [9/50], Class Loss: 0.0323, Discrepancy Loss: 0.0031
Epoch [10/50], Class Loss: 0.1184, Discrepancy Loss: 0.0047
Epoch [11/50], Class Loss: 0.1430, Discrepancy Loss: 0.0191
Epoch [12/50], Class Loss: 0.0273, Discrepancy Loss: 0.0068
Epoch [13/50], Class Loss: 0.0211, Discrepancy Loss: 0.0030
Epoch [14/50], Class Loss: 0.0605, Discrepancy Loss: 0.0036
Epoch [15/50], Class Loss: 0.0198, Discrepancy Loss: 0.0057
Epoch [16/50], Class Loss: 0.0137, Discrepancy Loss: 0.0044
Epoch [17/50], Class Loss: 0.1031, Discrepancy Loss: 0.0022
Epoch [18/50], Class Loss: 0.0359, Discrepancy Loss: 0.0065
Epoch [19/50], Class Loss: 0.1066, Discrepancy Loss: 0.0040
Epoch [20/50], Class Loss: 0.0583, Discrepancy Loss: 0.0051
Epoch [21/50], Class Loss: 0.0155, Discrepancy Loss: 0.0028
Epoch [22/50], Class Loss: 0.0190, Discrepancy Loss: 0.0028
Epoch [23/50], Class Loss: 0.0467, Discrepancy Loss: 0.0019
Epoch [24/50], Class Loss: 0.0135, Discrepancy Loss: 0.0028
Epoch [25/50], Class Loss: 0.0275, Discrepancy Loss: 0.0023
Epoch [26/50], Class Loss: 0.0156, Discrepancy Loss: 0.0036
Epoch [27/50], Class Loss: 0.0136, Discrepancy Loss: 0.0021
Epoch [28/50], Class Loss: 0.0117, Discrepancy Loss: 0.0023
Epoch [29/50], Class Loss: 0.0990, Discrepancy Loss: 0.0034
Epoch [30/50], Class Loss: 0.0117, Discrepancy Loss: 0.0024
Epoch [31/50], Class Loss: 0.0266, Discrepancy Loss: 0.0037
Epoch [32/50], Class Loss: 0.0097, Discrepancy Loss: 0.0026
Epoch [33/50], Class Loss: 0.0095, Discrepancy Loss: 0.0042
Epoch [34/50], Class Loss: 0.0917, Discrepancy Loss: 0.0031
Epoch [35/50], Class Loss: 0.0086, Discrepancy Loss: 0.0038
Epoch [36/50], Class Loss: 0.0108, Discrepancy Loss: 0.0035
Epoch [37/50], Class Loss: 0.0139, Discrepancy Loss: 0.0025
Epoch [38/50], Class Loss: 0.0180, Discrepancy Loss: 0.0018
Epoch [39/50], Class Loss: 0.0097, Discrepancy Loss: 0.0023
Epoch [40/50], Class Loss: 0.0089, Discrepancy Loss: 0.0023
Epoch [41/50], Class Loss: 0.0079, Discrepancy Loss: 0.0022
Epoch [42/50], Class Loss: 0.0243, Discrepancy Loss: 0.0026
Epoch [43/50], Class Loss: 0.0092, Discrepancy Loss: 0.0035
Epoch [44/50], Class Loss: 0.0134, Discrepancy Loss: 0.0032
Epoch [45/50], Class Loss: 0.0112, Discrepancy Loss: 0.0030
Epoch [46/50], Class Loss: 0.0106, Discrepancy Loss: 0.0023
Epoch [47/50], Class Loss: 0.0103, Discrepancy Loss: 0.0043
Epoch [48/50], Class Loss: 0.0102, Discrepancy Loss: 0.0023
Epoch [49/50], Class Loss: 0.0124, Discrepancy Loss: 0.0036
Epoch [50/50], Class Loss: 0.0089, Discrepancy Loss: 0.0029
Source Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 4/10
Epoch [1/50], Class Loss: 1.7197, Discrepancy Loss: 0.1154
Epoch [2/50], Class Loss: 0.5109, Discrepancy Loss: 0.0448
Epoch [3/50], Class Loss: 0.1043, Discrepancy Loss: 0.0165
Epoch [4/50], Class Loss: 0.0983, Discrepancy Loss: 0.0086
Epoch [5/50], Class Loss: 0.0423, Discrepancy Loss: 0.0049
Epoch [6/50], Class Loss: 0.0813, Discrepancy Loss: 0.0070
Epoch [7/50], Class Loss: 0.3197, Discrepancy Loss: 0.0144
Epoch [8/50], Class Loss: 0.1312, Discrepancy Loss: 0.0110
Epoch [9/50], Class Loss: 0.0840, Discrepancy Loss: 0.0194
Epoch [10/50], Class Loss: 0.0516, Discrepancy Loss: 0.0103
Epoch [11/50], Class Loss: 0.0554, Discrepancy Loss: 0.0087
Epoch [12/50], Class Loss: 0.0166, Discrepancy Loss: 0.0051
Epoch [13/50], Class Loss: 0.0190, Discrepancy Loss: 0.0063
Epoch [14/50], Class Loss: 0.0134, Discrepancy Loss: 0.0040
Epoch [15/50], Class Loss: 0.1884, Discrepancy Loss: 0.0026
Epoch [16/50], Class Loss: 0.0347, Discrepancy Loss: 0.0071
Epoch [17/50], Class Loss: 0.0188, Discrepancy Loss: 0.0032
Epoch [18/50], Class Loss: 0.0191, Discrepancy Loss: 0.0024
Epoch [19/50], Class Loss: 0.0130, Discrepancy Loss: 0.0016
Epoch [20/50], Class Loss: 0.0143, Discrepancy Loss: 0.0025
Epoch [21/50], Class Loss: 0.0095, Discrepancy Loss: 0.0011
Epoch [22/50], Class Loss: 0.0067, Discrepancy Loss: 0.0023
Epoch [23/50], Class Loss: 0.0166, Discrepancy Loss: 0.0024
Epoch [24/50], Class Loss: 0.0601, Discrepancy Loss: 0.0021
Epoch [25/50], Class Loss: 0.0348, Discrepancy Loss: 0.0020
Epoch [26/50], Class Loss: 0.0059, Discrepancy Loss: 0.0032
Epoch [27/50], Class Loss: 0.0059, Discrepancy Loss: 0.0023
Epoch [28/50], Class Loss: 0.0213, Discrepancy Loss: 0.0021
Epoch [29/50], Class Loss: 0.0100, Discrepancy Loss: 0.0014
Epoch [30/50], Class Loss: 0.0072, Discrepancy Loss: 0.0011
Epoch [31/50], Class Loss: 0.0074, Discrepancy Loss: 0.0015
Epoch [32/50], Class Loss: 0.0064, Discrepancy Loss: 0.0015
Epoch [33/50], Class Loss: 0.0166, Discrepancy Loss: 0.0018
Epoch [34/50], Class Loss: 0.0322, Discrepancy Loss: 0.0013
Epoch [35/50], Class Loss: 0.0084, Discrepancy Loss: 0.0009
Epoch [36/50], Class Loss: 0.0054, Discrepancy Loss: 0.0011
Epoch [37/50], Class Loss: 0.0055, Discrepancy Loss: 0.0013
Epoch [38/50], Class Loss: 0.0084, Discrepancy Loss: 0.0023
Epoch [39/50], Class Loss: 0.0149, Discrepancy Loss: 0.0019
Epoch [40/50], Class Loss: 0.0097, Discrepancy Loss: 0.0011
Epoch [41/50], Class Loss: 0.0060, Discrepancy Loss: 0.0023
Epoch [42/50], Class Loss: 0.0083, Discrepancy Loss: 0.0016
Epoch [43/50], Class Loss: 0.0095, Discrepancy Loss: 0.0012
Epoch [44/50], Class Loss: 0.0055, Discrepancy Loss: 0.0026
Epoch [45/50], Class Loss: 0.0816, Discrepancy Loss: 0.0013
Epoch [46/50], Class Loss: 0.0041, Discrepancy Loss: 0.0014
Epoch [47/50], Class Loss: 0.0683, Discrepancy Loss: 0.0019
Epoch [48/50], Class Loss: 0.0037, Discrepancy Loss: 0.0019
Epoch [49/50], Class Loss: 0.0118, Discrepancy Loss: 0.0012
Epoch [50/50], Class Loss: 0.0328, Discrepancy Loss: 0.0022
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 5/10
Epoch [1/50], Class Loss: 1.8664, Discrepancy Loss: 0.1265
Epoch [2/50], Class Loss: 0.5230, Discrepancy Loss: 0.0364
Epoch [3/50], Class Loss: 0.3425, Discrepancy Loss: 0.0263
Epoch [4/50], Class Loss: 0.1429, Discrepancy Loss: 0.0225
Epoch [5/50], Class Loss: 0.1953, Discrepancy Loss: 0.0144
Epoch [6/50], Class Loss: 0.0522, Discrepancy Loss: 0.0068
Epoch [7/50], Class Loss: 0.0475, Discrepancy Loss: 0.0056
Epoch [8/50], Class Loss: 0.0322, Discrepancy Loss: 0.0037
Epoch [9/50], Class Loss: 0.0872, Discrepancy Loss: 0.0099
Epoch [10/50], Class Loss: 0.0485, Discrepancy Loss: 0.0055
Epoch [11/50], Class Loss: 0.0143, Discrepancy Loss: 0.0020
Epoch [12/50], Class Loss: 0.0881, Discrepancy Loss: 0.0029
Epoch [13/50], Class Loss: 0.0287, Discrepancy Loss: 0.0023
Epoch [14/50], Class Loss: 0.0162, Discrepancy Loss: 0.0019
Epoch [15/50], Class Loss: 0.0081, Discrepancy Loss: 0.0016
Epoch [16/50], Class Loss: 0.0090, Discrepancy Loss: 0.0014
Epoch [17/50], Class Loss: 0.0039, Discrepancy Loss: 0.0015
Epoch [18/50], Class Loss: 0.0039, Discrepancy Loss: 0.0010
Epoch [19/50], Class Loss: 0.0136, Discrepancy Loss: 0.0019
Epoch [20/50], Class Loss: 0.0044, Discrepancy Loss: 0.0007
Epoch [21/50], Class Loss: 0.0042, Discrepancy Loss: 0.0009
Epoch [22/50], Class Loss: 0.0018, Discrepancy Loss: 0.0007
Epoch [23/50], Class Loss: 0.0138, Discrepancy Loss: 0.0015
Epoch [24/50], Class Loss: 0.0027, Discrepancy Loss: 0.0006
Epoch [25/50], Class Loss: 0.0053, Discrepancy Loss: 0.0012
Epoch [26/50], Class Loss: 0.0051, Discrepancy Loss: 0.0009
Epoch [27/50], Class Loss: 0.0013, Discrepancy Loss: 0.0009
Epoch [28/50], Class Loss: 0.0026, Discrepancy Loss: 0.0025
Epoch [29/50], Class Loss: 0.0029, Discrepancy Loss: 0.0008
Epoch [30/50], Class Loss: 0.0034, Discrepancy Loss: 0.0005
Epoch [31/50], Class Loss: 0.0070, Discrepancy Loss: 0.0013
Epoch [32/50], Class Loss: 0.0017, Discrepancy Loss: 0.0007
Epoch [33/50], Class Loss: 0.0024, Discrepancy Loss: 0.0012
Epoch [34/50], Class Loss: 0.0036, Discrepancy Loss: 0.0010
Epoch [35/50], Class Loss: 0.0036, Discrepancy Loss: 0.0014
Epoch [36/50], Class Loss: 0.0021, Discrepancy Loss: 0.0006
Epoch [37/50], Class Loss: 0.0014, Discrepancy Loss: 0.0009
Epoch [38/50], Class Loss: 0.0022, Discrepancy Loss: 0.0018
Epoch [39/50], Class Loss: 0.0028, Discrepancy Loss: 0.0029
Epoch [40/50], Class Loss: 0.0015, Discrepancy Loss: 0.0010
Epoch [41/50], Class Loss: 0.0307, Discrepancy Loss: 0.0008
Epoch [42/50], Class Loss: 0.0036, Discrepancy Loss: 0.0006
Epoch [43/50], Class Loss: 0.0022, Discrepancy Loss: 0.0009
Epoch [44/50], Class Loss: 0.0017, Discrepancy Loss: 0.0011
Epoch [45/50], Class Loss: 0.0029, Discrepancy Loss: 0.0009
Epoch [46/50], Class Loss: 0.0037, Discrepancy Loss: 0.0014
Epoch [47/50], Class Loss: 0.0041, Discrepancy Loss: 0.0017
Epoch [48/50], Class Loss: 0.0098, Discrepancy Loss: 0.0016
Epoch [49/50], Class Loss: 0.0099, Discrepancy Loss: 0.0008
Epoch [50/50], Class Loss: 0.0058, Discrepancy Loss: 0.0013
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 6/10
Epoch [1/50], Class Loss: 1.6362, Discrepancy Loss: 0.1016
Epoch [2/50], Class Loss: 0.2293, Discrepancy Loss: 0.0216
Epoch [3/50], Class Loss: 0.2527, Discrepancy Loss: 0.0226
Epoch [4/50], Class Loss: 0.0416, Discrepancy Loss: 0.0119
Epoch [5/50], Class Loss: 0.0364, Discrepancy Loss: 0.0054
Epoch [6/50], Class Loss: 0.0178, Discrepancy Loss: 0.0053
Epoch [7/50], Class Loss: 0.0332, Discrepancy Loss: 0.0034
Epoch [8/50], Class Loss: 0.0346, Discrepancy Loss: 0.0047
Epoch [9/50], Class Loss: 0.0501, Discrepancy Loss: 0.0094
Epoch [10/50], Class Loss: 0.0746, Discrepancy Loss: 0.0054
Epoch [11/50], Class Loss: 0.0336, Discrepancy Loss: 0.0046
Epoch [12/50], Class Loss: 0.1452, Discrepancy Loss: 0.0027
Epoch [13/50], Class Loss: 0.0074, Discrepancy Loss: 0.0032
Epoch [14/50], Class Loss: 0.0084, Discrepancy Loss: 0.0022
Epoch [15/50], Class Loss: 0.0076, Discrepancy Loss: 0.0021
Epoch [16/50], Class Loss: 0.0344, Discrepancy Loss: 0.0024
Epoch [17/50], Class Loss: 0.0098, Discrepancy Loss: 0.0017
Epoch [18/50], Class Loss: 0.0046, Discrepancy Loss: 0.0015
Epoch [19/50], Class Loss: 0.0031, Discrepancy Loss: 0.0021
Epoch [20/50], Class Loss: 0.0116, Discrepancy Loss: 0.0015
Epoch [21/50], Class Loss: 0.0067, Discrepancy Loss: 0.0011
Epoch [22/50], Class Loss: 0.0091, Discrepancy Loss: 0.0013
Epoch [23/50], Class Loss: 0.0038, Discrepancy Loss: 0.0016
Epoch [24/50], Class Loss: 0.0024, Discrepancy Loss: 0.0016
Epoch [25/50], Class Loss: 0.0072, Discrepancy Loss: 0.0015
Epoch [26/50], Class Loss: 0.0717, Discrepancy Loss: 0.0010
Epoch [27/50], Class Loss: 0.0115, Discrepancy Loss: 0.0012
Epoch [28/50], Class Loss: 0.0083, Discrepancy Loss: 0.0017
Epoch [29/50], Class Loss: 0.0048, Discrepancy Loss: 0.0017
Epoch [30/50], Class Loss: 0.0046, Discrepancy Loss: 0.0013
Epoch [31/50], Class Loss: 0.0033, Discrepancy Loss: 0.0012
Epoch [32/50], Class Loss: 0.0044, Discrepancy Loss: 0.0010
Epoch [33/50], Class Loss: 0.0022, Discrepancy Loss: 0.0010
Epoch [34/50], Class Loss: 0.0033, Discrepancy Loss: 0.0012
Epoch [35/50], Class Loss: 0.0041, Discrepancy Loss: 0.0008
Epoch [36/50], Class Loss: 0.0045, Discrepancy Loss: 0.0015
Epoch [37/50], Class Loss: 0.0033, Discrepancy Loss: 0.0013
Epoch [38/50], Class Loss: 0.0039, Discrepancy Loss: 0.0022
Epoch [39/50], Class Loss: 0.0026, Discrepancy Loss: 0.0008
Epoch [40/50], Class Loss: 0.0023, Discrepancy Loss: 0.0011
Epoch [41/50], Class Loss: 0.0123, Discrepancy Loss: 0.0013
Epoch [42/50], Class Loss: 0.0030, Discrepancy Loss: 0.0011
Epoch [43/50], Class Loss: 0.0963, Discrepancy Loss: 0.0023
Epoch [44/50], Class Loss: 0.0028, Discrepancy Loss: 0.0006
Epoch [45/50], Class Loss: 0.0211, Discrepancy Loss: 0.0011
Epoch [46/50], Class Loss: 0.1342, Discrepancy Loss: 0.0014
Epoch [47/50], Class Loss: 0.0034, Discrepancy Loss: 0.0010
Epoch [48/50], Class Loss: 0.0059, Discrepancy Loss: 0.0012
Epoch [49/50], Class Loss: 0.0033, Discrepancy Loss: 0.0012
Epoch [50/50], Class Loss: 0.0044, Discrepancy Loss: 0.0012
Source Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.76%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 7/10
Epoch [1/50], Class Loss: 1.9696, Discrepancy Loss: 0.1310
Epoch [2/50], Class Loss: 0.5283, Discrepancy Loss: 0.0493
Epoch [3/50], Class Loss: 0.1870, Discrepancy Loss: 0.0195
Epoch [4/50], Class Loss: 0.1443, Discrepancy Loss: 0.0108
Epoch [5/50], Class Loss: 0.5914, Discrepancy Loss: 0.0308
Epoch [6/50], Class Loss: 0.1489, Discrepancy Loss: 0.0130
Epoch [7/50], Class Loss: 0.1967, Discrepancy Loss: 0.0199
Epoch [8/50], Class Loss: 0.0428, Discrepancy Loss: 0.0062
Epoch [9/50], Class Loss: 0.0401, Discrepancy Loss: 0.0069
Epoch [10/50], Class Loss: 0.0333, Discrepancy Loss: 0.0045
Epoch [11/50], Class Loss: 0.0131, Discrepancy Loss: 0.0023
Epoch [12/50], Class Loss: 0.0150, Discrepancy Loss: 0.0019
Epoch [13/50], Class Loss: 0.0108, Discrepancy Loss: 0.0036
Epoch [14/50], Class Loss: 0.0098, Discrepancy Loss: 0.0017
Epoch [15/50], Class Loss: 0.0093, Discrepancy Loss: 0.0017
Epoch [16/50], Class Loss: 0.0105, Discrepancy Loss: 0.0012
Epoch [17/50], Class Loss: 0.0122, Discrepancy Loss: 0.0015
Epoch [18/50], Class Loss: 0.0063, Discrepancy Loss: 0.0015
Epoch [19/50], Class Loss: 0.0054, Discrepancy Loss: 0.0030
Epoch [20/50], Class Loss: 0.0918, Discrepancy Loss: 0.0027
Epoch [21/50], Class Loss: 0.0118, Discrepancy Loss: 0.0030
Epoch [22/50], Class Loss: 0.0148, Discrepancy Loss: 0.0026
Epoch [23/50], Class Loss: 0.0305, Discrepancy Loss: 0.0018
Epoch [24/50], Class Loss: 0.0095, Discrepancy Loss: 0.0025
Epoch [25/50], Class Loss: 0.0072, Discrepancy Loss: 0.0026
Epoch [26/50], Class Loss: 0.0952, Discrepancy Loss: 0.0020
Epoch [27/50], Class Loss: 0.0062, Discrepancy Loss: 0.0025
Epoch [28/50], Class Loss: 0.0267, Discrepancy Loss: 0.0025
Epoch [29/50], Class Loss: 0.0074, Discrepancy Loss: 0.0018
Epoch [30/50], Class Loss: 0.0180, Discrepancy Loss: 0.0015
Epoch [31/50], Class Loss: 0.0082, Discrepancy Loss: 0.0013
Epoch [32/50], Class Loss: 0.0083, Discrepancy Loss: 0.0023
Epoch [33/50], Class Loss: 0.0108, Discrepancy Loss: 0.0012
Epoch [34/50], Class Loss: 0.0080, Discrepancy Loss: 0.0019
Epoch [35/50], Class Loss: 0.0064, Discrepancy Loss: 0.0013
Epoch [36/50], Class Loss: 0.0050, Discrepancy Loss: 0.0017
Epoch [37/50], Class Loss: 0.0079, Discrepancy Loss: 0.0026
Epoch [38/50], Class Loss: 0.0047, Discrepancy Loss: 0.0013
Epoch [39/50], Class Loss: 0.0088, Discrepancy Loss: 0.0021
Epoch [40/50], Class Loss: 0.0055, Discrepancy Loss: 0.0022
Epoch [41/50], Class Loss: 0.0084, Discrepancy Loss: 0.0012
Epoch [42/50], Class Loss: 0.0144, Discrepancy Loss: 0.0014
Epoch [43/50], Class Loss: 0.0047, Discrepancy Loss: 0.0018
Epoch [44/50], Class Loss: 0.0102, Discrepancy Loss: 0.0019
Epoch [45/50], Class Loss: 0.0059, Discrepancy Loss: 0.0017
Epoch [46/50], Class Loss: 0.0128, Discrepancy Loss: 0.0017
Epoch [47/50], Class Loss: 0.0083, Discrepancy Loss: 0.0015
Epoch [48/50], Class Loss: 0.0091, Discrepancy Loss: 0.0043
Epoch [49/50], Class Loss: 0.0091, Discrepancy Loss: 0.0021
Epoch [50/50], Class Loss: 0.0064, Discrepancy Loss: 0.0025
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 8/10
Epoch [1/50], Class Loss: 1.9496, Discrepancy Loss: 0.1282
Epoch [2/50], Class Loss: 0.5686, Discrepancy Loss: 0.0574
Epoch [3/50], Class Loss: 0.1369, Discrepancy Loss: 0.0208
Epoch [4/50], Class Loss: 0.2034, Discrepancy Loss: 0.0089
Epoch [5/50], Class Loss: 0.0989, Discrepancy Loss: 0.0083
Epoch [6/50], Class Loss: 0.0485, Discrepancy Loss: 0.0071
Epoch [7/50], Class Loss: 0.0232, Discrepancy Loss: 0.0038
Epoch [8/50], Class Loss: 0.0625, Discrepancy Loss: 0.0067
Epoch [9/50], Class Loss: 0.1143, Discrepancy Loss: 0.0076
Epoch [10/50], Class Loss: 0.0878, Discrepancy Loss: 0.0103
Epoch [11/50], Class Loss: 0.0145, Discrepancy Loss: 0.0032
Epoch [12/50], Class Loss: 0.0135, Discrepancy Loss: 0.0020
Epoch [13/50], Class Loss: 0.0149, Discrepancy Loss: 0.0034
Epoch [14/50], Class Loss: 0.0073, Discrepancy Loss: 0.0026
Epoch [15/50], Class Loss: 0.0315, Discrepancy Loss: 0.0029
Epoch [16/50], Class Loss: 0.0238, Discrepancy Loss: 0.0024
Epoch [17/50], Class Loss: 0.0607, Discrepancy Loss: 0.0023
Epoch [18/50], Class Loss: 0.0221, Discrepancy Loss: 0.0082
Epoch [19/50], Class Loss: 0.0073, Discrepancy Loss: 0.0019
Epoch [20/50], Class Loss: 0.0068, Discrepancy Loss: 0.0029
Epoch [21/50], Class Loss: 0.0182, Discrepancy Loss: 0.0019
Epoch [22/50], Class Loss: 0.0049, Discrepancy Loss: 0.0018
Epoch [23/50], Class Loss: 0.0051, Discrepancy Loss: 0.0016
Epoch [24/50], Class Loss: 0.0152, Discrepancy Loss: 0.0021
Epoch [25/50], Class Loss: 0.0057, Discrepancy Loss: 0.0016
Epoch [26/50], Class Loss: 0.0082, Discrepancy Loss: 0.0012
Epoch [27/50], Class Loss: 0.0072, Discrepancy Loss: 0.0011
Epoch [28/50], Class Loss: 0.0121, Discrepancy Loss: 0.0032
Epoch [29/50], Class Loss: 0.0029, Discrepancy Loss: 0.0015
Epoch [30/50], Class Loss: 0.0180, Discrepancy Loss: 0.0013
Epoch [31/50], Class Loss: 0.0096, Discrepancy Loss: 0.0011
Epoch [32/50], Class Loss: 0.0075, Discrepancy Loss: 0.0011
Epoch [33/50], Class Loss: 0.0027, Discrepancy Loss: 0.0012
Epoch [34/50], Class Loss: 0.0029, Discrepancy Loss: 0.0017
Epoch [35/50], Class Loss: 0.0060, Discrepancy Loss: 0.0010
Epoch [36/50], Class Loss: 0.1024, Discrepancy Loss: 0.0012
Epoch [37/50], Class Loss: 0.0941, Discrepancy Loss: 0.0018
Epoch [38/50], Class Loss: 0.0377, Discrepancy Loss: 0.0015
Epoch [39/50], Class Loss: 0.0073, Discrepancy Loss: 0.0011
Epoch [40/50], Class Loss: 0.0045, Discrepancy Loss: 0.0015
Epoch [41/50], Class Loss: 0.1105, Discrepancy Loss: 0.0011
Epoch [42/50], Class Loss: 0.0081, Discrepancy Loss: 0.0031
Epoch [43/50], Class Loss: 0.0195, Discrepancy Loss: 0.0015
Epoch [44/50], Class Loss: 0.0033, Discrepancy Loss: 0.0018
Epoch [45/50], Class Loss: 0.0042, Discrepancy Loss: 0.0015
Epoch [46/50], Class Loss: 0.0069, Discrepancy Loss: 0.0031
Epoch [47/50], Class Loss: 0.0034, Discrepancy Loss: 0.0012
Epoch [48/50], Class Loss: 0.0086, Discrepancy Loss: 0.0014
Epoch [49/50], Class Loss: 0.0241, Discrepancy Loss: 0.0014
Epoch [50/50], Class Loss: 0.0030, Discrepancy Loss: 0.0018
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 9/10
Epoch [1/50], Class Loss: 1.6463, Discrepancy Loss: 0.1127
Epoch [2/50], Class Loss: 0.3113, Discrepancy Loss: 0.0368
Epoch [3/50], Class Loss: 0.1821, Discrepancy Loss: 0.0211
Epoch [4/50], Class Loss: 0.0569, Discrepancy Loss: 0.0089
Epoch [5/50], Class Loss: 0.0369, Discrepancy Loss: 0.0062
Epoch [6/50], Class Loss: 0.0582, Discrepancy Loss: 0.0113
Epoch [7/50], Class Loss: 0.0419, Discrepancy Loss: 0.0109
Epoch [8/50], Class Loss: 0.0425, Discrepancy Loss: 0.0052
Epoch [9/50], Class Loss: 0.2302, Discrepancy Loss: 0.0257
Epoch [10/50], Class Loss: 0.2760, Discrepancy Loss: 0.0155
Epoch [11/50], Class Loss: 0.0929, Discrepancy Loss: 0.0234
Epoch [12/50], Class Loss: 0.0476, Discrepancy Loss: 0.0078
Epoch [13/50], Class Loss: 0.0400, Discrepancy Loss: 0.0104
Epoch [14/50], Class Loss: 0.0492, Discrepancy Loss: 0.0065
Epoch [15/50], Class Loss: 0.0380, Discrepancy Loss: 0.0046
Epoch [16/50], Class Loss: 0.0317, Discrepancy Loss: 0.0083
Epoch [17/50], Class Loss: 0.0598, Discrepancy Loss: 0.0032
Epoch [18/50], Class Loss: 0.0252, Discrepancy Loss: 0.0042
Epoch [19/50], Class Loss: 0.0294, Discrepancy Loss: 0.0071
Epoch [20/50], Class Loss: 0.0270, Discrepancy Loss: 0.0029
Epoch [21/50], Class Loss: 0.0280, Discrepancy Loss: 0.0080
Epoch [22/50], Class Loss: 0.0161, Discrepancy Loss: 0.0084
Epoch [23/50], Class Loss: 0.0222, Discrepancy Loss: 0.0034
Epoch [24/50], Class Loss: 0.0175, Discrepancy Loss: 0.0032
Epoch [25/50], Class Loss: 0.0198, Discrepancy Loss: 0.0028
Epoch [26/50], Class Loss: 0.0163, Discrepancy Loss: 0.0049
Epoch [27/50], Class Loss: 0.0138, Discrepancy Loss: 0.0028
Epoch [28/50], Class Loss: 0.0144, Discrepancy Loss: 0.0025
Epoch [29/50], Class Loss: 0.0154, Discrepancy Loss: 0.0041
Epoch [30/50], Class Loss: 0.0149, Discrepancy Loss: 0.0038
Epoch [31/50], Class Loss: 0.0179, Discrepancy Loss: 0.0023
Epoch [32/50], Class Loss: 0.0131, Discrepancy Loss: 0.0059
Epoch [33/50], Class Loss: 0.0164, Discrepancy Loss: 0.0036
Epoch [34/50], Class Loss: 0.0197, Discrepancy Loss: 0.0037
Epoch [35/50], Class Loss: 0.0115, Discrepancy Loss: 0.0026
Epoch [36/50], Class Loss: 0.0165, Discrepancy Loss: 0.0027
Epoch [37/50], Class Loss: 0.0284, Discrepancy Loss: 0.0024
Epoch [38/50], Class Loss: 0.0153, Discrepancy Loss: 0.0033
Epoch [39/50], Class Loss: 0.0155, Discrepancy Loss: 0.0035
Epoch [40/50], Class Loss: 0.0123, Discrepancy Loss: 0.0023
Epoch [41/50], Class Loss: 0.0172, Discrepancy Loss: 0.0024
Epoch [42/50], Class Loss: 0.0164, Discrepancy Loss: 0.0024
Epoch [43/50], Class Loss: 0.0209, Discrepancy Loss: 0.0030
Epoch [44/50], Class Loss: 0.0672, Discrepancy Loss: 0.0050
Epoch [45/50], Class Loss: 0.0185, Discrepancy Loss: 0.0023
Epoch [46/50], Class Loss: 0.0322, Discrepancy Loss: 0.0027
Epoch [47/50], Class Loss: 0.0328, Discrepancy Loss: 0.0020
Epoch [48/50], Class Loss: 0.0307, Discrepancy Loss: 0.0032
Epoch [49/50], Class Loss: 0.0254, Discrepancy Loss: 0.0038
Epoch [50/50], Class Loss: 0.0126, Discrepancy Loss: 0.0024
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%

Run 10/10
Epoch [1/50], Class Loss: 1.5868, Discrepancy Loss: 0.1190
Epoch [2/50], Class Loss: 0.2488, Discrepancy Loss: 0.0234
Epoch [3/50], Class Loss: 0.1288, Discrepancy Loss: 0.0226
Epoch [4/50], Class Loss: 0.0842, Discrepancy Loss: 0.0094
Epoch [5/50], Class Loss: 0.1081, Discrepancy Loss: 0.0091
Epoch [6/50], Class Loss: 0.1407, Discrepancy Loss: 0.0152
Epoch [7/50], Class Loss: 0.0773, Discrepancy Loss: 0.0068
Epoch [8/50], Class Loss: 0.0474, Discrepancy Loss: 0.0095
Epoch [9/50], Class Loss: 0.0341, Discrepancy Loss: 0.0037
Epoch [10/50], Class Loss: 0.0510, Discrepancy Loss: 0.0038
Epoch [11/50], Class Loss: 0.0623, Discrepancy Loss: 0.0068
Epoch [12/50], Class Loss: 0.0332, Discrepancy Loss: 0.0051
Epoch [13/50], Class Loss: 0.0452, Discrepancy Loss: 0.0044
Epoch [14/50], Class Loss: 0.0325, Discrepancy Loss: 0.0049
Epoch [15/50], Class Loss: 0.0153, Discrepancy Loss: 0.0035
Epoch [16/50], Class Loss: 0.0112, Discrepancy Loss: 0.0029
Epoch [17/50], Class Loss: 0.0101, Discrepancy Loss: 0.0033
Epoch [18/50], Class Loss: 0.0082, Discrepancy Loss: 0.0026
Epoch [19/50], Class Loss: 0.0117, Discrepancy Loss: 0.0020
Epoch [20/50], Class Loss: 0.0115, Discrepancy Loss: 0.0025
Epoch [21/50], Class Loss: 0.0056, Discrepancy Loss: 0.0024
Epoch [22/50], Class Loss: 0.0094, Discrepancy Loss: 0.0019
Epoch [23/50], Class Loss: 0.0071, Discrepancy Loss: 0.0027
Epoch [24/50], Class Loss: 0.0065, Discrepancy Loss: 0.0023
Epoch [25/50], Class Loss: 0.0072, Discrepancy Loss: 0.0013
Epoch [26/50], Class Loss: 0.0058, Discrepancy Loss: 0.0022
Epoch [27/50], Class Loss: 0.0035, Discrepancy Loss: 0.0019
Epoch [28/50], Class Loss: 0.0087, Discrepancy Loss: 0.0015
Epoch [29/50], Class Loss: 0.1350, Discrepancy Loss: 0.0028
Epoch [30/50], Class Loss: 0.0061, Discrepancy Loss: 0.0015
Epoch [31/50], Class Loss: 0.0058, Discrepancy Loss: 0.0021
Epoch [32/50], Class Loss: 0.0130, Discrepancy Loss: 0.0012
Epoch [33/50], Class Loss: 0.0067, Discrepancy Loss: 0.0017
Epoch [34/50], Class Loss: 0.0623, Discrepancy Loss: 0.0018
Epoch [35/50], Class Loss: 0.0057, Discrepancy Loss: 0.0021
Epoch [36/50], Class Loss: 0.0244, Discrepancy Loss: 0.0014
Epoch [37/50], Class Loss: 0.0058, Discrepancy Loss: 0.0027
Epoch [38/50], Class Loss: 0.0066, Discrepancy Loss: 0.0021
Epoch [39/50], Class Loss: 0.0060, Discrepancy Loss: 0.0023
Epoch [40/50], Class Loss: 0.0086, Discrepancy Loss: 0.0023
Epoch [41/50], Class Loss: 0.0062, Discrepancy Loss: 0.0021
Epoch [42/50], Class Loss: 0.0052, Discrepancy Loss: 0.0026
Epoch [43/50], Class Loss: 0.0078, Discrepancy Loss: 0.0018
Epoch [44/50], Class Loss: 0.0097, Discrepancy Loss: 0.0024
Epoch [45/50], Class Loss: 0.0067, Discrepancy Loss: 0.0019
Epoch [46/50], Class Loss: 0.0190, Discrepancy Loss: 0.0019
Epoch [47/50], Class Loss: 0.0133, Discrepancy Loss: 0.0014
Epoch [48/50], Class Loss: 0.0053, Discrepancy Loss: 0.0028
Epoch [49/50], Class Loss: 0.0063, Discrepancy Loss: 0.0022
Epoch [50/50], Class Loss: 0.0046, Discrepancy Loss: 0.0015
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Source performance: 99.92% 99.92% 99.92% 99.92%
Target performance: 99.93% 99.93% 99.93% 99.93%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 99.81%
16qam: 99.93%
8apsk: 100.00%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.5903, Discrepancy Loss: 0.0204
Validation Loss: 0.9933
Epoch [2/50], Class Loss: 0.0735, Discrepancy Loss: 0.0020
Validation Loss: 0.0541
Epoch [3/50], Class Loss: 0.0426, Discrepancy Loss: 0.0025
Validation Loss: 0.3228
Epoch [4/50], Class Loss: 0.0564, Discrepancy Loss: 0.0014
Validation Loss: 1.5948
Epoch [5/50], Class Loss: 0.0397, Discrepancy Loss: 0.0016
Validation Loss: 2.0324
Epoch [6/50], Class Loss: 0.0391, Discrepancy Loss: 0.0022
Validation Loss: 0.1715
Epoch [7/50], Class Loss: 0.0426, Discrepancy Loss: 0.0024
Validation Loss: 3.0157
Early stopping!
Source Domain Performance - Accuracy: 79.68%, Precision: 87.04%, Recall: 79.67%, F1 Score: 75.83%
Target Domain Performance - Accuracy: 78.78%, Precision: 87.48%, Recall: 78.93%, F1 Score: 74.49%

Run 2/10
Epoch [1/50], Class Loss: 0.6049, Discrepancy Loss: 0.0201
Validation Loss: 1.7119
Epoch [2/50], Class Loss: 0.0530, Discrepancy Loss: 0.0010
Validation Loss: 1.1179
Epoch [3/50], Class Loss: 0.0859, Discrepancy Loss: 0.0026
Validation Loss: 0.0777
Epoch [4/50], Class Loss: 0.0648, Discrepancy Loss: 0.0033
Validation Loss: 0.0254
Epoch [5/50], Class Loss: 0.0298, Discrepancy Loss: 0.0009
Validation Loss: 0.0072
Epoch [6/50], Class Loss: 0.0266, Discrepancy Loss: 0.0005
Validation Loss: 0.0729
Epoch [7/50], Class Loss: 0.0408, Discrepancy Loss: 0.0010
Validation Loss: 3.1586
Epoch [8/50], Class Loss: 0.0390, Discrepancy Loss: 0.0017
Validation Loss: 2.8452
Epoch [9/50], Class Loss: 0.0196, Discrepancy Loss: 0.0010
Validation Loss: 1.2928
Epoch [10/50], Class Loss: 0.0058, Discrepancy Loss: 0.0005
Validation Loss: 0.0211
Early stopping!
Source Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.77%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%

Run 3/10
Epoch [1/50], Class Loss: 0.5639, Discrepancy Loss: 0.0204
Validation Loss: 12.4139
Epoch [2/50], Class Loss: 0.1193, Discrepancy Loss: 0.0027
Validation Loss: 0.0212
Epoch [3/50], Class Loss: 0.0494, Discrepancy Loss: 0.0008
Validation Loss: 3.8186
Epoch [4/50], Class Loss: 0.0424, Discrepancy Loss: 0.0006
Validation Loss: 0.0304
Epoch [5/50], Class Loss: 0.1780, Discrepancy Loss: 0.0040
Validation Loss: 0.0419
Epoch [6/50], Class Loss: 0.0750, Discrepancy Loss: 0.0027
Validation Loss: 2.3238
Epoch [7/50], Class Loss: 0.0189, Discrepancy Loss: 0.0007
Validation Loss: 4.2536
Early stopping!
Source Domain Performance - Accuracy: 73.74%, Precision: 62.19%, Recall: 75.00%, F1 Score: 66.39%
Target Domain Performance - Accuracy: 74.94%, Precision: 62.53%, Recall: 75.00%, F1 Score: 66.69%

Run 4/10
Epoch [1/50], Class Loss: 0.7400, Discrepancy Loss: 0.0198
Validation Loss: 14.0599
Epoch [2/50], Class Loss: 0.0943, Discrepancy Loss: 0.0030
Validation Loss: 0.2797
Epoch [3/50], Class Loss: 0.0303, Discrepancy Loss: 0.0009
Validation Loss: 4.0414
Epoch [4/50], Class Loss: 0.1083, Discrepancy Loss: 0.0025
Validation Loss: 0.2187
Epoch [5/50], Class Loss: 0.0227, Discrepancy Loss: 0.0009
Validation Loss: 0.6661
Epoch [6/50], Class Loss: 0.0391, Discrepancy Loss: 0.0025
Validation Loss: 0.1645
Epoch [7/50], Class Loss: 0.0252, Discrepancy Loss: 0.0019
Validation Loss: 0.7866
Epoch [8/50], Class Loss: 0.0254, Discrepancy Loss: 0.0033
Validation Loss: 0.5131
Epoch [9/50], Class Loss: 0.0584, Discrepancy Loss: 0.0025
Validation Loss: 2.1473
Epoch [10/50], Class Loss: 0.0097, Discrepancy Loss: 0.0009
Validation Loss: 1.2556
Epoch [11/50], Class Loss: 0.0056, Discrepancy Loss: 0.0002
Validation Loss: 0.0005
Epoch [12/50], Class Loss: 0.0008, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [13/50], Class Loss: 0.0018, Discrepancy Loss: 0.0001
Validation Loss: 0.0005
Epoch [14/50], Class Loss: 0.0014, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [15/50], Class Loss: 0.0023, Discrepancy Loss: 0.0001
Validation Loss: 0.0002
Epoch [16/50], Class Loss: 0.0005, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [18/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0003
Epoch [19/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [20/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0007
Epoch [21/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 5/10
Epoch [1/50], Class Loss: 0.6854, Discrepancy Loss: 0.0208
Validation Loss: 0.5456
Epoch [2/50], Class Loss: 0.0551, Discrepancy Loss: 0.0033
Validation Loss: 1.5827
Epoch [3/50], Class Loss: 0.0264, Discrepancy Loss: 0.0015
Validation Loss: 0.1531
Epoch [4/50], Class Loss: 0.0534, Discrepancy Loss: 0.0019
Validation Loss: 0.0783
Epoch [5/50], Class Loss: 0.0387, Discrepancy Loss: 0.0020
Validation Loss: 3.2692
Epoch [6/50], Class Loss: 0.0094, Discrepancy Loss: 0.0005
Validation Loss: 0.0068
Epoch [7/50], Class Loss: 0.0159, Discrepancy Loss: 0.0004
Validation Loss: 0.0063
Epoch [8/50], Class Loss: 0.0398, Discrepancy Loss: 0.0011
Validation Loss: 1.5273
Epoch [9/50], Class Loss: 0.0409, Discrepancy Loss: 0.0015
Validation Loss: 3.2316
Epoch [10/50], Class Loss: 0.0515, Discrepancy Loss: 0.0020
Validation Loss: 0.2107
Epoch [11/50], Class Loss: 0.0028, Discrepancy Loss: 0.0003
Validation Loss: 0.0012
Epoch [12/50], Class Loss: 0.0012, Discrepancy Loss: 0.0001
Validation Loss: 0.0013
Epoch [13/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [14/50], Class Loss: 0.0012, Discrepancy Loss: 0.0005
Validation Loss: 0.0007
Epoch [15/50], Class Loss: 0.0004, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [16/50], Class Loss: 0.0004, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [17/50], Class Loss: 0.0070, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [18/50], Class Loss: 0.0113, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [19/50], Class Loss: 0.0020, Discrepancy Loss: 0.0000
Validation Loss: 0.0005
Epoch [20/50], Class Loss: 0.0004, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [21/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [22/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [23/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [24/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [25/50], Class Loss: 0.0678, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [26/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [27/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [28/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [29/50], Class Loss: 0.0046, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [30/50], Class Loss: 0.0051, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [31/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [32/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [33/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [34/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [35/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [36/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [37/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [38/50], Class Loss: 0.0005, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [39/50], Class Loss: 0.0138, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [40/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [41/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [42/50], Class Loss: 0.0029, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 6/10
Epoch [1/50], Class Loss: 0.5293, Discrepancy Loss: 0.0121
Validation Loss: 11.5065
Epoch [2/50], Class Loss: 0.1121, Discrepancy Loss: 0.0017
Validation Loss: 2.3149
Epoch [3/50], Class Loss: 0.1030, Discrepancy Loss: 0.0032
Validation Loss: 0.1068
Epoch [4/50], Class Loss: 0.0302, Discrepancy Loss: 0.0025
Validation Loss: 0.3606
Epoch [5/50], Class Loss: 0.0966, Discrepancy Loss: 0.0049
Validation Loss: 0.0462
Epoch [6/50], Class Loss: 0.0457, Discrepancy Loss: 0.0021
Validation Loss: 1.4278
Epoch [7/50], Class Loss: 0.0155, Discrepancy Loss: 0.0006
Validation Loss: 0.0155
Epoch [8/50], Class Loss: 0.0129, Discrepancy Loss: 0.0008
Validation Loss: 2.1618
Epoch [9/50], Class Loss: 0.0031, Discrepancy Loss: 0.0001
Validation Loss: 0.0010
Epoch [10/50], Class Loss: 0.0044, Discrepancy Loss: 0.0001
Validation Loss: 0.0241
Epoch [11/50], Class Loss: 0.0036, Discrepancy Loss: 0.0000
Validation Loss: 0.0010
Epoch [12/50], Class Loss: 0.0011, Discrepancy Loss: 0.0000
Validation Loss: 0.0035
Epoch [13/50], Class Loss: 0.0022, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [14/50], Class Loss: 0.0015, Discrepancy Loss: 0.0000
Validation Loss: 0.0026
Epoch [15/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0018
Epoch [16/50], Class Loss: 0.0005, Discrepancy Loss: 0.0000
Validation Loss: 0.0013
Epoch [17/50], Class Loss: 0.0067, Discrepancy Loss: 0.0000
Validation Loss: 0.0015
Epoch [18/50], Class Loss: 0.0009, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [19/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [20/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [21/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [22/50], Class Loss: 0.0009, Discrepancy Loss: 0.0001
Validation Loss: 0.0007
Epoch [23/50], Class Loss: 0.0467, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [24/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [25/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0005
Epoch [26/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [27/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [28/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0006
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 7/10
Epoch [1/50], Class Loss: 0.6593, Discrepancy Loss: 0.0203
Validation Loss: 6.4272
Epoch [2/50], Class Loss: 0.0641, Discrepancy Loss: 0.0024
Validation Loss: 5.0231
Epoch [3/50], Class Loss: 0.0966, Discrepancy Loss: 0.0021
Validation Loss: 0.0275
Epoch [4/50], Class Loss: 0.0306, Discrepancy Loss: 0.0007
Validation Loss: 0.1866
Epoch [5/50], Class Loss: 0.0404, Discrepancy Loss: 0.0013
Validation Loss: 0.2863
Epoch [6/50], Class Loss: 0.0349, Discrepancy Loss: 0.0010
Validation Loss: 0.1557
Epoch [7/50], Class Loss: 0.0430, Discrepancy Loss: 0.0013
Validation Loss: 5.4202
Epoch [8/50], Class Loss: 0.0294, Discrepancy Loss: 0.0014
Validation Loss: 3.0428
Early stopping!
Source Domain Performance - Accuracy: 62.05%, Precision: 52.54%, Recall: 63.23%, F1 Score: 56.17%
Target Domain Performance - Accuracy: 62.89%, Precision: 52.86%, Recall: 63.00%, F1 Score: 56.27%

Run 8/10
Epoch [1/50], Class Loss: 0.6103, Discrepancy Loss: 0.0253
Validation Loss: 5.6581
Epoch [2/50], Class Loss: 0.0591, Discrepancy Loss: 0.0022
Validation Loss: 0.1105
Epoch [3/50], Class Loss: 0.0467, Discrepancy Loss: 0.0013
Validation Loss: 4.6198
Epoch [4/50], Class Loss: 0.0268, Discrepancy Loss: 0.0014
Validation Loss: 1.1074
Epoch [5/50], Class Loss: 0.0748, Discrepancy Loss: 0.0015
Validation Loss: 0.0657
Epoch [6/50], Class Loss: 0.0290, Discrepancy Loss: 0.0010
Validation Loss: 0.0178
Epoch [7/50], Class Loss: 0.0451, Discrepancy Loss: 0.0011
Validation Loss: 0.0102
Epoch [8/50], Class Loss: 0.0235, Discrepancy Loss: 0.0009
Validation Loss: 0.0144
Epoch [9/50], Class Loss: 0.0276, Discrepancy Loss: 0.0008
Validation Loss: 2.5429
Epoch [10/50], Class Loss: 0.0245, Discrepancy Loss: 0.0014
Validation Loss: 0.0601
Epoch [11/50], Class Loss: 0.0016, Discrepancy Loss: 0.0002
Validation Loss: 0.0002
Epoch [12/50], Class Loss: 0.0004, Discrepancy Loss: 0.0000
Validation Loss: 0.0006
Epoch [13/50], Class Loss: 0.0009, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [14/50], Class Loss: 0.0004, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [15/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [16/50], Class Loss: 0.0081, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0152, Discrepancy Loss: 0.0000
Validation Loss: 0.0009
Epoch [18/50], Class Loss: 0.0004, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [19/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [20/50], Class Loss: 0.0323, Discrepancy Loss: 0.0001
Validation Loss: 0.0003
Epoch [21/50], Class Loss: 0.0005, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 9/10
Epoch [1/50], Class Loss: 0.5063, Discrepancy Loss: 0.0257
Validation Loss: 9.4579
Epoch [2/50], Class Loss: 0.1173, Discrepancy Loss: 0.0046
Validation Loss: 0.3885
Epoch [3/50], Class Loss: 0.0388, Discrepancy Loss: 0.0021
Validation Loss: 0.0600
Epoch [4/50], Class Loss: 0.0428, Discrepancy Loss: 0.0020
Validation Loss: 2.9520
Epoch [5/50], Class Loss: 0.0223, Discrepancy Loss: 0.0015
Validation Loss: 0.7228
Epoch [6/50], Class Loss: 0.3045, Discrepancy Loss: 0.0026
Validation Loss: 0.7043
Epoch [7/50], Class Loss: 0.0350, Discrepancy Loss: 0.0014
Validation Loss: 2.1003
Epoch [8/50], Class Loss: 0.0227, Discrepancy Loss: 0.0009
Validation Loss: 2.2760
Early stopping!
Source Domain Performance - Accuracy: 68.41%, Precision: 78.78%, Recall: 69.49%, F1 Score: 64.79%
Target Domain Performance - Accuracy: 69.30%, Precision: 78.93%, Recall: 69.42%, F1 Score: 65.05%

Run 10/10
Epoch [1/50], Class Loss: 0.5393, Discrepancy Loss: 0.0199
Validation Loss: 4.9813
Epoch [2/50], Class Loss: 0.0782, Discrepancy Loss: 0.0042
Validation Loss: 3.3244
Epoch [3/50], Class Loss: 0.0588, Discrepancy Loss: 0.0017
Validation Loss: 0.0109
Epoch [4/50], Class Loss: 0.0560, Discrepancy Loss: 0.0016
Validation Loss: 2.6931
Epoch [5/50], Class Loss: 0.0407, Discrepancy Loss: 0.0018
Validation Loss: 0.0482
Epoch [6/50], Class Loss: 0.0437, Discrepancy Loss: 0.0016
Validation Loss: 1.9802
Epoch [7/50], Class Loss: 0.0469, Discrepancy Loss: 0.0012
Validation Loss: 3.6453
Epoch [8/50], Class Loss: 0.0234, Discrepancy Loss: 0.0014
Validation Loss: 0.5482
Early stopping!
Source Domain Performance - Accuracy: 94.36%, Precision: 94.64%, Recall: 94.31%, F1 Score: 94.28%
Target Domain Performance - Accuracy: 95.56%, Precision: 95.77%, Recall: 95.57%, F1 Score: 95.56%

Source performance: 87.80% 87.50% 88.15% 85.72%
Target performance: 88.13% 87.74% 88.17% 85.79%

Per-Class Accuracy on Target Domain:
bpsk: 71.34%
qpsk: 99.20%
16qam: 83.60%
8apsk: 98.56%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.3378, JMMD Loss: 0.0410
Validation Loss: 3.4598
Epoch [2/50], Class Loss: 0.1000, JMMD Loss: 0.0313
Validation Loss: 4.6508
Epoch [3/50], Class Loss: 0.1154, JMMD Loss: 0.0339
Validation Loss: 0.0878
Epoch [4/50], Class Loss: 0.1304, JMMD Loss: 0.0349
Validation Loss: 0.5864
Epoch [5/50], Class Loss: 0.1753, JMMD Loss: 0.0383
Validation Loss: 0.2816
Epoch [6/50], Class Loss: 0.1266, JMMD Loss: 0.0406
Validation Loss: 0.0104
Epoch [7/50], Class Loss: 0.0297, JMMD Loss: 0.0415
Validation Loss: 0.0155
Epoch [8/50], Class Loss: 0.0210, JMMD Loss: 0.0385
Validation Loss: 0.0049
Epoch [9/50], Class Loss: 0.0758, JMMD Loss: 0.0419
Validation Loss: 0.9173
Epoch [10/50], Class Loss: 0.0904, JMMD Loss: 0.0367
Validation Loss: 0.0067
Epoch [11/50], Class Loss: 0.0238, JMMD Loss: 0.0332
Validation Loss: 0.0054
Epoch [12/50], Class Loss: 0.0151, JMMD Loss: 0.0409
Validation Loss: 0.0052
Epoch [13/50], Class Loss: 0.0112, JMMD Loss: 0.0359
Validation Loss: 0.0073
Early stopping!
Source Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 2/10
Epoch [1/50], Class Loss: 0.2925, JMMD Loss: 0.0310
Validation Loss: 0.5247
Epoch [2/50], Class Loss: 0.1218, JMMD Loss: 0.0335
Validation Loss: 2.1837
Epoch [3/50], Class Loss: 0.2853, JMMD Loss: 0.0277
Validation Loss: 0.4771
Epoch [4/50], Class Loss: 0.0710, JMMD Loss: 0.0311
Validation Loss: 0.1569
Epoch [5/50], Class Loss: 0.0391, JMMD Loss: 0.0284
Validation Loss: 0.0125
Epoch [6/50], Class Loss: 0.0262, JMMD Loss: 0.0247
Validation Loss: 0.0209
Epoch [7/50], Class Loss: 0.0253, JMMD Loss: 0.0268
Validation Loss: 0.0045
Epoch [8/50], Class Loss: 0.0209, JMMD Loss: 0.0297
Validation Loss: 0.0089
Epoch [9/50], Class Loss: 0.0217, JMMD Loss: 0.0332
Validation Loss: 0.0078
Epoch [10/50], Class Loss: 0.0166, JMMD Loss: 0.0340
Validation Loss: 0.0096
Epoch [11/50], Class Loss: 0.0095, JMMD Loss: 0.0318
Validation Loss: 0.0031
Epoch [12/50], Class Loss: 0.0059, JMMD Loss: 0.0378
Validation Loss: 0.0046
Epoch [13/50], Class Loss: 0.0056, JMMD Loss: 0.0303
Validation Loss: 0.0020
Epoch [14/50], Class Loss: 0.0060, JMMD Loss: 0.0291
Validation Loss: 0.0029
Epoch [15/50], Class Loss: 0.0096, JMMD Loss: 0.0290
Validation Loss: 0.0042
Epoch [16/50], Class Loss: 0.0052, JMMD Loss: 0.0255
Validation Loss: 0.0023
Epoch [17/50], Class Loss: 0.0054, JMMD Loss: 0.0343
Validation Loss: 0.0012
Epoch [18/50], Class Loss: 0.0034, JMMD Loss: 0.0284
Validation Loss: 0.0012
Epoch [19/50], Class Loss: 0.0039, JMMD Loss: 0.0300
Validation Loss: 0.0013
Epoch [20/50], Class Loss: 0.0028, JMMD Loss: 0.0233
Validation Loss: 0.0011
Epoch [21/50], Class Loss: 0.0035, JMMD Loss: 0.0313
Validation Loss: 0.0014
Epoch [22/50], Class Loss: 0.0024, JMMD Loss: 0.0318
Validation Loss: 0.0018
Epoch [23/50], Class Loss: 0.0023, JMMD Loss: 0.0395
Validation Loss: 0.0023
Epoch [24/50], Class Loss: 0.0058, JMMD Loss: 0.0377
Validation Loss: 0.0015
Epoch [25/50], Class Loss: 0.0468, JMMD Loss: 0.0373
Validation Loss: 0.0028
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 3/10
Epoch [1/50], Class Loss: 0.4020, JMMD Loss: 0.0374
Validation Loss: 4.3143
Epoch [2/50], Class Loss: 0.0633, JMMD Loss: 0.0516
Validation Loss: 0.0371
Epoch [3/50], Class Loss: 0.0635, JMMD Loss: 0.0326
Validation Loss: 1.5225
Epoch [4/50], Class Loss: 0.0377, JMMD Loss: 0.0316
Validation Loss: 0.0871
Epoch [5/50], Class Loss: 0.0571, JMMD Loss: 0.0393
Validation Loss: 0.0175
Epoch [6/50], Class Loss: 0.0144, JMMD Loss: 0.0378
Validation Loss: 0.9133
Epoch [7/50], Class Loss: 0.0312, JMMD Loss: 0.0441
Validation Loss: 0.7148
Epoch [8/50], Class Loss: 0.1136, JMMD Loss: 0.0331
Validation Loss: 0.2698
Epoch [9/50], Class Loss: 0.0660, JMMD Loss: 0.0352
Validation Loss: 0.0623
Epoch [10/50], Class Loss: 0.0259, JMMD Loss: 0.0339
Validation Loss: 0.0170
Epoch [11/50], Class Loss: 0.0161, JMMD Loss: 0.0304
Validation Loss: 0.0051
Epoch [12/50], Class Loss: 0.0136, JMMD Loss: 0.0346
Validation Loss: 0.0073
Epoch [13/50], Class Loss: 0.0105, JMMD Loss: 0.0371
Validation Loss: 0.0042
Epoch [14/50], Class Loss: 0.0091, JMMD Loss: 0.0391
Validation Loss: 0.0040
Epoch [15/50], Class Loss: 0.0068, JMMD Loss: 0.0411
Validation Loss: 0.0029
Epoch [16/50], Class Loss: 0.0117, JMMD Loss: 0.0390
Validation Loss: 0.0067
Epoch [17/50], Class Loss: 0.0077, JMMD Loss: 0.0276
Validation Loss: 0.0049
Epoch [18/50], Class Loss: 0.0044, JMMD Loss: 0.0277
Validation Loss: 0.0027
Epoch [19/50], Class Loss: 0.0061, JMMD Loss: 0.0324
Validation Loss: 0.0046
Epoch [20/50], Class Loss: 0.0183, JMMD Loss: 0.0402
Validation Loss: 0.0020
Epoch [21/50], Class Loss: 0.0096, JMMD Loss: 0.0411
Validation Loss: 0.0028
Epoch [22/50], Class Loss: 0.0063, JMMD Loss: 0.0337
Validation Loss: 0.0026
Epoch [23/50], Class Loss: 0.0034, JMMD Loss: 0.0311
Validation Loss: 0.0012
Epoch [24/50], Class Loss: 0.0038, JMMD Loss: 0.0347
Validation Loss: 0.0016
Epoch [25/50], Class Loss: 0.0032, JMMD Loss: 0.0321
Validation Loss: 0.0012
Epoch [26/50], Class Loss: 0.0037, JMMD Loss: 0.0308
Validation Loss: 0.0012
Epoch [27/50], Class Loss: 0.0035, JMMD Loss: 0.0331
Validation Loss: 0.0013
Epoch [28/50], Class Loss: 0.0138, JMMD Loss: 0.0509
Validation Loss: 0.0014
Epoch [29/50], Class Loss: 0.0041, JMMD Loss: 0.0367
Validation Loss: 0.0012
Epoch [30/50], Class Loss: 0.0064, JMMD Loss: 0.0409
Validation Loss: 0.0020
Epoch [31/50], Class Loss: 0.0027, JMMD Loss: 0.0331
Validation Loss: 0.0012
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 4/10
Epoch [1/50], Class Loss: 0.4626, JMMD Loss: 0.0357
Validation Loss: 0.4860
Epoch [2/50], Class Loss: 0.0499, JMMD Loss: 0.0274
Validation Loss: 1.8061
Epoch [3/50], Class Loss: 0.0657, JMMD Loss: 0.0279
Validation Loss: 0.9980
Epoch [4/50], Class Loss: 0.0766, JMMD Loss: 0.0349
Validation Loss: 1.1226
Epoch [5/50], Class Loss: 0.1235, JMMD Loss: 0.0338
Validation Loss: 0.2174
Epoch [6/50], Class Loss: 0.1725, JMMD Loss: 0.0300
Validation Loss: 0.1654
Epoch [7/50], Class Loss: 0.0713, JMMD Loss: 0.0370
Validation Loss: 1.0553
Epoch [8/50], Class Loss: 0.0392, JMMD Loss: 0.0302
Validation Loss: 0.0791
Epoch [9/50], Class Loss: 0.0218, JMMD Loss: 0.0402
Validation Loss: 0.0054
Epoch [10/50], Class Loss: 0.0402, JMMD Loss: 0.0386
Validation Loss: 0.0610
Epoch [11/50], Class Loss: 0.0153, JMMD Loss: 0.0400
Validation Loss: 0.0033
Epoch [12/50], Class Loss: 0.0260, JMMD Loss: 0.0373
Validation Loss: 0.0035
Epoch [13/50], Class Loss: 0.0125, JMMD Loss: 0.0345
Validation Loss: 0.0034
Epoch [14/50], Class Loss: 0.0131, JMMD Loss: 0.0353
Validation Loss: 0.0027
Epoch [15/50], Class Loss: 0.0141, JMMD Loss: 0.0423
Validation Loss: 0.0027
Epoch [16/50], Class Loss: 0.0061, JMMD Loss: 0.0373
Validation Loss: 0.0029
Epoch [17/50], Class Loss: 0.0077, JMMD Loss: 0.0427
Validation Loss: 0.0022
Epoch [18/50], Class Loss: 0.0066, JMMD Loss: 0.0391
Validation Loss: 0.0032
Epoch [19/50], Class Loss: 0.0063, JMMD Loss: 0.0374
Validation Loss: 0.0028
Epoch [20/50], Class Loss: 0.0074, JMMD Loss: 0.0385
Validation Loss: 0.0025
Epoch [21/50], Class Loss: 0.0043, JMMD Loss: 0.0325
Validation Loss: 0.0022
Epoch [22/50], Class Loss: 0.0049, JMMD Loss: 0.0468
Validation Loss: 0.0021
Epoch [23/50], Class Loss: 0.0035, JMMD Loss: 0.0385
Validation Loss: 0.0021
Epoch [24/50], Class Loss: 0.0057, JMMD Loss: 0.0452
Validation Loss: 0.0019
Epoch [25/50], Class Loss: 0.0049, JMMD Loss: 0.0656
Validation Loss: 0.0023
Epoch [26/50], Class Loss: 0.0033, JMMD Loss: 0.0351
Validation Loss: 0.0023
Epoch [27/50], Class Loss: 0.0115, JMMD Loss: 0.0356
Validation Loss: 0.0027
Epoch [28/50], Class Loss: 0.0128, JMMD Loss: 0.0399
Validation Loss: 0.0043
Epoch [29/50], Class Loss: 0.0929, JMMD Loss: 0.0450
Validation Loss: 0.0032
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 5/10
Epoch [1/50], Class Loss: 0.4070, JMMD Loss: 0.0356
Validation Loss: 4.5103
Epoch [2/50], Class Loss: 0.1191, JMMD Loss: 0.0341
Validation Loss: 2.6959
Epoch [3/50], Class Loss: 0.0757, JMMD Loss: 0.0256
Validation Loss: 1.3220
Epoch [4/50], Class Loss: 0.0231, JMMD Loss: 0.0367
Validation Loss: 0.1472
Epoch [5/50], Class Loss: 0.0309, JMMD Loss: 0.0320
Validation Loss: 0.0208
Epoch [6/50], Class Loss: 0.0306, JMMD Loss: 0.0303
Validation Loss: 2.1809
Epoch [7/50], Class Loss: 0.1565, JMMD Loss: 0.0275
Validation Loss: 9.9136
Epoch [8/50], Class Loss: 0.1719, JMMD Loss: 0.0402
Validation Loss: 0.1434
Epoch [9/50], Class Loss: 0.1388, JMMD Loss: 0.0353
Validation Loss: 0.7066
Epoch [10/50], Class Loss: 0.0304, JMMD Loss: 0.0376
Validation Loss: 0.0064
Epoch [11/50], Class Loss: 0.0688, JMMD Loss: 0.0447
Validation Loss: 0.0031
Epoch [12/50], Class Loss: 0.0143, JMMD Loss: 0.0300
Validation Loss: 0.0049
Epoch [13/50], Class Loss: 0.0162, JMMD Loss: 0.0341
Validation Loss: 0.0113
Epoch [14/50], Class Loss: 0.0103, JMMD Loss: 0.0479
Validation Loss: 0.0024
Epoch [15/50], Class Loss: 0.0052, JMMD Loss: 0.0521
Validation Loss: 0.0026
Epoch [16/50], Class Loss: 0.0071, JMMD Loss: 0.0473
Validation Loss: 0.0015
Epoch [17/50], Class Loss: 0.0070, JMMD Loss: 0.0431
Validation Loss: 0.0023
Epoch [18/50], Class Loss: 0.0037, JMMD Loss: 0.0362
Validation Loss: 0.0034
Epoch [19/50], Class Loss: 0.0044, JMMD Loss: 0.0421
Validation Loss: 0.0013
Epoch [20/50], Class Loss: 0.0068, JMMD Loss: 0.0411
Validation Loss: 0.0014
Epoch [21/50], Class Loss: 0.0138, JMMD Loss: 0.0393
Validation Loss: 0.0011
Epoch [22/50], Class Loss: 0.0112, JMMD Loss: 0.0465
Validation Loss: 0.0011
Epoch [23/50], Class Loss: 0.0072, JMMD Loss: 0.0344
Validation Loss: 0.0015
Epoch [24/50], Class Loss: 0.0054, JMMD Loss: 0.0426
Validation Loss: 0.0015
Epoch [25/50], Class Loss: 0.0031, JMMD Loss: 0.0479
Validation Loss: 0.0015
Epoch [26/50], Class Loss: 0.0031, JMMD Loss: 0.0380
Validation Loss: 0.0014
Epoch [27/50], Class Loss: 0.0032, JMMD Loss: 0.0381
Validation Loss: 0.0026
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 6/10
Epoch [1/50], Class Loss: 0.3300, JMMD Loss: 0.0437
Validation Loss: 12.2087
Epoch [2/50], Class Loss: 0.0701, JMMD Loss: 0.0320
Validation Loss: 0.2621
Epoch [3/50], Class Loss: 0.0462, JMMD Loss: 0.0402
Validation Loss: 0.0278
Epoch [4/50], Class Loss: 0.0217, JMMD Loss: 0.0323
Validation Loss: 0.0076
Epoch [5/50], Class Loss: 0.0270, JMMD Loss: 0.0358
Validation Loss: 0.7156
Epoch [6/50], Class Loss: 0.0095, JMMD Loss: 0.0375
Validation Loss: 0.0014
Epoch [7/50], Class Loss: 0.0066, JMMD Loss: 0.0405
Validation Loss: 0.0054
Epoch [8/50], Class Loss: 0.0117, JMMD Loss: 0.0360
Validation Loss: 1.5364
Epoch [9/50], Class Loss: 0.2543, JMMD Loss: 0.0364
Validation Loss: 0.6878
Epoch [10/50], Class Loss: 0.1388, JMMD Loss: 0.0372
Validation Loss: 0.0200
Epoch [11/50], Class Loss: 0.0207, JMMD Loss: 0.0295
Validation Loss: 0.0113
Early stopping!
Source Domain Performance - Accuracy: 99.70%, Precision: 99.70%, Recall: 99.70%, F1 Score: 99.70%
Target Domain Performance - Accuracy: 99.52%, Precision: 99.52%, Recall: 99.52%, F1 Score: 99.52%

Run 7/10
Epoch [1/50], Class Loss: 0.4307, JMMD Loss: 0.0291
Validation Loss: 2.3029
Epoch [2/50], Class Loss: 0.1117, JMMD Loss: 0.0325
Validation Loss: 0.1042
Epoch [3/50], Class Loss: 0.0291, JMMD Loss: 0.0344
Validation Loss: 0.0314
Epoch [4/50], Class Loss: 0.0229, JMMD Loss: 0.0336
Validation Loss: 0.0356
Epoch [5/50], Class Loss: 0.0950, JMMD Loss: 0.0346
Validation Loss: 1.1472
Epoch [6/50], Class Loss: 0.1663, JMMD Loss: 0.0384
Validation Loss: 1.6151
Epoch [7/50], Class Loss: 0.0508, JMMD Loss: 0.0371
Validation Loss: 0.0132
Epoch [8/50], Class Loss: 0.0200, JMMD Loss: 0.0315
Validation Loss: 0.0076
Epoch [9/50], Class Loss: 0.0227, JMMD Loss: 0.0428
Validation Loss: 0.0603
Epoch [10/50], Class Loss: 0.0992, JMMD Loss: 0.0347
Validation Loss: 2.3306
Epoch [11/50], Class Loss: 0.2483, JMMD Loss: 0.0555
Validation Loss: 0.0158
Epoch [12/50], Class Loss: 0.0209, JMMD Loss: 0.0371
Validation Loss: 0.0087
Epoch [13/50], Class Loss: 0.0143, JMMD Loss: 0.0390
Validation Loss: 0.0071
Epoch [14/50], Class Loss: 0.0326, JMMD Loss: 0.0414
Validation Loss: 0.0068
Epoch [15/50], Class Loss: 0.0159, JMMD Loss: 0.0411
Validation Loss: 0.0067
Epoch [16/50], Class Loss: 0.0085, JMMD Loss: 0.0307
Validation Loss: 0.0048
Epoch [17/50], Class Loss: 0.0067, JMMD Loss: 0.0338
Validation Loss: 0.0045
Epoch [18/50], Class Loss: 0.0094, JMMD Loss: 0.0421
Validation Loss: 0.0047
Epoch [19/50], Class Loss: 0.0103, JMMD Loss: 0.0407
Validation Loss: 0.0068
Epoch [20/50], Class Loss: 0.0056, JMMD Loss: 0.0299
Validation Loss: 0.0039
Epoch [21/50], Class Loss: 0.0052, JMMD Loss: 0.0388
Validation Loss: 0.0036
Epoch [22/50], Class Loss: 0.0188, JMMD Loss: 0.0397
Validation Loss: 0.0042
Epoch [23/50], Class Loss: 0.0049, JMMD Loss: 0.0339
Validation Loss: 0.0037
Epoch [24/50], Class Loss: 0.0054, JMMD Loss: 0.0412
Validation Loss: 0.0042
Epoch [25/50], Class Loss: 0.0048, JMMD Loss: 0.0399
Validation Loss: 0.0036
Epoch [26/50], Class Loss: 0.0047, JMMD Loss: 0.0403
Validation Loss: 0.0035
Epoch [27/50], Class Loss: 0.0047, JMMD Loss: 0.0365
Validation Loss: 0.0037
Epoch [28/50], Class Loss: 0.0041, JMMD Loss: 0.0387
Validation Loss: 0.0035
Epoch [29/50], Class Loss: 0.0049, JMMD Loss: 0.0357
Validation Loss: 0.0037
Epoch [30/50], Class Loss: 0.0083, JMMD Loss: 0.0441
Validation Loss: 0.0039
Epoch [31/50], Class Loss: 0.0093, JMMD Loss: 0.0381
Validation Loss: 0.0037
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 8/10
Epoch [1/50], Class Loss: 0.2982, JMMD Loss: 0.0392
Validation Loss: 0.1684
Epoch [2/50], Class Loss: 0.0450, JMMD Loss: 0.0380
Validation Loss: 0.0170
Epoch [3/50], Class Loss: 0.0287, JMMD Loss: 0.0404
Validation Loss: 0.0136
Epoch [4/50], Class Loss: 0.0274, JMMD Loss: 0.0329
Validation Loss: 0.1000
Epoch [5/50], Class Loss: 0.1972, JMMD Loss: 0.0448
Validation Loss: 0.5830
Epoch [6/50], Class Loss: 0.1160, JMMD Loss: 0.0419
Validation Loss: 0.0862
Epoch [7/50], Class Loss: 0.0780, JMMD Loss: 0.0450
Validation Loss: 0.3854
Epoch [8/50], Class Loss: 0.1630, JMMD Loss: 0.0378
Validation Loss: 0.7809
Early stopping!
Source Domain Performance - Accuracy: 75.96%, Precision: 82.60%, Recall: 75.53%, F1 Score: 73.13%
Target Domain Performance - Accuracy: 77.10%, Precision: 83.78%, Recall: 77.09%, F1 Score: 74.86%

Run 9/10
Epoch [1/50], Class Loss: 0.3912, JMMD Loss: 0.0353
Validation Loss: 2.1359
Epoch [2/50], Class Loss: 0.0633, JMMD Loss: 0.0328
Validation Loss: 1.3914
Epoch [3/50], Class Loss: 0.0506, JMMD Loss: 0.0383
Validation Loss: 0.0617
Epoch [4/50], Class Loss: 0.0531, JMMD Loss: 0.0258
Validation Loss: 0.0083
Epoch [5/50], Class Loss: 0.0294, JMMD Loss: 0.0341
Validation Loss: 0.0050
Epoch [6/50], Class Loss: 0.0546, JMMD Loss: 0.0384
Validation Loss: 0.0401
Epoch [7/50], Class Loss: 0.0982, JMMD Loss: 0.0420
Validation Loss: 1.3226
Epoch [8/50], Class Loss: 0.0563, JMMD Loss: 0.0286
Validation Loss: 0.0153
Epoch [9/50], Class Loss: 0.0247, JMMD Loss: 0.0406
Validation Loss: 1.6409
Epoch [10/50], Class Loss: 0.1049, JMMD Loss: 0.0397
Validation Loss: 0.0072
Early stopping!
Source Domain Performance - Accuracy: 99.82%, Precision: 99.83%, Recall: 99.82%, F1 Score: 99.82%
Target Domain Performance - Accuracy: 99.58%, Precision: 99.58%, Recall: 99.58%, F1 Score: 99.58%

Run 10/10
Epoch [1/50], Class Loss: 0.3754, JMMD Loss: 0.0374
Validation Loss: 1.1341
Epoch [2/50], Class Loss: 0.1310, JMMD Loss: 0.0342
Validation Loss: 0.0532
Epoch [3/50], Class Loss: 0.0401, JMMD Loss: 0.0417
Validation Loss: 0.0546
Epoch [4/50], Class Loss: 0.0209, JMMD Loss: 0.0376
Validation Loss: 0.0975
Epoch [5/50], Class Loss: 0.0849, JMMD Loss: 0.0353
Validation Loss: 0.0580
Epoch [6/50], Class Loss: 0.0268, JMMD Loss: 0.0333
Validation Loss: 1.2213
Epoch [7/50], Class Loss: 0.1045, JMMD Loss: 0.0249
Validation Loss: 0.0166
Epoch [8/50], Class Loss: 0.0315, JMMD Loss: 0.0340
Validation Loss: 0.1762
Epoch [9/50], Class Loss: 0.0190, JMMD Loss: 0.0305
Validation Loss: 0.0197
Epoch [10/50], Class Loss: 0.0234, JMMD Loss: 0.0388
Validation Loss: 0.0082
Epoch [11/50], Class Loss: 0.0133, JMMD Loss: 0.0352
Validation Loss: 0.0039
Epoch [12/50], Class Loss: 0.0103, JMMD Loss: 0.0477
Validation Loss: 0.0037
Epoch [13/50], Class Loss: 0.0601, JMMD Loss: 0.0392
Validation Loss: 0.0032
Epoch [14/50], Class Loss: 0.0260, JMMD Loss: 0.0386
Validation Loss: 0.0041
Epoch [15/50], Class Loss: 0.0099, JMMD Loss: 0.0380
Validation Loss: 0.0031
Epoch [16/50], Class Loss: 0.0074, JMMD Loss: 0.0397
Validation Loss: 0.0031
Epoch [17/50], Class Loss: 0.0057, JMMD Loss: 0.0406
Validation Loss: 0.0027
Epoch [18/50], Class Loss: 0.0058, JMMD Loss: 0.0357
Validation Loss: 0.0032
Epoch [19/50], Class Loss: 0.0164, JMMD Loss: 0.0448
Validation Loss: 0.0023
Epoch [20/50], Class Loss: 0.0141, JMMD Loss: 0.0425
Validation Loss: 0.0033
Epoch [21/50], Class Loss: 0.0397, JMMD Loss: 0.0463
Validation Loss: 0.0040
Epoch [22/50], Class Loss: 0.0064, JMMD Loss: 0.0452
Validation Loss: 0.0030
Epoch [23/50], Class Loss: 0.0058, JMMD Loss: 0.0445
Validation Loss: 0.0029
Epoch [24/50], Class Loss: 0.0053, JMMD Loss: 0.0428
Validation Loss: 0.0026
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Source performance: 97.50% 98.16% 97.46% 97.22%
Target performance: 97.58% 98.25% 97.58% 97.36%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 99.98%
  Class 1: 99.76%
  Class 2: 97.67%
  Class 3: 92.93%
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=5a75f4d8-b552-4e01-8024-24f80cc6eaa3">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_snr</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_deep_coral_acc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_base_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Base'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_dann_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'DANN'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_star_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Star'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_mcd_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'D'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'MCD'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'v'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'DCORAL'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_jan_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'x'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'JANN'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'SNR (dB)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Acc (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'OTA SNR-to-SNR DA'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>6
6
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=fe817268-bf24-4e1c-8b14-62914fd2e267">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [4]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="c1"># Load testbed data</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"/home/ash/ic3/testbed_da/data"</span>

<span class="c1"># Classes</span>
<span class="n">class_subset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bpsk"</span><span class="p">,</span> <span class="s2">"qpsk"</span><span class="p">,</span> <span class="s2">"16qam"</span><span class="p">,</span> <span class="s2">"8apsk"</span><span class="p">]</span>

<span class="c1"># Split source, target</span>
<span class="c1"># try selecting some of the mods, not all</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_X.npy"</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_Y.npy"</span><span class="p">)</span>

<span class="n">sou_snr</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">tar_snr</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">t_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_base_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_dann_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_mcd_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_star_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_jan_acc</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_epochs</span><span class="o">=</span> <span class="mi">50</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">n_snr</span><span class="o">=</span> <span class="mi">4</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_snr</span><span class="p">):</span>    
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">sou_snr</span><span class="p">)</span>
    <span class="n">target_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">tar_snr</span><span class="p">)</span>
    
    <span class="n">X_s</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_s</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="n">X_t</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">target_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">target_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_t</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>

    
    <span class="c1"># Dataloaders</span>
    <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">Y_s</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_t</span><span class="p">,</span> <span class="n">Y_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'Base'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_base</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">Base</span><span class="p">(</span>
        <span class="n">model_cls</span><span class="o">=</span><span class="n">DeepResNet</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span> 
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span> 
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_base_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_base</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'DANN'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_dann</span> <span class="o">=</span> <span class="n">dann</span><span class="o">.</span><span class="n">DAN</span><span class="p">(</span>
        <span class="n">dann</span><span class="o">.</span><span class="n">DANN</span><span class="p">,</span>
        <span class="n">FA</span><span class="o">=</span><span class="n">DANN_F</span><span class="p">,</span>
        <span class="n">LP</span><span class="o">=</span><span class="n">DANN_LP</span><span class="p">,</span>
        <span class="n">DC</span><span class="o">=</span><span class="n">DANN_DC</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_dann_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_dann</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'Deep CORAL'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_deep</span> <span class="o">=</span> <span class="n">DeepCORAL</span><span class="p">(</span>
        <span class="n">G</span><span class="o">=</span><span class="n">CORAL_G</span><span class="p">,</span> 
        <span class="n">C</span><span class="o">=</span><span class="n">CORAL_C</span><span class="p">,</span> 
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> 
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> 
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> 
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">lambda_coral</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">deep_weights</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_deep_coral_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_deep</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'STAR'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_star</span> <span class="o">=</span>  <span class="n">star</span><span class="o">.</span><span class="n">Star</span><span class="p">(</span>
        <span class="n">G</span><span class="o">=</span><span class="n">STAR_G</span><span class="p">,</span>
        <span class="n">C</span><span class="o">=</span><span class="n">STAR_C</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>  
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_star_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_star</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'MCD'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_mcd</span> <span class="o">=</span> <span class="n">mcd</span><span class="o">.</span><span class="n">Mcd</span><span class="p">(</span>
        <span class="n">G</span><span class="o">=</span><span class="n">MCD_G</span><span class="p">,</span>
        <span class="n">C</span><span class="o">=</span><span class="n">MCD_C</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>  
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_mcd_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_mcd</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'JAN'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_jan</span> <span class="o">=</span> <span class="n">jan</span><span class="o">.</span><span class="n">Jan</span><span class="p">(</span>
        <span class="n">C</span><span class="o">=</span><span class="n">C_JAN</span><span class="p">,</span>
        <span class="n">G</span><span class="o">=</span><span class="n">JAN_G</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">lambda_jmmd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_jan_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_jan</span><span class="p">)</span>
    
    <span class="n">tar_snr</span> <span class="o">+=</span> <span class="mi">4</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Base

Run 1/10
Epoch 1/50, Train Loss: 0.3820, Train Acc: 0.8464, Val Loss: 2.3482, Val Acc: 0.7650
Epoch 2/50, Train Loss: 0.1123, Train Acc: 0.9610, Val Loss: 0.4687, Val Acc: 0.8501
Epoch 3/50, Train Loss: 0.0435, Train Acc: 0.9858, Val Loss: 0.5571, Val Acc: 0.8489
Epoch 4/50, Train Loss: 0.0202, Train Acc: 0.9936, Val Loss: 1.0855, Val Acc: 0.7092
Epoch 5/50, Train Loss: 0.0207, Train Acc: 0.9934, Val Loss: 0.0150, Val Acc: 0.9958
Epoch 6/50, Train Loss: 0.0102, Train Acc: 0.9975, Val Loss: 0.0151, Val Acc: 0.9964
Epoch 7/50, Train Loss: 0.0024, Train Acc: 0.9994, Val Loss: 0.0112, Val Acc: 0.9958
Epoch 8/50, Train Loss: 0.0047, Train Acc: 0.9984, Val Loss: 0.0256, Val Acc: 0.9904
Epoch 9/50, Train Loss: 0.0080, Train Acc: 0.9981, Val Loss: 0.0551, Val Acc: 0.9808
Epoch 10/50, Train Loss: 0.0036, Train Acc: 0.9993, Val Loss: 0.0140, Val Acc: 0.9940
Epoch 11/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0035, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0022, Train Acc: 0.9999, Val Loss: 0.0033, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0026, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0041, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0032, Val Acc: 0.9988
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9988
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988
Epoch 22/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 23/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0022, Val Acc: 0.9988
Early stopping!

Run 2/10
Epoch 1/50, Train Loss: 0.3940, Train Acc: 0.8425, Val Loss: 0.3968, Val Acc: 0.8915
Epoch 2/50, Train Loss: 0.0621, Train Acc: 0.9790, Val Loss: 0.5206, Val Acc: 0.8405
Epoch 3/50, Train Loss: 0.0235, Train Acc: 0.9921, Val Loss: 0.0218, Val Acc: 0.9928
Epoch 4/50, Train Loss: 0.0122, Train Acc: 0.9963, Val Loss: 0.1206, Val Acc: 0.9610
Epoch 5/50, Train Loss: 0.0057, Train Acc: 0.9982, Val Loss: 0.9856, Val Acc: 0.8459
Epoch 6/50, Train Loss: 0.0106, Train Acc: 0.9964, Val Loss: 0.2888, Val Acc: 0.9221
Epoch 7/50, Train Loss: 0.0150, Train Acc: 0.9951, Val Loss: 0.0118, Val Acc: 0.9958
Epoch 8/50, Train Loss: 0.0040, Train Acc: 0.9991, Val Loss: 0.0217, Val Acc: 0.9940
Epoch 9/50, Train Loss: 0.0036, Train Acc: 0.9993, Val Loss: 0.0194, Val Acc: 0.9934
Epoch 10/50, Train Loss: 0.0037, Train Acc: 0.9988, Val Loss: 0.0522, Val Acc: 0.9826
Epoch 11/50, Train Loss: 0.0010, Train Acc: 0.9997, Val Loss: 0.0041, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0044, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0032, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0042, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0035, Val Acc: 0.9982
Epoch 16/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0038, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0023, Val Acc: 0.9982
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9982
Epoch 19/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0031, Val Acc: 0.9988
Epoch 20/50, Train Loss: 0.0034, Train Acc: 0.9997, Val Loss: 0.0041, Val Acc: 0.9988
Epoch 21/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9988
Epoch 22/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0034, Val Acc: 0.9988
Early stopping!

Run 3/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4216, Train Acc: 0.8308, Val Loss: 3.7897, Val Acc: 0.6835
Epoch 2/50, Train Loss: 0.1014, Train Acc: 0.9640, Val Loss: 0.4131, Val Acc: 0.8675
Epoch 3/50, Train Loss: 0.0310, Train Acc: 0.9912, Val Loss: 0.3290, Val Acc: 0.9035
Epoch 4/50, Train Loss: 0.0112, Train Acc: 0.9976, Val Loss: 0.4464, Val Acc: 0.8483
Epoch 5/50, Train Loss: 0.0039, Train Acc: 0.9991, Val Loss: 0.4705, Val Acc: 0.8585
Epoch 6/50, Train Loss: 0.0055, Train Acc: 0.9985, Val Loss: 10.6719, Val Acc: 0.4988
Epoch 7/50, Train Loss: 0.0078, Train Acc: 0.9981, Val Loss: 0.0022, Val Acc: 0.9994
Epoch 8/50, Train Loss: 0.0041, Train Acc: 0.9991, Val Loss: 0.0114, Val Acc: 0.9964
Epoch 9/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 10/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0004, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0014, Train Acc: 0.9993, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0004, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Early stopping!

Run 4/10
Epoch 1/50, Train Loss: 0.4225, Train Acc: 0.8247, Val Loss: 4.1346, Val Acc: 0.5294
Epoch 2/50, Train Loss: 0.1087, Train Acc: 0.9619, Val Loss: 1.5606, Val Acc: 0.7170
Epoch 3/50, Train Loss: 0.0449, Train Acc: 0.9850, Val Loss: 3.8955, Val Acc: 0.6109
Epoch 4/50, Train Loss: 0.0244, Train Acc: 0.9925, Val Loss: 6.0029, Val Acc: 0.5048
Epoch 5/50, Train Loss: 0.0037, Train Acc: 0.9997, Val Loss: 0.0126, Val Acc: 0.9958
Epoch 6/50, Train Loss: 0.0061, Train Acc: 0.9987, Val Loss: 0.0111, Val Acc: 0.9970
Epoch 7/50, Train Loss: 0.0091, Train Acc: 0.9969, Val Loss: 0.0546, Val Acc: 0.9838
Epoch 8/50, Train Loss: 0.0110, Train Acc: 0.9970, Val Loss: 3.8840, Val Acc: 0.5006
Epoch 9/50, Train Loss: 0.0024, Train Acc: 0.9997, Val Loss: 0.0177, Val Acc: 0.9946
Epoch 10/50, Train Loss: 0.0036, Train Acc: 0.9994, Val Loss: 0.4930, Val Acc: 0.8597
Epoch 11/50, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0018, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0010, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 32/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 33/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 34/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 35/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 36/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 37/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 38/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 39/50, Train Loss: 0.0042, Train Acc: 0.9996, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 40/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 5/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3807, Train Acc: 0.8514, Val Loss: 3.1374, Val Acc: 0.7128
Epoch 2/50, Train Loss: 0.0902, Train Acc: 0.9682, Val Loss: 2.4593, Val Acc: 0.6918
Epoch 3/50, Train Loss: 0.0385, Train Acc: 0.9871, Val Loss: 0.3241, Val Acc: 0.9053
Epoch 4/50, Train Loss: 0.0260, Train Acc: 0.9921, Val Loss: 16.1177, Val Acc: 0.4976
Epoch 5/50, Train Loss: 0.0194, Train Acc: 0.9939, Val Loss: 2.2224, Val Acc: 0.5629
Epoch 6/50, Train Loss: 0.0068, Train Acc: 0.9978, Val Loss: 0.6876, Val Acc: 0.7308
Epoch 7/50, Train Loss: 0.0051, Train Acc: 0.9987, Val Loss: 0.0398, Val Acc: 0.9886
Epoch 8/50, Train Loss: 0.0026, Train Acc: 0.9990, Val Loss: 0.0088, Val Acc: 0.9970
Epoch 9/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0046, Val Acc: 0.9982
Epoch 10/50, Train Loss: 0.0021, Train Acc: 0.9999, Val Loss: 0.0041, Val Acc: 0.9982
Epoch 11/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0036, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0039, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0019, Train Acc: 0.9997, Val Loss: 0.0035, Val Acc: 0.9982
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0035, Val Acc: 0.9982
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0028, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0028, Train Acc: 0.9996, Val Loss: 0.0031, Val Acc: 0.9988
Epoch 19/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0034, Val Acc: 0.9988
Epoch 20/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0037, Val Acc: 0.9988
Epoch 21/50, Train Loss: 0.0036, Train Acc: 0.9999, Val Loss: 0.0039, Val Acc: 0.9988
Early stopping!

Run 6/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4137, Train Acc: 0.8281, Val Loss: 1.7389, Val Acc: 0.7560
Epoch 2/50, Train Loss: 0.1079, Train Acc: 0.9622, Val Loss: 0.2859, Val Acc: 0.9059
Epoch 3/50, Train Loss: 0.0530, Train Acc: 0.9819, Val Loss: 1.0347, Val Acc: 0.7872
Epoch 4/50, Train Loss: 0.0270, Train Acc: 0.9919, Val Loss: 0.1144, Val Acc: 0.9598
Epoch 5/50, Train Loss: 0.0166, Train Acc: 0.9952, Val Loss: 0.1033, Val Acc: 0.9598
Epoch 6/50, Train Loss: 0.0109, Train Acc: 0.9967, Val Loss: 12.5898, Val Acc: 0.6871
Epoch 7/50, Train Loss: 0.0113, Train Acc: 0.9964, Val Loss: 0.0059, Val Acc: 0.9976
Epoch 8/50, Train Loss: 0.0052, Train Acc: 0.9985, Val Loss: 1.7388, Val Acc: 0.7536
Epoch 9/50, Train Loss: 0.0030, Train Acc: 0.9996, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 10/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 7/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4679, Train Acc: 0.8062, Val Loss: 16.0356, Val Acc: 0.2512
Epoch 2/50, Train Loss: 0.1509, Train Acc: 0.9394, Val Loss: 0.3044, Val Acc: 0.8771
Epoch 3/50, Train Loss: 0.0537, Train Acc: 0.9828, Val Loss: 0.2987, Val Acc: 0.8969
Epoch 4/50, Train Loss: 0.0151, Train Acc: 0.9957, Val Loss: 0.0270, Val Acc: 0.9898
Epoch 5/50, Train Loss: 0.0107, Train Acc: 0.9966, Val Loss: 0.2941, Val Acc: 0.9185
Epoch 6/50, Train Loss: 0.0139, Train Acc: 0.9954, Val Loss: 1.3920, Val Acc: 0.7770
Epoch 7/50, Train Loss: 0.0098, Train Acc: 0.9969, Val Loss: 0.0289, Val Acc: 0.9892
Epoch 8/50, Train Loss: 0.0094, Train Acc: 0.9978, Val Loss: 0.0856, Val Acc: 0.9946
Epoch 9/50, Train Loss: 0.0054, Train Acc: 0.9985, Val Loss: 0.0255, Val Acc: 0.9904
Epoch 10/50, Train Loss: 0.0037, Train Acc: 0.9988, Val Loss: 0.0089, Val Acc: 0.9958
Epoch 11/50, Train Loss: 0.0021, Train Acc: 0.9996, Val Loss: 0.0076, Val Acc: 0.9970
Epoch 12/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0078, Val Acc: 0.9976
Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0066, Val Acc: 0.9970
Epoch 14/50, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0074, Val Acc: 0.9976
Epoch 15/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0093, Val Acc: 0.9970
Epoch 16/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0069, Val Acc: 0.9964
Epoch 17/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0069, Val Acc: 0.9976
Epoch 18/50, Train Loss: 0.0046, Train Acc: 0.9997, Val Loss: 0.0075, Val Acc: 0.9976
Early stopping!

Run 8/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4253, Train Acc: 0.8347, Val Loss: 3.9403, Val Acc: 0.5528
Epoch 2/50, Train Loss: 0.0869, Train Acc: 0.9702, Val Loss: 0.6039, Val Acc: 0.8411
Epoch 3/50, Train Loss: 0.0715, Train Acc: 0.9757, Val Loss: 0.3173, Val Acc: 0.8885
Epoch 4/50, Train Loss: 0.0275, Train Acc: 0.9910, Val Loss: 0.3111, Val Acc: 0.8837
Epoch 5/50, Train Loss: 0.0137, Train Acc: 0.9966, Val Loss: 0.8450, Val Acc: 0.7752
Epoch 6/50, Train Loss: 0.0140, Train Acc: 0.9952, Val Loss: 0.1225, Val Acc: 0.9592
Epoch 7/50, Train Loss: 0.0105, Train Acc: 0.9972, Val Loss: 0.0176, Val Acc: 0.9916
Epoch 8/50, Train Loss: 0.0030, Train Acc: 0.9994, Val Loss: 0.0105, Val Acc: 0.9964
Epoch 9/50, Train Loss: 0.0014, Train Acc: 0.9996, Val Loss: 0.0081, Val Acc: 0.9982
Epoch 10/50, Train Loss: 0.0036, Train Acc: 0.9988, Val Loss: 3.8596, Val Acc: 0.7074
Epoch 11/50, Train Loss: 0.0035, Train Acc: 0.9994, Val Loss: 0.0088, Val Acc: 0.9964
Epoch 12/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0036, Val Acc: 0.9982
Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0022, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0020, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9994
Early stopping!

Run 9/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4239, Train Acc: 0.8221, Val Loss: 0.5578, Val Acc: 0.8177
Epoch 2/50, Train Loss: 0.1252, Train Acc: 0.9531, Val Loss: 0.2865, Val Acc: 0.8705
Epoch 3/50, Train Loss: 0.0668, Train Acc: 0.9751, Val Loss: 0.4466, Val Acc: 0.8327
Epoch 4/50, Train Loss: 0.0386, Train Acc: 0.9871, Val Loss: 1.2201, Val Acc: 0.7854
Epoch 5/50, Train Loss: 0.0223, Train Acc: 0.9928, Val Loss: 0.0327, Val Acc: 0.9904
Epoch 6/50, Train Loss: 0.0110, Train Acc: 0.9975, Val Loss: 0.1114, Val Acc: 0.9628
Epoch 7/50, Train Loss: 0.0051, Train Acc: 0.9987, Val Loss: 0.0134, Val Acc: 0.9958
Epoch 8/50, Train Loss: 0.0112, Train Acc: 0.9976, Val Loss: 5.6848, Val Acc: 0.5600
Epoch 9/50, Train Loss: 0.0082, Train Acc: 0.9985, Val Loss: 0.6367, Val Acc: 0.8753
Epoch 10/50, Train Loss: 0.0029, Train Acc: 0.9991, Val Loss: 0.0062, Val Acc: 0.9976
Epoch 11/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0033, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0025, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0013, Train Acc: 0.9996, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0027, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0018, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9994
Early stopping!

Run 10/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4167, Train Acc: 0.8359, Val Loss: 7.4548, Val Acc: 0.4934
Epoch 2/50, Train Loss: 0.1304, Train Acc: 0.9493, Val Loss: 0.0782, Val Acc: 0.9712
Epoch 3/50, Train Loss: 0.0494, Train Acc: 0.9832, Val Loss: 0.1502, Val Acc: 0.9496
Epoch 4/50, Train Loss: 0.0224, Train Acc: 0.9928, Val Loss: 0.0145, Val Acc: 0.9946
Epoch 5/50, Train Loss: 0.0175, Train Acc: 0.9948, Val Loss: 0.2575, Val Acc: 0.9215
Epoch 6/50, Train Loss: 0.0114, Train Acc: 0.9966, Val Loss: 0.0936, Val Acc: 0.9694
Epoch 7/50, Train Loss: 0.0047, Train Acc: 0.9987, Val Loss: 0.0491, Val Acc: 0.9814
Epoch 8/50, Train Loss: 0.0018, Train Acc: 0.9996, Val Loss: 0.0104, Val Acc: 0.9952
Epoch 9/50, Train Loss: 0.0050, Train Acc: 0.9985, Val Loss: 0.4941, Val Acc: 0.8771
Epoch 10/50, Train Loss: 0.0123, Train Acc: 0.9955, Val Loss: 0.2990, Val Acc: 0.9203
Epoch 11/50, Train Loss: 0.0048, Train Acc: 0.9991, Val Loss: 0.0096, Val Acc: 0.9982
Epoch 12/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0089, Val Acc: 0.9982
Epoch 13/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0059, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0071, Val Acc: 0.9982
Epoch 15/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0061, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0087, Val Acc: 0.9982
Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0063, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0058, Val Acc: 0.9988
Epoch 19/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0055, Val Acc: 0.9982
Epoch 20/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0060, Val Acc: 0.9982
Epoch 21/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0052, Val Acc: 0.9982
Epoch 22/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0070, Val Acc: 0.9982
Epoch 23/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0048, Val Acc: 0.9976
Epoch 24/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0061, Val Acc: 0.9982
Epoch 25/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0045, Val Acc: 0.9982
Epoch 26/50, Train Loss: 0.0055, Train Acc: 0.9997, Val Loss: 0.0062, Val Acc: 0.9982
Epoch 27/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0056, Val Acc: 0.9982
Epoch 28/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0055, Val Acc: 0.9982
Epoch 29/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0066, Val Acc: 0.9982
Epoch 30/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0049, Val Acc: 0.9982
Early stopping!

Source performance: 99.91 99.91 99.90 99.91
Target performance: 48.78 33.34 48.51 36.69

bpsk: 100.00
qpsk: 0.00
16qam: 4.80
8apsk: 89.24
DANN
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2964, Domain Loss: 1.3158, Class Loss: 0.9806
Epoch 2/50, Loss: 1.8903, Domain Loss: 1.4565, Class Loss: 0.4338
Epoch 3/50, Loss: 2.4818, Domain Loss: 2.1027, Class Loss: 0.3790
Epoch 4/50, Loss: 10.6560, Domain Loss: 10.2819, Class Loss: 0.3741
Epoch 5/50, Loss: 7.5426, Domain Loss: 7.1788, Class Loss: 0.3638
Epoch 6/50, Loss: 12.8229, Domain Loss: 12.3427, Class Loss: 0.4802
Epoch 7/50, Loss: 10.8920, Domain Loss: 10.4370, Class Loss: 0.4550
Epoch 8/50, Loss: 4.1137, Domain Loss: 3.4502, Class Loss: 0.6635
Epoch 9/50, Loss: 2.5998, Domain Loss: 2.0767, Class Loss: 0.5231
Epoch 10/50, Loss: 2.1911, Domain Loss: 1.7510, Class Loss: 0.4400
Epoch 11/50, Loss: 1.9348, Domain Loss: 1.5613, Class Loss: 0.3735
Epoch 12/50, Loss: 2.6355, Domain Loss: 2.2502, Class Loss: 0.3853
Epoch 13/50, Loss: 2.6605, Domain Loss: 2.2600, Class Loss: 0.4005
Epoch 14/50, Loss: 2.9323, Domain Loss: 2.5065, Class Loss: 0.4258
Epoch 15/50, Loss: 3.8034, Domain Loss: 3.1482, Class Loss: 0.6552
Epoch 16/50, Loss: 3.0299, Domain Loss: 2.5507, Class Loss: 0.4792
Epoch 17/50, Loss: 3.2619, Domain Loss: 2.7910, Class Loss: 0.4709
Epoch 18/50, Loss: 3.5693, Domain Loss: 2.7386, Class Loss: 0.8308
Epoch 19/50, Loss: 3.5027, Domain Loss: 3.1144, Class Loss: 0.3883
Epoch 20/50, Loss: 2.8390, Domain Loss: 2.4629, Class Loss: 0.3761
Epoch 21/50, Loss: 2.5919, Domain Loss: 2.1836, Class Loss: 0.4082
Epoch 22/50, Loss: 2.8071, Domain Loss: 2.3997, Class Loss: 0.4074
Epoch 23/50, Loss: 2.4193, Domain Loss: 1.9960, Class Loss: 0.4233
Epoch 24/50, Loss: 2.0080, Domain Loss: 1.6350, Class Loss: 0.3730
Epoch 25/50, Loss: 1.9260, Domain Loss: 1.5884, Class Loss: 0.3377
Epoch 26/50, Loss: 1.9671, Domain Loss: 1.6462, Class Loss: 0.3209
Epoch 27/50, Loss: 1.7714, Domain Loss: 1.4554, Class Loss: 0.3159
Epoch 28/50, Loss: 1.6840, Domain Loss: 1.4201, Class Loss: 0.2638
Epoch 29/50, Loss: 1.7218, Domain Loss: 1.4339, Class Loss: 0.2879
Epoch 30/50, Loss: 1.6011, Domain Loss: 1.3402, Class Loss: 0.2609
Epoch 31/50, Loss: 1.9295, Domain Loss: 1.7182, Class Loss: 0.2113
Epoch 32/50, Loss: 1.9036, Domain Loss: 1.6251, Class Loss: 0.2785
Epoch 33/50, Loss: 2.2232, Domain Loss: 1.8511, Class Loss: 0.3721
Epoch 34/50, Loss: 1.5489, Domain Loss: 1.2568, Class Loss: 0.2921
Epoch 35/50, Loss: 1.8028, Domain Loss: 1.5600, Class Loss: 0.2429
Epoch 36/50, Loss: 1.2989, Domain Loss: 1.1430, Class Loss: 0.1559
Epoch 37/50, Loss: 2.4921, Domain Loss: 2.2729, Class Loss: 0.2192
Epoch 38/50, Loss: 2.5194, Domain Loss: 2.0649, Class Loss: 0.4545
Epoch 39/50, Loss: 2.0410, Domain Loss: 1.5765, Class Loss: 0.4645
Epoch 40/50, Loss: 1.8493, Domain Loss: 1.5094, Class Loss: 0.3399
Epoch 41/50, Loss: 1.6614, Domain Loss: 1.3674, Class Loss: 0.2941
Epoch 42/50, Loss: 1.6516, Domain Loss: 1.4113, Class Loss: 0.2404
Epoch 43/50, Loss: 1.6029, Domain Loss: 1.3829, Class Loss: 0.2200
Epoch 44/50, Loss: 1.6605, Domain Loss: 1.4707, Class Loss: 0.1898
Epoch 45/50, Loss: 1.5305, Domain Loss: 1.4031, Class Loss: 0.1274
Epoch 46/50, Loss: 1.5916, Domain Loss: 1.4751, Class Loss: 0.1164
Epoch 47/50, Loss: 1.5945, Domain Loss: 1.4734, Class Loss: 0.1211
Epoch 48/50, Loss: 1.4734, Domain Loss: 1.3562, Class Loss: 0.1172
Epoch 49/50, Loss: 1.5257, Domain Loss: 1.4199, Class Loss: 0.1058
Epoch 50/50, Loss: 1.6884, Domain Loss: 1.5816, Class Loss: 0.1068
46.10


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2118, Domain Loss: 1.3230, Class Loss: 0.8888
Epoch 2/50, Loss: 1.6626, Domain Loss: 1.2337, Class Loss: 0.4289
Epoch 3/50, Loss: 2.2288, Domain Loss: 1.8401, Class Loss: 0.3887
Epoch 4/50, Loss: 7.0137, Domain Loss: 6.6059, Class Loss: 0.4078
Epoch 5/50, Loss: 7.6845, Domain Loss: 7.2667, Class Loss: 0.4179
Epoch 6/50, Loss: 5.7102, Domain Loss: 5.3909, Class Loss: 0.3193
Epoch 7/50, Loss: 15.8873, Domain Loss: 15.4536, Class Loss: 0.4336
Epoch 8/50, Loss: 27.9675, Domain Loss: 27.1221, Class Loss: 0.8454
Epoch 9/50, Loss: 44.9700, Domain Loss: 43.9483, Class Loss: 1.0218
Epoch 10/50, Loss: 10.9192, Domain Loss: 10.2893, Class Loss: 0.6300
Epoch 11/50, Loss: 6.3422, Domain Loss: 5.5336, Class Loss: 0.8086
Epoch 12/50, Loss: 2.9245, Domain Loss: 2.3450, Class Loss: 0.5794
Epoch 13/50, Loss: 2.4964, Domain Loss: 1.9771, Class Loss: 0.5193
Epoch 14/50, Loss: 2.3550, Domain Loss: 1.9132, Class Loss: 0.4418
Epoch 15/50, Loss: 2.8526, Domain Loss: 2.4390, Class Loss: 0.4136
Epoch 16/50, Loss: 4.6377, Domain Loss: 4.0573, Class Loss: 0.5804
Epoch 17/50, Loss: 4.5929, Domain Loss: 4.1166, Class Loss: 0.4763
Epoch 18/50, Loss: 3.3302, Domain Loss: 2.8609, Class Loss: 0.4693
Epoch 19/50, Loss: 2.9009, Domain Loss: 2.3609, Class Loss: 0.5400
Epoch 20/50, Loss: 2.0469, Domain Loss: 1.6165, Class Loss: 0.4303
Epoch 21/50, Loss: 2.0637, Domain Loss: 1.6722, Class Loss: 0.3914
Epoch 22/50, Loss: 1.9607, Domain Loss: 1.5666, Class Loss: 0.3941
Epoch 23/50, Loss: 2.4123, Domain Loss: 1.9986, Class Loss: 0.4138
Epoch 24/50, Loss: 2.0238, Domain Loss: 1.6363, Class Loss: 0.3875
Epoch 25/50, Loss: 2.5560, Domain Loss: 2.1658, Class Loss: 0.3902
Epoch 26/50, Loss: 2.1525, Domain Loss: 1.7837, Class Loss: 0.3687
Epoch 27/50, Loss: 2.3162, Domain Loss: 1.8499, Class Loss: 0.4663
Epoch 28/50, Loss: 2.0555, Domain Loss: 1.6447, Class Loss: 0.4108
Epoch 29/50, Loss: 1.8348, Domain Loss: 1.4736, Class Loss: 0.3613
Epoch 30/50, Loss: 1.9060, Domain Loss: 1.5374, Class Loss: 0.3686
Epoch 31/50, Loss: 1.9227, Domain Loss: 1.5433, Class Loss: 0.3794
Epoch 32/50, Loss: 1.8025, Domain Loss: 1.4511, Class Loss: 0.3514
Epoch 33/50, Loss: 2.0958, Domain Loss: 1.7506, Class Loss: 0.3452
Epoch 34/50, Loss: 1.5994, Domain Loss: 1.3141, Class Loss: 0.2853
Epoch 35/50, Loss: 1.6165, Domain Loss: 1.3574, Class Loss: 0.2591
Epoch 36/50, Loss: 1.9527, Domain Loss: 1.6588, Class Loss: 0.2939
Epoch 37/50, Loss: 1.7253, Domain Loss: 1.4815, Class Loss: 0.2438
Epoch 38/50, Loss: 1.9065, Domain Loss: 1.5917, Class Loss: 0.3148
Epoch 39/50, Loss: 1.8194, Domain Loss: 1.5041, Class Loss: 0.3153
Epoch 40/50, Loss: 1.8331, Domain Loss: 1.5038, Class Loss: 0.3292
Epoch 41/50, Loss: 1.9905, Domain Loss: 1.6618, Class Loss: 0.3287
Epoch 42/50, Loss: 1.9042, Domain Loss: 1.6174, Class Loss: 0.2868
Epoch 43/50, Loss: 1.5580, Domain Loss: 1.3108, Class Loss: 0.2472
Epoch 44/50, Loss: 1.8111, Domain Loss: 1.4963, Class Loss: 0.3149
Epoch 45/50, Loss: 2.0180, Domain Loss: 1.7438, Class Loss: 0.2742
Epoch 46/50, Loss: 1.8134, Domain Loss: 1.5746, Class Loss: 0.2387
Epoch 47/50, Loss: 1.8483, Domain Loss: 1.6161, Class Loss: 0.2321
Epoch 48/50, Loss: 1.5094, Domain Loss: 1.3677, Class Loss: 0.1417
Epoch 49/50, Loss: 1.6819, Domain Loss: 1.5125, Class Loss: 0.1694
Epoch 50/50, Loss: 1.8590, Domain Loss: 1.6360, Class Loss: 0.2230
46.16


Epoch 1/50, Loss: 2.2995, Domain Loss: 1.3112, Class Loss: 0.9883
Epoch 2/50, Loss: 1.7737, Domain Loss: 1.3354, Class Loss: 0.4383
Epoch 3/50, Loss: 2.4259, Domain Loss: 2.0530, Class Loss: 0.3729
Epoch 4/50, Loss: 3.8262, Domain Loss: 3.5203, Class Loss: 0.3059
Epoch 5/50, Loss: 6.9408, Domain Loss: 6.6362, Class Loss: 0.3046
Epoch 6/50, Loss: 5.5807, Domain Loss: 5.2280, Class Loss: 0.3526
Epoch 7/50, Loss: 10.4257, Domain Loss: 9.9625, Class Loss: 0.4632
Epoch 8/50, Loss: 7.3650, Domain Loss: 6.8540, Class Loss: 0.5110
Epoch 9/50, Loss: 6.0579, Domain Loss: 5.3406, Class Loss: 0.7173
Epoch 10/50, Loss: 3.3182, Domain Loss: 2.7725, Class Loss: 0.5457
Epoch 11/50, Loss: 8.8372, Domain Loss: 8.1086, Class Loss: 0.7286
Epoch 12/50, Loss: 7.8568, Domain Loss: 7.2505, Class Loss: 0.6063
Epoch 13/50, Loss: 5.4770, Domain Loss: 4.9937, Class Loss: 0.4833
Epoch 14/50, Loss: 5.6848, Domain Loss: 5.2330, Class Loss: 0.4518
Epoch 15/50, Loss: 3.7592, Domain Loss: 3.3119, Class Loss: 0.4473
Epoch 16/50, Loss: 5.2976, Domain Loss: 4.5207, Class Loss: 0.7769
Epoch 17/50, Loss: 2.5818, Domain Loss: 2.1065, Class Loss: 0.4754
Epoch 18/50, Loss: 2.6306, Domain Loss: 2.1138, Class Loss: 0.5168
Epoch 19/50, Loss: 2.6354, Domain Loss: 2.0995, Class Loss: 0.5359
Epoch 20/50, Loss: 2.6415, Domain Loss: 2.1690, Class Loss: 0.4725
Epoch 21/50, Loss: 2.6945, Domain Loss: 2.3495, Class Loss: 0.3450
Epoch 22/50, Loss: 2.6816, Domain Loss: 2.3623, Class Loss: 0.3193
Epoch 23/50, Loss: 1.8682, Domain Loss: 1.5619, Class Loss: 0.3063
Epoch 24/50, Loss: 2.4700, Domain Loss: 2.0952, Class Loss: 0.3748
Epoch 25/50, Loss: 3.6494, Domain Loss: 3.1824, Class Loss: 0.4670
Epoch 26/50, Loss: 3.4964, Domain Loss: 3.0499, Class Loss: 0.4464
Epoch 27/50, Loss: 3.7378, Domain Loss: 3.2223, Class Loss: 0.5155
Epoch 28/50, Loss: 5.1504, Domain Loss: 4.2463, Class Loss: 0.9042
Epoch 29/50, Loss: 3.7402, Domain Loss: 3.1923, Class Loss: 0.5478
Epoch 30/50, Loss: 2.0832, Domain Loss: 1.6828, Class Loss: 0.4005
Epoch 31/50, Loss: 3.1240, Domain Loss: 2.7249, Class Loss: 0.3991
Epoch 32/50, Loss: 3.4150, Domain Loss: 2.9563, Class Loss: 0.4587
Epoch 33/50, Loss: 1.8734, Domain Loss: 1.5181, Class Loss: 0.3553
Epoch 34/50, Loss: 3.3403, Domain Loss: 2.3339, Class Loss: 1.0064
Epoch 35/50, Loss: 2.2668, Domain Loss: 1.7039, Class Loss: 0.5629
Epoch 36/50, Loss: 1.8383, Domain Loss: 1.4811, Class Loss: 0.3572
Epoch 37/50, Loss: 1.7643, Domain Loss: 1.4133, Class Loss: 0.3510
Epoch 38/50, Loss: 1.6995, Domain Loss: 1.3882, Class Loss: 0.3112
Epoch 39/50, Loss: 1.6273, Domain Loss: 1.3776, Class Loss: 0.2498
Epoch 40/50, Loss: 1.6063, Domain Loss: 1.3956, Class Loss: 0.2107
Epoch 41/50, Loss: 1.6065, Domain Loss: 1.4244, Class Loss: 0.1821
Epoch 42/50, Loss: 1.6120, Domain Loss: 1.4238, Class Loss: 0.1882
Epoch 43/50, Loss: 1.6825, Domain Loss: 1.5121, Class Loss: 0.1704
Epoch 44/50, Loss: 1.6910, Domain Loss: 1.5482, Class Loss: 0.1428
Epoch 45/50, Loss: 1.4981, Domain Loss: 1.3343, Class Loss: 0.1638
Epoch 46/50, Loss: 1.6293, Domain Loss: 1.4720, Class Loss: 0.1573
Epoch 47/50, Loss: 1.6539, Domain Loss: 1.4986, Class Loss: 0.1553
Epoch 48/50, Loss: 1.5873, Domain Loss: 1.4106, Class Loss: 0.1767
Epoch 49/50, Loss: 1.5403, Domain Loss: 1.3660, Class Loss: 0.1743
Epoch 50/50, Loss: 1.4659, Domain Loss: 1.3527, Class Loss: 0.1132
69.30


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2351, Domain Loss: 1.3134, Class Loss: 0.9217
Epoch 2/50, Loss: 1.8398, Domain Loss: 1.3807, Class Loss: 0.4591
Epoch 3/50, Loss: 4.4383, Domain Loss: 3.9981, Class Loss: 0.4402
Epoch 4/50, Loss: 7.4546, Domain Loss: 7.0534, Class Loss: 0.4013
Epoch 5/50, Loss: 6.3911, Domain Loss: 6.0136, Class Loss: 0.3775
Epoch 6/50, Loss: 3.7598, Domain Loss: 3.4672, Class Loss: 0.2926
Epoch 7/50, Loss: 4.2512, Domain Loss: 3.8471, Class Loss: 0.4041
Epoch 8/50, Loss: 2.6366, Domain Loss: 2.3082, Class Loss: 0.3284
Epoch 9/50, Loss: 1.8008, Domain Loss: 1.5045, Class Loss: 0.2963
Epoch 10/50, Loss: 2.6571, Domain Loss: 2.4329, Class Loss: 0.2242
Epoch 11/50, Loss: 2.2428, Domain Loss: 2.0260, Class Loss: 0.2168
Epoch 12/50, Loss: 1.6282, Domain Loss: 1.4528, Class Loss: 0.1754
Epoch 13/50, Loss: 1.8014, Domain Loss: 1.6002, Class Loss: 0.2011
Epoch 14/50, Loss: 1.7940, Domain Loss: 1.6315, Class Loss: 0.1624
Epoch 15/50, Loss: 1.9443, Domain Loss: 1.8251, Class Loss: 0.1192
Epoch 16/50, Loss: 2.4980, Domain Loss: 2.3027, Class Loss: 0.1952
Epoch 17/50, Loss: 2.2380, Domain Loss: 2.0935, Class Loss: 0.1445
Epoch 18/50, Loss: 2.1982, Domain Loss: 1.9405, Class Loss: 0.2577
Epoch 19/50, Loss: 2.3385, Domain Loss: 2.1291, Class Loss: 0.2093
Epoch 20/50, Loss: 1.7845, Domain Loss: 1.6271, Class Loss: 0.1574
Epoch 21/50, Loss: 1.6671, Domain Loss: 1.5621, Class Loss: 0.1049
Epoch 22/50, Loss: 1.6847, Domain Loss: 1.5828, Class Loss: 0.1019
Epoch 23/50, Loss: 1.6272, Domain Loss: 1.4866, Class Loss: 0.1405
Epoch 24/50, Loss: 1.4926, Domain Loss: 1.4099, Class Loss: 0.0828
Epoch 25/50, Loss: 1.5206, Domain Loss: 1.3916, Class Loss: 0.1290
Epoch 26/50, Loss: 1.4783, Domain Loss: 1.3777, Class Loss: 0.1007
Epoch 27/50, Loss: 1.4521, Domain Loss: 1.3917, Class Loss: 0.0604
Epoch 28/50, Loss: 1.4230, Domain Loss: 1.3818, Class Loss: 0.0412
Epoch 29/50, Loss: 1.4142, Domain Loss: 1.3808, Class Loss: 0.0334
Epoch 30/50, Loss: 1.4160, Domain Loss: 1.3828, Class Loss: 0.0332
Epoch 31/50, Loss: 1.4042, Domain Loss: 1.3800, Class Loss: 0.0242
Epoch 32/50, Loss: 1.4004, Domain Loss: 1.3828, Class Loss: 0.0177
Epoch 33/50, Loss: 1.4169, Domain Loss: 1.3934, Class Loss: 0.0235
Epoch 34/50, Loss: 1.4290, Domain Loss: 1.4010, Class Loss: 0.0279
Epoch 35/50, Loss: 1.4334, Domain Loss: 1.4039, Class Loss: 0.0295
Epoch 36/50, Loss: 1.4378, Domain Loss: 1.4134, Class Loss: 0.0244
Epoch 37/50, Loss: 1.4200, Domain Loss: 1.4029, Class Loss: 0.0171
Epoch 38/50, Loss: 1.4106, Domain Loss: 1.3933, Class Loss: 0.0173
Epoch 39/50, Loss: 1.4401, Domain Loss: 1.4147, Class Loss: 0.0254
Epoch 40/50, Loss: 1.4367, Domain Loss: 1.4025, Class Loss: 0.0341
Epoch 41/50, Loss: 1.4921, Domain Loss: 1.4607, Class Loss: 0.0314
Epoch 42/50, Loss: 1.4521, Domain Loss: 1.4186, Class Loss: 0.0335
Epoch 43/50, Loss: 1.4078, Domain Loss: 1.3760, Class Loss: 0.0318
Epoch 44/50, Loss: 1.4207, Domain Loss: 1.3906, Class Loss: 0.0301
Epoch 45/50, Loss: 1.3862, Domain Loss: 1.3620, Class Loss: 0.0242
Epoch 46/50, Loss: 1.4067, Domain Loss: 1.3758, Class Loss: 0.0308
Epoch 47/50, Loss: 1.4737, Domain Loss: 1.4425, Class Loss: 0.0313
Epoch 48/50, Loss: 1.4387, Domain Loss: 1.4061, Class Loss: 0.0326
Epoch 49/50, Loss: 1.3956, Domain Loss: 1.3875, Class Loss: 0.0082
Epoch 50/50, Loss: 1.3957, Domain Loss: 1.3867, Class Loss: 0.0090
63.67


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2233, Domain Loss: 1.3264, Class Loss: 0.8969
Epoch 2/50, Loss: 1.7161, Domain Loss: 1.2967, Class Loss: 0.4194
Epoch 3/50, Loss: 2.0564, Domain Loss: 1.7180, Class Loss: 0.3383
Epoch 4/50, Loss: 3.4694, Domain Loss: 3.1585, Class Loss: 0.3108
Epoch 5/50, Loss: 5.0754, Domain Loss: 4.8066, Class Loss: 0.2688
Epoch 6/50, Loss: 4.0444, Domain Loss: 3.7639, Class Loss: 0.2805
Epoch 7/50, Loss: 4.2909, Domain Loss: 4.0950, Class Loss: 0.1959
Epoch 8/50, Loss: 5.4140, Domain Loss: 5.1388, Class Loss: 0.2752
Epoch 9/50, Loss: 7.5961, Domain Loss: 6.9379, Class Loss: 0.6582
Epoch 10/50, Loss: 13.7518, Domain Loss: 13.0272, Class Loss: 0.7246
Epoch 11/50, Loss: 8.1697, Domain Loss: 7.5926, Class Loss: 0.5771
Epoch 12/50, Loss: 5.8523, Domain Loss: 5.3311, Class Loss: 0.5212
Epoch 13/50, Loss: 3.9869, Domain Loss: 3.5012, Class Loss: 0.4857
Epoch 14/50, Loss: 4.8488, Domain Loss: 4.4002, Class Loss: 0.4486
Epoch 15/50, Loss: 12.9014, Domain Loss: 12.3180, Class Loss: 0.5834
Epoch 16/50, Loss: 8.5047, Domain Loss: 6.4320, Class Loss: 2.0727
Epoch 17/50, Loss: 7.9185, Domain Loss: 7.1651, Class Loss: 0.7534
Epoch 18/50, Loss: 5.0343, Domain Loss: 4.5233, Class Loss: 0.5110
Epoch 19/50, Loss: 3.2149, Domain Loss: 2.6730, Class Loss: 0.5419
Epoch 20/50, Loss: 3.1037, Domain Loss: 2.5709, Class Loss: 0.5329
Epoch 21/50, Loss: 3.4195, Domain Loss: 2.9077, Class Loss: 0.5118
Epoch 22/50, Loss: 2.1093, Domain Loss: 1.6766, Class Loss: 0.4327
Epoch 23/50, Loss: 2.3194, Domain Loss: 1.8927, Class Loss: 0.4267
Epoch 24/50, Loss: 3.6840, Domain Loss: 3.1732, Class Loss: 0.5108
Epoch 25/50, Loss: 3.8537, Domain Loss: 3.4052, Class Loss: 0.4485
Epoch 26/50, Loss: 3.9910, Domain Loss: 3.5151, Class Loss: 0.4759
Epoch 27/50, Loss: 3.7911, Domain Loss: 2.7301, Class Loss: 1.0610
Epoch 28/50, Loss: 3.8766, Domain Loss: 3.2890, Class Loss: 0.5876
Epoch 29/50, Loss: 3.5855, Domain Loss: 2.9323, Class Loss: 0.6532
Epoch 30/50, Loss: 3.7333, Domain Loss: 3.1221, Class Loss: 0.6112
Epoch 31/50, Loss: 3.0279, Domain Loss: 2.4288, Class Loss: 0.5991
Epoch 32/50, Loss: 3.3676, Domain Loss: 2.8198, Class Loss: 0.5478
Epoch 33/50, Loss: 4.7114, Domain Loss: 3.5859, Class Loss: 1.1256
Epoch 34/50, Loss: 4.0964, Domain Loss: 3.2908, Class Loss: 0.8056
Epoch 35/50, Loss: 4.0965, Domain Loss: 3.3544, Class Loss: 0.7422
Epoch 36/50, Loss: 2.8860, Domain Loss: 2.3009, Class Loss: 0.5851
Epoch 37/50, Loss: 2.1246, Domain Loss: 1.6891, Class Loss: 0.4355
Epoch 38/50, Loss: 2.1872, Domain Loss: 1.7528, Class Loss: 0.4345
Epoch 39/50, Loss: 2.2962, Domain Loss: 1.8061, Class Loss: 0.4902
Epoch 40/50, Loss: 2.8576, Domain Loss: 2.3239, Class Loss: 0.5337
Epoch 41/50, Loss: 2.5203, Domain Loss: 2.0380, Class Loss: 0.4823
Epoch 42/50, Loss: 2.3215, Domain Loss: 1.9278, Class Loss: 0.3937
Epoch 43/50, Loss: 1.6262, Domain Loss: 1.2455, Class Loss: 0.3807
Epoch 44/50, Loss: 1.7035, Domain Loss: 1.2707, Class Loss: 0.4328
Epoch 45/50, Loss: 1.9062, Domain Loss: 1.4795, Class Loss: 0.4267
Epoch 46/50, Loss: 2.2055, Domain Loss: 1.8070, Class Loss: 0.3984
Epoch 47/50, Loss: 2.2524, Domain Loss: 1.8743, Class Loss: 0.3781
Epoch 48/50, Loss: 2.2882, Domain Loss: 1.9391, Class Loss: 0.3492
Epoch 49/50, Loss: 2.1348, Domain Loss: 1.7829, Class Loss: 0.3519
Epoch 50/50, Loss: 2.1298, Domain Loss: 1.7436, Class Loss: 0.3861
29.56


Epoch 1/50, Loss: 2.2713, Domain Loss: 1.3260, Class Loss: 0.9453
Epoch 2/50, Loss: 1.6536, Domain Loss: 1.2374, Class Loss: 0.4162
Epoch 3/50, Loss: 2.0239, Domain Loss: 1.6893, Class Loss: 0.3346
Epoch 4/50, Loss: 5.6791, Domain Loss: 5.2804, Class Loss: 0.3987
Epoch 5/50, Loss: 11.1518, Domain Loss: 10.7591, Class Loss: 0.3926
Epoch 6/50, Loss: 8.2891, Domain Loss: 7.9783, Class Loss: 0.3109
Epoch 7/50, Loss: 3.2956, Domain Loss: 3.0337, Class Loss: 0.2619
Epoch 8/50, Loss: 2.7084, Domain Loss: 2.5126, Class Loss: 0.1957
Epoch 9/50, Loss: 14.4696, Domain Loss: 14.0250, Class Loss: 0.4446
Epoch 10/50, Loss: 5.9127, Domain Loss: 5.5414, Class Loss: 0.3713
Epoch 11/50, Loss: 2.5535, Domain Loss: 2.2018, Class Loss: 0.3517
Epoch 12/50, Loss: 2.2135, Domain Loss: 1.8079, Class Loss: 0.4055
Epoch 13/50, Loss: 2.1919, Domain Loss: 1.8607, Class Loss: 0.3312
Epoch 14/50, Loss: 1.8858, Domain Loss: 1.5698, Class Loss: 0.3161
Epoch 15/50, Loss: 2.1110, Domain Loss: 1.8056, Class Loss: 0.3054
Epoch 16/50, Loss: 2.3995, Domain Loss: 2.0241, Class Loss: 0.3753
Epoch 17/50, Loss: 2.2044, Domain Loss: 1.8847, Class Loss: 0.3197
Epoch 18/50, Loss: 1.9626, Domain Loss: 1.6489, Class Loss: 0.3137
Epoch 19/50, Loss: 1.8950, Domain Loss: 1.6578, Class Loss: 0.2372
Epoch 20/50, Loss: 1.9692, Domain Loss: 1.7699, Class Loss: 0.1993
Epoch 21/50, Loss: 2.7109, Domain Loss: 2.4033, Class Loss: 0.3076
Epoch 22/50, Loss: 3.3097, Domain Loss: 3.0019, Class Loss: 0.3079
Epoch 23/50, Loss: 2.8274, Domain Loss: 2.4816, Class Loss: 0.3458
Epoch 24/50, Loss: 2.4792, Domain Loss: 2.2622, Class Loss: 0.2170
Epoch 25/50, Loss: 2.7644, Domain Loss: 2.4623, Class Loss: 0.3020
Epoch 26/50, Loss: 2.6674, Domain Loss: 2.4096, Class Loss: 0.2578
Epoch 27/50, Loss: 2.1922, Domain Loss: 1.8795, Class Loss: 0.3127
Epoch 28/50, Loss: 1.8865, Domain Loss: 1.6340, Class Loss: 0.2525
Epoch 29/50, Loss: 2.0060, Domain Loss: 1.7771, Class Loss: 0.2289
Epoch 30/50, Loss: 2.3019, Domain Loss: 2.0678, Class Loss: 0.2341
Epoch 31/50, Loss: 2.6303, Domain Loss: 2.2731, Class Loss: 0.3572
Epoch 32/50, Loss: 4.9890, Domain Loss: 4.0290, Class Loss: 0.9600
Epoch 33/50, Loss: 3.7069, Domain Loss: 3.0478, Class Loss: 0.6591
Epoch 34/50, Loss: 3.0144, Domain Loss: 2.5185, Class Loss: 0.4959
Epoch 35/50, Loss: 2.7447, Domain Loss: 2.2746, Class Loss: 0.4701
Epoch 36/50, Loss: 2.1964, Domain Loss: 1.7811, Class Loss: 0.4153
Epoch 37/50, Loss: 2.3791, Domain Loss: 1.9124, Class Loss: 0.4666
Epoch 38/50, Loss: 2.2888, Domain Loss: 1.8350, Class Loss: 0.4538
Epoch 39/50, Loss: 2.0924, Domain Loss: 1.7363, Class Loss: 0.3562
Epoch 40/50, Loss: 1.9279, Domain Loss: 1.5390, Class Loss: 0.3889
Epoch 41/50, Loss: 1.8990, Domain Loss: 1.5508, Class Loss: 0.3482
Epoch 42/50, Loss: 2.1288, Domain Loss: 1.8634, Class Loss: 0.2654
Epoch 43/50, Loss: 1.7599, Domain Loss: 1.4831, Class Loss: 0.2768
Epoch 44/50, Loss: 1.8534, Domain Loss: 1.6003, Class Loss: 0.2531
Epoch 45/50, Loss: 2.0046, Domain Loss: 1.7337, Class Loss: 0.2709
Epoch 46/50, Loss: 1.8685, Domain Loss: 1.6306, Class Loss: 0.2379
Epoch 47/50, Loss: 1.7401, Domain Loss: 1.5587, Class Loss: 0.1814
Epoch 48/50, Loss: 1.7103, Domain Loss: 1.5681, Class Loss: 0.1422
Epoch 49/50, Loss: 1.5758, Domain Loss: 1.4225, Class Loss: 0.1533
Epoch 50/50, Loss: 1.7510, Domain Loss: 1.5701, Class Loss: 0.1809
63.25


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2454, Domain Loss: 1.3143, Class Loss: 0.9311
Epoch 2/50, Loss: 1.9726, Domain Loss: 1.5092, Class Loss: 0.4633
Epoch 3/50, Loss: 3.5706, Domain Loss: 3.1481, Class Loss: 0.4225
Epoch 4/50, Loss: 12.2438, Domain Loss: 11.8584, Class Loss: 0.3854
Epoch 5/50, Loss: 17.0744, Domain Loss: 16.6873, Class Loss: 0.3871
Epoch 6/50, Loss: 17.7589, Domain Loss: 17.0991, Class Loss: 0.6598
Epoch 7/50, Loss: 39.8803, Domain Loss: 39.3968, Class Loss: 0.4835
Epoch 8/50, Loss: 17.1722, Domain Loss: 16.6909, Class Loss: 0.4813
Epoch 9/50, Loss: 11.2341, Domain Loss: 10.7912, Class Loss: 0.4428
Epoch 10/50, Loss: 3.9603, Domain Loss: 3.5930, Class Loss: 0.3674
Epoch 11/50, Loss: 2.2563, Domain Loss: 1.9071, Class Loss: 0.3493
Epoch 12/50, Loss: 1.7602, Domain Loss: 1.4702, Class Loss: 0.2900
Epoch 13/50, Loss: 1.6359, Domain Loss: 1.3940, Class Loss: 0.2419
Epoch 14/50, Loss: 1.8484, Domain Loss: 1.6057, Class Loss: 0.2427
Epoch 15/50, Loss: 2.1031, Domain Loss: 1.8410, Class Loss: 0.2622
Epoch 16/50, Loss: 3.4733, Domain Loss: 3.2227, Class Loss: 0.2507
Epoch 17/50, Loss: 4.0812, Domain Loss: 3.7883, Class Loss: 0.2929
Epoch 18/50, Loss: 2.1976, Domain Loss: 1.9632, Class Loss: 0.2344
Epoch 19/50, Loss: 1.8831, Domain Loss: 1.6517, Class Loss: 0.2314
Epoch 20/50, Loss: 1.9238, Domain Loss: 1.6062, Class Loss: 0.3176
Epoch 21/50, Loss: 1.9960, Domain Loss: 1.7483, Class Loss: 0.2477
Epoch 22/50, Loss: 2.3052, Domain Loss: 2.0194, Class Loss: 0.2858
Epoch 23/50, Loss: 1.6878, Domain Loss: 1.5549, Class Loss: 0.1329
Epoch 24/50, Loss: 1.7604, Domain Loss: 1.6125, Class Loss: 0.1480
Epoch 25/50, Loss: 1.6019, Domain Loss: 1.4801, Class Loss: 0.1217
Epoch 26/50, Loss: 1.6326, Domain Loss: 1.4796, Class Loss: 0.1530
Epoch 27/50, Loss: 1.5929, Domain Loss: 1.4543, Class Loss: 0.1386
Epoch 28/50, Loss: 1.6691, Domain Loss: 1.5564, Class Loss: 0.1126
Epoch 29/50, Loss: 1.6078, Domain Loss: 1.4942, Class Loss: 0.1136
Epoch 30/50, Loss: 1.5713, Domain Loss: 1.4800, Class Loss: 0.0913
Epoch 31/50, Loss: 1.7083, Domain Loss: 1.6403, Class Loss: 0.0681
Epoch 32/50, Loss: 1.7415, Domain Loss: 1.6498, Class Loss: 0.0917
Epoch 33/50, Loss: 1.7212, Domain Loss: 1.6297, Class Loss: 0.0915
Epoch 34/50, Loss: 1.6655, Domain Loss: 1.6023, Class Loss: 0.0632
Epoch 35/50, Loss: 1.6183, Domain Loss: 1.5494, Class Loss: 0.0690
Epoch 36/50, Loss: 1.5348, Domain Loss: 1.4819, Class Loss: 0.0529
Epoch 37/50, Loss: 1.4859, Domain Loss: 1.4553, Class Loss: 0.0306
Epoch 38/50, Loss: 1.5853, Domain Loss: 1.5295, Class Loss: 0.0558
Epoch 39/50, Loss: 1.5194, Domain Loss: 1.4314, Class Loss: 0.0880
Epoch 40/50, Loss: 1.7439, Domain Loss: 1.6710, Class Loss: 0.0730
Epoch 41/50, Loss: 1.4619, Domain Loss: 1.4038, Class Loss: 0.0581
Epoch 42/50, Loss: 1.5243, Domain Loss: 1.4865, Class Loss: 0.0378
Epoch 43/50, Loss: 1.5652, Domain Loss: 1.4981, Class Loss: 0.0671
Epoch 44/50, Loss: 1.4928, Domain Loss: 1.4294, Class Loss: 0.0634
Epoch 45/50, Loss: 1.5854, Domain Loss: 1.5188, Class Loss: 0.0666
Epoch 46/50, Loss: 1.5115, Domain Loss: 1.4604, Class Loss: 0.0511
Epoch 47/50, Loss: 1.4238, Domain Loss: 1.3992, Class Loss: 0.0246
Epoch 48/50, Loss: 1.5474, Domain Loss: 1.4888, Class Loss: 0.0586
Epoch 49/50, Loss: 1.4077, Domain Loss: 1.3931, Class Loss: 0.0146
Epoch 50/50, Loss: 1.4749, Domain Loss: 1.4577, Class Loss: 0.0172
51.44


Epoch 1/50, Loss: 2.3283, Domain Loss: 1.3392, Class Loss: 0.9891
Epoch 2/50, Loss: 1.7722, Domain Loss: 1.3447, Class Loss: 0.4275
Epoch 3/50, Loss: 2.1713, Domain Loss: 1.7963, Class Loss: 0.3749
Epoch 4/50, Loss: 10.2844, Domain Loss: 9.8813, Class Loss: 0.4031
Epoch 5/50, Loss: 18.9653, Domain Loss: 18.5783, Class Loss: 0.3870
Epoch 6/50, Loss: 8.6223, Domain Loss: 8.2621, Class Loss: 0.3601
Epoch 7/50, Loss: 3.7939, Domain Loss: 3.4123, Class Loss: 0.3816
Epoch 8/50, Loss: 2.7559, Domain Loss: 2.3335, Class Loss: 0.4224
Epoch 9/50, Loss: 2.4956, Domain Loss: 2.0920, Class Loss: 0.4036
Epoch 10/50, Loss: 3.4108, Domain Loss: 3.0870, Class Loss: 0.3238
Epoch 11/50, Loss: 3.1190, Domain Loss: 2.7987, Class Loss: 0.3202
Epoch 12/50, Loss: 2.6051, Domain Loss: 2.1817, Class Loss: 0.4233
Epoch 13/50, Loss: 2.0873, Domain Loss: 1.7235, Class Loss: 0.3637
Epoch 14/50, Loss: 3.1207, Domain Loss: 2.6120, Class Loss: 0.5087
Epoch 15/50, Loss: 3.4829, Domain Loss: 3.0160, Class Loss: 0.4668
Epoch 16/50, Loss: 2.7646, Domain Loss: 2.2285, Class Loss: 0.5361
Epoch 17/50, Loss: 2.5178, Domain Loss: 2.0369, Class Loss: 0.4809
Epoch 18/50, Loss: 3.0589, Domain Loss: 2.4881, Class Loss: 0.5708
Epoch 19/50, Loss: 9.8733, Domain Loss: 9.0232, Class Loss: 0.8501
Epoch 20/50, Loss: 11.0444, Domain Loss: 10.0656, Class Loss: 0.9788
Epoch 21/50, Loss: 5.6555, Domain Loss: 4.8589, Class Loss: 0.7966
Epoch 22/50, Loss: 4.3587, Domain Loss: 3.6741, Class Loss: 0.6846
Epoch 23/50, Loss: 4.4504, Domain Loss: 3.8114, Class Loss: 0.6390
Epoch 24/50, Loss: 5.6465, Domain Loss: 4.7023, Class Loss: 0.9441
Epoch 25/50, Loss: 3.7256, Domain Loss: 3.1366, Class Loss: 0.5890
Epoch 26/50, Loss: 3.9462, Domain Loss: 3.3021, Class Loss: 0.6441
Epoch 27/50, Loss: 2.5328, Domain Loss: 1.9916, Class Loss: 0.5413
Epoch 28/50, Loss: 2.9325, Domain Loss: 2.0466, Class Loss: 0.8860
Epoch 29/50, Loss: 3.3495, Domain Loss: 2.4233, Class Loss: 0.9262
Epoch 30/50, Loss: 2.2681, Domain Loss: 1.8159, Class Loss: 0.4522
Epoch 31/50, Loss: 1.9951, Domain Loss: 1.6159, Class Loss: 0.3793
Epoch 32/50, Loss: 1.6857, Domain Loss: 1.3300, Class Loss: 0.3557
Epoch 33/50, Loss: 1.7190, Domain Loss: 1.3721, Class Loss: 0.3469
Epoch 34/50, Loss: 1.7971, Domain Loss: 1.4714, Class Loss: 0.3257
Epoch 35/50, Loss: 2.0336, Domain Loss: 1.6735, Class Loss: 0.3602
Epoch 36/50, Loss: 2.3729, Domain Loss: 1.9561, Class Loss: 0.4168
Epoch 37/50, Loss: 2.4187, Domain Loss: 2.0261, Class Loss: 0.3926
Epoch 38/50, Loss: 2.1396, Domain Loss: 1.7330, Class Loss: 0.4066
Epoch 39/50, Loss: 1.7663, Domain Loss: 1.4016, Class Loss: 0.3647
Epoch 40/50, Loss: 1.6175, Domain Loss: 1.3290, Class Loss: 0.2885
Epoch 41/50, Loss: 1.6633, Domain Loss: 1.3714, Class Loss: 0.2919
Epoch 42/50, Loss: 1.6222, Domain Loss: 1.3304, Class Loss: 0.2918
Epoch 43/50, Loss: 1.5945, Domain Loss: 1.3608, Class Loss: 0.2338
Epoch 44/50, Loss: 1.7030, Domain Loss: 1.5003, Class Loss: 0.2027
Epoch 45/50, Loss: 1.7849, Domain Loss: 1.5264, Class Loss: 0.2585
Epoch 46/50, Loss: 1.7856, Domain Loss: 1.5615, Class Loss: 0.2241
Epoch 47/50, Loss: 1.8952, Domain Loss: 1.6427, Class Loss: 0.2525
Epoch 48/50, Loss: 1.8175, Domain Loss: 1.5109, Class Loss: 0.3066
Epoch 49/50, Loss: 1.7778, Domain Loss: 1.5410, Class Loss: 0.2368
Epoch 50/50, Loss: 1.7573, Domain Loss: 1.5377, Class Loss: 0.2195
52.22


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2161, Domain Loss: 1.3267, Class Loss: 0.8894
Epoch 2/50, Loss: 1.6946, Domain Loss: 1.2578, Class Loss: 0.4368
Epoch 3/50, Loss: 2.9435, Domain Loss: 2.5732, Class Loss: 0.3702
Epoch 4/50, Loss: 4.6937, Domain Loss: 4.3270, Class Loss: 0.3667
Epoch 5/50, Loss: 12.1799, Domain Loss: 11.7965, Class Loss: 0.3833
Epoch 6/50, Loss: 5.9437, Domain Loss: 5.5608, Class Loss: 0.3829
Epoch 7/50, Loss: 3.6910, Domain Loss: 3.3195, Class Loss: 0.3715
Epoch 8/50, Loss: 3.8074, Domain Loss: 3.2892, Class Loss: 0.5182
Epoch 9/50, Loss: 6.4347, Domain Loss: 5.8547, Class Loss: 0.5799
Epoch 10/50, Loss: 6.7048, Domain Loss: 6.2295, Class Loss: 0.4753
Epoch 11/50, Loss: 9.0724, Domain Loss: 8.7025, Class Loss: 0.3698
Epoch 12/50, Loss: 6.3211, Domain Loss: 5.9079, Class Loss: 0.4132
Epoch 13/50, Loss: 7.1909, Domain Loss: 6.4614, Class Loss: 0.7295
Epoch 14/50, Loss: 5.1156, Domain Loss: 4.5016, Class Loss: 0.6140
Epoch 15/50, Loss: 7.5765, Domain Loss: 7.0617, Class Loss: 0.5148
Epoch 16/50, Loss: 4.7248, Domain Loss: 4.3532, Class Loss: 0.3716
Epoch 17/50, Loss: 3.5601, Domain Loss: 3.0811, Class Loss: 0.4790
Epoch 18/50, Loss: 3.4246, Domain Loss: 2.8575, Class Loss: 0.5672
Epoch 19/50, Loss: 4.8584, Domain Loss: 4.4023, Class Loss: 0.4560
Epoch 20/50, Loss: 5.6824, Domain Loss: 5.2946, Class Loss: 0.3878
Epoch 21/50, Loss: 3.3507, Domain Loss: 2.9155, Class Loss: 0.4352
Epoch 22/50, Loss: 2.4246, Domain Loss: 1.9804, Class Loss: 0.4443
Epoch 23/50, Loss: 1.9572, Domain Loss: 1.5400, Class Loss: 0.4171
Epoch 24/50, Loss: 2.2068, Domain Loss: 1.8209, Class Loss: 0.3858
Epoch 25/50, Loss: 3.9315, Domain Loss: 3.5284, Class Loss: 0.4030
Epoch 26/50, Loss: 3.8176, Domain Loss: 3.2945, Class Loss: 0.5232
Epoch 27/50, Loss: 3.9416, Domain Loss: 3.4749, Class Loss: 0.4667
Epoch 28/50, Loss: 3.5270, Domain Loss: 3.1079, Class Loss: 0.4191
Epoch 29/50, Loss: 2.8514, Domain Loss: 2.4353, Class Loss: 0.4161
Epoch 30/50, Loss: 2.9352, Domain Loss: 2.4945, Class Loss: 0.4407
Epoch 31/50, Loss: 2.7953, Domain Loss: 2.3579, Class Loss: 0.4373
Epoch 32/50, Loss: 1.8313, Domain Loss: 1.4378, Class Loss: 0.3935
Epoch 33/50, Loss: 1.8687, Domain Loss: 1.5028, Class Loss: 0.3659
Epoch 34/50, Loss: 2.0052, Domain Loss: 1.6743, Class Loss: 0.3309
Epoch 35/50, Loss: 2.2082, Domain Loss: 1.8331, Class Loss: 0.3751
Epoch 36/50, Loss: 2.1974, Domain Loss: 1.8399, Class Loss: 0.3575
Epoch 37/50, Loss: 2.1758, Domain Loss: 1.8670, Class Loss: 0.3087
Epoch 38/50, Loss: 1.9801, Domain Loss: 1.6303, Class Loss: 0.3498
Epoch 39/50, Loss: 1.9936, Domain Loss: 1.5499, Class Loss: 0.4437
Epoch 40/50, Loss: 1.5307, Domain Loss: 1.2361, Class Loss: 0.2947
Epoch 41/50, Loss: 1.7232, Domain Loss: 1.4173, Class Loss: 0.3059
Epoch 42/50, Loss: 2.0089, Domain Loss: 1.6242, Class Loss: 0.3847
Epoch 43/50, Loss: 1.9244, Domain Loss: 1.5870, Class Loss: 0.3374
Epoch 44/50, Loss: 1.8595, Domain Loss: 1.5015, Class Loss: 0.3580
Epoch 45/50, Loss: 1.7454, Domain Loss: 1.4612, Class Loss: 0.2841
Epoch 46/50, Loss: 1.6476, Domain Loss: 1.3898, Class Loss: 0.2578
Epoch 47/50, Loss: 1.6747, Domain Loss: 1.4350, Class Loss: 0.2397
Epoch 48/50, Loss: 1.7141, Domain Loss: 1.4997, Class Loss: 0.2145
Epoch 49/50, Loss: 1.6564, Domain Loss: 1.4112, Class Loss: 0.2452
Epoch 50/50, Loss: 1.9391, Domain Loss: 1.6717, Class Loss: 0.2674
46.70


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.3041, Domain Loss: 1.3296, Class Loss: 0.9745
Epoch 2/50, Loss: 1.8352, Domain Loss: 1.3944, Class Loss: 0.4408
Epoch 3/50, Loss: 2.7353, Domain Loss: 2.3036, Class Loss: 0.4317
Epoch 4/50, Loss: 5.7098, Domain Loss: 5.3440, Class Loss: 0.3657
Epoch 5/50, Loss: 13.5518, Domain Loss: 13.1267, Class Loss: 0.4252
Epoch 6/50, Loss: 6.1286, Domain Loss: 5.6528, Class Loss: 0.4757
Epoch 7/50, Loss: 4.0483, Domain Loss: 3.6588, Class Loss: 0.3895
Epoch 8/50, Loss: 4.4703, Domain Loss: 4.0501, Class Loss: 0.4203
Epoch 9/50, Loss: 3.8334, Domain Loss: 3.4277, Class Loss: 0.4057
Epoch 10/50, Loss: 2.8626, Domain Loss: 2.5763, Class Loss: 0.2863
Epoch 11/50, Loss: 2.6505, Domain Loss: 2.3583, Class Loss: 0.2922
Epoch 12/50, Loss: 2.3899, Domain Loss: 2.0891, Class Loss: 0.3008
Epoch 13/50, Loss: 2.0772, Domain Loss: 1.7288, Class Loss: 0.3485
Epoch 14/50, Loss: 1.8271, Domain Loss: 1.5181, Class Loss: 0.3090
Epoch 15/50, Loss: 1.7877, Domain Loss: 1.5738, Class Loss: 0.2139
Epoch 16/50, Loss: 3.2467, Domain Loss: 2.9730, Class Loss: 0.2737
Epoch 17/50, Loss: 3.7656, Domain Loss: 3.4069, Class Loss: 0.3586
Epoch 18/50, Loss: 2.1587, Domain Loss: 1.9144, Class Loss: 0.2443
Epoch 19/50, Loss: 1.9503, Domain Loss: 1.7452, Class Loss: 0.2052
Epoch 20/50, Loss: 2.3608, Domain Loss: 2.1316, Class Loss: 0.2292
Epoch 21/50, Loss: 1.9200, Domain Loss: 1.7199, Class Loss: 0.2001
Epoch 22/50, Loss: 2.6884, Domain Loss: 2.1872, Class Loss: 0.5012
Epoch 23/50, Loss: 1.8247, Domain Loss: 1.6266, Class Loss: 0.1982
Epoch 24/50, Loss: 2.1162, Domain Loss: 1.9036, Class Loss: 0.2126
Epoch 25/50, Loss: 1.9950, Domain Loss: 1.7536, Class Loss: 0.2415
Epoch 26/50, Loss: 2.3514, Domain Loss: 2.0838, Class Loss: 0.2676
Epoch 27/50, Loss: 1.7065, Domain Loss: 1.4205, Class Loss: 0.2860
Epoch 28/50, Loss: 1.7189, Domain Loss: 1.4771, Class Loss: 0.2417
Epoch 29/50, Loss: 1.6805, Domain Loss: 1.5098, Class Loss: 0.1707
Epoch 30/50, Loss: 1.5657, Domain Loss: 1.4103, Class Loss: 0.1554
Epoch 31/50, Loss: 1.5248, Domain Loss: 1.3840, Class Loss: 0.1407
Epoch 32/50, Loss: 1.5490, Domain Loss: 1.3903, Class Loss: 0.1588
Epoch 33/50, Loss: 1.5672, Domain Loss: 1.4454, Class Loss: 0.1218
Epoch 34/50, Loss: 1.6683, Domain Loss: 1.5296, Class Loss: 0.1387
Epoch 35/50, Loss: 1.5641, Domain Loss: 1.4691, Class Loss: 0.0950
Epoch 36/50, Loss: 1.5721, Domain Loss: 1.4637, Class Loss: 0.1084
Epoch 37/50, Loss: 1.7964, Domain Loss: 1.6109, Class Loss: 0.1855
Epoch 38/50, Loss: 1.5340, Domain Loss: 1.3718, Class Loss: 0.1622
Epoch 39/50, Loss: 1.4928, Domain Loss: 1.3907, Class Loss: 0.1021
Epoch 40/50, Loss: 1.5772, Domain Loss: 1.4884, Class Loss: 0.0888
Epoch 41/50, Loss: 1.5042, Domain Loss: 1.4409, Class Loss: 0.0632
Epoch 42/50, Loss: 1.5248, Domain Loss: 1.4509, Class Loss: 0.0739
Epoch 43/50, Loss: 1.4039, Domain Loss: 1.3527, Class Loss: 0.0512
Epoch 44/50, Loss: 1.6130, Domain Loss: 1.5582, Class Loss: 0.0547
Epoch 45/50, Loss: 1.4288, Domain Loss: 1.3850, Class Loss: 0.0438
Epoch 46/50, Loss: 1.7030, Domain Loss: 1.6546, Class Loss: 0.0484
Epoch 47/50, Loss: 1.5020, Domain Loss: 1.4180, Class Loss: 0.0841
Epoch 48/50, Loss: 1.4820, Domain Loss: 1.4105, Class Loss: 0.0715
Epoch 49/50, Loss: 1.5428, Domain Loss: 1.4888, Class Loss: 0.0539
Epoch 50/50, Loss: 1.5851, Domain Loss: 1.5141, Class Loss: 0.0710
51.44


Source performance:
57.59 50.41 56.34 46.01 
Target performance:
51.98 49.12 51.85 44.04 

Per-class target performance: 100.00 11.52 38.31 57.58 Deep CORAL
Deep CORAL Run 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1: Source Val Acc = 0.7020, Target Val Acc = 0.3429
Epoch 2: Source Val Acc = 0.5174, Target Val Acc = 0.4514
Epoch 3: Source Val Acc = 0.5180, Target Val Acc = 0.4946
Epoch 4: Source Val Acc = 0.5264, Target Val Acc = 0.5252
Epoch 5: Source Val Acc = 0.5264, Target Val Acc = 0.5102
Epoch 6: Source Val Acc = 0.5168, Target Val Acc = 0.3477
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.5168, Target Val Acc = 0.3477

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.6571, Target Val Acc = 0.5510
Epoch 2: Source Val Acc = 0.5534, Target Val Acc = 0.5390
Epoch 3: Source Val Acc = 0.5234, Target Val Acc = 0.4484
Epoch 4: Source Val Acc = 0.4089, Target Val Acc = 0.2806
Epoch 5: Source Val Acc = 0.7656, Target Val Acc = 0.4538
Epoch 6: Source Val Acc = 0.7626, Target Val Acc = 0.4460
Epoch 7: Source Val Acc = 0.5348, Target Val Acc = 0.4568
Epoch 8: Source Val Acc = 0.6037, Target Val Acc = 0.5084
Epoch 9: Source Val Acc = 0.7164, Target Val Acc = 0.4490
Epoch 10: Source Val Acc = 0.6049, Target Val Acc = 0.4946
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.6049, Target Val Acc = 0.4946

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.8088, Target Val Acc = 0.5024
Epoch 2: Source Val Acc = 0.5180, Target Val Acc = 0.5402
Epoch 3: Source Val Acc = 0.9347, Target Val Acc = 0.4335
Epoch 4: Source Val Acc = 0.2530, Target Val Acc = 0.1013
Epoch 5: Source Val Acc = 0.5222, Target Val Acc = 0.2896
Epoch 6: Source Val Acc = 0.5246, Target Val Acc = 0.3621
Epoch 7: Source Val Acc = 0.5318, Target Val Acc = 0.3513
Epoch 8: Source Val Acc = 0.5707, Target Val Acc = 0.3495
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.5707, Target Val Acc = 0.3495

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.5282, Target Val Acc = 0.4335
Epoch 2: Source Val Acc = 0.5186, Target Val Acc = 0.5090
Epoch 3: Source Val Acc = 0.5827, Target Val Acc = 0.5162
Epoch 4: Source Val Acc = 0.7524, Target Val Acc = 0.4712
Epoch 5: Source Val Acc = 0.7230, Target Val Acc = 0.4880
Epoch 6: Source Val Acc = 0.3669, Target Val Acc = 0.4448
Epoch 7: Source Val Acc = 0.5168, Target Val Acc = 0.3531
Epoch 8: Source Val Acc = 0.5450, Target Val Acc = 0.5090
Epoch 9: Source Val Acc = 0.5911, Target Val Acc = 0.4844
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.5911, Target Val Acc = 0.4844

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.4329, Target Val Acc = 0.2098
Epoch 2: Source Val Acc = 0.5222, Target Val Acc = 0.5839
Epoch 3: Source Val Acc = 0.5162, Target Val Acc = 0.3615
Epoch 4: Source Val Acc = 0.6984, Target Val Acc = 0.5989
Epoch 5: Source Val Acc = 0.5695, Target Val Acc = 0.5468
Epoch 6: Source Val Acc = 0.5168, Target Val Acc = 0.4958
Epoch 7: Source Val Acc = 0.6841, Target Val Acc = 0.4988
Epoch 8: Source Val Acc = 0.5486, Target Val Acc = 0.5384
Epoch 9: Source Val Acc = 0.7926, Target Val Acc = 0.4814
Epoch 10: Source Val Acc = 0.5390, Target Val Acc = 0.4095
Epoch 11: Source Val Acc = 0.5683, Target Val Acc = 0.4712
Epoch 12: Source Val Acc = 0.7464, Target Val Acc = 0.3417
Epoch 13: Source Val Acc = 0.5360, Target Val Acc = 0.5893
Epoch 14: Source Val Acc = 0.7710, Target Val Acc = 0.5048
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.7710, Target Val Acc = 0.5048

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.3945, Target Val Acc = 0.3022
Epoch 2: Source Val Acc = 0.7230, Target Val Acc = 0.4251
Epoch 3: Source Val Acc = 0.5641, Target Val Acc = 0.4808
Epoch 4: Source Val Acc = 0.5576, Target Val Acc = 0.5624
Epoch 5: Source Val Acc = 0.5959, Target Val Acc = 0.4994
Epoch 6: Source Val Acc = 0.5444, Target Val Acc = 0.6067
Epoch 7: Source Val Acc = 0.5761, Target Val Acc = 0.5809
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 0.5761, Target Val Acc = 0.5809

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.7284, Target Val Acc = 0.4958
Epoch 2: Source Val Acc = 0.7572, Target Val Acc = 0.4400
Epoch 3: Source Val Acc = 0.5252, Target Val Acc = 0.4814
Epoch 4: Source Val Acc = 0.5168, Target Val Acc = 0.6139
Epoch 5: Source Val Acc = 0.6457, Target Val Acc = 0.4502
Epoch 6: Source Val Acc = 0.6769, Target Val Acc = 0.4760
Epoch 7: Source Val Acc = 0.5330, Target Val Acc = 0.4143
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.5330, Target Val Acc = 0.4143

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.6559, Target Val Acc = 0.4748
Epoch 2: Source Val Acc = 0.6307, Target Val Acc = 0.4041
Epoch 3: Source Val Acc = 0.8399, Target Val Acc = 0.5641
Epoch 4: Source Val Acc = 0.5234, Target Val Acc = 0.6187
Epoch 5: Source Val Acc = 0.7722, Target Val Acc = 0.5204
Epoch 6: Source Val Acc = 0.7440, Target Val Acc = 0.4143
Epoch 7: Source Val Acc = 0.7098, Target Val Acc = 0.3165
Epoch 8: Source Val Acc = 0.6319, Target Val Acc = 0.4772
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.6319, Target Val Acc = 0.4772

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.7590, Target Val Acc = 0.4059
Epoch 2: Source Val Acc = 0.5258, Target Val Acc = 0.5030
Epoch 3: Source Val Acc = 0.7362, Target Val Acc = 0.4017
Epoch 4: Source Val Acc = 0.5216, Target Val Acc = 0.4952
Epoch 5: Source Val Acc = 0.6607, Target Val Acc = 0.5504
Epoch 6: Source Val Acc = 0.4430, Target Val Acc = 0.5444
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 0.4430, Target Val Acc = 0.5444

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.5204, Target Val Acc = 0.4394
Epoch 2: Source Val Acc = 0.5264, Target Val Acc = 0.5018
Epoch 3: Source Val Acc = 0.5168, Target Val Acc = 0.6888
Epoch 4: Source Val Acc = 0.5168, Target Val Acc = 0.4826
Epoch 5: Source Val Acc = 0.5564, Target Val Acc = 0.6607
Epoch 6: Source Val Acc = 0.6187, Target Val Acc = 0.5737
Epoch 7: Source Val Acc = 0.5180, Target Val Acc = 0.4628
Epoch 8: Source Val Acc = 0.5935, Target Val Acc = 0.2800
Epoch 9: Source Val Acc = 0.5168, Target Val Acc = 0.4916
Epoch 10: Source Val Acc = 0.5264, Target Val Acc = 0.5606
Epoch 11: Source Val Acc = 0.5462, Target Val Acc = 0.5246
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.5462, Target Val Acc = 0.5246

Deep CORAL: Average Source Val Acc = 0.5785, Average Target Val Acc = 0.4722
STAR

Run 1/10
Epoch [1/50], Class Loss: 1.7047, Discrepancy Loss: 0.1175
Epoch [2/50], Class Loss: 0.8505, Discrepancy Loss: 0.0816
Epoch [3/50], Class Loss: 0.6452, Discrepancy Loss: 0.0790
Epoch [4/50], Class Loss: 0.2163, Discrepancy Loss: 0.0520
Epoch [5/50], Class Loss: 0.1644, Discrepancy Loss: 0.0449
Epoch [6/50], Class Loss: 0.2033, Discrepancy Loss: 0.0361
Epoch [7/50], Class Loss: 0.0596, Discrepancy Loss: 0.0351
Epoch [8/50], Class Loss: 0.0335, Discrepancy Loss: 0.0259
Epoch [9/50], Class Loss: 0.0936, Discrepancy Loss: 0.0193
Epoch [10/50], Class Loss: 0.2323, Discrepancy Loss: 0.0379
Epoch [11/50], Class Loss: 0.0214, Discrepancy Loss: 0.0368
Epoch [12/50], Class Loss: 0.0153, Discrepancy Loss: 0.0284
Epoch [13/50], Class Loss: 0.0331, Discrepancy Loss: 0.0268
Epoch [14/50], Class Loss: 0.0137, Discrepancy Loss: 0.0221
Epoch [15/50], Class Loss: 0.0071, Discrepancy Loss: 0.0216
Epoch [16/50], Class Loss: 0.0077, Discrepancy Loss: 0.0188
Epoch [17/50], Class Loss: 0.0064, Discrepancy Loss: 0.0165
Epoch [18/50], Class Loss: 0.0114, Discrepancy Loss: 0.0151
Epoch [19/50], Class Loss: 0.0075, Discrepancy Loss: 0.0167
Epoch [20/50], Class Loss: 0.0065, Discrepancy Loss: 0.0106
Epoch [21/50], Class Loss: 0.0140, Discrepancy Loss: 0.0100
Epoch [22/50], Class Loss: 0.0041, Discrepancy Loss: 0.0107
Epoch [23/50], Class Loss: 0.0040, Discrepancy Loss: 0.0110
Epoch [24/50], Class Loss: 0.0030, Discrepancy Loss: 0.0105
Epoch [25/50], Class Loss: 0.0051, Discrepancy Loss: 0.0125
Epoch [26/50], Class Loss: 0.0031, Discrepancy Loss: 0.0101
Epoch [27/50], Class Loss: 0.0034, Discrepancy Loss: 0.0102
Epoch [28/50], Class Loss: 0.0094, Discrepancy Loss: 0.0105
Epoch [29/50], Class Loss: 0.0709, Discrepancy Loss: 0.0118
Epoch [30/50], Class Loss: 0.0033, Discrepancy Loss: 0.0120
Epoch [31/50], Class Loss: 0.0074, Discrepancy Loss: 0.0128
Epoch [32/50], Class Loss: 0.0050, Discrepancy Loss: 0.0112
Epoch [33/50], Class Loss: 0.0029, Discrepancy Loss: 0.0125
Epoch [34/50], Class Loss: 0.0042, Discrepancy Loss: 0.0103
Epoch [35/50], Class Loss: 0.0076, Discrepancy Loss: 0.0103
Epoch [36/50], Class Loss: 0.0028, Discrepancy Loss: 0.0114
Epoch [37/50], Class Loss: 0.0017, Discrepancy Loss: 0.0120
Epoch [38/50], Class Loss: 0.0023, Discrepancy Loss: 0.0109
Epoch [39/50], Class Loss: 0.0039, Discrepancy Loss: 0.0117
Epoch [40/50], Class Loss: 0.0024, Discrepancy Loss: 0.0104
Epoch [41/50], Class Loss: 0.0028, Discrepancy Loss: 0.0110
Epoch [42/50], Class Loss: 0.0131, Discrepancy Loss: 0.0107
Epoch [43/50], Class Loss: 0.0036, Discrepancy Loss: 0.0113
Epoch [44/50], Class Loss: 0.0021, Discrepancy Loss: 0.0099
Epoch [45/50], Class Loss: 0.0128, Discrepancy Loss: 0.0117
Epoch [46/50], Class Loss: 0.0040, Discrepancy Loss: 0.0109
Epoch [47/50], Class Loss: 0.0070, Discrepancy Loss: 0.0115
Epoch [48/50], Class Loss: 0.0029, Discrepancy Loss: 0.0104
Epoch [49/50], Class Loss: 0.0129, Discrepancy Loss: 0.0105
Epoch [50/50], Class Loss: 0.0037, Discrepancy Loss: 0.0120
Source Domain Performance - Accuracy: 51.62%, Precision: 33.52%, Recall: 49.94%, F1 Score: 37.69%
Target Domain Performance - Accuracy: 44.12%, Precision: 45.29%, Recall: 44.13%, F1 Score: 44.65%

Run 2/10
Epoch [1/50], Class Loss: 1.8533, Discrepancy Loss: 0.1587
Epoch [2/50], Class Loss: 0.9929, Discrepancy Loss: 0.0928
Epoch [3/50], Class Loss: 0.5137, Discrepancy Loss: 0.0766
Epoch [4/50], Class Loss: 0.2637, Discrepancy Loss: 0.0550
Epoch [5/50], Class Loss: 0.1883, Discrepancy Loss: 0.0598
Epoch [6/50], Class Loss: 0.1736, Discrepancy Loss: 0.0575
Epoch [7/50], Class Loss: 0.0873, Discrepancy Loss: 0.0321
Epoch [8/50], Class Loss: 0.1698, Discrepancy Loss: 0.0409
Epoch [9/50], Class Loss: 0.0574, Discrepancy Loss: 0.0229
Epoch [10/50], Class Loss: 0.0591, Discrepancy Loss: 0.0247
Epoch [11/50], Class Loss: 0.0151, Discrepancy Loss: 0.0165
Epoch [12/50], Class Loss: 0.0413, Discrepancy Loss: 0.0157
Epoch [13/50], Class Loss: 0.0568, Discrepancy Loss: 0.0173
Epoch [14/50], Class Loss: 0.0182, Discrepancy Loss: 0.0155
Epoch [15/50], Class Loss: 0.0133, Discrepancy Loss: 0.0129
Epoch [16/50], Class Loss: 0.0662, Discrepancy Loss: 0.0103
Epoch [17/50], Class Loss: 0.0226, Discrepancy Loss: 0.0137
Epoch [18/50], Class Loss: 0.0087, Discrepancy Loss: 0.0120
Epoch [19/50], Class Loss: 0.0756, Discrepancy Loss: 0.0112
Epoch [20/50], Class Loss: 0.0072, Discrepancy Loss: 0.0159
Epoch [21/50], Class Loss: 0.0274, Discrepancy Loss: 0.0158
Epoch [22/50], Class Loss: 0.0058, Discrepancy Loss: 0.0136
Epoch [23/50], Class Loss: 0.0557, Discrepancy Loss: 0.0140
Epoch [24/50], Class Loss: 0.0114, Discrepancy Loss: 0.0142
Epoch [25/50], Class Loss: 0.0508, Discrepancy Loss: 0.0109
Epoch [26/50], Class Loss: 0.0024, Discrepancy Loss: 0.0138
Epoch [27/50], Class Loss: 0.0034, Discrepancy Loss: 0.0109
Epoch [28/50], Class Loss: 0.0055, Discrepancy Loss: 0.0131
Epoch [29/50], Class Loss: 0.0028, Discrepancy Loss: 0.0124
Epoch [30/50], Class Loss: 0.0172, Discrepancy Loss: 0.0182
Epoch [31/50], Class Loss: 0.0155, Discrepancy Loss: 0.0129
Epoch [32/50], Class Loss: 0.0205, Discrepancy Loss: 0.0145
Epoch [33/50], Class Loss: 0.0080, Discrepancy Loss: 0.0097
Epoch [34/50], Class Loss: 0.0038, Discrepancy Loss: 0.0119
Epoch [35/50], Class Loss: 0.0281, Discrepancy Loss: 0.0130
Epoch [36/50], Class Loss: 0.0047, Discrepancy Loss: 0.0119
Epoch [37/50], Class Loss: 0.0019, Discrepancy Loss: 0.0110
Epoch [38/50], Class Loss: 0.0037, Discrepancy Loss: 0.0091
Epoch [39/50], Class Loss: 0.0067, Discrepancy Loss: 0.0193
Epoch [40/50], Class Loss: 0.0036, Discrepancy Loss: 0.0105
Epoch [41/50], Class Loss: 0.0101, Discrepancy Loss: 0.0111
Epoch [42/50], Class Loss: 0.0030, Discrepancy Loss: 0.0105
Epoch [43/50], Class Loss: 0.0023, Discrepancy Loss: 0.0115
Epoch [44/50], Class Loss: 0.0025, Discrepancy Loss: 0.0132
Epoch [45/50], Class Loss: 0.0054, Discrepancy Loss: 0.0119
Epoch [46/50], Class Loss: 0.0024, Discrepancy Loss: 0.0133
Epoch [47/50], Class Loss: 0.0727, Discrepancy Loss: 0.0130
Epoch [48/50], Class Loss: 0.0020, Discrepancy Loss: 0.0098
Epoch [49/50], Class Loss: 0.0015, Discrepancy Loss: 0.0111
Epoch [50/50], Class Loss: 0.0071, Discrepancy Loss: 0.0125
Source Domain Performance - Accuracy: 51.68%, Precision: 33.52%, Recall: 50.00%, F1 Score: 37.71%
Target Domain Performance - Accuracy: 34.83%, Precision: 50.11%, Recall: 34.78%, F1 Score: 36.59%

Run 3/10
Epoch [1/50], Class Loss: 1.4555, Discrepancy Loss: 0.1294
Epoch [2/50], Class Loss: 0.8170, Discrepancy Loss: 0.0741
Epoch [3/50], Class Loss: 0.5031, Discrepancy Loss: 0.0702
Epoch [4/50], Class Loss: 0.1163, Discrepancy Loss: 0.0458
Epoch [5/50], Class Loss: 0.2747, Discrepancy Loss: 0.0422
Epoch [6/50], Class Loss: 0.0972, Discrepancy Loss: 0.0577
Epoch [7/50], Class Loss: 0.0795, Discrepancy Loss: 0.0392
Epoch [8/50], Class Loss: 0.7389, Discrepancy Loss: 0.0648
Epoch [9/50], Class Loss: 0.0942, Discrepancy Loss: 0.0504
Epoch [10/50], Class Loss: 0.0672, Discrepancy Loss: 0.0494
Epoch [11/50], Class Loss: 0.0320, Discrepancy Loss: 0.0272
Epoch [12/50], Class Loss: 0.0125, Discrepancy Loss: 0.0258
Epoch [13/50], Class Loss: 0.0141, Discrepancy Loss: 0.0308
Epoch [14/50], Class Loss: 0.0118, Discrepancy Loss: 0.0271
Epoch [15/50], Class Loss: 0.0068, Discrepancy Loss: 0.0233
Epoch [16/50], Class Loss: 0.0070, Discrepancy Loss: 0.0231
Epoch [17/50], Class Loss: 0.0075, Discrepancy Loss: 0.0253
Epoch [18/50], Class Loss: 0.0996, Discrepancy Loss: 0.0218
Epoch [19/50], Class Loss: 0.0122, Discrepancy Loss: 0.0266
Epoch [20/50], Class Loss: 0.0061, Discrepancy Loss: 0.0284
Epoch [21/50], Class Loss: 0.0111, Discrepancy Loss: 0.0254
Epoch [22/50], Class Loss: 0.0084, Discrepancy Loss: 0.0232
Epoch [23/50], Class Loss: 0.0067, Discrepancy Loss: 0.0268
Epoch [24/50], Class Loss: 0.0536, Discrepancy Loss: 0.0281
Epoch [25/50], Class Loss: 0.0833, Discrepancy Loss: 0.0266
Epoch [26/50], Class Loss: 0.0073, Discrepancy Loss: 0.0241
Epoch [27/50], Class Loss: 0.0102, Discrepancy Loss: 0.0225
Epoch [28/50], Class Loss: 0.0064, Discrepancy Loss: 0.0219
Epoch [29/50], Class Loss: 0.0215, Discrepancy Loss: 0.0234
Epoch [30/50], Class Loss: 0.0070, Discrepancy Loss: 0.0229
Epoch [31/50], Class Loss: 0.0169, Discrepancy Loss: 0.0222
Epoch [32/50], Class Loss: 0.0095, Discrepancy Loss: 0.0249
Epoch [33/50], Class Loss: 0.0094, Discrepancy Loss: 0.0223
Epoch [34/50], Class Loss: 0.0064, Discrepancy Loss: 0.0204
Epoch [35/50], Class Loss: 0.0098, Discrepancy Loss: 0.0207
Epoch [36/50], Class Loss: 0.1713, Discrepancy Loss: 0.0209
Epoch [37/50], Class Loss: 0.0074, Discrepancy Loss: 0.0216
Epoch [38/50], Class Loss: 0.0074, Discrepancy Loss: 0.0210
Epoch [39/50], Class Loss: 0.0079, Discrepancy Loss: 0.0239
Epoch [40/50], Class Loss: 0.0600, Discrepancy Loss: 0.0225
Epoch [41/50], Class Loss: 0.0079, Discrepancy Loss: 0.0191
Epoch [42/50], Class Loss: 0.0085, Discrepancy Loss: 0.0207
Epoch [43/50], Class Loss: 0.0154, Discrepancy Loss: 0.0208
Epoch [44/50], Class Loss: 0.0065, Discrepancy Loss: 0.0209
Epoch [45/50], Class Loss: 0.0059, Discrepancy Loss: 0.0228
Epoch [46/50], Class Loss: 0.0053, Discrepancy Loss: 0.0232
Epoch [47/50], Class Loss: 0.0099, Discrepancy Loss: 0.0241
Epoch [48/50], Class Loss: 0.0051, Discrepancy Loss: 0.0223
Epoch [49/50], Class Loss: 0.0065, Discrepancy Loss: 0.0226
Epoch [50/50], Class Loss: 0.0117, Discrepancy Loss: 0.0222
Source Domain Performance - Accuracy: 51.68%, Precision: 33.52%, Recall: 50.00%, F1 Score: 37.71%
Target Domain Performance - Accuracy: 48.74%, Precision: 52.61%, Recall: 48.78%, F1 Score: 37.23%

Run 4/10
Epoch [1/50], Class Loss: 1.6453, Discrepancy Loss: 0.1237
Epoch [2/50], Class Loss: 0.7627, Discrepancy Loss: 0.0746
Epoch [3/50], Class Loss: 0.8571, Discrepancy Loss: 0.0820
Epoch [4/50], Class Loss: 0.3043, Discrepancy Loss: 0.0502
Epoch [5/50], Class Loss: 0.1771, Discrepancy Loss: 0.0415
Epoch [6/50], Class Loss: 0.0960, Discrepancy Loss: 0.0188
Epoch [7/50], Class Loss: 0.0989, Discrepancy Loss: 0.0225
Epoch [8/50], Class Loss: 0.4110, Discrepancy Loss: 0.0532
Epoch [9/50], Class Loss: 0.2441, Discrepancy Loss: 0.0506
Epoch [10/50], Class Loss: 0.1746, Discrepancy Loss: 0.0279
Epoch [11/50], Class Loss: 0.0495, Discrepancy Loss: 0.0263
Epoch [12/50], Class Loss: 0.0287, Discrepancy Loss: 0.0271
Epoch [13/50], Class Loss: 0.0372, Discrepancy Loss: 0.0226
Epoch [14/50], Class Loss: 0.0281, Discrepancy Loss: 0.0228
Epoch [15/50], Class Loss: 0.0252, Discrepancy Loss: 0.0227
Epoch [16/50], Class Loss: 0.0257, Discrepancy Loss: 0.0197
Epoch [17/50], Class Loss: 0.0409, Discrepancy Loss: 0.0207
Epoch [18/50], Class Loss: 0.0292, Discrepancy Loss: 0.0212
Epoch [19/50], Class Loss: 0.0161, Discrepancy Loss: 0.0192
Epoch [20/50], Class Loss: 0.0216, Discrepancy Loss: 0.0179
Epoch [21/50], Class Loss: 0.0184, Discrepancy Loss: 0.0127
Epoch [22/50], Class Loss: 0.0090, Discrepancy Loss: 0.0164
Epoch [23/50], Class Loss: 0.0352, Discrepancy Loss: 0.0175
Epoch [24/50], Class Loss: 0.0250, Discrepancy Loss: 0.0132
Epoch [25/50], Class Loss: 0.0141, Discrepancy Loss: 0.0147
Epoch [26/50], Class Loss: 0.0119, Discrepancy Loss: 0.0173
Epoch [27/50], Class Loss: 0.0129, Discrepancy Loss: 0.0128
Epoch [28/50], Class Loss: 0.0180, Discrepancy Loss: 0.0142
Epoch [29/50], Class Loss: 0.0157, Discrepancy Loss: 0.0150
Epoch [30/50], Class Loss: 0.0093, Discrepancy Loss: 0.0151
Epoch [31/50], Class Loss: 0.0115, Discrepancy Loss: 0.0149
Epoch [32/50], Class Loss: 0.0090, Discrepancy Loss: 0.0133
Epoch [33/50], Class Loss: 0.0113, Discrepancy Loss: 0.0141
Epoch [34/50], Class Loss: 0.0178, Discrepancy Loss: 0.0158
Epoch [35/50], Class Loss: 0.0112, Discrepancy Loss: 0.0150
Epoch [36/50], Class Loss: 0.0183, Discrepancy Loss: 0.0173
Epoch [37/50], Class Loss: 0.0206, Discrepancy Loss: 0.0142
Epoch [38/50], Class Loss: 0.0081, Discrepancy Loss: 0.0171
Epoch [39/50], Class Loss: 0.0092, Discrepancy Loss: 0.0145
Epoch [40/50], Class Loss: 0.0120, Discrepancy Loss: 0.0138
Epoch [41/50], Class Loss: 0.0151, Discrepancy Loss: 0.0153
Epoch [42/50], Class Loss: 0.0156, Discrepancy Loss: 0.0162
Epoch [43/50], Class Loss: 0.0129, Discrepancy Loss: 0.0141
Epoch [44/50], Class Loss: 0.0152, Discrepancy Loss: 0.0125
Epoch [45/50], Class Loss: 0.0144, Discrepancy Loss: 0.0119
Epoch [46/50], Class Loss: 0.0118, Discrepancy Loss: 0.0150
Epoch [47/50], Class Loss: 0.0096, Discrepancy Loss: 0.0157
Epoch [48/50], Class Loss: 0.0123, Discrepancy Loss: 0.0152
Epoch [49/50], Class Loss: 0.0406, Discrepancy Loss: 0.0160
Epoch [50/50], Class Loss: 0.0135, Discrepancy Loss: 0.0125
Source Domain Performance - Accuracy: 51.68%, Precision: 33.52%, Recall: 50.00%, F1 Score: 37.71%
Target Domain Performance - Accuracy: 54.92%, Precision: 53.48%, Recall: 54.79%, F1 Score: 52.85%

Run 5/10
Epoch [1/50], Class Loss: 1.8293, Discrepancy Loss: 0.1166
Epoch [2/50], Class Loss: 0.7621, Discrepancy Loss: 0.0827
Epoch [3/50], Class Loss: 0.3301, Discrepancy Loss: 0.0423
Epoch [4/50], Class Loss: 0.1387, Discrepancy Loss: 0.0284
Epoch [5/50], Class Loss: 0.1190, Discrepancy Loss: 0.0230
Epoch [6/50], Class Loss: 0.0802, Discrepancy Loss: 0.0230
Epoch [7/50], Class Loss: 0.0741, Discrepancy Loss: 0.0189
Epoch [8/50], Class Loss: 0.0259, Discrepancy Loss: 0.0143
Epoch [9/50], Class Loss: 0.0371, Discrepancy Loss: 0.0184
Epoch [10/50], Class Loss: 0.0181, Discrepancy Loss: 0.0152
Epoch [11/50], Class Loss: 0.0096, Discrepancy Loss: 0.0114
Epoch [12/50], Class Loss: 0.0059, Discrepancy Loss: 0.0097
Epoch [13/50], Class Loss: 0.0047, Discrepancy Loss: 0.0115
Epoch [14/50], Class Loss: 0.0017, Discrepancy Loss: 0.0096
Epoch [15/50], Class Loss: 0.0034, Discrepancy Loss: 0.0083
Epoch [16/50], Class Loss: 0.0017, Discrepancy Loss: 0.0092
Epoch [17/50], Class Loss: 0.0046, Discrepancy Loss: 0.0072
Epoch [18/50], Class Loss: 0.0008, Discrepancy Loss: 0.0079
Epoch [19/50], Class Loss: 0.0596, Discrepancy Loss: 0.0075
Epoch [20/50], Class Loss: 0.0219, Discrepancy Loss: 0.0285
Epoch [21/50], Class Loss: 0.0289, Discrepancy Loss: 0.0256
Epoch [22/50], Class Loss: 0.0139, Discrepancy Loss: 0.0256
Epoch [23/50], Class Loss: 0.0069, Discrepancy Loss: 0.0315
Epoch [24/50], Class Loss: 0.0106, Discrepancy Loss: 0.0268
Epoch [25/50], Class Loss: 0.0043, Discrepancy Loss: 0.0250
Epoch [26/50], Class Loss: 0.0057, Discrepancy Loss: 0.0274
Epoch [27/50], Class Loss: 0.0049, Discrepancy Loss: 0.0293
Epoch [28/50], Class Loss: 0.0042, Discrepancy Loss: 0.0265
Epoch [29/50], Class Loss: 0.0040, Discrepancy Loss: 0.0236
Epoch [30/50], Class Loss: 0.0037, Discrepancy Loss: 0.0231
Epoch [31/50], Class Loss: 0.0033, Discrepancy Loss: 0.0234
Epoch [32/50], Class Loss: 0.0064, Discrepancy Loss: 0.0229
Epoch [33/50], Class Loss: 0.0236, Discrepancy Loss: 0.0283
Epoch [34/50], Class Loss: 0.0071, Discrepancy Loss: 0.0248
Epoch [35/50], Class Loss: 0.0036, Discrepancy Loss: 0.0228
Epoch [36/50], Class Loss: 0.0037, Discrepancy Loss: 0.0222
Epoch [37/50], Class Loss: 0.0043, Discrepancy Loss: 0.0220
Epoch [38/50], Class Loss: 0.0031, Discrepancy Loss: 0.0259
Epoch [39/50], Class Loss: 0.0037, Discrepancy Loss: 0.0239
Epoch [40/50], Class Loss: 0.0554, Discrepancy Loss: 0.0222
Epoch [41/50], Class Loss: 0.0037, Discrepancy Loss: 0.0234
Epoch [42/50], Class Loss: 0.0103, Discrepancy Loss: 0.0261
Epoch [43/50], Class Loss: 0.0025, Discrepancy Loss: 0.0231
Epoch [44/50], Class Loss: 0.0209, Discrepancy Loss: 0.0220
Epoch [45/50], Class Loss: 0.0035, Discrepancy Loss: 0.0224
Epoch [46/50], Class Loss: 0.0034, Discrepancy Loss: 0.0230
Epoch [47/50], Class Loss: 0.0033, Discrepancy Loss: 0.0245
Epoch [48/50], Class Loss: 0.0137, Discrepancy Loss: 0.0260
Epoch [49/50], Class Loss: 0.0076, Discrepancy Loss: 0.0240
Epoch [50/50], Class Loss: 0.0034, Discrepancy Loss: 0.0273
Source Domain Performance - Accuracy: 51.68%, Precision: 33.52%, Recall: 50.00%, F1 Score: 37.71%
Target Domain Performance - Accuracy: 49.76%, Precision: 33.25%, Recall: 49.47%, F1 Score: 33.24%

Run 6/10
Epoch [1/50], Class Loss: 1.5646, Discrepancy Loss: 0.1211
Epoch [2/50], Class Loss: 0.8567, Discrepancy Loss: 0.0982
Epoch [3/50], Class Loss: 0.4557, Discrepancy Loss: 0.0607
Epoch [4/50], Class Loss: 0.2458, Discrepancy Loss: 0.0544
Epoch [5/50], Class Loss: 0.3179, Discrepancy Loss: 0.0649
Epoch [6/50], Class Loss: 0.0435, Discrepancy Loss: 0.0437
Epoch [7/50], Class Loss: 0.0278, Discrepancy Loss: 0.0219
Epoch [8/50], Class Loss: 0.0443, Discrepancy Loss: 0.0266
Epoch [9/50], Class Loss: 0.0550, Discrepancy Loss: 0.0313
Epoch [10/50], Class Loss: 0.0262, Discrepancy Loss: 0.0300
Epoch [11/50], Class Loss: 0.0105, Discrepancy Loss: 0.0226
Epoch [12/50], Class Loss: 0.0038, Discrepancy Loss: 0.0213
Epoch [13/50], Class Loss: 0.0064, Discrepancy Loss: 0.0194
Epoch [14/50], Class Loss: 0.0072, Discrepancy Loss: 0.0199
Epoch [15/50], Class Loss: 0.0024, Discrepancy Loss: 0.0219
Epoch [16/50], Class Loss: 0.0026, Discrepancy Loss: 0.0206
Epoch [17/50], Class Loss: 0.0028, Discrepancy Loss: 0.0174
Epoch [18/50], Class Loss: 0.0050, Discrepancy Loss: 0.0228
Epoch [19/50], Class Loss: 0.0169, Discrepancy Loss: 0.0161
Epoch [20/50], Class Loss: 0.0050, Discrepancy Loss: 0.0176
Epoch [21/50], Class Loss: 0.0042, Discrepancy Loss: 0.0207
Epoch [22/50], Class Loss: 0.0082, Discrepancy Loss: 0.0190
Epoch [23/50], Class Loss: 0.0047, Discrepancy Loss: 0.0158
Epoch [24/50], Class Loss: 0.0034, Discrepancy Loss: 0.0162
Epoch [25/50], Class Loss: 0.0086, Discrepancy Loss: 0.0197
Epoch [26/50], Class Loss: 0.0024, Discrepancy Loss: 0.0186
Epoch [27/50], Class Loss: 0.0120, Discrepancy Loss: 0.0164
Epoch [28/50], Class Loss: 0.0038, Discrepancy Loss: 0.0199
Epoch [29/50], Class Loss: 0.0019, Discrepancy Loss: 0.0157
Epoch [30/50], Class Loss: 0.0063, Discrepancy Loss: 0.0188
Epoch [31/50], Class Loss: 0.0089, Discrepancy Loss: 0.0160
Epoch [32/50], Class Loss: 0.0297, Discrepancy Loss: 0.0156
Epoch [33/50], Class Loss: 0.0682, Discrepancy Loss: 0.0151
Epoch [34/50], Class Loss: 0.0067, Discrepancy Loss: 0.0161
Epoch [35/50], Class Loss: 0.0025, Discrepancy Loss: 0.0162
Epoch [36/50], Class Loss: 0.0028, Discrepancy Loss: 0.0141
Epoch [37/50], Class Loss: 0.0028, Discrepancy Loss: 0.0187
Epoch [38/50], Class Loss: 0.0026, Discrepancy Loss: 0.0172
Epoch [39/50], Class Loss: 0.0020, Discrepancy Loss: 0.0167
Epoch [40/50], Class Loss: 0.0024, Discrepancy Loss: 0.0159
Epoch [41/50], Class Loss: 0.0068, Discrepancy Loss: 0.0160
Epoch [42/50], Class Loss: 0.0907, Discrepancy Loss: 0.0144
Epoch [43/50], Class Loss: 0.0012, Discrepancy Loss: 0.0177
Epoch [44/50], Class Loss: 0.0020, Discrepancy Loss: 0.0154
Epoch [45/50], Class Loss: 0.0021, Discrepancy Loss: 0.0161
Epoch [46/50], Class Loss: 0.0086, Discrepancy Loss: 0.0157
Epoch [47/50], Class Loss: 0.0016, Discrepancy Loss: 0.0166
Epoch [48/50], Class Loss: 0.0031, Discrepancy Loss: 0.0152
Epoch [49/50], Class Loss: 0.0074, Discrepancy Loss: 0.0174
Epoch [50/50], Class Loss: 0.0013, Discrepancy Loss: 0.0171
Source Domain Performance - Accuracy: 51.68%, Precision: 33.52%, Recall: 50.00%, F1 Score: 37.71%
Target Domain Performance - Accuracy: 45.86%, Precision: 42.97%, Recall: 45.82%, F1 Score: 44.23%

Run 7/10
Epoch [1/50], Class Loss: 1.9541, Discrepancy Loss: 0.1412
Epoch [2/50], Class Loss: 0.8935, Discrepancy Loss: 0.0989
Epoch [3/50], Class Loss: 0.4378, Discrepancy Loss: 0.0774
Epoch [4/50], Class Loss: 0.1434, Discrepancy Loss: 0.0484
Epoch [5/50], Class Loss: 0.0519, Discrepancy Loss: 0.0269
Epoch [6/50], Class Loss: 0.3356, Discrepancy Loss: 0.0533
Epoch [7/50], Class Loss: 0.0582, Discrepancy Loss: 0.0321
Epoch [8/50], Class Loss: 0.0388, Discrepancy Loss: 0.0259
Epoch [9/50], Class Loss: 0.1103, Discrepancy Loss: 0.0585
Epoch [10/50], Class Loss: 0.0473, Discrepancy Loss: 0.0405
Epoch [11/50], Class Loss: 0.0796, Discrepancy Loss: 0.0343
Epoch [12/50], Class Loss: 0.0113, Discrepancy Loss: 0.0234
Epoch [13/50], Class Loss: 0.0099, Discrepancy Loss: 0.0220
Epoch [14/50], Class Loss: 0.0160, Discrepancy Loss: 0.0260
Epoch [15/50], Class Loss: 0.0330, Discrepancy Loss: 0.0288
Epoch [16/50], Class Loss: 0.1664, Discrepancy Loss: 0.0216
Epoch [17/50], Class Loss: 0.0638, Discrepancy Loss: 0.0604
Epoch [18/50], Class Loss: 0.0188, Discrepancy Loss: 0.0576
Epoch [19/50], Class Loss: 0.0072, Discrepancy Loss: 0.0585
Epoch [20/50], Class Loss: 0.0356, Discrepancy Loss: 0.0668
Epoch [21/50], Class Loss: 0.0083, Discrepancy Loss: 0.0417
Epoch [22/50], Class Loss: 0.0196, Discrepancy Loss: 0.0415
Epoch [23/50], Class Loss: 0.0120, Discrepancy Loss: 0.0425
Epoch [24/50], Class Loss: 0.0117, Discrepancy Loss: 0.0382
Epoch [25/50], Class Loss: 0.0382, Discrepancy Loss: 0.0365
Epoch [26/50], Class Loss: 0.0156, Discrepancy Loss: 0.0374
Epoch [27/50], Class Loss: 0.0106, Discrepancy Loss: 0.0401
Epoch [28/50], Class Loss: 0.0116, Discrepancy Loss: 0.0354
Epoch [29/50], Class Loss: 0.0059, Discrepancy Loss: 0.0346
Epoch [30/50], Class Loss: 0.0072, Discrepancy Loss: 0.0309
Epoch [31/50], Class Loss: 0.0129, Discrepancy Loss: 0.0298
Epoch [32/50], Class Loss: 0.0044, Discrepancy Loss: 0.0281
Epoch [33/50], Class Loss: 0.0143, Discrepancy Loss: 0.0313
Epoch [34/50], Class Loss: 0.0610, Discrepancy Loss: 0.0307
Epoch [35/50], Class Loss: 0.0116, Discrepancy Loss: 0.0316
Epoch [36/50], Class Loss: 0.0073, Discrepancy Loss: 0.0308
Epoch [37/50], Class Loss: 0.0032, Discrepancy Loss: 0.0298
Epoch [38/50], Class Loss: 0.0052, Discrepancy Loss: 0.0329
Epoch [39/50], Class Loss: 0.0759, Discrepancy Loss: 0.0296
Epoch [40/50], Class Loss: 0.0059, Discrepancy Loss: 0.0334
Epoch [41/50], Class Loss: 0.0202, Discrepancy Loss: 0.0346
Epoch [42/50], Class Loss: 0.0070, Discrepancy Loss: 0.0312
Epoch [43/50], Class Loss: 0.0061, Discrepancy Loss: 0.0301
Epoch [44/50], Class Loss: 0.0327, Discrepancy Loss: 0.0260
Epoch [45/50], Class Loss: 0.0068, Discrepancy Loss: 0.0295
Epoch [46/50], Class Loss: 0.0081, Discrepancy Loss: 0.0295
Epoch [47/50], Class Loss: 0.0081, Discrepancy Loss: 0.0290
Epoch [48/50], Class Loss: 0.0029, Discrepancy Loss: 0.0301
Epoch [49/50], Class Loss: 0.0204, Discrepancy Loss: 0.0281
Epoch [50/50], Class Loss: 0.0068, Discrepancy Loss: 0.0350
Source Domain Performance - Accuracy: 63.97%, Precision: 60.24%, Recall: 63.24%, F1 Score: 56.84%
Target Domain Performance - Accuracy: 56.35%, Precision: 62.41%, Recall: 56.45%, F1 Score: 48.97%

Run 8/10
Epoch [1/50], Class Loss: 1.6372, Discrepancy Loss: 0.1075
Epoch [2/50], Class Loss: 0.7667, Discrepancy Loss: 0.0732
Epoch [3/50], Class Loss: 0.5187, Discrepancy Loss: 0.0705
Epoch [4/50], Class Loss: 0.3117, Discrepancy Loss: 0.0512
Epoch [5/50], Class Loss: 0.1524, Discrepancy Loss: 0.0287
Epoch [6/50], Class Loss: 0.0761, Discrepancy Loss: 0.0235
Epoch [7/50], Class Loss: 0.0879, Discrepancy Loss: 0.0201
Epoch [8/50], Class Loss: 0.0419, Discrepancy Loss: 0.0122
Epoch [9/50], Class Loss: 0.0443, Discrepancy Loss: 0.0126
Epoch [10/50], Class Loss: 0.0746, Discrepancy Loss: 0.0178
Epoch [11/50], Class Loss: 0.0136, Discrepancy Loss: 0.0167
Epoch [12/50], Class Loss: 0.0067, Discrepancy Loss: 0.0149
Epoch [13/50], Class Loss: 0.0520, Discrepancy Loss: 0.0141
Epoch [14/50], Class Loss: 0.0130, Discrepancy Loss: 0.0124
Epoch [15/50], Class Loss: 0.0048, Discrepancy Loss: 0.0094
Epoch [16/50], Class Loss: 0.0099, Discrepancy Loss: 0.0094
Epoch [17/50], Class Loss: 0.0106, Discrepancy Loss: 0.0150
Epoch [18/50], Class Loss: 0.0061, Discrepancy Loss: 0.0112
Epoch [19/50], Class Loss: 0.0031, Discrepancy Loss: 0.0142
Epoch [20/50], Class Loss: 0.0087, Discrepancy Loss: 0.0152
Epoch [21/50], Class Loss: 0.0024, Discrepancy Loss: 0.0125
Epoch [22/50], Class Loss: 0.0237, Discrepancy Loss: 0.0121
Epoch [23/50], Class Loss: 0.0039, Discrepancy Loss: 0.0126
Epoch [24/50], Class Loss: 0.0033, Discrepancy Loss: 0.0105
Epoch [25/50], Class Loss: 0.0028, Discrepancy Loss: 0.0109
Epoch [26/50], Class Loss: 0.0297, Discrepancy Loss: 0.0119
Epoch [27/50], Class Loss: 0.0028, Discrepancy Loss: 0.0129
Epoch [28/50], Class Loss: 0.0016, Discrepancy Loss: 0.0127
Epoch [29/50], Class Loss: 0.0035, Discrepancy Loss: 0.0123
Epoch [30/50], Class Loss: 0.0015, Discrepancy Loss: 0.0085
Epoch [31/50], Class Loss: 0.0022, Discrepancy Loss: 0.0120
Epoch [32/50], Class Loss: 0.0220, Discrepancy Loss: 0.0130
Epoch [33/50], Class Loss: 0.0940, Discrepancy Loss: 0.0115
Epoch [34/50], Class Loss: 0.0284, Discrepancy Loss: 0.0087
Epoch [35/50], Class Loss: 0.0016, Discrepancy Loss: 0.0087
Epoch [36/50], Class Loss: 0.0063, Discrepancy Loss: 0.0115
Epoch [37/50], Class Loss: 0.0010, Discrepancy Loss: 0.0095
Epoch [38/50], Class Loss: 0.0024, Discrepancy Loss: 0.0121
Epoch [39/50], Class Loss: 0.0019, Discrepancy Loss: 0.0092
Epoch [40/50], Class Loss: 0.0035, Discrepancy Loss: 0.0094
Epoch [41/50], Class Loss: 0.0100, Discrepancy Loss: 0.0090
Epoch [42/50], Class Loss: 0.0020, Discrepancy Loss: 0.0091
Epoch [43/50], Class Loss: 0.0026, Discrepancy Loss: 0.0138
Epoch [44/50], Class Loss: 0.0095, Discrepancy Loss: 0.0083
Epoch [45/50], Class Loss: 0.0061, Discrepancy Loss: 0.0098
Epoch [46/50], Class Loss: 0.0019, Discrepancy Loss: 0.0107
Epoch [47/50], Class Loss: 0.0067, Discrepancy Loss: 0.0102
Epoch [48/50], Class Loss: 0.0026, Discrepancy Loss: 0.0124
Epoch [49/50], Class Loss: 0.0020, Discrepancy Loss: 0.0103
Epoch [50/50], Class Loss: 0.0090, Discrepancy Loss: 0.0097
Source Domain Performance - Accuracy: 51.68%, Precision: 33.52%, Recall: 50.00%, F1 Score: 37.71%
Target Domain Performance - Accuracy: 47.00%, Precision: 39.81%, Recall: 47.01%, F1 Score: 42.52%

Run 9/10
Epoch [1/50], Class Loss: 1.7464, Discrepancy Loss: 0.1310
Epoch [2/50], Class Loss: 0.8018, Discrepancy Loss: 0.0792
Epoch [3/50], Class Loss: 0.5145, Discrepancy Loss: 0.0716
Epoch [4/50], Class Loss: 0.5992, Discrepancy Loss: 0.0911
Epoch [5/50], Class Loss: 0.2278, Discrepancy Loss: 0.0484
Epoch [6/50], Class Loss: 0.1536, Discrepancy Loss: 0.0392
Epoch [7/50], Class Loss: 0.1308, Discrepancy Loss: 0.0430
Epoch [8/50], Class Loss: 0.1112, Discrepancy Loss: 0.0362
Epoch [9/50], Class Loss: 0.0699, Discrepancy Loss: 0.0249
Epoch [10/50], Class Loss: 0.0728, Discrepancy Loss: 0.0326
Epoch [11/50], Class Loss: 0.0668, Discrepancy Loss: 0.0212
Epoch [12/50], Class Loss: 0.0192, Discrepancy Loss: 0.0168
Epoch [13/50], Class Loss: 0.0202, Discrepancy Loss: 0.0151
Epoch [14/50], Class Loss: 0.0201, Discrepancy Loss: 0.0112
Epoch [15/50], Class Loss: 0.0157, Discrepancy Loss: 0.0140
Epoch [16/50], Class Loss: 0.0103, Discrepancy Loss: 0.0131
Epoch [17/50], Class Loss: 0.0095, Discrepancy Loss: 0.0112
Epoch [18/50], Class Loss: 0.0080, Discrepancy Loss: 0.0100
Epoch [19/50], Class Loss: 0.0053, Discrepancy Loss: 0.0093
Epoch [20/50], Class Loss: 0.0060, Discrepancy Loss: 0.0063
Epoch [21/50], Class Loss: 0.0086, Discrepancy Loss: 0.0069
Epoch [22/50], Class Loss: 0.0068, Discrepancy Loss: 0.0092
Epoch [23/50], Class Loss: 0.0020, Discrepancy Loss: 0.0110
Epoch [24/50], Class Loss: 0.0015, Discrepancy Loss: 0.0080
Epoch [25/50], Class Loss: 0.0072, Discrepancy Loss: 0.0082
Epoch [26/50], Class Loss: 0.0027, Discrepancy Loss: 0.0095
Epoch [27/50], Class Loss: 0.0030, Discrepancy Loss: 0.0083
Epoch [28/50], Class Loss: 0.0013, Discrepancy Loss: 0.0073
Epoch [29/50], Class Loss: 0.0027, Discrepancy Loss: 0.0064
Epoch [30/50], Class Loss: 0.0030, Discrepancy Loss: 0.0061
Epoch [31/50], Class Loss: 0.0016, Discrepancy Loss: 0.0081
Epoch [32/50], Class Loss: 0.0030, Discrepancy Loss: 0.0064
Epoch [33/50], Class Loss: 0.0018, Discrepancy Loss: 0.0075
Epoch [34/50], Class Loss: 0.0025, Discrepancy Loss: 0.0082
Epoch [35/50], Class Loss: 0.0068, Discrepancy Loss: 0.0071
Epoch [36/50], Class Loss: 0.0023, Discrepancy Loss: 0.0075
Epoch [37/50], Class Loss: 0.0030, Discrepancy Loss: 0.0085
Epoch [38/50], Class Loss: 0.0019, Discrepancy Loss: 0.0101
Epoch [39/50], Class Loss: 0.0045, Discrepancy Loss: 0.0066
Epoch [40/50], Class Loss: 0.0034, Discrepancy Loss: 0.0073
Epoch [41/50], Class Loss: 0.0025, Discrepancy Loss: 0.0077
Epoch [42/50], Class Loss: 0.0904, Discrepancy Loss: 0.0093
Epoch [43/50], Class Loss: 0.0038, Discrepancy Loss: 0.0078
Epoch [44/50], Class Loss: 0.0017, Discrepancy Loss: 0.0075
Epoch [45/50], Class Loss: 0.0018, Discrepancy Loss: 0.0073
Epoch [46/50], Class Loss: 0.0021, Discrepancy Loss: 0.0081
Epoch [47/50], Class Loss: 0.0331, Discrepancy Loss: 0.0063
Epoch [48/50], Class Loss: 0.0072, Discrepancy Loss: 0.0089
Epoch [49/50], Class Loss: 0.0308, Discrepancy Loss: 0.0060
Epoch [50/50], Class Loss: 0.0035, Discrepancy Loss: 0.0103
Source Domain Performance - Accuracy: 51.68%, Precision: 33.52%, Recall: 50.00%, F1 Score: 37.71%
Target Domain Performance - Accuracy: 40.47%, Precision: 51.08%, Recall: 40.36%, F1 Score: 40.73%

Run 10/10
Epoch [1/50], Class Loss: 1.9067, Discrepancy Loss: 0.1159
Epoch [2/50], Class Loss: 0.7332, Discrepancy Loss: 0.0861
Epoch [3/50], Class Loss: 0.2562, Discrepancy Loss: 0.0569
Epoch [4/50], Class Loss: 0.2792, Discrepancy Loss: 0.0485
Epoch [5/50], Class Loss: 0.1458, Discrepancy Loss: 0.0385
Epoch [6/50], Class Loss: 0.0893, Discrepancy Loss: 0.0184
Epoch [7/50], Class Loss: 0.1022, Discrepancy Loss: 0.0254
Epoch [8/50], Class Loss: 0.1673, Discrepancy Loss: 0.0247
Epoch [9/50], Class Loss: 0.1049, Discrepancy Loss: 0.0235
Epoch [10/50], Class Loss: 0.0387, Discrepancy Loss: 0.0152
Epoch [11/50], Class Loss: 0.0096, Discrepancy Loss: 0.0125
Epoch [12/50], Class Loss: 0.0104, Discrepancy Loss: 0.0149
Epoch [13/50], Class Loss: 0.0228, Discrepancy Loss: 0.0122
Epoch [14/50], Class Loss: 0.0155, Discrepancy Loss: 0.0159
Epoch [15/50], Class Loss: 0.0099, Discrepancy Loss: 0.0134
Epoch [16/50], Class Loss: 0.0104, Discrepancy Loss: 0.0106
Epoch [17/50], Class Loss: 0.0135, Discrepancy Loss: 0.0139
Epoch [18/50], Class Loss: 0.0051, Discrepancy Loss: 0.0120
Epoch [19/50], Class Loss: 0.1331, Discrepancy Loss: 0.0096
Epoch [20/50], Class Loss: 0.0172, Discrepancy Loss: 0.0179
Epoch [21/50], Class Loss: 0.0051, Discrepancy Loss: 0.0129
Epoch [22/50], Class Loss: 0.0105, Discrepancy Loss: 0.0125
Epoch [23/50], Class Loss: 0.0052, Discrepancy Loss: 0.0125
Epoch [24/50], Class Loss: 0.0048, Discrepancy Loss: 0.0139
Epoch [25/50], Class Loss: 0.0096, Discrepancy Loss: 0.0115
Epoch [26/50], Class Loss: 0.0216, Discrepancy Loss: 0.0130
Epoch [27/50], Class Loss: 0.0182, Discrepancy Loss: 0.0111
Epoch [28/50], Class Loss: 0.0085, Discrepancy Loss: 0.0147
Epoch [29/50], Class Loss: 0.0086, Discrepancy Loss: 0.0108
Epoch [30/50], Class Loss: 0.0048, Discrepancy Loss: 0.0113
Epoch [31/50], Class Loss: 0.0049, Discrepancy Loss: 0.0101
Epoch [32/50], Class Loss: 0.0082, Discrepancy Loss: 0.0099
Epoch [33/50], Class Loss: 0.0039, Discrepancy Loss: 0.0105
Epoch [34/50], Class Loss: 0.0072, Discrepancy Loss: 0.0081
Epoch [35/50], Class Loss: 0.0057, Discrepancy Loss: 0.0098
Epoch [36/50], Class Loss: 0.0051, Discrepancy Loss: 0.0087
Epoch [37/50], Class Loss: 0.0035, Discrepancy Loss: 0.0111
Epoch [38/50], Class Loss: 0.0440, Discrepancy Loss: 0.0125
Epoch [39/50], Class Loss: 0.0155, Discrepancy Loss: 0.0079
Epoch [40/50], Class Loss: 0.0614, Discrepancy Loss: 0.0121
Epoch [41/50], Class Loss: 0.0073, Discrepancy Loss: 0.0093
Epoch [42/50], Class Loss: 0.0043, Discrepancy Loss: 0.0133
Epoch [43/50], Class Loss: 0.0054, Discrepancy Loss: 0.0080
Epoch [44/50], Class Loss: 0.0045, Discrepancy Loss: 0.0083
Epoch [45/50], Class Loss: 0.0066, Discrepancy Loss: 0.0094
Epoch [46/50], Class Loss: 0.0087, Discrepancy Loss: 0.0145
Epoch [47/50], Class Loss: 0.0098, Discrepancy Loss: 0.0109
Epoch [48/50], Class Loss: 0.0072, Discrepancy Loss: 0.0120
Epoch [49/50], Class Loss: 0.0044, Discrepancy Loss: 0.0105
Epoch [50/50], Class Loss: 0.0068, Discrepancy Loss: 0.0084
Source Domain Performance - Accuracy: 51.56%, Precision: 33.52%, Recall: 49.89%, F1 Score: 37.66%
Target Domain Performance - Accuracy: 47.72%, Precision: 52.09%, Recall: 47.61%, F1 Score: 47.44%

Source performance: 52.89% 36.20% 51.31% 39.62%
Target performance: 46.98% 48.31% 46.92% 42.84%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 10.85%
16qam: 46.48%
8apsk: 30.36%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.6989, Discrepancy Loss: 0.1078
Validation Loss: 0.5962
Epoch [2/50], Class Loss: 0.3110, Discrepancy Loss: 0.2109
Validation Loss: 0.2023
Epoch [3/50], Class Loss: 0.1528, Discrepancy Loss: 0.1624
Validation Loss: 4.4064
Epoch [4/50], Class Loss: 0.1339, Discrepancy Loss: 0.1068
Validation Loss: 35.7918
Epoch [5/50], Class Loss: 0.1044, Discrepancy Loss: 0.0970
Validation Loss: 22.1103
Epoch [6/50], Class Loss: 0.0641, Discrepancy Loss: 0.0104
Validation Loss: 0.1572
Epoch [7/50], Class Loss: 0.0287, Discrepancy Loss: 0.0054
Validation Loss: 0.0012
Epoch [8/50], Class Loss: 0.0459, Discrepancy Loss: 0.0234
Validation Loss: 17.9846
Epoch [9/50], Class Loss: 0.0617, Discrepancy Loss: 0.0601
Validation Loss: 0.4978
Epoch [10/50], Class Loss: 0.0238, Discrepancy Loss: 0.0077
Validation Loss: 23.6916
Epoch [11/50], Class Loss: 0.0020, Discrepancy Loss: 0.0097
Validation Loss: 0.0074
Epoch [12/50], Class Loss: 0.0016, Discrepancy Loss: 0.0252
Validation Loss: 0.0000
Epoch [13/50], Class Loss: 0.0022, Discrepancy Loss: 0.0287
Validation Loss: 0.0000
Epoch [14/50], Class Loss: 0.0115, Discrepancy Loss: 0.0372
Validation Loss: 0.0019
Epoch [15/50], Class Loss: 0.0030, Discrepancy Loss: 0.0483
Validation Loss: 0.0857
Epoch [16/50], Class Loss: 0.0028, Discrepancy Loss: 0.0637
Validation Loss: 0.0018
Epoch [17/50], Class Loss: 0.0057, Discrepancy Loss: 0.0546
Validation Loss: 0.0278
Epoch [18/50], Class Loss: 0.0037, Discrepancy Loss: 0.0334
Validation Loss: 0.0584
Early stopping!
Source Domain Performance - Accuracy: 98.98%, Precision: 98.95%, Recall: 98.99%, F1 Score: 98.95%
Target Domain Performance - Accuracy: 35.73%, Precision: 23.96%, Recall: 35.60%, F1 Score: 28.42%

Run 2/10
Epoch [1/50], Class Loss: 0.7631, Discrepancy Loss: 0.0532
Validation Loss: 2.0818
Epoch [2/50], Class Loss: 0.2901, Discrepancy Loss: 0.0518
Validation Loss: 0.2834
Epoch [3/50], Class Loss: 0.2359, Discrepancy Loss: 0.0419
Validation Loss: 0.3111
Epoch [4/50], Class Loss: 0.1248, Discrepancy Loss: 0.0548
Validation Loss: 0.0626
Epoch [5/50], Class Loss: 0.3214, Discrepancy Loss: 0.0748
Validation Loss: 0.7545
Epoch [6/50], Class Loss: 0.1088, Discrepancy Loss: 0.1266
Validation Loss: 8.0162
Epoch [7/50], Class Loss: 0.3300, Discrepancy Loss: 0.0884
Validation Loss: 14.2651
Epoch [8/50], Class Loss: 0.0284, Discrepancy Loss: 0.0114
Validation Loss: 44.2465
Epoch [9/50], Class Loss: 0.0486, Discrepancy Loss: 0.0349
Validation Loss: 2.4538
Early stopping!
Source Domain Performance - Accuracy: 81.29%, Precision: 88.73%, Recall: 81.37%, F1 Score: 78.10%
Target Domain Performance - Accuracy: 50.30%, Precision: 29.58%, Recall: 50.00%, F1 Score: 35.85%

Run 3/10
Epoch [1/50], Class Loss: 0.8033, Discrepancy Loss: 0.0714
Validation Loss: 3.3755
Epoch [2/50], Class Loss: 0.2732, Discrepancy Loss: 0.0222
Validation Loss: 1.1002
Epoch [3/50], Class Loss: 0.1348, Discrepancy Loss: 0.0332
Validation Loss: 3.6956
Epoch [4/50], Class Loss: 0.1463, Discrepancy Loss: 0.0323
Validation Loss: 41.3170
Epoch [5/50], Class Loss: 0.1279, Discrepancy Loss: 0.0774
Validation Loss: 14.2899
Epoch [6/50], Class Loss: 0.1338, Discrepancy Loss: 0.0804
Validation Loss: 0.1393
Epoch [7/50], Class Loss: 0.0491, Discrepancy Loss: 0.1013
Validation Loss: 0.7310
Epoch [8/50], Class Loss: 0.0334, Discrepancy Loss: 0.0711
Validation Loss: 0.8429
Epoch [9/50], Class Loss: 0.0301, Discrepancy Loss: 0.1995
Validation Loss: 35.7526
Epoch [10/50], Class Loss: 0.0460, Discrepancy Loss: 0.0181
Validation Loss: 1.3908
Epoch [11/50], Class Loss: 0.0008, Discrepancy Loss: 0.0048
Validation Loss: 0.0000
Epoch [12/50], Class Loss: 0.0004, Discrepancy Loss: 0.0011
Validation Loss: 0.0000
Epoch [13/50], Class Loss: 0.0000, Discrepancy Loss: 0.0015
Validation Loss: 0.0000
Epoch [14/50], Class Loss: 0.0001, Discrepancy Loss: 0.0020
Validation Loss: 0.0000
Epoch [15/50], Class Loss: 0.0033, Discrepancy Loss: 0.0008
Validation Loss: 0.0000
Epoch [16/50], Class Loss: 0.0000, Discrepancy Loss: 0.0008
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0007, Discrepancy Loss: 0.0009
Validation Loss: 0.0000
Epoch [18/50], Class Loss: 0.0051, Discrepancy Loss: 0.0010
Validation Loss: 0.0000
Epoch [19/50], Class Loss: 0.0004, Discrepancy Loss: 0.0017
Validation Loss: 0.0000
Epoch [20/50], Class Loss: 0.0001, Discrepancy Loss: 0.0031
Validation Loss: 0.0000
Epoch [21/50], Class Loss: 0.0000, Discrepancy Loss: 0.0019
Validation Loss: 0.0000
Epoch [22/50], Class Loss: 0.0000, Discrepancy Loss: 0.0008
Validation Loss: 0.0000
Epoch [23/50], Class Loss: 0.0000, Discrepancy Loss: 0.0008
Validation Loss: 0.0000
Epoch [24/50], Class Loss: 0.0000, Discrepancy Loss: 0.0009
Validation Loss: 0.0000
Epoch [25/50], Class Loss: 0.0000, Discrepancy Loss: 0.0009
Validation Loss: 0.0000
Epoch [26/50], Class Loss: 0.0000, Discrepancy Loss: 0.0009
Validation Loss: 0.0000
Epoch [27/50], Class Loss: 0.0000, Discrepancy Loss: 0.0009
Validation Loss: 0.0000
Epoch [28/50], Class Loss: 0.0000, Discrepancy Loss: 0.0010
Validation Loss: 0.0000
Epoch [29/50], Class Loss: 0.0000, Discrepancy Loss: 0.0009
Validation Loss: 0.0000
Epoch [30/50], Class Loss: 0.0917, Discrepancy Loss: 0.0012
Validation Loss: 0.0000
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 59.59%, Precision: 55.96%, Recall: 59.53%, F1 Score: 52.41%

Run 4/10
Epoch [1/50], Class Loss: 0.7528, Discrepancy Loss: 0.1029
Validation Loss: 10.2271
Epoch [2/50], Class Loss: 0.2921, Discrepancy Loss: 0.0327
Validation Loss: 2.4714
Epoch [3/50], Class Loss: 0.1801, Discrepancy Loss: 0.0712
Validation Loss: 0.2979
Epoch [4/50], Class Loss: 0.1353, Discrepancy Loss: 0.2067
Validation Loss: 0.4640
Epoch [5/50], Class Loss: 0.0778, Discrepancy Loss: 0.0266
Validation Loss: 2.3614
Epoch [6/50], Class Loss: 0.0506, Discrepancy Loss: 0.0035
Validation Loss: 0.6394
Epoch [7/50], Class Loss: 0.0166, Discrepancy Loss: 0.0070
Validation Loss: 0.1015
Epoch [8/50], Class Loss: 0.0698, Discrepancy Loss: 0.0305
Validation Loss: 0.0408
Epoch [9/50], Class Loss: 0.0650, Discrepancy Loss: 0.0648
Validation Loss: 1.9965
Epoch [10/50], Class Loss: 0.0253, Discrepancy Loss: 0.1371
Validation Loss: 0.0042
Epoch [11/50], Class Loss: 0.0027, Discrepancy Loss: 0.0009
Validation Loss: 0.0002
Epoch [12/50], Class Loss: 0.0002, Discrepancy Loss: 0.0007
Validation Loss: 0.0004
Epoch [13/50], Class Loss: 0.0168, Discrepancy Loss: 0.0008
Validation Loss: 0.0002
Epoch [14/50], Class Loss: 0.0480, Discrepancy Loss: 0.0009
Validation Loss: 0.0027
Epoch [15/50], Class Loss: 0.0001, Discrepancy Loss: 0.0008
Validation Loss: 0.0000
Epoch [16/50], Class Loss: 0.0361, Discrepancy Loss: 0.0011
Validation Loss: 0.0002
Epoch [17/50], Class Loss: 0.0000, Discrepancy Loss: 0.0106
Validation Loss: 0.0006
Epoch [18/50], Class Loss: 0.0092, Discrepancy Loss: 0.0577
Validation Loss: 0.0000
Epoch [19/50], Class Loss: 0.0068, Discrepancy Loss: 0.1828
Validation Loss: 0.8358
Epoch [20/50], Class Loss: 0.0078, Discrepancy Loss: 0.3106
Validation Loss: 2.9888
Epoch [21/50], Class Loss: 0.0408, Discrepancy Loss: 0.3014
Validation Loss: 0.0010
Epoch [22/50], Class Loss: 0.0061, Discrepancy Loss: 0.3323
Validation Loss: 0.0060
Epoch [23/50], Class Loss: 0.0034, Discrepancy Loss: 0.3368
Validation Loss: 0.0099
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 36.21%, Precision: 24.30%, Recall: 36.05%, F1 Score: 24.88%

Run 5/10
Epoch [1/50], Class Loss: 0.7833, Discrepancy Loss: 0.0672
Validation Loss: 0.8692
Epoch [2/50], Class Loss: 0.3004, Discrepancy Loss: 0.1074
Validation Loss: 4.2549
Epoch [3/50], Class Loss: 0.1429, Discrepancy Loss: 0.0943
Validation Loss: 1.4774
Epoch [4/50], Class Loss: 0.1294, Discrepancy Loss: 0.1295
Validation Loss: 6.0924
Epoch [5/50], Class Loss: 0.1270, Discrepancy Loss: 0.0815
Validation Loss: 0.7584
Epoch [6/50], Class Loss: 0.0574, Discrepancy Loss: 0.0500
Validation Loss: 4.5938
Epoch [7/50], Class Loss: 0.0383, Discrepancy Loss: 0.0050
Validation Loss: 11.8887
Epoch [8/50], Class Loss: 0.0071, Discrepancy Loss: 0.0106
Validation Loss: 1.5101
Epoch [9/50], Class Loss: 0.0226, Discrepancy Loss: 0.0063
Validation Loss: 0.0521
Epoch [10/50], Class Loss: 0.0027, Discrepancy Loss: 0.0041
Validation Loss: 0.0710
Epoch [11/50], Class Loss: 0.0002, Discrepancy Loss: 0.0082
Validation Loss: 0.0016
Epoch [12/50], Class Loss: 0.0004, Discrepancy Loss: 0.0019
Validation Loss: 0.0002
Epoch [13/50], Class Loss: 0.0031, Discrepancy Loss: 0.0025
Validation Loss: 0.0003
Epoch [14/50], Class Loss: 0.0000, Discrepancy Loss: 0.0025
Validation Loss: 0.0002
Epoch [15/50], Class Loss: 0.0003, Discrepancy Loss: 0.0024
Validation Loss: 0.0007
Epoch [16/50], Class Loss: 0.0000, Discrepancy Loss: 0.0024
Validation Loss: 0.0002
Epoch [17/50], Class Loss: 0.0001, Discrepancy Loss: 0.0025
Validation Loss: 0.0003
Epoch [18/50], Class Loss: 0.0000, Discrepancy Loss: 0.0052
Validation Loss: 0.0003
Epoch [19/50], Class Loss: 0.0008, Discrepancy Loss: 0.0530
Validation Loss: 0.2335
Early stopping!
Source Domain Performance - Accuracy: 96.28%, Precision: 96.78%, Recall: 96.28%, F1 Score: 96.27%
Target Domain Performance - Accuracy: 50.18%, Precision: 26.90%, Recall: 49.88%, F1 Score: 34.43%

Run 6/10
Epoch [1/50], Class Loss: 0.8468, Discrepancy Loss: 0.0573
Validation Loss: 51.8236
Epoch [2/50], Class Loss: 0.5356, Discrepancy Loss: 0.0738
Validation Loss: 3.3315
Epoch [3/50], Class Loss: 0.1553, Discrepancy Loss: 0.0950
Validation Loss: 0.6842
Epoch [4/50], Class Loss: 0.1275, Discrepancy Loss: 0.0356
Validation Loss: 3.0278
Epoch [5/50], Class Loss: 0.1307, Discrepancy Loss: 0.1188
Validation Loss: 11.3460
Epoch [6/50], Class Loss: 0.0506, Discrepancy Loss: 0.0905
Validation Loss: 0.8280
Epoch [7/50], Class Loss: 0.0313, Discrepancy Loss: 0.0459
Validation Loss: 0.1214
Epoch [8/50], Class Loss: 0.1077, Discrepancy Loss: 0.0507
Validation Loss: 8.1379
Epoch [9/50], Class Loss: 0.0266, Discrepancy Loss: 0.0812
Validation Loss: 37.1073
Epoch [10/50], Class Loss: 0.0361, Discrepancy Loss: 0.0217
Validation Loss: 0.0417
Epoch [11/50], Class Loss: 0.0005, Discrepancy Loss: 0.0006
Validation Loss: 0.0002
Epoch [12/50], Class Loss: 0.0001, Discrepancy Loss: 0.0009
Validation Loss: 0.0001
Epoch [13/50], Class Loss: 0.0001, Discrepancy Loss: 0.0020
Validation Loss: 0.0000
Epoch [14/50], Class Loss: 0.0002, Discrepancy Loss: 0.0013
Validation Loss: 0.0000
Epoch [15/50], Class Loss: 0.0000, Discrepancy Loss: 0.0015
Validation Loss: 0.0000
Epoch [16/50], Class Loss: 0.0000, Discrepancy Loss: 0.0016
Validation Loss: 0.0000
Epoch [17/50], Class Loss: 0.0064, Discrepancy Loss: 0.0018
Validation Loss: 0.0002
Epoch [18/50], Class Loss: 0.0000, Discrepancy Loss: 0.0032
Validation Loss: 0.0000
Epoch [19/50], Class Loss: 0.0000, Discrepancy Loss: 0.0025
Validation Loss: 0.0000
Epoch [20/50], Class Loss: 0.0000, Discrepancy Loss: 0.0026
Validation Loss: 0.0000
Epoch [21/50], Class Loss: 0.0001, Discrepancy Loss: 0.0025
Validation Loss: 0.0000
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 50.12%, Precision: 25.50%, Recall: 49.82%, F1 Score: 33.61%

Run 7/10
Epoch [1/50], Class Loss: 0.8258, Discrepancy Loss: 0.1214
Validation Loss: 5.9501
Epoch [2/50], Class Loss: 0.3228, Discrepancy Loss: 0.0175
Validation Loss: 0.6168
Epoch [3/50], Class Loss: 0.2059, Discrepancy Loss: 0.0222
Validation Loss: 1.3295
Epoch [4/50], Class Loss: 0.0959, Discrepancy Loss: 0.1478
Validation Loss: 0.4439
Epoch [5/50], Class Loss: 0.0996, Discrepancy Loss: 0.1977
Validation Loss: 3.9611
Epoch [6/50], Class Loss: 0.0374, Discrepancy Loss: 0.1580
Validation Loss: 0.2549
Epoch [7/50], Class Loss: 0.0212, Discrepancy Loss: 0.1167
Validation Loss: 0.6223
Epoch [8/50], Class Loss: 0.0425, Discrepancy Loss: 0.1525
Validation Loss: 0.8975
Epoch [9/50], Class Loss: 0.0248, Discrepancy Loss: 0.0288
Validation Loss: 4.8667
Epoch [10/50], Class Loss: 0.0180, Discrepancy Loss: 0.0032
Validation Loss: 1.3294
Epoch [11/50], Class Loss: 0.0457, Discrepancy Loss: 0.0030
Validation Loss: 0.0125
Epoch [12/50], Class Loss: 0.0006, Discrepancy Loss: 0.0018
Validation Loss: 0.0007
Epoch [13/50], Class Loss: 0.0003, Discrepancy Loss: 0.0012
Validation Loss: 0.0014
Epoch [14/50], Class Loss: 0.0054, Discrepancy Loss: 0.0014
Validation Loss: 0.0043
Epoch [15/50], Class Loss: 0.0007, Discrepancy Loss: 0.0018
Validation Loss: 0.0053
Epoch [16/50], Class Loss: 0.0001, Discrepancy Loss: 0.0015
Validation Loss: 0.0104
Epoch [17/50], Class Loss: 0.0003, Discrepancy Loss: 0.0025
Validation Loss: 0.0088
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 48.02%, Precision: 25.77%, Recall: 47.74%, F1 Score: 33.18%

Run 8/10
Epoch [1/50], Class Loss: 0.7599, Discrepancy Loss: 0.0684
Validation Loss: 1.8793
Epoch [2/50], Class Loss: 0.1869, Discrepancy Loss: 0.0774
Validation Loss: 0.6007
Epoch [3/50], Class Loss: 0.1829, Discrepancy Loss: 0.0387
Validation Loss: 4.1431
Epoch [4/50], Class Loss: 0.1117, Discrepancy Loss: 0.1461
Validation Loss: 3.7876
Epoch [5/50], Class Loss: 0.0440, Discrepancy Loss: 0.1838
Validation Loss: 5.3697
Epoch [6/50], Class Loss: 0.0948, Discrepancy Loss: 0.1400
Validation Loss: 6.7629
Epoch [7/50], Class Loss: 0.0777, Discrepancy Loss: 0.1096
Validation Loss: 6.9873
Early stopping!
Source Domain Performance - Accuracy: 76.80%, Precision: 78.03%, Recall: 75.02%, F1 Score: 67.43%
Target Domain Performance - Accuracy: 50.42%, Precision: 58.33%, Recall: 50.12%, F1 Score: 37.81%

Run 9/10
Epoch [1/50], Class Loss: 0.8353, Discrepancy Loss: 0.0465
Validation Loss: 1.8002
Epoch [2/50], Class Loss: 0.2193, Discrepancy Loss: 0.0493
Validation Loss: 0.8605
Epoch [3/50], Class Loss: 0.1842, Discrepancy Loss: 0.0592
Validation Loss: 14.4833
Epoch [4/50], Class Loss: 0.1717, Discrepancy Loss: 0.0129
Validation Loss: 5.0111
Epoch [5/50], Class Loss: 0.1754, Discrepancy Loss: 0.0315
Validation Loss: 0.8342
Epoch [6/50], Class Loss: 0.1628, Discrepancy Loss: 0.1834
Validation Loss: 11.6768
Epoch [7/50], Class Loss: 0.0866, Discrepancy Loss: 0.0804
Validation Loss: 2.1324
Epoch [8/50], Class Loss: 0.0918, Discrepancy Loss: 0.0228
Validation Loss: 0.0379
Epoch [9/50], Class Loss: 0.0936, Discrepancy Loss: 0.0699
Validation Loss: 0.1106
Epoch [10/50], Class Loss: 0.0869, Discrepancy Loss: 0.1722
Validation Loss: 2.9542
Epoch [11/50], Class Loss: 0.0029, Discrepancy Loss: 0.1684
Validation Loss: 0.0047
Epoch [12/50], Class Loss: 0.0012, Discrepancy Loss: 0.0601
Validation Loss: 0.0056
Epoch [13/50], Class Loss: 0.0102, Discrepancy Loss: 0.1355
Validation Loss: 0.0041
Epoch [14/50], Class Loss: 0.0025, Discrepancy Loss: 0.1755
Validation Loss: 0.1664
Epoch [15/50], Class Loss: 0.0049, Discrepancy Loss: 0.2430
Validation Loss: 0.0001
Epoch [16/50], Class Loss: 0.0022, Discrepancy Loss: 0.2467
Validation Loss: 0.0007
Epoch [17/50], Class Loss: 0.0017, Discrepancy Loss: 0.2857
Validation Loss: 0.0307
Epoch [18/50], Class Loss: 0.0018, Discrepancy Loss: 0.2889
Validation Loss: 0.0004
Epoch [19/50], Class Loss: 0.0265, Discrepancy Loss: 0.2520
Validation Loss: 0.0004
Epoch [20/50], Class Loss: 0.0040, Discrepancy Loss: 0.2616
Validation Loss: 0.2816
Early stopping!
Source Domain Performance - Accuracy: 95.68%, Precision: 96.08%, Recall: 95.70%, F1 Score: 95.52%
Target Domain Performance - Accuracy: 26.14%, Precision: 31.34%, Recall: 26.07%, F1 Score: 12.17%

Run 10/10
Epoch [1/50], Class Loss: 0.7189, Discrepancy Loss: 0.0917
Validation Loss: 3.1878
Epoch [2/50], Class Loss: 0.2883, Discrepancy Loss: 0.1254
Validation Loss: 1.1580
Epoch [3/50], Class Loss: 0.1410, Discrepancy Loss: 0.0287
Validation Loss: 3.6709
Epoch [4/50], Class Loss: 0.0517, Discrepancy Loss: 0.0597
Validation Loss: 71.2708
Epoch [5/50], Class Loss: 0.0505, Discrepancy Loss: 0.2427
Validation Loss: 19.3196
Epoch [6/50], Class Loss: 0.1122, Discrepancy Loss: 0.1011
Validation Loss: 0.6189
Epoch [7/50], Class Loss: 0.0290, Discrepancy Loss: 0.0497
Validation Loss: 1.2918
Epoch [8/50], Class Loss: 0.0049, Discrepancy Loss: 0.0018
Validation Loss: 0.5094
Epoch [9/50], Class Loss: 0.0236, Discrepancy Loss: 0.0154
Validation Loss: 0.0355
Epoch [10/50], Class Loss: 0.0263, Discrepancy Loss: 0.1093
Validation Loss: 0.1433
Epoch [11/50], Class Loss: 0.0020, Discrepancy Loss: 0.2210
Validation Loss: 0.0011
Epoch [12/50], Class Loss: 0.0647, Discrepancy Loss: 0.0522
Validation Loss: 0.0002
Epoch [13/50], Class Loss: 0.0015, Discrepancy Loss: 0.0422
Validation Loss: 0.0004
Epoch [14/50], Class Loss: 0.0005, Discrepancy Loss: 0.0810
Validation Loss: 0.0006
Epoch [15/50], Class Loss: 0.0067, Discrepancy Loss: 0.0807
Validation Loss: 0.1927
Epoch [16/50], Class Loss: 0.0098, Discrepancy Loss: 0.2438
Validation Loss: 1.1830
Epoch [17/50], Class Loss: 0.0224, Discrepancy Loss: 0.2179
Validation Loss: 0.4304
Early stopping!
Source Domain Performance - Accuracy: 92.27%, Precision: 93.75%, Recall: 92.29%, F1 Score: 92.25%
Target Domain Performance - Accuracy: 47.54%, Precision: 26.47%, Recall: 47.27%, F1 Score: 33.31%

Source performance: 94.12% 95.22% 93.95% 92.84%
Target performance: 45.43% 32.81% 45.21% 32.61%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 0.05%
16qam: 10.05%
8apsk: 70.74%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.3839, JMMD Loss: 0.1871
Validation Loss: 0.7760
Epoch [2/50], Class Loss: 0.1290, JMMD Loss: 0.1233
Validation Loss: 0.6786
Epoch [3/50], Class Loss: 0.0988, JMMD Loss: 0.1192
Validation Loss: 2.6056
Epoch [4/50], Class Loss: 0.0458, JMMD Loss: 0.1182
Validation Loss: 1.7263
Epoch [5/50], Class Loss: 0.0891, JMMD Loss: 0.1339
Validation Loss: 2.4499
Epoch [6/50], Class Loss: 0.0868, JMMD Loss: 0.1388
Validation Loss: 6.0668
Epoch [7/50], Class Loss: 0.0155, JMMD Loss: 0.1140
Validation Loss: 5.0205
Early stopping!
Source Domain Performance - Accuracy: 51.92%, Precision: 59.54%, Recall: 50.24%, F1 Score: 39.28%
Target Domain Performance - Accuracy: 55.46%, Precision: 48.38%, Recall: 55.20%, F1 Score: 45.29%

Run 2/10
Epoch [1/50], Class Loss: 0.4338, JMMD Loss: 0.1686
Validation Loss: 1.7010
Epoch [2/50], Class Loss: 0.1895, JMMD Loss: 0.1832
Validation Loss: 2.3532
Epoch [3/50], Class Loss: 0.1421, JMMD Loss: 0.1052
Validation Loss: 3.4759
Epoch [4/50], Class Loss: 0.0836, JMMD Loss: 0.1050
Validation Loss: 5.5081
Epoch [5/50], Class Loss: 0.0440, JMMD Loss: 0.1195
Validation Loss: 4.8091
Epoch [6/50], Class Loss: 0.0696, JMMD Loss: 0.0899
Validation Loss: 4.6917
Early stopping!
Source Domain Performance - Accuracy: 54.98%, Precision: 66.54%, Recall: 53.35%, F1 Score: 44.45%
Target Domain Performance - Accuracy: 51.02%, Precision: 39.33%, Recall: 50.76%, F1 Score: 38.54%

Run 3/10
Epoch [1/50], Class Loss: 0.4005, JMMD Loss: 0.1807
Validation Loss: 2.5496
Epoch [2/50], Class Loss: 0.1468, JMMD Loss: 0.1130
Validation Loss: 2.8048
Epoch [3/50], Class Loss: 0.0799, JMMD Loss: 0.1439
Validation Loss: 6.3969
Epoch [4/50], Class Loss: 0.0611, JMMD Loss: 0.1190
Validation Loss: 3.1336
Epoch [5/50], Class Loss: 0.0985, JMMD Loss: 0.1395
Validation Loss: 12.3984
Epoch [6/50], Class Loss: 0.1036, JMMD Loss: 0.1350
Validation Loss: 2.7440
Early stopping!
Source Domain Performance - Accuracy: 52.04%, Precision: 62.03%, Recall: 50.38%, F1 Score: 41.36%
Target Domain Performance - Accuracy: 53.84%, Precision: 53.69%, Recall: 53.57%, F1 Score: 45.02%

Run 4/10
Epoch [1/50], Class Loss: 0.3753, JMMD Loss: 0.1451
Validation Loss: 2.7810
Epoch [2/50], Class Loss: 0.1379, JMMD Loss: 0.1061
Validation Loss: 1.1094
Epoch [3/50], Class Loss: 0.0844, JMMD Loss: 0.1082
Validation Loss: 2.8379
Epoch [4/50], Class Loss: 0.0876, JMMD Loss: 0.0959
Validation Loss: 0.2175
Epoch [5/50], Class Loss: 0.0825, JMMD Loss: 0.1563
Validation Loss: 9.1423
Epoch [6/50], Class Loss: 0.0670, JMMD Loss: 0.1339
Validation Loss: 3.9544
Epoch [7/50], Class Loss: 0.0298, JMMD Loss: 0.1353
Validation Loss: 3.3424
Epoch [8/50], Class Loss: 0.0333, JMMD Loss: 0.0999
Validation Loss: 3.0840
Epoch [9/50], Class Loss: 0.0255, JMMD Loss: 0.1283
Validation Loss: 6.2718
Early stopping!
Source Domain Performance - Accuracy: 51.68%, Precision: 33.78%, Recall: 50.00%, F1 Score: 37.99%
Target Domain Performance - Accuracy: 50.24%, Precision: 50.55%, Recall: 49.95%, F1 Score: 35.13%

Run 5/10
Epoch [1/50], Class Loss: 0.4060, JMMD Loss: 0.1436
Validation Loss: 3.7979
Epoch [2/50], Class Loss: 0.1938, JMMD Loss: 0.1110
Validation Loss: 5.9986
Epoch [3/50], Class Loss: 0.0802, JMMD Loss: 0.0945
Validation Loss: 2.3211
Epoch [4/50], Class Loss: 0.0685, JMMD Loss: 0.0970
Validation Loss: 5.9204
Epoch [5/50], Class Loss: 0.0542, JMMD Loss: 0.1169
Validation Loss: 3.6934
Epoch [6/50], Class Loss: 0.0315, JMMD Loss: 0.0855
Validation Loss: 6.9473
Epoch [7/50], Class Loss: 0.1758, JMMD Loss: 0.1358
Validation Loss: 22.8169
Epoch [8/50], Class Loss: 0.0734, JMMD Loss: 0.1026
Validation Loss: 7.1307
Early stopping!
Source Domain Performance - Accuracy: 52.04%, Precision: 59.43%, Recall: 50.36%, F1 Score: 39.40%
Target Domain Performance - Accuracy: 49.34%, Precision: 36.84%, Recall: 49.28%, F1 Score: 42.00%

Run 6/10
Epoch [1/50], Class Loss: 0.4274, JMMD Loss: 0.1985
Validation Loss: 0.3320
Epoch [2/50], Class Loss: 0.1959, JMMD Loss: 0.1427
Validation Loss: 1.5745
Epoch [3/50], Class Loss: 0.1523, JMMD Loss: 0.1496
Validation Loss: 5.1272
Epoch [4/50], Class Loss: 0.1683, JMMD Loss: 0.1474
Validation Loss: 5.8457
Epoch [5/50], Class Loss: 0.0805, JMMD Loss: 0.1229
Validation Loss: 4.1427
Epoch [6/50], Class Loss: 0.0426, JMMD Loss: 0.1124
Validation Loss: 4.3394
Early stopping!
Source Domain Performance - Accuracy: 51.98%, Precision: 61.27%, Recall: 50.32%, F1 Score: 40.60%
Target Domain Performance - Accuracy: 55.52%, Precision: 47.90%, Recall: 55.27%, F1 Score: 45.96%

Run 7/10
Epoch [1/50], Class Loss: 0.4159, JMMD Loss: 0.1720
Validation Loss: 0.7704
Epoch [2/50], Class Loss: 0.2261, JMMD Loss: 0.1516
Validation Loss: 1.5594
Epoch [3/50], Class Loss: 0.1241, JMMD Loss: 0.0901
Validation Loss: 2.8285
Epoch [4/50], Class Loss: 0.1196, JMMD Loss: 0.1160
Validation Loss: 3.7052
Epoch [5/50], Class Loss: 0.0786, JMMD Loss: 0.1263
Validation Loss: 9.2789
Epoch [6/50], Class Loss: 0.0440, JMMD Loss: 0.1015
Validation Loss: 4.2479
Early stopping!
Source Domain Performance - Accuracy: 51.68%, Precision: 35.29%, Recall: 50.00%, F1 Score: 39.58%
Target Domain Performance - Accuracy: 50.48%, Precision: 50.50%, Recall: 50.20%, F1 Score: 36.36%

Run 8/10
Epoch [1/50], Class Loss: 0.3798, JMMD Loss: 0.1141
Validation Loss: 0.8418
Epoch [2/50], Class Loss: 0.1802, JMMD Loss: 0.1002
Validation Loss: 2.1947
Epoch [3/50], Class Loss: 0.1393, JMMD Loss: 0.1161
Validation Loss: 2.6235
Epoch [4/50], Class Loss: 0.0752, JMMD Loss: 0.1438
Validation Loss: 4.4184
Epoch [5/50], Class Loss: 0.0253, JMMD Loss: 0.1134
Validation Loss: 5.1868
Epoch [6/50], Class Loss: 0.0573, JMMD Loss: 0.1593
Validation Loss: 3.9978
Early stopping!
Source Domain Performance - Accuracy: 52.04%, Precision: 73.58%, Recall: 50.37%, F1 Score: 38.51%
Target Domain Performance - Accuracy: 50.06%, Precision: 50.03%, Recall: 49.76%, F1 Score: 33.41%

Run 9/10
Epoch [1/50], Class Loss: 0.3688, JMMD Loss: 0.1658
Validation Loss: 1.3458
Epoch [2/50], Class Loss: 0.1620, JMMD Loss: 0.1039
Validation Loss: 2.3884
Epoch [3/50], Class Loss: 0.0809, JMMD Loss: 0.1075
Validation Loss: 1.4194
Epoch [4/50], Class Loss: 0.0483, JMMD Loss: 0.1043
Validation Loss: 6.2684
Epoch [5/50], Class Loss: 0.0549, JMMD Loss: 0.0972
Validation Loss: 7.3671
Epoch [6/50], Class Loss: 0.0377, JMMD Loss: 0.1064
Validation Loss: 0.5013
Epoch [7/50], Class Loss: 0.0910, JMMD Loss: 0.1282
Validation Loss: 6.0606
Epoch [8/50], Class Loss: 0.0384, JMMD Loss: 0.0941
Validation Loss: 8.1005
Epoch [9/50], Class Loss: 0.0562, JMMD Loss: 0.0975
Validation Loss: 4.8629
Epoch [10/50], Class Loss: 0.0172, JMMD Loss: 0.1158
Validation Loss: 5.3884
Epoch [11/50], Class Loss: 0.0131, JMMD Loss: 0.1197
Validation Loss: 4.9968
Early stopping!
Source Domain Performance - Accuracy: 51.68%, Precision: 33.59%, Recall: 50.00%, F1 Score: 37.78%
Target Domain Performance - Accuracy: 54.62%, Precision: 45.79%, Recall: 54.37%, F1 Score: 43.88%

Run 10/10
Epoch [1/50], Class Loss: 0.3935, JMMD Loss: 0.1327
Validation Loss: 0.1341
Epoch [2/50], Class Loss: 0.1995, JMMD Loss: 0.1440
Validation Loss: 3.3789
Epoch [3/50], Class Loss: 0.0941, JMMD Loss: 0.1062
Validation Loss: 2.3112
Epoch [4/50], Class Loss: 0.0475, JMMD Loss: 0.1024
Validation Loss: 1.7857
Epoch [5/50], Class Loss: 0.0491, JMMD Loss: 0.1044
Validation Loss: 2.6648
Epoch [6/50], Class Loss: 0.0345, JMMD Loss: 0.1079
Validation Loss: 4.3131
Early stopping!
Source Domain Performance - Accuracy: 51.80%, Precision: 59.00%, Recall: 50.12%, F1 Score: 38.48%
Target Domain Performance - Accuracy: 51.68%, Precision: 45.06%, Recall: 51.42%, F1 Score: 40.96%

Source performance: 52.18% 54.41% 50.51% 39.74%
Target performance: 52.22% 46.81% 51.98% 40.65%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 100.00%
  Class 1: 0.02%
  Class 2: 20.31%
  Class 3: 87.58%
Base

Run 1/10
Epoch 1/50, Train Loss: 0.4268, Train Acc: 0.8314, Val Loss: 4.4530, Val Acc: 0.7458
Epoch 2/50, Train Loss: 0.1200, Train Acc: 0.9538, Val Loss: 0.9896, Val Acc: 0.7734
Epoch 3/50, Train Loss: 0.0627, Train Acc: 0.9787, Val Loss: 1.2100, Val Acc: 0.7794
Epoch 4/50, Train Loss: 0.0505, Train Acc: 0.9823, Val Loss: 1.0036, Val Acc: 0.7950
Epoch 5/50, Train Loss: 0.0184, Train Acc: 0.9942, Val Loss: 0.0375, Val Acc: 0.9856
Epoch 6/50, Train Loss: 0.0105, Train Acc: 0.9967, Val Loss: 0.1150, Val Acc: 0.9568
Epoch 7/50, Train Loss: 0.0074, Train Acc: 0.9979, Val Loss: 0.7292, Val Acc: 0.8297
Epoch 8/50, Train Loss: 0.0128, Train Acc: 0.9961, Val Loss: 2.2294, Val Acc: 0.6445
Epoch 9/50, Train Loss: 0.0052, Train Acc: 0.9982, Val Loss: 0.0082, Val Acc: 0.9976
Epoch 10/50, Train Loss: 0.0060, Train Acc: 0.9991, Val Loss: 0.0017, Val Acc: 0.9988
Epoch 11/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0017, Train Acc: 0.9996, Val Loss: 0.0024, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0034, Train Acc: 0.9997, Val Loss: 0.0012, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0032, Train Acc: 0.9996, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0040, Train Acc: 0.9994, Val Loss: 0.0013, Val Acc: 0.9988
Epoch 20/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 21/50, Train Loss: 0.0020, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Early stopping!

Run 2/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4131, Train Acc: 0.8403, Val Loss: 1.3051, Val Acc: 0.7458
Epoch 2/50, Train Loss: 0.1076, Train Acc: 0.9631, Val Loss: 0.9590, Val Acc: 0.7854
Epoch 3/50, Train Loss: 0.0924, Train Acc: 0.9693, Val Loss: 0.0677, Val Acc: 0.9760
Epoch 4/50, Train Loss: 0.0241, Train Acc: 0.9934, Val Loss: 2.5287, Val Acc: 0.7104
Epoch 5/50, Train Loss: 0.0422, Train Acc: 0.9871, Val Loss: 0.7091, Val Acc: 0.8129
Epoch 6/50, Train Loss: 0.0181, Train Acc: 0.9937, Val Loss: 0.0154, Val Acc: 0.9964
Epoch 7/50, Train Loss: 0.0091, Train Acc: 0.9978, Val Loss: 0.1860, Val Acc: 0.9371
Epoch 8/50, Train Loss: 0.0013, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 9/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 10/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0022, Train Acc: 0.9996, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Early stopping!

Run 3/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4312, Train Acc: 0.8340, Val Loss: 3.9367, Val Acc: 0.7362
Epoch 2/50, Train Loss: 0.1375, Train Acc: 0.9484, Val Loss: 1.6524, Val Acc: 0.7536
Epoch 3/50, Train Loss: 0.0562, Train Acc: 0.9805, Val Loss: 4.7066, Val Acc: 0.5689
Epoch 4/50, Train Loss: 0.0347, Train Acc: 0.9894, Val Loss: 5.6173, Val Acc: 0.5444
Epoch 5/50, Train Loss: 0.0313, Train Acc: 0.9879, Val Loss: 0.1007, Val Acc: 0.9616
Epoch 6/50, Train Loss: 0.0142, Train Acc: 0.9948, Val Loss: 1.2922, Val Acc: 0.7770
Epoch 7/50, Train Loss: 0.0033, Train Acc: 0.9993, Val Loss: 0.0177, Val Acc: 0.9940
Epoch 8/50, Train Loss: 0.0028, Train Acc: 0.9994, Val Loss: 0.0171, Val Acc: 0.9952
Epoch 9/50, Train Loss: 0.0145, Train Acc: 0.9973, Val Loss: 0.0046, Val Acc: 0.9982
Epoch 10/50, Train Loss: 0.0062, Train Acc: 0.9984, Val Loss: 1.4031, Val Acc: 0.7680
Epoch 11/50, Train Loss: 0.0029, Train Acc: 0.9993, Val Loss: 0.0018, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0018, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0020, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 24/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 25/50, Train Loss: 0.0028, Train Acc: 0.9997, Val Loss: 0.0009, Val Acc: 0.9994
Early stopping!

Run 4/10
Epoch 1/50, Train Loss: 0.4172, Train Acc: 0.8371, Val Loss: 5.1909, Val Acc: 0.6475
Epoch 2/50, Train Loss: 0.1077, Train Acc: 0.9615, Val Loss: 0.0748, Val Acc: 0.9718
Epoch 3/50, Train Loss: 0.0727, Train Acc: 0.9748, Val Loss: 0.0641, Val Acc: 0.9754
Epoch 4/50, Train Loss: 0.0308, Train Acc: 0.9922, Val Loss: 1.3289, Val Acc: 0.6859
Epoch 5/50, Train Loss: 0.0240, Train Acc: 0.9924, Val Loss: 3.4237, Val Acc: 0.7302
Epoch 6/50, Train Loss: 0.0088, Train Acc: 0.9976, Val Loss: 0.3157, Val Acc: 0.8663
Epoch 7/50, Train Loss: 0.0095, Train Acc: 0.9976, Val Loss: 0.1415, Val Acc: 0.9484
Epoch 8/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0045, Val Acc: 0.9988
Epoch 9/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 10/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Early stopping!

Run 5/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4037, Train Acc: 0.8370, Val Loss: 3.5396, Val Acc: 0.5917
Epoch 2/50, Train Loss: 0.0984, Train Acc: 0.9639, Val Loss: 0.9710, Val Acc: 0.7260
Epoch 3/50, Train Loss: 0.0507, Train Acc: 0.9829, Val Loss: 0.1658, Val Acc: 0.9388
Epoch 4/50, Train Loss: 0.0212, Train Acc: 0.9937, Val Loss: 0.4856, Val Acc: 0.8423
Epoch 5/50, Train Loss: 0.0200, Train Acc: 0.9942, Val Loss: 1.7876, Val Acc: 0.7482
Epoch 6/50, Train Loss: 0.0168, Train Acc: 0.9942, Val Loss: 0.0220, Val Acc: 0.9922
Epoch 7/50, Train Loss: 0.0110, Train Acc: 0.9967, Val Loss: 2.1049, Val Acc: 0.7572
Epoch 8/50, Train Loss: 0.0059, Train Acc: 0.9984, Val Loss: 0.1187, Val Acc: 0.9628
Epoch 9/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0031, Val Acc: 0.9994
Epoch 10/50, Train Loss: 0.0020, Train Acc: 0.9997, Val Loss: 0.1501, Val Acc: 0.9466
Epoch 11/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0018, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0056, Train Acc: 0.9993, Val Loss: 0.0005, Val Acc: 1.0000
Early stopping!

Run 6/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3995, Train Acc: 0.8367, Val Loss: 0.2385, Val Acc: 0.9227
Epoch 2/50, Train Loss: 0.0868, Train Acc: 0.9678, Val Loss: 2.3175, Val Acc: 0.7350
Epoch 3/50, Train Loss: 0.0497, Train Acc: 0.9831, Val Loss: 0.0421, Val Acc: 0.9868
Epoch 4/50, Train Loss: 0.0224, Train Acc: 0.9924, Val Loss: 0.3807, Val Acc: 0.9077
Epoch 5/50, Train Loss: 0.0105, Train Acc: 0.9969, Val Loss: 0.0184, Val Acc: 0.9928
Epoch 6/50, Train Loss: 0.0041, Train Acc: 0.9993, Val Loss: 0.8833, Val Acc: 0.7680
Epoch 7/50, Train Loss: 0.0083, Train Acc: 0.9969, Val Loss: 0.0188, Val Acc: 0.9928
Epoch 8/50, Train Loss: 0.0115, Train Acc: 0.9964, Val Loss: 0.0936, Val Acc: 0.9700
Epoch 9/50, Train Loss: 0.0042, Train Acc: 0.9988, Val Loss: 0.0105, Val Acc: 0.9970
Epoch 10/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0012, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0026, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0021, Train Acc: 0.9999, Val Loss: 0.0051, Val Acc: 0.9970
Epoch 21/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0022, Train Acc: 0.9996, Val Loss: 0.0012, Val Acc: 1.0000
Early stopping!

Run 7/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4560, Train Acc: 0.8089, Val Loss: 1.7027, Val Acc: 0.6942
Epoch 2/50, Train Loss: 0.1442, Train Acc: 0.9456, Val Loss: 0.1736, Val Acc: 0.9305
Epoch 3/50, Train Loss: 0.0685, Train Acc: 0.9766, Val Loss: 3.1057, Val Acc: 0.5024
Epoch 4/50, Train Loss: 0.0638, Train Acc: 0.9774, Val Loss: 4.6650, Val Acc: 0.5036
Epoch 5/50, Train Loss: 0.0360, Train Acc: 0.9877, Val Loss: 0.0419, Val Acc: 0.9868
Epoch 6/50, Train Loss: 0.0120, Train Acc: 0.9963, Val Loss: 0.0121, Val Acc: 0.9970
Epoch 7/50, Train Loss: 0.0076, Train Acc: 0.9982, Val Loss: 0.5208, Val Acc: 0.8459
Epoch 8/50, Train Loss: 0.0070, Train Acc: 0.9982, Val Loss: 1.5857, Val Acc: 0.7224
Epoch 9/50, Train Loss: 0.0026, Train Acc: 0.9994, Val Loss: 0.1013, Val Acc: 0.9610
Epoch 10/50, Train Loss: 0.0013, Train Acc: 1.0000, Val Loss: 1.4275, Val Acc: 0.7530
Epoch 11/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0027, Train Acc: 0.9997, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0019, Train Acc: 0.9997, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0040, Train Acc: 0.9996, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Early stopping!

Run 8/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4262, Train Acc: 0.8271, Val Loss: 3.4210, Val Acc: 0.5911
Epoch 2/50, Train Loss: 0.1226, Train Acc: 0.9546, Val Loss: 0.0553, Val Acc: 0.9790
Epoch 3/50, Train Loss: 0.0635, Train Acc: 0.9771, Val Loss: 0.2889, Val Acc: 0.8927
Epoch 4/50, Train Loss: 0.0382, Train Acc: 0.9876, Val Loss: 3.1701, Val Acc: 0.7518
Epoch 5/50, Train Loss: 0.0178, Train Acc: 0.9945, Val Loss: 6.0411, Val Acc: 0.5917
Epoch 6/50, Train Loss: 0.0112, Train Acc: 0.9973, Val Loss: 0.0856, Val Acc: 0.9688
Epoch 7/50, Train Loss: 0.0055, Train Acc: 0.9982, Val Loss: 0.0157, Val Acc: 0.9952
Epoch 8/50, Train Loss: 0.0148, Train Acc: 0.9954, Val Loss: 0.2266, Val Acc: 0.9227
Epoch 9/50, Train Loss: 0.0035, Train Acc: 0.9993, Val Loss: 0.0351, Val Acc: 0.9874
Epoch 10/50, Train Loss: 0.0037, Train Acc: 0.9993, Val Loss: 0.7469, Val Acc: 0.8046
Epoch 11/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0033, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 9/10
Epoch 1/50, Train Loss: 0.4909, Train Acc: 0.7956, Val Loss: 4.0732, Val Acc: 0.4388
Epoch 2/50, Train Loss: 0.1110, Train Acc: 0.9580, Val Loss: 4.4408, Val Acc: 0.5066
Epoch 3/50, Train Loss: 0.0410, Train Acc: 0.9870, Val Loss: 1.1674, Val Acc: 0.7512
Epoch 4/50, Train Loss: 0.0191, Train Acc: 0.9946, Val Loss: 0.2905, Val Acc: 0.9149
Epoch 5/50, Train Loss: 0.0152, Train Acc: 0.9958, Val Loss: 0.0094, Val Acc: 0.9970
Epoch 6/50, Train Loss: 0.0051, Train Acc: 0.9984, Val Loss: 3.6145, Val Acc: 0.7038
Epoch 7/50, Train Loss: 0.0068, Train Acc: 0.9984, Val Loss: 1.3722, Val Acc: 0.7602
Epoch 8/50, Train Loss: 0.0046, Train Acc: 0.9990, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 9/50, Train Loss: 0.0031, Train Acc: 0.9994, Val Loss: 3.7940, Val Acc: 0.7386
Epoch 10/50, Train Loss: 0.0136, Train Acc: 0.9976, Val Loss: 0.0094, Val Acc: 0.9964
Epoch 11/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Early stopping!

Run 10/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4526, Train Acc: 0.8209, Val Loss: 2.6663, Val Acc: 0.7392
Epoch 2/50, Train Loss: 0.1046, Train Acc: 0.9610, Val Loss: 0.1778, Val Acc: 0.9388
Epoch 3/50, Train Loss: 0.0802, Train Acc: 0.9730, Val Loss: 20.1656, Val Acc: 0.5072
Epoch 4/50, Train Loss: 0.0404, Train Acc: 0.9862, Val Loss: 1.4310, Val Acc: 0.7758
Epoch 5/50, Train Loss: 0.0284, Train Acc: 0.9898, Val Loss: 2.4841, Val Acc: 0.5743
Epoch 6/50, Train Loss: 0.0118, Train Acc: 0.9961, Val Loss: 0.0173, Val Acc: 0.9946
Epoch 7/50, Train Loss: 0.0065, Train Acc: 0.9981, Val Loss: 0.0131, Val Acc: 0.9946
Epoch 8/50, Train Loss: 0.0096, Train Acc: 0.9969, Val Loss: 1.0588, Val Acc: 0.8082
Epoch 9/50, Train Loss: 0.0046, Train Acc: 0.9988, Val Loss: 0.0105, Val Acc: 0.9976
Epoch 10/50, Train Loss: 0.0047, Train Acc: 0.9991, Val Loss: 0.0236, Val Acc: 0.9922
Epoch 11/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0018, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0008, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0021, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Early stopping!

Source performance: 99.99 99.99 99.99 99.99
Target performance: 52.85 39.88 53.79 40.70

bpsk: 100.00
qpsk: 0.00
16qam: 15.27
8apsk: 99.88
DANN
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.4084, Domain Loss: 1.3837, Class Loss: 1.0247
Epoch 2/50, Loss: 1.7233, Domain Loss: 1.2642, Class Loss: 0.4592
Epoch 3/50, Loss: 1.9679, Domain Loss: 1.5407, Class Loss: 0.4273
Epoch 4/50, Loss: 1.9962, Domain Loss: 1.6808, Class Loss: 0.3154
Epoch 5/50, Loss: 2.6318, Domain Loss: 2.3695, Class Loss: 0.2623
Epoch 6/50, Loss: 3.9681, Domain Loss: 3.7452, Class Loss: 0.2229
Epoch 7/50, Loss: 4.9134, Domain Loss: 4.5379, Class Loss: 0.3755
Epoch 8/50, Loss: 6.0642, Domain Loss: 5.6590, Class Loss: 0.4052
Epoch 9/50, Loss: 3.5945, Domain Loss: 3.0519, Class Loss: 0.5426
Epoch 10/50, Loss: 11.0973, Domain Loss: 10.5886, Class Loss: 0.5087
Epoch 11/50, Loss: 8.8410, Domain Loss: 8.1453, Class Loss: 0.6957
Epoch 12/50, Loss: 8.9778, Domain Loss: 8.1822, Class Loss: 0.7956
Epoch 13/50, Loss: 4.5040, Domain Loss: 3.9714, Class Loss: 0.5326
Epoch 14/50, Loss: 2.9345, Domain Loss: 2.4119, Class Loss: 0.5225
Epoch 15/50, Loss: 2.4005, Domain Loss: 1.9751, Class Loss: 0.4254
Epoch 16/50, Loss: 3.3125, Domain Loss: 2.7348, Class Loss: 0.5777
Epoch 17/50, Loss: 5.4698, Domain Loss: 4.2002, Class Loss: 1.2696
Epoch 18/50, Loss: 4.7148, Domain Loss: 4.1233, Class Loss: 0.5915
Epoch 19/50, Loss: 3.8013, Domain Loss: 3.3222, Class Loss: 0.4791
Epoch 20/50, Loss: 4.1300, Domain Loss: 3.5765, Class Loss: 0.5535
Epoch 21/50, Loss: 3.8992, Domain Loss: 3.1983, Class Loss: 0.7008
Epoch 22/50, Loss: 10.1890, Domain Loss: 8.8801, Class Loss: 1.3089
Epoch 23/50, Loss: 9.6804, Domain Loss: 8.3186, Class Loss: 1.3619
Epoch 24/50, Loss: 5.2236, Domain Loss: 4.6369, Class Loss: 0.5868
Epoch 25/50, Loss: 6.1066, Domain Loss: 5.3789, Class Loss: 0.7277
Epoch 26/50, Loss: 3.3752, Domain Loss: 2.7279, Class Loss: 0.6473
Epoch 27/50, Loss: 2.9923, Domain Loss: 2.3813, Class Loss: 0.6111
Epoch 28/50, Loss: 2.2878, Domain Loss: 1.7177, Class Loss: 0.5701
Epoch 29/50, Loss: 2.3043, Domain Loss: 1.7844, Class Loss: 0.5199
Epoch 30/50, Loss: 2.1439, Domain Loss: 1.6739, Class Loss: 0.4700
Epoch 31/50, Loss: 2.1021, Domain Loss: 1.6180, Class Loss: 0.4841
Epoch 32/50, Loss: 2.0024, Domain Loss: 1.5722, Class Loss: 0.4302
Epoch 33/50, Loss: 2.2215, Domain Loss: 1.6906, Class Loss: 0.5309
Epoch 34/50, Loss: 2.0110, Domain Loss: 1.6033, Class Loss: 0.4077
Epoch 35/50, Loss: 1.9499, Domain Loss: 1.5703, Class Loss: 0.3796
Epoch 36/50, Loss: 2.1045, Domain Loss: 1.7531, Class Loss: 0.3514
Epoch 37/50, Loss: 1.9326, Domain Loss: 1.5548, Class Loss: 0.3778
Epoch 38/50, Loss: 1.9588, Domain Loss: 1.6041, Class Loss: 0.3548
Epoch 39/50, Loss: 2.3285, Domain Loss: 1.9565, Class Loss: 0.3719
Epoch 40/50, Loss: 3.1241, Domain Loss: 2.7516, Class Loss: 0.3725
Epoch 41/50, Loss: 3.2132, Domain Loss: 2.8390, Class Loss: 0.3742
Epoch 42/50, Loss: 1.9602, Domain Loss: 1.5591, Class Loss: 0.4011
Epoch 43/50, Loss: 2.8541, Domain Loss: 2.3406, Class Loss: 0.5135
Epoch 44/50, Loss: 2.3097, Domain Loss: 1.9499, Class Loss: 0.3598
Epoch 45/50, Loss: 2.3291, Domain Loss: 1.9319, Class Loss: 0.3972
Epoch 46/50, Loss: 2.3600, Domain Loss: 1.9672, Class Loss: 0.3928
Epoch 47/50, Loss: 2.3941, Domain Loss: 2.0226, Class Loss: 0.3715
Epoch 48/50, Loss: 2.7330, Domain Loss: 2.3818, Class Loss: 0.3512
Epoch 49/50, Loss: 1.8404, Domain Loss: 1.4553, Class Loss: 0.3851
Epoch 50/50, Loss: 2.5527, Domain Loss: 2.2292, Class Loss: 0.3234
56.35


Epoch 1/50, Loss: 2.3304, Domain Loss: 1.3711, Class Loss: 0.9593
Epoch 2/50, Loss: 1.7655, Domain Loss: 1.3422, Class Loss: 0.4233
Epoch 3/50, Loss: 1.8885, Domain Loss: 1.4841, Class Loss: 0.4044
Epoch 4/50, Loss: 2.0473, Domain Loss: 1.7274, Class Loss: 0.3199
Epoch 5/50, Loss: 8.8790, Domain Loss: 8.4502, Class Loss: 0.4288
Epoch 6/50, Loss: 14.8509, Domain Loss: 14.1961, Class Loss: 0.6548
Epoch 7/50, Loss: 17.2817, Domain Loss: 15.6304, Class Loss: 1.6513
Epoch 8/50, Loss: 12.1624, Domain Loss: 10.8709, Class Loss: 1.2916
Epoch 9/50, Loss: 16.7071, Domain Loss: 15.7832, Class Loss: 0.9239
Epoch 10/50, Loss: 9.9497, Domain Loss: 9.1684, Class Loss: 0.7813
Epoch 11/50, Loss: 4.1037, Domain Loss: 3.5083, Class Loss: 0.5955
Epoch 12/50, Loss: 2.7335, Domain Loss: 2.2164, Class Loss: 0.5171
Epoch 13/50, Loss: 5.6439, Domain Loss: 5.1106, Class Loss: 0.5333
Epoch 14/50, Loss: 4.4467, Domain Loss: 3.9954, Class Loss: 0.4513
Epoch 15/50, Loss: 3.0653, Domain Loss: 2.5764, Class Loss: 0.4889
Epoch 16/50, Loss: 2.4049, Domain Loss: 1.9836, Class Loss: 0.4213
Epoch 17/50, Loss: 2.5036, Domain Loss: 2.0143, Class Loss: 0.4893
Epoch 18/50, Loss: 5.8611, Domain Loss: 5.2878, Class Loss: 0.5733
Epoch 19/50, Loss: 4.4450, Domain Loss: 3.9777, Class Loss: 0.4672
Epoch 20/50, Loss: 4.5811, Domain Loss: 4.1068, Class Loss: 0.4744
Epoch 21/50, Loss: 3.2853, Domain Loss: 2.7072, Class Loss: 0.5781
Epoch 22/50, Loss: 2.0090, Domain Loss: 1.4683, Class Loss: 0.5407
Epoch 23/50, Loss: 1.8230, Domain Loss: 1.4008, Class Loss: 0.4221
Epoch 24/50, Loss: 1.9664, Domain Loss: 1.5332, Class Loss: 0.4332
Epoch 25/50, Loss: 2.5249, Domain Loss: 2.0920, Class Loss: 0.4330
Epoch 26/50, Loss: 2.0237, Domain Loss: 1.5445, Class Loss: 0.4792
Epoch 27/50, Loss: 2.0991, Domain Loss: 1.6039, Class Loss: 0.4953
Epoch 28/50, Loss: 1.8965, Domain Loss: 1.5061, Class Loss: 0.3905
Epoch 29/50, Loss: 1.7814, Domain Loss: 1.3924, Class Loss: 0.3891
Epoch 30/50, Loss: 1.8227, Domain Loss: 1.3233, Class Loss: 0.4994
Epoch 31/50, Loss: 2.0373, Domain Loss: 1.5208, Class Loss: 0.5166
Epoch 32/50, Loss: 1.7978, Domain Loss: 1.4115, Class Loss: 0.3863
Epoch 33/50, Loss: 2.0268, Domain Loss: 1.5148, Class Loss: 0.5120
Epoch 34/50, Loss: 1.8564, Domain Loss: 1.4051, Class Loss: 0.4513
Epoch 35/50, Loss: 1.7935, Domain Loss: 1.3973, Class Loss: 0.3962
Epoch 36/50, Loss: 1.8702, Domain Loss: 1.4612, Class Loss: 0.4090
Epoch 37/50, Loss: 1.7165, Domain Loss: 1.3608, Class Loss: 0.3557
Epoch 38/50, Loss: 1.6880, Domain Loss: 1.3276, Class Loss: 0.3603
Epoch 39/50, Loss: 1.6705, Domain Loss: 1.3520, Class Loss: 0.3185
Epoch 40/50, Loss: 1.7070, Domain Loss: 1.3917, Class Loss: 0.3153
Epoch 41/50, Loss: 1.6477, Domain Loss: 1.3496, Class Loss: 0.2981
Epoch 42/50, Loss: 1.6787, Domain Loss: 1.3722, Class Loss: 0.3064
Epoch 43/50, Loss: 1.6905, Domain Loss: 1.3615, Class Loss: 0.3290
Epoch 44/50, Loss: 1.9181, Domain Loss: 1.5042, Class Loss: 0.4139
Epoch 45/50, Loss: 1.8398, Domain Loss: 1.4695, Class Loss: 0.3703
Epoch 46/50, Loss: 1.7398, Domain Loss: 1.4058, Class Loss: 0.3339
Epoch 47/50, Loss: 1.6271, Domain Loss: 1.3255, Class Loss: 0.3016
Epoch 48/50, Loss: 1.7297, Domain Loss: 1.4126, Class Loss: 0.3171
Epoch 49/50, Loss: 1.8121, Domain Loss: 1.4337, Class Loss: 0.3785
Epoch 50/50, Loss: 1.7236, Domain Loss: 1.4171, Class Loss: 0.3065
58.75


Epoch 1/50, Loss: 2.3608, Domain Loss: 1.3856, Class Loss: 0.9752
Epoch 2/50, Loss: 1.7474, Domain Loss: 1.3066, Class Loss: 0.4407
Epoch 3/50, Loss: 1.7807, Domain Loss: 1.4275, Class Loss: 0.3532
Epoch 4/50, Loss: 3.2395, Domain Loss: 2.9040, Class Loss: 0.3355
Epoch 5/50, Loss: 6.3957, Domain Loss: 6.0274, Class Loss: 0.3683
Epoch 6/50, Loss: 3.9877, Domain Loss: 3.6040, Class Loss: 0.3837
Epoch 7/50, Loss: 2.5777, Domain Loss: 2.2458, Class Loss: 0.3320
Epoch 8/50, Loss: 5.9925, Domain Loss: 5.6064, Class Loss: 0.3861
Epoch 9/50, Loss: 6.1766, Domain Loss: 5.6628, Class Loss: 0.5137
Epoch 10/50, Loss: 7.8387, Domain Loss: 7.3839, Class Loss: 0.4548
Epoch 11/50, Loss: 7.0726, Domain Loss: 6.6302, Class Loss: 0.4424
Epoch 12/50, Loss: 4.2947, Domain Loss: 3.6649, Class Loss: 0.6298
Epoch 13/50, Loss: 3.3041, Domain Loss: 2.7374, Class Loss: 0.5667
Epoch 14/50, Loss: 2.3647, Domain Loss: 1.9871, Class Loss: 0.3776
Epoch 15/50, Loss: 2.4225, Domain Loss: 2.0432, Class Loss: 0.3794
Epoch 16/50, Loss: 3.4937, Domain Loss: 3.1050, Class Loss: 0.3887
Epoch 17/50, Loss: 2.8723, Domain Loss: 2.5166, Class Loss: 0.3557
Epoch 18/50, Loss: 3.0397, Domain Loss: 2.5740, Class Loss: 0.4657
Epoch 19/50, Loss: 3.3107, Domain Loss: 2.9467, Class Loss: 0.3640
Epoch 20/50, Loss: 2.3296, Domain Loss: 2.0046, Class Loss: 0.3250
Epoch 21/50, Loss: 2.4166, Domain Loss: 1.7917, Class Loss: 0.6249
Epoch 22/50, Loss: 2.0421, Domain Loss: 1.6155, Class Loss: 0.4267
Epoch 23/50, Loss: 1.8898, Domain Loss: 1.5502, Class Loss: 0.3395
Epoch 24/50, Loss: 2.1709, Domain Loss: 1.8309, Class Loss: 0.3400
Epoch 25/50, Loss: 1.5706, Domain Loss: 1.2770, Class Loss: 0.2936
Epoch 26/50, Loss: 2.3917, Domain Loss: 1.8480, Class Loss: 0.5437
Epoch 27/50, Loss: 1.9153, Domain Loss: 1.5447, Class Loss: 0.3705
Epoch 28/50, Loss: 2.1003, Domain Loss: 1.7567, Class Loss: 0.3436
Epoch 29/50, Loss: 2.3530, Domain Loss: 1.8194, Class Loss: 0.5336
Epoch 30/50, Loss: 1.9853, Domain Loss: 1.5670, Class Loss: 0.4183
Epoch 31/50, Loss: 1.8135, Domain Loss: 1.4877, Class Loss: 0.3258
Epoch 32/50, Loss: 1.9100, Domain Loss: 1.5715, Class Loss: 0.3385
Epoch 33/50, Loss: 1.7549, Domain Loss: 1.4513, Class Loss: 0.3036
Epoch 34/50, Loss: 1.5108, Domain Loss: 1.2353, Class Loss: 0.2755
Epoch 35/50, Loss: 1.8776, Domain Loss: 1.5829, Class Loss: 0.2947
Epoch 36/50, Loss: 1.9287, Domain Loss: 1.6107, Class Loss: 0.3181
Epoch 37/50, Loss: 1.7480, Domain Loss: 1.4297, Class Loss: 0.3184
Epoch 38/50, Loss: 2.0903, Domain Loss: 1.6264, Class Loss: 0.4639
Epoch 39/50, Loss: 1.7922, Domain Loss: 1.4761, Class Loss: 0.3161
Epoch 40/50, Loss: 1.6508, Domain Loss: 1.3812, Class Loss: 0.2696
Epoch 41/50, Loss: 1.5164, Domain Loss: 1.2667, Class Loss: 0.2497
Epoch 42/50, Loss: 1.6624, Domain Loss: 1.4147, Class Loss: 0.2478
Epoch 43/50, Loss: 1.6524, Domain Loss: 1.3696, Class Loss: 0.2828
Epoch 44/50, Loss: 1.5802, Domain Loss: 1.3239, Class Loss: 0.2564
Epoch 45/50, Loss: 1.7225, Domain Loss: 1.4732, Class Loss: 0.2493
Epoch 46/50, Loss: 1.7149, Domain Loss: 1.4728, Class Loss: 0.2421
Epoch 47/50, Loss: 1.7978, Domain Loss: 1.4884, Class Loss: 0.3094
Epoch 48/50, Loss: 1.9747, Domain Loss: 1.5994, Class Loss: 0.3753
Epoch 49/50, Loss: 2.5785, Domain Loss: 1.9614, Class Loss: 0.6171
Epoch 50/50, Loss: 2.1799, Domain Loss: 1.7371, Class Loss: 0.4428
55.16


Epoch 1/50, Loss: 2.3298, Domain Loss: 1.3774, Class Loss: 0.9524
Epoch 2/50, Loss: 1.7207, Domain Loss: 1.3010, Class Loss: 0.4198
Epoch 3/50, Loss: 1.6886, Domain Loss: 1.3395, Class Loss: 0.3491
Epoch 4/50, Loss: 4.0005, Domain Loss: 3.6969, Class Loss: 0.3036
Epoch 5/50, Loss: 9.6378, Domain Loss: 9.2262, Class Loss: 0.4116
Epoch 6/50, Loss: 13.9748, Domain Loss: 13.5500, Class Loss: 0.4247
Epoch 7/50, Loss: 9.7735, Domain Loss: 9.3299, Class Loss: 0.4435
Epoch 8/50, Loss: 14.0157, Domain Loss: 13.1386, Class Loss: 0.8770
Epoch 9/50, Loss: 10.4117, Domain Loss: 9.5003, Class Loss: 0.9114
Epoch 10/50, Loss: 4.7307, Domain Loss: 3.8187, Class Loss: 0.9120
Epoch 11/50, Loss: 8.3745, Domain Loss: 6.5807, Class Loss: 1.7938
Epoch 12/50, Loss: 14.1739, Domain Loss: 13.0550, Class Loss: 1.1189
Epoch 13/50, Loss: 6.5721, Domain Loss: 5.8845, Class Loss: 0.6877
Epoch 14/50, Loss: 5.8399, Domain Loss: 5.0570, Class Loss: 0.7829
Epoch 15/50, Loss: 7.8139, Domain Loss: 6.4873, Class Loss: 1.3267
Epoch 16/50, Loss: 4.2792, Domain Loss: 3.3403, Class Loss: 0.9389
Epoch 17/50, Loss: 3.2128, Domain Loss: 2.3915, Class Loss: 0.8213
Epoch 18/50, Loss: 4.1262, Domain Loss: 3.3798, Class Loss: 0.7463
Epoch 19/50, Loss: 3.8969, Domain Loss: 3.1602, Class Loss: 0.7367
Epoch 20/50, Loss: 2.6686, Domain Loss: 2.0937, Class Loss: 0.5749
Epoch 21/50, Loss: 2.1131, Domain Loss: 1.5666, Class Loss: 0.5465
Epoch 22/50, Loss: 2.1124, Domain Loss: 1.5537, Class Loss: 0.5587
Epoch 23/50, Loss: 1.9727, Domain Loss: 1.5231, Class Loss: 0.4495
Epoch 24/50, Loss: 1.8139, Domain Loss: 1.3806, Class Loss: 0.4333
Epoch 25/50, Loss: 2.1105, Domain Loss: 1.6443, Class Loss: 0.4662
Epoch 26/50, Loss: 2.4606, Domain Loss: 1.7749, Class Loss: 0.6856
Epoch 27/50, Loss: 2.1446, Domain Loss: 1.6733, Class Loss: 0.4713
Epoch 28/50, Loss: 2.1710, Domain Loss: 1.7115, Class Loss: 0.4595
Epoch 29/50, Loss: 2.1445, Domain Loss: 1.6550, Class Loss: 0.4895
Epoch 30/50, Loss: 1.9793, Domain Loss: 1.5516, Class Loss: 0.4278
Epoch 31/50, Loss: 1.9733, Domain Loss: 1.5399, Class Loss: 0.4333
Epoch 32/50, Loss: 1.8217, Domain Loss: 1.4154, Class Loss: 0.4063
Epoch 33/50, Loss: 1.7947, Domain Loss: 1.4142, Class Loss: 0.3805
Epoch 34/50, Loss: 1.8099, Domain Loss: 1.4591, Class Loss: 0.3509
Epoch 35/50, Loss: 2.0423, Domain Loss: 1.6123, Class Loss: 0.4300
Epoch 36/50, Loss: 1.8759, Domain Loss: 1.4739, Class Loss: 0.4020
Epoch 37/50, Loss: 1.8018, Domain Loss: 1.3839, Class Loss: 0.4179
Epoch 38/50, Loss: 1.8111, Domain Loss: 1.4075, Class Loss: 0.4036
Epoch 39/50, Loss: 1.8893, Domain Loss: 1.4917, Class Loss: 0.3976
Epoch 40/50, Loss: 2.0245, Domain Loss: 1.6516, Class Loss: 0.3729
Epoch 41/50, Loss: 2.3125, Domain Loss: 1.8974, Class Loss: 0.4151
Epoch 42/50, Loss: 2.6290, Domain Loss: 2.2339, Class Loss: 0.3951
Epoch 43/50, Loss: 3.0428, Domain Loss: 2.6757, Class Loss: 0.3670
Epoch 44/50, Loss: 2.3278, Domain Loss: 1.9643, Class Loss: 0.3635
Epoch 45/50, Loss: 1.9832, Domain Loss: 1.6295, Class Loss: 0.3537
Epoch 46/50, Loss: 1.9776, Domain Loss: 1.5415, Class Loss: 0.4362
Epoch 47/50, Loss: 1.8688, Domain Loss: 1.5252, Class Loss: 0.3436
Epoch 48/50, Loss: 1.8615, Domain Loss: 1.5157, Class Loss: 0.3457
Epoch 49/50, Loss: 1.9654, Domain Loss: 1.5980, Class Loss: 0.3674
Epoch 50/50, Loss: 1.8865, Domain Loss: 1.5509, Class Loss: 0.3356
58.57


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.3504, Domain Loss: 1.3854, Class Loss: 0.9650
Epoch 2/50, Loss: 1.8020, Domain Loss: 1.3833, Class Loss: 0.4187
Epoch 3/50, Loss: 1.6806, Domain Loss: 1.3946, Class Loss: 0.2860
Epoch 4/50, Loss: 1.6587, Domain Loss: 1.4848, Class Loss: 0.1739
Epoch 5/50, Loss: 2.5525, Domain Loss: 2.3700, Class Loss: 0.1825
Epoch 6/50, Loss: 2.6634, Domain Loss: 2.3906, Class Loss: 0.2728
Epoch 7/50, Loss: 2.7415, Domain Loss: 2.4880, Class Loss: 0.2535
Epoch 8/50, Loss: 2.1974, Domain Loss: 2.0174, Class Loss: 0.1800
Epoch 9/50, Loss: 3.2747, Domain Loss: 3.0200, Class Loss: 0.2547
Epoch 10/50, Loss: 5.3747, Domain Loss: 5.0626, Class Loss: 0.3121
Epoch 11/50, Loss: 3.9297, Domain Loss: 3.4578, Class Loss: 0.4719
Epoch 12/50, Loss: 3.3058, Domain Loss: 2.6066, Class Loss: 0.6993
Epoch 13/50, Loss: 2.3423, Domain Loss: 1.7580, Class Loss: 0.5843
Epoch 14/50, Loss: 2.6166, Domain Loss: 2.0972, Class Loss: 0.5194
Epoch 15/50, Loss: 2.8972, Domain Loss: 2.3867, Class Loss: 0.5106
Epoch 16/50, Loss: 2.1061, Domain Loss: 1.6855, Class Loss: 0.4206
Epoch 17/50, Loss: 2.5109, Domain Loss: 1.8986, Class Loss: 0.6123
Epoch 18/50, Loss: 1.9708, Domain Loss: 1.5332, Class Loss: 0.4376
Epoch 19/50, Loss: 1.8739, Domain Loss: 1.5371, Class Loss: 0.3368
Epoch 20/50, Loss: 2.0138, Domain Loss: 1.6855, Class Loss: 0.3283
Epoch 21/50, Loss: 2.1750, Domain Loss: 1.8305, Class Loss: 0.3444
Epoch 22/50, Loss: 1.8487, Domain Loss: 1.5052, Class Loss: 0.3435
Epoch 23/50, Loss: 2.0803, Domain Loss: 1.7533, Class Loss: 0.3270
Epoch 24/50, Loss: 1.6639, Domain Loss: 1.3092, Class Loss: 0.3547
Epoch 25/50, Loss: 1.9365, Domain Loss: 1.5591, Class Loss: 0.3773
Epoch 26/50, Loss: 2.0280, Domain Loss: 1.6659, Class Loss: 0.3622
Epoch 27/50, Loss: 1.6357, Domain Loss: 1.3601, Class Loss: 0.2756
Epoch 28/50, Loss: 1.7281, Domain Loss: 1.4538, Class Loss: 0.2743
Epoch 29/50, Loss: 2.3067, Domain Loss: 2.0309, Class Loss: 0.2758
Epoch 30/50, Loss: 2.0147, Domain Loss: 1.6471, Class Loss: 0.3676
Epoch 31/50, Loss: 3.4553, Domain Loss: 3.0129, Class Loss: 0.4424
Epoch 32/50, Loss: 2.4399, Domain Loss: 1.9428, Class Loss: 0.4971
Epoch 33/50, Loss: 1.8197, Domain Loss: 1.4343, Class Loss: 0.3854
Epoch 34/50, Loss: 2.6187, Domain Loss: 2.2806, Class Loss: 0.3382
Epoch 35/50, Loss: 2.8536, Domain Loss: 2.4471, Class Loss: 0.4065
Epoch 36/50, Loss: 3.1555, Domain Loss: 2.6896, Class Loss: 0.4659
Epoch 37/50, Loss: 4.6957, Domain Loss: 4.1363, Class Loss: 0.5593
Epoch 38/50, Loss: 3.9075, Domain Loss: 3.5803, Class Loss: 0.3273
Epoch 39/50, Loss: 3.2945, Domain Loss: 2.9247, Class Loss: 0.3697
Epoch 40/50, Loss: 3.4587, Domain Loss: 2.9947, Class Loss: 0.4640
Epoch 41/50, Loss: 2.7507, Domain Loss: 2.2504, Class Loss: 0.5003
Epoch 42/50, Loss: 2.2773, Domain Loss: 1.7923, Class Loss: 0.4850
Epoch 43/50, Loss: 2.3385, Domain Loss: 1.8240, Class Loss: 0.5145
Epoch 44/50, Loss: 1.7098, Domain Loss: 1.3331, Class Loss: 0.3766
Epoch 45/50, Loss: 1.6089, Domain Loss: 1.2689, Class Loss: 0.3400
Epoch 46/50, Loss: 1.8504, Domain Loss: 1.4148, Class Loss: 0.4356
Epoch 47/50, Loss: 1.7669, Domain Loss: 1.4059, Class Loss: 0.3610
Epoch 48/50, Loss: 1.7392, Domain Loss: 1.4056, Class Loss: 0.3336
Epoch 49/50, Loss: 1.6954, Domain Loss: 1.3707, Class Loss: 0.3248
Epoch 50/50, Loss: 1.6954, Domain Loss: 1.3993, Class Loss: 0.2961
66.13


Epoch 1/50, Loss: 2.2297, Domain Loss: 1.3657, Class Loss: 0.8640
Epoch 2/50, Loss: 1.6800, Domain Loss: 1.2676, Class Loss: 0.4123
Epoch 3/50, Loss: 2.2997, Domain Loss: 1.9219, Class Loss: 0.3778
Epoch 4/50, Loss: 9.2602, Domain Loss: 8.8274, Class Loss: 0.4328
Epoch 5/50, Loss: 13.3781, Domain Loss: 12.9440, Class Loss: 0.4340
Epoch 6/50, Loss: 20.3771, Domain Loss: 19.8793, Class Loss: 0.4978
Epoch 7/50, Loss: 10.0998, Domain Loss: 9.5378, Class Loss: 0.5620
Epoch 8/50, Loss: 6.2245, Domain Loss: 5.5097, Class Loss: 0.7148
Epoch 9/50, Loss: 3.9726, Domain Loss: 3.4438, Class Loss: 0.5288
Epoch 10/50, Loss: 3.8069, Domain Loss: 3.2321, Class Loss: 0.5748
Epoch 11/50, Loss: 3.1887, Domain Loss: 2.7033, Class Loss: 0.4855
Epoch 12/50, Loss: 2.7152, Domain Loss: 2.3123, Class Loss: 0.4028
Epoch 13/50, Loss: 2.7810, Domain Loss: 2.3685, Class Loss: 0.4124
Epoch 14/50, Loss: 2.5783, Domain Loss: 2.2154, Class Loss: 0.3629
Epoch 15/50, Loss: 2.3568, Domain Loss: 2.0359, Class Loss: 0.3209
Epoch 16/50, Loss: 2.3458, Domain Loss: 2.0007, Class Loss: 0.3452
Epoch 17/50, Loss: 2.4180, Domain Loss: 2.0824, Class Loss: 0.3356
Epoch 18/50, Loss: 1.8594, Domain Loss: 1.5414, Class Loss: 0.3180
Epoch 19/50, Loss: 1.8472, Domain Loss: 1.5265, Class Loss: 0.3208
Epoch 20/50, Loss: 2.3769, Domain Loss: 2.0456, Class Loss: 0.3313
Epoch 21/50, Loss: 2.1828, Domain Loss: 1.8765, Class Loss: 0.3063
Epoch 22/50, Loss: 2.9628, Domain Loss: 2.5977, Class Loss: 0.3652
Epoch 23/50, Loss: 2.3266, Domain Loss: 1.9983, Class Loss: 0.3282
Epoch 24/50, Loss: 1.9782, Domain Loss: 1.6884, Class Loss: 0.2898
Epoch 25/50, Loss: 1.7795, Domain Loss: 1.5029, Class Loss: 0.2766
Epoch 26/50, Loss: 1.7444, Domain Loss: 1.5001, Class Loss: 0.2443
Epoch 27/50, Loss: 2.2334, Domain Loss: 1.7451, Class Loss: 0.4883
Epoch 28/50, Loss: 1.8517, Domain Loss: 1.4994, Class Loss: 0.3523
Epoch 29/50, Loss: 1.7464, Domain Loss: 1.4093, Class Loss: 0.3371
Epoch 30/50, Loss: 1.6261, Domain Loss: 1.3897, Class Loss: 0.2364
Epoch 31/50, Loss: 1.8772, Domain Loss: 1.5446, Class Loss: 0.3326
Epoch 32/50, Loss: 2.2518, Domain Loss: 1.9148, Class Loss: 0.3370
Epoch 33/50, Loss: 1.8598, Domain Loss: 1.5812, Class Loss: 0.2786
Epoch 34/50, Loss: 1.6323, Domain Loss: 1.4161, Class Loss: 0.2163
Epoch 35/50, Loss: 1.7710, Domain Loss: 1.5446, Class Loss: 0.2264
Epoch 36/50, Loss: 1.8566, Domain Loss: 1.6073, Class Loss: 0.2494
Epoch 37/50, Loss: 1.6434, Domain Loss: 1.4078, Class Loss: 0.2356
Epoch 38/50, Loss: 1.7418, Domain Loss: 1.4865, Class Loss: 0.2553
Epoch 39/50, Loss: 1.7495, Domain Loss: 1.5096, Class Loss: 0.2399
Epoch 40/50, Loss: 1.6238, Domain Loss: 1.2995, Class Loss: 0.3243
Epoch 41/50, Loss: 1.5740, Domain Loss: 1.3499, Class Loss: 0.2241
Epoch 42/50, Loss: 1.7239, Domain Loss: 1.5238, Class Loss: 0.2000
Epoch 43/50, Loss: 1.6061, Domain Loss: 1.4333, Class Loss: 0.1728
Epoch 44/50, Loss: 1.7597, Domain Loss: 1.5198, Class Loss: 0.2399
Epoch 45/50, Loss: 1.6005, Domain Loss: 1.4129, Class Loss: 0.1876
Epoch 46/50, Loss: 1.5194, Domain Loss: 1.3871, Class Loss: 0.1323
Epoch 47/50, Loss: 1.5367, Domain Loss: 1.3671, Class Loss: 0.1696
Epoch 48/50, Loss: 1.6620, Domain Loss: 1.4813, Class Loss: 0.1808
Epoch 49/50, Loss: 1.5384, Domain Loss: 1.4115, Class Loss: 0.1269
Epoch 50/50, Loss: 1.4829, Domain Loss: 1.3905, Class Loss: 0.0924
63.07


Epoch 1/50, Loss: 2.3566, Domain Loss: 1.3765, Class Loss: 0.9801
Epoch 2/50, Loss: 1.7196, Domain Loss: 1.2572, Class Loss: 0.4624
Epoch 3/50, Loss: 1.6780, Domain Loss: 1.3082, Class Loss: 0.3698
Epoch 4/50, Loss: 1.8570, Domain Loss: 1.5525, Class Loss: 0.3045
Epoch 5/50, Loss: 2.3711, Domain Loss: 2.1434, Class Loss: 0.2277
Epoch 6/50, Loss: 12.8459, Domain Loss: 12.3570, Class Loss: 0.4889
Epoch 7/50, Loss: 6.9737, Domain Loss: 6.4936, Class Loss: 0.4801
Epoch 8/50, Loss: 7.1931, Domain Loss: 6.6155, Class Loss: 0.5776
Epoch 9/50, Loss: 6.5904, Domain Loss: 6.0081, Class Loss: 0.5823
Epoch 10/50, Loss: 7.1904, Domain Loss: 6.6059, Class Loss: 0.5845
Epoch 11/50, Loss: 6.8761, Domain Loss: 6.0891, Class Loss: 0.7870
Epoch 12/50, Loss: 5.0677, Domain Loss: 4.3208, Class Loss: 0.7469
Epoch 13/50, Loss: 2.6299, Domain Loss: 2.1615, Class Loss: 0.4685
Epoch 14/50, Loss: 2.2246, Domain Loss: 1.7819, Class Loss: 0.4427
Epoch 15/50, Loss: 2.2534, Domain Loss: 1.7805, Class Loss: 0.4728
Epoch 16/50, Loss: 2.4033, Domain Loss: 2.0006, Class Loss: 0.4026
Epoch 17/50, Loss: 3.2982, Domain Loss: 2.8587, Class Loss: 0.4395
Epoch 18/50, Loss: 3.6064, Domain Loss: 3.1591, Class Loss: 0.4473
Epoch 19/50, Loss: 3.5425, Domain Loss: 3.1647, Class Loss: 0.3779
Epoch 20/50, Loss: 2.4044, Domain Loss: 2.0377, Class Loss: 0.3667
Epoch 21/50, Loss: 2.7601, Domain Loss: 2.3801, Class Loss: 0.3800
Epoch 22/50, Loss: 3.0733, Domain Loss: 2.6577, Class Loss: 0.4156
Epoch 23/50, Loss: 4.4669, Domain Loss: 3.6983, Class Loss: 0.7686
Epoch 24/50, Loss: 3.9484, Domain Loss: 3.4409, Class Loss: 0.5075
Epoch 25/50, Loss: 3.0440, Domain Loss: 2.2441, Class Loss: 0.7999
Epoch 26/50, Loss: 2.6778, Domain Loss: 2.0242, Class Loss: 0.6535
Epoch 27/50, Loss: 2.0616, Domain Loss: 1.4318, Class Loss: 0.6298
Epoch 28/50, Loss: 2.0322, Domain Loss: 1.4166, Class Loss: 0.6156
Epoch 29/50, Loss: 1.9169, Domain Loss: 1.4049, Class Loss: 0.5120
Epoch 30/50, Loss: 1.9385, Domain Loss: 1.5044, Class Loss: 0.4341
Epoch 31/50, Loss: 2.0086, Domain Loss: 1.5766, Class Loss: 0.4319
Epoch 32/50, Loss: 1.9821, Domain Loss: 1.5829, Class Loss: 0.3992
Epoch 33/50, Loss: 1.8240, Domain Loss: 1.4325, Class Loss: 0.3915
Epoch 34/50, Loss: 1.7099, Domain Loss: 1.3905, Class Loss: 0.3194
Epoch 35/50, Loss: 1.8392, Domain Loss: 1.5097, Class Loss: 0.3295
Epoch 36/50, Loss: 1.7713, Domain Loss: 1.4406, Class Loss: 0.3307
Epoch 37/50, Loss: 1.6772, Domain Loss: 1.3697, Class Loss: 0.3075
Epoch 38/50, Loss: 1.7464, Domain Loss: 1.4263, Class Loss: 0.3202
Epoch 39/50, Loss: 1.6638, Domain Loss: 1.3449, Class Loss: 0.3189
Epoch 40/50, Loss: 1.8449, Domain Loss: 1.5198, Class Loss: 0.3251
Epoch 41/50, Loss: 1.8967, Domain Loss: 1.5751, Class Loss: 0.3216
Epoch 42/50, Loss: 1.8026, Domain Loss: 1.5088, Class Loss: 0.2938
Epoch 43/50, Loss: 1.7327, Domain Loss: 1.4778, Class Loss: 0.2549
Epoch 44/50, Loss: 1.8331, Domain Loss: 1.5773, Class Loss: 0.2558
Epoch 45/50, Loss: 1.8327, Domain Loss: 1.5804, Class Loss: 0.2523
Epoch 46/50, Loss: 1.7723, Domain Loss: 1.5223, Class Loss: 0.2500
Epoch 47/50, Loss: 1.8022, Domain Loss: 1.5498, Class Loss: 0.2524
Epoch 48/50, Loss: 1.7094, Domain Loss: 1.4944, Class Loss: 0.2150
Epoch 49/50, Loss: 1.7319, Domain Loss: 1.5671, Class Loss: 0.1648
Epoch 50/50, Loss: 1.6707, Domain Loss: 1.5142, Class Loss: 0.1565
63.31


Epoch 1/50, Loss: 2.2900, Domain Loss: 1.3742, Class Loss: 0.9159
Epoch 2/50, Loss: 1.7089, Domain Loss: 1.2806, Class Loss: 0.4283
Epoch 3/50, Loss: 2.0284, Domain Loss: 1.6850, Class Loss: 0.3434
Epoch 4/50, Loss: 4.8309, Domain Loss: 4.4737, Class Loss: 0.3572
Epoch 5/50, Loss: 11.3635, Domain Loss: 10.7693, Class Loss: 0.5941
Epoch 6/50, Loss: 15.9861, Domain Loss: 15.2967, Class Loss: 0.6895
Epoch 7/50, Loss: 9.3707, Domain Loss: 8.8468, Class Loss: 0.5238
Epoch 8/50, Loss: 3.9437, Domain Loss: 3.4104, Class Loss: 0.5333
Epoch 9/50, Loss: 2.5319, Domain Loss: 2.0486, Class Loss: 0.4833
Epoch 10/50, Loss: 2.9743, Domain Loss: 2.5726, Class Loss: 0.4017
Epoch 11/50, Loss: 12.4712, Domain Loss: 11.8320, Class Loss: 0.6391
Epoch 12/50, Loss: 13.6139, Domain Loss: 12.8634, Class Loss: 0.7505
Epoch 13/50, Loss: 6.8758, Domain Loss: 6.0350, Class Loss: 0.8407
Epoch 14/50, Loss: 3.4009, Domain Loss: 2.7057, Class Loss: 0.6951
Epoch 15/50, Loss: 2.2410, Domain Loss: 1.7144, Class Loss: 0.5266
Epoch 16/50, Loss: 1.8658, Domain Loss: 1.4446, Class Loss: 0.4213
Epoch 17/50, Loss: 2.0657, Domain Loss: 1.6235, Class Loss: 0.4422
Epoch 18/50, Loss: 2.4909, Domain Loss: 2.0318, Class Loss: 0.4591
Epoch 19/50, Loss: 2.7079, Domain Loss: 2.2760, Class Loss: 0.4320
Epoch 20/50, Loss: 2.8913, Domain Loss: 2.3766, Class Loss: 0.5146
Epoch 21/50, Loss: 5.8993, Domain Loss: 5.1892, Class Loss: 0.7101
Epoch 22/50, Loss: 2.7942, Domain Loss: 2.2364, Class Loss: 0.5577
Epoch 23/50, Loss: 3.8283, Domain Loss: 3.0704, Class Loss: 0.7580
Epoch 24/50, Loss: 2.6201, Domain Loss: 2.1497, Class Loss: 0.4704
Epoch 25/50, Loss: 1.8989, Domain Loss: 1.5047, Class Loss: 0.3942
Epoch 26/50, Loss: 2.1182, Domain Loss: 1.6033, Class Loss: 0.5149
Epoch 27/50, Loss: 2.1186, Domain Loss: 1.6736, Class Loss: 0.4449
Epoch 28/50, Loss: 2.6006, Domain Loss: 2.1621, Class Loss: 0.4385
Epoch 29/50, Loss: 3.2807, Domain Loss: 2.8622, Class Loss: 0.4185
Epoch 30/50, Loss: 2.2624, Domain Loss: 1.8339, Class Loss: 0.4286
Epoch 31/50, Loss: 1.6452, Domain Loss: 1.2460, Class Loss: 0.3992
Epoch 32/50, Loss: 1.8667, Domain Loss: 1.4465, Class Loss: 0.4203
Epoch 33/50, Loss: 1.6880, Domain Loss: 1.3401, Class Loss: 0.3479
Epoch 34/50, Loss: 1.8136, Domain Loss: 1.4923, Class Loss: 0.3213
Epoch 35/50, Loss: 1.8077, Domain Loss: 1.4863, Class Loss: 0.3214
Epoch 36/50, Loss: 1.9468, Domain Loss: 1.5896, Class Loss: 0.3572
Epoch 37/50, Loss: 1.9110, Domain Loss: 1.5699, Class Loss: 0.3411
Epoch 38/50, Loss: 1.9864, Domain Loss: 1.6088, Class Loss: 0.3776
Epoch 39/50, Loss: 2.5999, Domain Loss: 2.1369, Class Loss: 0.4630
Epoch 40/50, Loss: 2.1358, Domain Loss: 1.7760, Class Loss: 0.3597
Epoch 41/50, Loss: 1.7875, Domain Loss: 1.4541, Class Loss: 0.3335
Epoch 42/50, Loss: 1.8558, Domain Loss: 1.5402, Class Loss: 0.3156
Epoch 43/50, Loss: 1.8258, Domain Loss: 1.5281, Class Loss: 0.2977
Epoch 44/50, Loss: 1.7062, Domain Loss: 1.3941, Class Loss: 0.3121
Epoch 45/50, Loss: 1.6874, Domain Loss: 1.4063, Class Loss: 0.2811
Epoch 46/50, Loss: 1.6398, Domain Loss: 1.3904, Class Loss: 0.2495
Epoch 47/50, Loss: 1.5851, Domain Loss: 1.3570, Class Loss: 0.2281
Epoch 48/50, Loss: 1.5980, Domain Loss: 1.3762, Class Loss: 0.2219
Epoch 49/50, Loss: 1.6076, Domain Loss: 1.4040, Class Loss: 0.2036
Epoch 50/50, Loss: 1.6674, Domain Loss: 1.4403, Class Loss: 0.2272
65.59


Epoch 1/50, Loss: 2.3133, Domain Loss: 1.3868, Class Loss: 0.9265
Epoch 2/50, Loss: 1.7548, Domain Loss: 1.3436, Class Loss: 0.4112
Epoch 3/50, Loss: 1.7732, Domain Loss: 1.4434, Class Loss: 0.3298
Epoch 4/50, Loss: 2.4801, Domain Loss: 2.1925, Class Loss: 0.2876
Epoch 5/50, Loss: 11.6437, Domain Loss: 11.1821, Class Loss: 0.4616
Epoch 6/50, Loss: 13.8696, Domain Loss: 13.2911, Class Loss: 0.5785
Epoch 7/50, Loss: 6.7215, Domain Loss: 6.2658, Class Loss: 0.4557
Epoch 8/50, Loss: 3.3001, Domain Loss: 2.9116, Class Loss: 0.3885
Epoch 9/50, Loss: 2.4422, Domain Loss: 1.9651, Class Loss: 0.4771
Epoch 10/50, Loss: 2.2056, Domain Loss: 1.8001, Class Loss: 0.4056
Epoch 11/50, Loss: 2.2394, Domain Loss: 1.7319, Class Loss: 0.5074
Epoch 12/50, Loss: 2.8884, Domain Loss: 2.2987, Class Loss: 0.5897
Epoch 13/50, Loss: 6.4539, Domain Loss: 6.0092, Class Loss: 0.4447
Epoch 14/50, Loss: 5.4709, Domain Loss: 4.9458, Class Loss: 0.5252
Epoch 15/50, Loss: 4.3579, Domain Loss: 3.7740, Class Loss: 0.5839
Epoch 16/50, Loss: 4.9458, Domain Loss: 4.4284, Class Loss: 0.5174
Epoch 17/50, Loss: 2.9641, Domain Loss: 2.3505, Class Loss: 0.6136
Epoch 18/50, Loss: 2.0858, Domain Loss: 1.6038, Class Loss: 0.4820
Epoch 19/50, Loss: 1.8656, Domain Loss: 1.4649, Class Loss: 0.4008
Epoch 20/50, Loss: 4.3192, Domain Loss: 2.9300, Class Loss: 1.3892
Epoch 21/50, Loss: 4.6389, Domain Loss: 2.9343, Class Loss: 1.7046
Epoch 22/50, Loss: 3.4249, Domain Loss: 2.8644, Class Loss: 0.5605
Epoch 23/50, Loss: 3.4184, Domain Loss: 2.8948, Class Loss: 0.5236
Epoch 24/50, Loss: 3.9073, Domain Loss: 3.3866, Class Loss: 0.5207
Epoch 25/50, Loss: 3.4772, Domain Loss: 3.0013, Class Loss: 0.4759
Epoch 26/50, Loss: 4.2797, Domain Loss: 3.7305, Class Loss: 0.5492
Epoch 27/50, Loss: 5.9170, Domain Loss: 5.2580, Class Loss: 0.6591
Epoch 28/50, Loss: 9.0940, Domain Loss: 8.5229, Class Loss: 0.5710
Epoch 29/50, Loss: 6.4219, Domain Loss: 5.8764, Class Loss: 0.5455
Epoch 30/50, Loss: 5.0831, Domain Loss: 4.5283, Class Loss: 0.5548
Epoch 31/50, Loss: 4.4007, Domain Loss: 3.7699, Class Loss: 0.6308
Epoch 32/50, Loss: 4.2365, Domain Loss: 3.5068, Class Loss: 0.7298
Epoch 33/50, Loss: 4.7804, Domain Loss: 4.0410, Class Loss: 0.7394
Epoch 34/50, Loss: 3.2898, Domain Loss: 2.7228, Class Loss: 0.5670
Epoch 35/50, Loss: 3.0746, Domain Loss: 2.2154, Class Loss: 0.8592
Epoch 36/50, Loss: 2.2899, Domain Loss: 1.8219, Class Loss: 0.4680
Epoch 37/50, Loss: 2.9124, Domain Loss: 2.3958, Class Loss: 0.5166
Epoch 38/50, Loss: 3.1879, Domain Loss: 2.7197, Class Loss: 0.4682
Epoch 39/50, Loss: 2.6512, Domain Loss: 2.1400, Class Loss: 0.5112
Epoch 40/50, Loss: 1.8980, Domain Loss: 1.5009, Class Loss: 0.3971
Epoch 41/50, Loss: 1.8096, Domain Loss: 1.4242, Class Loss: 0.3855
Epoch 42/50, Loss: 2.4970, Domain Loss: 1.9259, Class Loss: 0.5710
Epoch 43/50, Loss: 2.1250, Domain Loss: 1.7608, Class Loss: 0.3642
Epoch 44/50, Loss: 2.3574, Domain Loss: 1.9446, Class Loss: 0.4128
Epoch 45/50, Loss: 1.9637, Domain Loss: 1.5724, Class Loss: 0.3913
Epoch 46/50, Loss: 1.8515, Domain Loss: 1.4944, Class Loss: 0.3571
Epoch 47/50, Loss: 2.0310, Domain Loss: 1.6845, Class Loss: 0.3466
Epoch 48/50, Loss: 1.7248, Domain Loss: 1.4111, Class Loss: 0.3137
Epoch 49/50, Loss: 1.6129, Domain Loss: 1.2761, Class Loss: 0.3369
Epoch 50/50, Loss: 1.5662, Domain Loss: 1.2750, Class Loss: 0.2911
71.28


Epoch 1/50, Loss: 2.3559, Domain Loss: 1.3757, Class Loss: 0.9802
Epoch 2/50, Loss: 1.6941, Domain Loss: 1.2915, Class Loss: 0.4026
Epoch 3/50, Loss: 1.8375, Domain Loss: 1.4658, Class Loss: 0.3717
Epoch 4/50, Loss: 3.5008, Domain Loss: 3.1982, Class Loss: 0.3027
Epoch 5/50, Loss: 7.2431, Domain Loss: 6.8778, Class Loss: 0.3652
Epoch 6/50, Loss: 18.4572, Domain Loss: 17.5430, Class Loss: 0.9142
Epoch 7/50, Loss: 21.3556, Domain Loss: 20.5706, Class Loss: 0.7850
Epoch 8/50, Loss: 6.3857, Domain Loss: 5.7741, Class Loss: 0.6116
Epoch 9/50, Loss: 3.6420, Domain Loss: 3.0965, Class Loss: 0.5455
Epoch 10/50, Loss: 6.0150, Domain Loss: 5.0736, Class Loss: 0.9414
Epoch 11/50, Loss: 8.1320, Domain Loss: 6.7751, Class Loss: 1.3569
Epoch 12/50, Loss: 3.7794, Domain Loss: 3.1031, Class Loss: 0.6763
Epoch 13/50, Loss: 2.4352, Domain Loss: 1.8942, Class Loss: 0.5410
Epoch 14/50, Loss: 1.9892, Domain Loss: 1.5013, Class Loss: 0.4879
Epoch 15/50, Loss: 2.0234, Domain Loss: 1.5703, Class Loss: 0.4530
Epoch 16/50, Loss: 1.9432, Domain Loss: 1.5223, Class Loss: 0.4209
Epoch 17/50, Loss: 1.8305, Domain Loss: 1.4401, Class Loss: 0.3904
Epoch 18/50, Loss: 2.0777, Domain Loss: 1.5711, Class Loss: 0.5067
Epoch 19/50, Loss: 1.8312, Domain Loss: 1.4492, Class Loss: 0.3820
Epoch 20/50, Loss: 1.9012, Domain Loss: 1.5436, Class Loss: 0.3577
Epoch 21/50, Loss: 2.2383, Domain Loss: 1.7573, Class Loss: 0.4810
Epoch 22/50, Loss: 1.8602, Domain Loss: 1.5405, Class Loss: 0.3197
Epoch 23/50, Loss: 2.0042, Domain Loss: 1.6846, Class Loss: 0.3195
Epoch 24/50, Loss: 3.3360, Domain Loss: 2.9875, Class Loss: 0.3486
Epoch 25/50, Loss: 5.9859, Domain Loss: 5.6913, Class Loss: 0.2946
Epoch 26/50, Loss: 3.1227, Domain Loss: 2.6741, Class Loss: 0.4486
Epoch 27/50, Loss: 2.4266, Domain Loss: 1.9540, Class Loss: 0.4726
Epoch 28/50, Loss: 2.1849, Domain Loss: 1.7390, Class Loss: 0.4459
Epoch 29/50, Loss: 2.1402, Domain Loss: 1.7325, Class Loss: 0.4076
Epoch 30/50, Loss: 1.8457, Domain Loss: 1.4688, Class Loss: 0.3768
Epoch 31/50, Loss: 1.7774, Domain Loss: 1.4490, Class Loss: 0.3284
Epoch 32/50, Loss: 1.8260, Domain Loss: 1.4453, Class Loss: 0.3807
Epoch 33/50, Loss: 1.6607, Domain Loss: 1.4056, Class Loss: 0.2551
Epoch 34/50, Loss: 2.0017, Domain Loss: 1.6362, Class Loss: 0.3655
Epoch 35/50, Loss: 1.9077, Domain Loss: 1.5347, Class Loss: 0.3730
Epoch 36/50, Loss: 1.9226, Domain Loss: 1.6363, Class Loss: 0.2863
Epoch 37/50, Loss: 1.7403, Domain Loss: 1.4834, Class Loss: 0.2569
Epoch 38/50, Loss: 1.5856, Domain Loss: 1.3799, Class Loss: 0.2056
Epoch 39/50, Loss: 2.0625, Domain Loss: 1.7435, Class Loss: 0.3189
Epoch 40/50, Loss: 1.5551, Domain Loss: 1.3546, Class Loss: 0.2004
Epoch 41/50, Loss: 1.9295, Domain Loss: 1.6761, Class Loss: 0.2533
Epoch 42/50, Loss: 1.8183, Domain Loss: 1.5390, Class Loss: 0.2793
Epoch 43/50, Loss: 1.5897, Domain Loss: 1.3780, Class Loss: 0.2117
Epoch 44/50, Loss: 1.5602, Domain Loss: 1.3722, Class Loss: 0.1880
Epoch 45/50, Loss: 1.5632, Domain Loss: 1.4136, Class Loss: 0.1496
Epoch 46/50, Loss: 1.5854, Domain Loss: 1.4426, Class Loss: 0.1427
Epoch 47/50, Loss: 1.5450, Domain Loss: 1.4347, Class Loss: 0.1103
Epoch 48/50, Loss: 1.5186, Domain Loss: 1.4013, Class Loss: 0.1173
Epoch 49/50, Loss: 1.5060, Domain Loss: 1.3998, Class Loss: 0.1062
Epoch 50/50, Loss: 1.4676, Domain Loss: 1.3944, Class Loss: 0.0732
41.97


Source performance:
56.06 66.87 57.14 48.65 
Target performance:
60.02 53.37 59.85 54.01 

Per-class target performance: 100.00 15.61 63.21 60.57 Deep CORAL
Deep CORAL Run 1/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1: Source Val Acc = 0.5492, Target Val Acc = 0.4784
Epoch 2: Source Val Acc = 0.5270, Target Val Acc = 0.5180
Epoch 3: Source Val Acc = 0.7926, Target Val Acc = 0.5336
Epoch 4: Source Val Acc = 0.8441, Target Val Acc = 0.6199
Epoch 5: Source Val Acc = 0.7158, Target Val Acc = 0.4143
Epoch 6: Source Val Acc = 0.8219, Target Val Acc = 0.4898
Epoch 7: Source Val Acc = 0.6241, Target Val Acc = 0.5959
Epoch 8: Source Val Acc = 0.6601, Target Val Acc = 0.5432
Epoch 9: Source Val Acc = 0.9496, Target Val Acc = 0.5390
Epoch 10: Source Val Acc = 0.9436, Target Val Acc = 0.5893
Epoch 11: Source Val Acc = 0.7530, Target Val Acc = 0.5132
Epoch 12: Source Val Acc = 0.7032, Target Val Acc = 0.6253
Epoch 13: Source Val Acc = 0.9544, Target Val Acc = 0.4880
Epoch 14: Source Val Acc = 0.9940, Target Val Acc = 0.5090
Epoch 15: Source Val Acc = 0.9814, Target Val Acc = 0.5618
Epoch 16: Source Val Acc = 0.9808, Target Val Acc = 0.5653
Epoch 17: Source Val Acc = 0.7332, Target Val Acc = 0.4880
Epoch 18: Source Val Acc = 0.7518, Target Val Acc = 0.4898
Epoch 19: Source Val Acc = 0.8393, Target Val Acc = 0.5078
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.8393, Target Val Acc = 0.5078

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.6499, Target Val Acc = 0.5252
Epoch 2: Source Val Acc = 0.8076, Target Val Acc = 0.5689
Epoch 3: Source Val Acc = 0.8399, Target Val Acc = 0.6421
Epoch 4: Source Val Acc = 0.7914, Target Val Acc = 0.7026
Epoch 5: Source Val Acc = 0.9514, Target Val Acc = 0.5336
Epoch 6: Source Val Acc = 0.7734, Target Val Acc = 0.5270
Epoch 7: Source Val Acc = 0.9406, Target Val Acc = 0.5761
Epoch 8: Source Val Acc = 0.9311, Target Val Acc = 0.5923
Epoch 9: Source Val Acc = 0.8669, Target Val Acc = 0.6960
Epoch 10: Source Val Acc = 0.7710, Target Val Acc = 0.5420
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.7710, Target Val Acc = 0.5420

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.5797, Target Val Acc = 0.5905
Epoch 2: Source Val Acc = 0.2644, Target Val Acc = 0.4233
Epoch 3: Source Val Acc = 0.7518, Target Val Acc = 0.5096
Epoch 4: Source Val Acc = 0.9628, Target Val Acc = 0.6691
Epoch 5: Source Val Acc = 0.7842, Target Val Acc = 0.4946
Epoch 6: Source Val Acc = 0.9191, Target Val Acc = 0.3471
Epoch 7: Source Val Acc = 0.8867, Target Val Acc = 0.6169
Epoch 8: Source Val Acc = 0.9311, Target Val Acc = 0.6223
Epoch 9: Source Val Acc = 0.9365, Target Val Acc = 0.5420
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.9365, Target Val Acc = 0.5420

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.8933, Target Val Acc = 0.5510
Epoch 2: Source Val Acc = 0.7776, Target Val Acc = 0.5138
Epoch 3: Source Val Acc = 0.7524, Target Val Acc = 0.5504
Epoch 4: Source Val Acc = 0.7128, Target Val Acc = 0.5929
Epoch 5: Source Val Acc = 0.7044, Target Val Acc = 0.6103
Epoch 6: Source Val Acc = 0.9442, Target Val Acc = 0.6367
Epoch 7: Source Val Acc = 0.9622, Target Val Acc = 0.6936
Epoch 8: Source Val Acc = 0.8771, Target Val Acc = 0.5971
Epoch 9: Source Val Acc = 0.7470, Target Val Acc = 0.6403
Epoch 10: Source Val Acc = 0.9862, Target Val Acc = 0.5624
Epoch 11: Source Val Acc = 0.9167, Target Val Acc = 0.5665
Epoch 12: Source Val Acc = 0.9406, Target Val Acc = 0.6343
Epoch 13: Source Val Acc = 0.9682, Target Val Acc = 0.6487
Epoch 14: Source Val Acc = 0.8513, Target Val Acc = 0.6871
Epoch 15: Source Val Acc = 0.9059, Target Val Acc = 0.6691
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.9059, Target Val Acc = 0.6691

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.8777, Target Val Acc = 0.4958
Epoch 2: Source Val Acc = 0.7704, Target Val Acc = 0.4940
Epoch 3: Source Val Acc = 0.6990, Target Val Acc = 0.6157
Epoch 4: Source Val Acc = 0.7392, Target Val Acc = 0.5372
Epoch 5: Source Val Acc = 0.4011, Target Val Acc = 0.2662
Epoch 6: Source Val Acc = 0.8393, Target Val Acc = 0.5647
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.8393, Target Val Acc = 0.5647

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.7836, Target Val Acc = 0.6049
Epoch 2: Source Val Acc = 0.6918, Target Val Acc = 0.4922
Epoch 3: Source Val Acc = 0.7740, Target Val Acc = 0.5743
Epoch 4: Source Val Acc = 0.9634, Target Val Acc = 0.5606
Epoch 5: Source Val Acc = 0.8273, Target Val Acc = 0.6223
Epoch 6: Source Val Acc = 0.9874, Target Val Acc = 0.6121
Epoch 7: Source Val Acc = 0.7548, Target Val Acc = 0.5384
Epoch 8: Source Val Acc = 0.9844, Target Val Acc = 0.5713
Epoch 9: Source Val Acc = 0.9017, Target Val Acc = 0.6247
Epoch 10: Source Val Acc = 0.8399, Target Val Acc = 0.4940
Epoch 11: Source Val Acc = 0.9886, Target Val Acc = 0.7062
Epoch 12: Source Val Acc = 0.9832, Target Val Acc = 0.5971
Epoch 13: Source Val Acc = 0.9922, Target Val Acc = 0.6888
Epoch 14: Source Val Acc = 0.9940, Target Val Acc = 0.6769
Epoch 15: Source Val Acc = 0.9706, Target Val Acc = 0.5923
Epoch 16: Source Val Acc = 0.9880, Target Val Acc = 0.5977
Epoch 17: Source Val Acc = 0.7692, Target Val Acc = 0.7836
Epoch 18: Source Val Acc = 0.9275, Target Val Acc = 0.8429
Epoch 19: Source Val Acc = 0.9832, Target Val Acc = 0.6930
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 0.9832, Target Val Acc = 0.6930

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.7680, Target Val Acc = 0.4952
Epoch 2: Source Val Acc = 0.7476, Target Val Acc = 0.6277
Epoch 3: Source Val Acc = 0.9394, Target Val Acc = 0.6882
Epoch 4: Source Val Acc = 0.9574, Target Val Acc = 0.4910
Epoch 5: Source Val Acc = 0.5887, Target Val Acc = 0.5510
Epoch 6: Source Val Acc = 0.5797, Target Val Acc = 0.5408
Epoch 7: Source Val Acc = 0.7854, Target Val Acc = 0.4718
Epoch 8: Source Val Acc = 0.9299, Target Val Acc = 0.6649
Epoch 9: Source Val Acc = 0.9418, Target Val Acc = 0.6349
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.9418, Target Val Acc = 0.6349

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.7902, Target Val Acc = 0.4526
Epoch 2: Source Val Acc = 0.7590, Target Val Acc = 0.7044
Epoch 3: Source Val Acc = 0.4796, Target Val Acc = 0.5540
Epoch 4: Source Val Acc = 0.5366, Target Val Acc = 0.4814
Epoch 5: Source Val Acc = 0.7512, Target Val Acc = 0.5390
Epoch 6: Source Val Acc = 0.7530, Target Val Acc = 0.4988
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.7530, Target Val Acc = 0.4988

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.6763, Target Val Acc = 0.4922
Epoch 2: Source Val Acc = 0.7380, Target Val Acc = 0.5971
Epoch 3: Source Val Acc = 0.8741, Target Val Acc = 0.7008
Epoch 4: Source Val Acc = 0.6205, Target Val Acc = 0.3873
Epoch 5: Source Val Acc = 0.7272, Target Val Acc = 0.6595
Epoch 6: Source Val Acc = 0.8177, Target Val Acc = 0.7212
Epoch 7: Source Val Acc = 0.7446, Target Val Acc = 0.6253
Epoch 8: Source Val Acc = 0.9484, Target Val Acc = 0.5444
Epoch 9: Source Val Acc = 0.8423, Target Val Acc = 0.6733
Epoch 10: Source Val Acc = 0.7710, Target Val Acc = 0.7104
Epoch 11: Source Val Acc = 0.8004, Target Val Acc = 0.7254
Epoch 12: Source Val Acc = 0.8927, Target Val Acc = 0.7236
Epoch 13: Source Val Acc = 0.9257, Target Val Acc = 0.6817
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 0.9257, Target Val Acc = 0.6817

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.7956, Target Val Acc = 0.4904
Epoch 2: Source Val Acc = 0.5234, Target Val Acc = 0.3118
Epoch 3: Source Val Acc = 0.9460, Target Val Acc = 0.6361
Epoch 4: Source Val Acc = 0.9418, Target Val Acc = 0.6067
Epoch 5: Source Val Acc = 0.8573, Target Val Acc = 0.6145
Epoch 6: Source Val Acc = 0.6601, Target Val Acc = 0.6019
Epoch 7: Source Val Acc = 0.8501, Target Val Acc = 0.6847
Epoch 8: Source Val Acc = 0.8861, Target Val Acc = 0.6817
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.8861, Target Val Acc = 0.6817

Deep CORAL: Average Source Val Acc = 0.8782, Average Target Val Acc = 0.6016
STAR

Run 1/10
Epoch [1/50], Class Loss: 1.3721, Discrepancy Loss: 0.1252
Epoch [2/50], Class Loss: 0.6335, Discrepancy Loss: 0.0701
Epoch [3/50], Class Loss: 0.2421, Discrepancy Loss: 0.0477
Epoch [4/50], Class Loss: 0.4399, Discrepancy Loss: 0.0719
Epoch [5/50], Class Loss: 0.2852, Discrepancy Loss: 0.0578
Epoch [6/50], Class Loss: 0.5244, Discrepancy Loss: 0.0822
Epoch [7/50], Class Loss: 0.1830, Discrepancy Loss: 0.0343
Epoch [8/50], Class Loss: 0.0640, Discrepancy Loss: 0.0344
Epoch [9/50], Class Loss: 0.0858, Discrepancy Loss: 0.0377
Epoch [10/50], Class Loss: 0.0807, Discrepancy Loss: 0.0274
Epoch [11/50], Class Loss: 0.0416, Discrepancy Loss: 0.0272
Epoch [12/50], Class Loss: 0.1062, Discrepancy Loss: 0.0228
Epoch [13/50], Class Loss: 0.0248, Discrepancy Loss: 0.0228
Epoch [14/50], Class Loss: 0.0619, Discrepancy Loss: 0.0255
Epoch [15/50], Class Loss: 0.0245, Discrepancy Loss: 0.0152
Epoch [16/50], Class Loss: 0.0236, Discrepancy Loss: 0.0153
Epoch [17/50], Class Loss: 0.0201, Discrepancy Loss: 0.0189
Epoch [18/50], Class Loss: 0.0310, Discrepancy Loss: 0.0156
Epoch [19/50], Class Loss: 0.0178, Discrepancy Loss: 0.0148
Epoch [20/50], Class Loss: 0.0286, Discrepancy Loss: 0.0137
Epoch [21/50], Class Loss: 0.0091, Discrepancy Loss: 0.0150
Epoch [22/50], Class Loss: 0.0623, Discrepancy Loss: 0.0138
Epoch [23/50], Class Loss: 0.0230, Discrepancy Loss: 0.0113
Epoch [24/50], Class Loss: 0.0063, Discrepancy Loss: 0.0119
Epoch [25/50], Class Loss: 0.0122, Discrepancy Loss: 0.0145
Epoch [26/50], Class Loss: 0.0078, Discrepancy Loss: 0.0133
Epoch [27/50], Class Loss: 0.0275, Discrepancy Loss: 0.0143
Epoch [28/50], Class Loss: 0.0069, Discrepancy Loss: 0.0131
Epoch [29/50], Class Loss: 0.0126, Discrepancy Loss: 0.0144
Epoch [30/50], Class Loss: 0.0136, Discrepancy Loss: 0.0118
Epoch [31/50], Class Loss: 0.0090, Discrepancy Loss: 0.0124
Epoch [32/50], Class Loss: 0.0071, Discrepancy Loss: 0.0135
Epoch [33/50], Class Loss: 0.0044, Discrepancy Loss: 0.0123
Epoch [34/50], Class Loss: 0.0149, Discrepancy Loss: 0.0132
Epoch [35/50], Class Loss: 0.0060, Discrepancy Loss: 0.0131
Epoch [36/50], Class Loss: 0.0057, Discrepancy Loss: 0.0126
Epoch [37/50], Class Loss: 0.0053, Discrepancy Loss: 0.0115
Epoch [38/50], Class Loss: 0.0659, Discrepancy Loss: 0.0131
Epoch [39/50], Class Loss: 0.0050, Discrepancy Loss: 0.0127
Epoch [40/50], Class Loss: 0.0052, Discrepancy Loss: 0.0142
Epoch [41/50], Class Loss: 0.0046, Discrepancy Loss: 0.0124
Epoch [42/50], Class Loss: 0.0054, Discrepancy Loss: 0.0130
Epoch [43/50], Class Loss: 0.0059, Discrepancy Loss: 0.0107
Epoch [44/50], Class Loss: 0.0105, Discrepancy Loss: 0.0141
Epoch [45/50], Class Loss: 0.0152, Discrepancy Loss: 0.0140
Epoch [46/50], Class Loss: 0.0829, Discrepancy Loss: 0.0132
Epoch [47/50], Class Loss: 0.0190, Discrepancy Loss: 0.0126
Epoch [48/50], Class Loss: 0.0057, Discrepancy Loss: 0.0152
Epoch [49/50], Class Loss: 0.0049, Discrepancy Loss: 0.0134
Epoch [50/50], Class Loss: 0.0064, Discrepancy Loss: 0.0140
Source Domain Performance - Accuracy: 48.74%, Precision: 33.37%, Recall: 50.00%, F1 Score: 37.55%
Target Domain Performance - Accuracy: 47.66%, Precision: 51.49%, Recall: 47.10%, F1 Score: 48.41%

Run 2/10
Epoch [1/50], Class Loss: 1.6623, Discrepancy Loss: 0.1205
Epoch [2/50], Class Loss: 0.9430, Discrepancy Loss: 0.0905
Epoch [3/50], Class Loss: 0.6211, Discrepancy Loss: 0.0757
Epoch [4/50], Class Loss: 0.1818, Discrepancy Loss: 0.0596
Epoch [5/50], Class Loss: 0.1286, Discrepancy Loss: 0.0502
Epoch [6/50], Class Loss: 0.0570, Discrepancy Loss: 0.0337
Epoch [7/50], Class Loss: 1.0198, Discrepancy Loss: 0.0877
Epoch [8/50], Class Loss: 0.2174, Discrepancy Loss: 0.0531
Epoch [9/50], Class Loss: 0.1693, Discrepancy Loss: 0.0635
Epoch [10/50], Class Loss: 0.0379, Discrepancy Loss: 0.0578
Epoch [11/50], Class Loss: 0.0490, Discrepancy Loss: 0.0388
Epoch [12/50], Class Loss: 0.0207, Discrepancy Loss: 0.0489
Epoch [13/50], Class Loss: 0.0183, Discrepancy Loss: 0.0502
Epoch [14/50], Class Loss: 0.0174, Discrepancy Loss: 0.0572
Epoch [15/50], Class Loss: 0.0190, Discrepancy Loss: 0.0497
Epoch [16/50], Class Loss: 0.0145, Discrepancy Loss: 0.0432
Epoch [17/50], Class Loss: 0.0312, Discrepancy Loss: 0.0378
Epoch [18/50], Class Loss: 0.0227, Discrepancy Loss: 0.0296
Epoch [19/50], Class Loss: 0.0140, Discrepancy Loss: 0.0258
Epoch [20/50], Class Loss: 0.0190, Discrepancy Loss: 0.0228
Epoch [21/50], Class Loss: 0.0116, Discrepancy Loss: 0.0237
Epoch [22/50], Class Loss: 0.0062, Discrepancy Loss: 0.0204
Epoch [23/50], Class Loss: 0.0045, Discrepancy Loss: 0.0249
Epoch [24/50], Class Loss: 0.0066, Discrepancy Loss: 0.0205
Epoch [25/50], Class Loss: 0.0096, Discrepancy Loss: 0.0227
Epoch [26/50], Class Loss: 0.0048, Discrepancy Loss: 0.0199
Epoch [27/50], Class Loss: 0.0035, Discrepancy Loss: 0.0209
Epoch [28/50], Class Loss: 0.0047, Discrepancy Loss: 0.0191
Epoch [29/50], Class Loss: 0.0037, Discrepancy Loss: 0.0203
Epoch [30/50], Class Loss: 0.0053, Discrepancy Loss: 0.0181
Epoch [31/50], Class Loss: 0.0074, Discrepancy Loss: 0.0188
Epoch [32/50], Class Loss: 0.0053, Discrepancy Loss: 0.0164
Epoch [33/50], Class Loss: 0.0277, Discrepancy Loss: 0.0175
Epoch [34/50], Class Loss: 0.0109, Discrepancy Loss: 0.0192
Epoch [35/50], Class Loss: 0.0068, Discrepancy Loss: 0.0194
Epoch [36/50], Class Loss: 0.0059, Discrepancy Loss: 0.0170
Epoch [37/50], Class Loss: 0.0385, Discrepancy Loss: 0.0187
Epoch [38/50], Class Loss: 0.0067, Discrepancy Loss: 0.0160
Epoch [39/50], Class Loss: 0.0069, Discrepancy Loss: 0.0162
Epoch [40/50], Class Loss: 0.0044, Discrepancy Loss: 0.0164
Epoch [41/50], Class Loss: 0.0052, Discrepancy Loss: 0.0158
Epoch [42/50], Class Loss: 0.0033, Discrepancy Loss: 0.0192
Epoch [43/50], Class Loss: 0.0129, Discrepancy Loss: 0.0165
Epoch [44/50], Class Loss: 0.0568, Discrepancy Loss: 0.0165
Epoch [45/50], Class Loss: 0.0076, Discrepancy Loss: 0.0170
Epoch [46/50], Class Loss: 0.0053, Discrepancy Loss: 0.0181
Epoch [47/50], Class Loss: 0.0039, Discrepancy Loss: 0.0167
Epoch [48/50], Class Loss: 0.0071, Discrepancy Loss: 0.0175
Epoch [49/50], Class Loss: 0.0086, Discrepancy Loss: 0.0167
Epoch [50/50], Class Loss: 0.0040, Discrepancy Loss: 0.0141
Source Domain Performance - Accuracy: 49.04%, Precision: 83.16%, Recall: 50.29%, F1 Score: 37.88%
Target Domain Performance - Accuracy: 50.78%, Precision: 45.26%, Recall: 50.40%, F1 Score: 47.27%

Run 3/10
Epoch [1/50], Class Loss: 1.9078, Discrepancy Loss: 0.1331
Epoch [2/50], Class Loss: 0.9394, Discrepancy Loss: 0.0902
Epoch [3/50], Class Loss: 0.7002, Discrepancy Loss: 0.0577
Epoch [4/50], Class Loss: 0.4311, Discrepancy Loss: 0.0858
Epoch [5/50], Class Loss: 0.4030, Discrepancy Loss: 0.0473
Epoch [6/50], Class Loss: 0.1945, Discrepancy Loss: 0.0376
Epoch [7/50], Class Loss: 0.0790, Discrepancy Loss: 0.0229
Epoch [8/50], Class Loss: 0.0700, Discrepancy Loss: 0.0234
Epoch [9/50], Class Loss: 0.1006, Discrepancy Loss: 0.0268
Epoch [10/50], Class Loss: 0.1095, Discrepancy Loss: 0.0185
Epoch [11/50], Class Loss: 0.0309, Discrepancy Loss: 0.0189
Epoch [12/50], Class Loss: 0.0089, Discrepancy Loss: 0.0114
Epoch [13/50], Class Loss: 0.0247, Discrepancy Loss: 0.0126
Epoch [14/50], Class Loss: 0.0152, Discrepancy Loss: 0.0124
Epoch [15/50], Class Loss: 0.0122, Discrepancy Loss: 0.0080
Epoch [16/50], Class Loss: 0.0077, Discrepancy Loss: 0.0081
Epoch [17/50], Class Loss: 0.0045, Discrepancy Loss: 0.0072
Epoch [18/50], Class Loss: 0.0308, Discrepancy Loss: 0.0087
Epoch [19/50], Class Loss: 0.0806, Discrepancy Loss: 0.0097
Epoch [20/50], Class Loss: 0.0187, Discrepancy Loss: 0.0132
Epoch [21/50], Class Loss: 0.0064, Discrepancy Loss: 0.0111
Epoch [22/50], Class Loss: 0.0057, Discrepancy Loss: 0.0073
Epoch [23/50], Class Loss: 0.0062, Discrepancy Loss: 0.0069
Epoch [24/50], Class Loss: 0.0035, Discrepancy Loss: 0.0092
Epoch [25/50], Class Loss: 0.0053, Discrepancy Loss: 0.0086
Epoch [26/50], Class Loss: 0.0049, Discrepancy Loss: 0.0083
Epoch [27/50], Class Loss: 0.0118, Discrepancy Loss: 0.0078
Epoch [28/50], Class Loss: 0.0277, Discrepancy Loss: 0.0076
Epoch [29/50], Class Loss: 0.0097, Discrepancy Loss: 0.0083
Epoch [30/50], Class Loss: 0.0072, Discrepancy Loss: 0.0063
Epoch [31/50], Class Loss: 0.0028, Discrepancy Loss: 0.0081
Epoch [32/50], Class Loss: 0.0257, Discrepancy Loss: 0.0065
Epoch [33/50], Class Loss: 0.0095, Discrepancy Loss: 0.0072
Epoch [34/50], Class Loss: 0.0054, Discrepancy Loss: 0.0079
Epoch [35/50], Class Loss: 0.0269, Discrepancy Loss: 0.0070
Epoch [36/50], Class Loss: 0.0095, Discrepancy Loss: 0.0072
Epoch [37/50], Class Loss: 0.0037, Discrepancy Loss: 0.0084
Epoch [38/50], Class Loss: 0.0061, Discrepancy Loss: 0.0080
Epoch [39/50], Class Loss: 0.0338, Discrepancy Loss: 0.0055
Epoch [40/50], Class Loss: 0.2037, Discrepancy Loss: 0.0094
Epoch [41/50], Class Loss: 0.1078, Discrepancy Loss: 0.0074
Epoch [42/50], Class Loss: 0.0036, Discrepancy Loss: 0.0052
Epoch [43/50], Class Loss: 0.0041, Discrepancy Loss: 0.0075
Epoch [44/50], Class Loss: 0.0030, Discrepancy Loss: 0.0065
Epoch [45/50], Class Loss: 0.0045, Discrepancy Loss: 0.0070
Epoch [46/50], Class Loss: 0.0046, Discrepancy Loss: 0.0073
Epoch [47/50], Class Loss: 0.0039, Discrepancy Loss: 0.0071
Epoch [48/50], Class Loss: 0.0082, Discrepancy Loss: 0.0088
Epoch [49/50], Class Loss: 0.0084, Discrepancy Loss: 0.0079
Epoch [50/50], Class Loss: 0.0033, Discrepancy Loss: 0.0088
Source Domain Performance - Accuracy: 48.92%, Precision: 58.15%, Recall: 50.17%, F1 Score: 37.63%
Target Domain Performance - Accuracy: 55.70%, Precision: 52.88%, Recall: 54.94%, F1 Score: 53.48%

Run 4/10
Epoch [1/50], Class Loss: 1.4675, Discrepancy Loss: 0.1306
Epoch [2/50], Class Loss: 0.6624, Discrepancy Loss: 0.0824
Epoch [3/50], Class Loss: 0.2919, Discrepancy Loss: 0.0917
Epoch [4/50], Class Loss: 0.6045, Discrepancy Loss: 0.0911
Epoch [5/50], Class Loss: 0.3372, Discrepancy Loss: 0.0773
Epoch [6/50], Class Loss: 0.1853, Discrepancy Loss: 0.0453
Epoch [7/50], Class Loss: 0.2682, Discrepancy Loss: 0.0561
Epoch [8/50], Class Loss: 0.1378, Discrepancy Loss: 0.0452
Epoch [9/50], Class Loss: 0.0666, Discrepancy Loss: 0.0279
Epoch [10/50], Class Loss: 0.0526, Discrepancy Loss: 0.0263
Epoch [11/50], Class Loss: 0.0220, Discrepancy Loss: 0.0250
Epoch [12/50], Class Loss: 0.0163, Discrepancy Loss: 0.0218
Epoch [13/50], Class Loss: 0.1060, Discrepancy Loss: 0.0191
Epoch [14/50], Class Loss: 0.0209, Discrepancy Loss: 0.0199
Epoch [15/50], Class Loss: 0.0055, Discrepancy Loss: 0.0174
Epoch [16/50], Class Loss: 0.0069, Discrepancy Loss: 0.0131
Epoch [17/50], Class Loss: 0.0051, Discrepancy Loss: 0.0108
Epoch [18/50], Class Loss: 0.0075, Discrepancy Loss: 0.0097
Epoch [19/50], Class Loss: 0.0042, Discrepancy Loss: 0.0095
Epoch [20/50], Class Loss: 0.0026, Discrepancy Loss: 0.0086
Epoch [21/50], Class Loss: 0.0545, Discrepancy Loss: 0.0070
Epoch [22/50], Class Loss: 0.0052, Discrepancy Loss: 0.0075
Epoch [23/50], Class Loss: 0.0032, Discrepancy Loss: 0.0074
Epoch [24/50], Class Loss: 0.0053, Discrepancy Loss: 0.0067
Epoch [25/50], Class Loss: 0.0035, Discrepancy Loss: 0.0074
Epoch [26/50], Class Loss: 0.0495, Discrepancy Loss: 0.0077
Epoch [27/50], Class Loss: 0.0045, Discrepancy Loss: 0.0105
Epoch [28/50], Class Loss: 0.0031, Discrepancy Loss: 0.0089
Epoch [29/50], Class Loss: 0.1153, Discrepancy Loss: 0.0079
Epoch [30/50], Class Loss: 0.0021, Discrepancy Loss: 0.0074
Epoch [31/50], Class Loss: 0.0029, Discrepancy Loss: 0.0086
Epoch [32/50], Class Loss: 0.1485, Discrepancy Loss: 0.0073
Epoch [33/50], Class Loss: 0.0022, Discrepancy Loss: 0.0084
Epoch [34/50], Class Loss: 0.0015, Discrepancy Loss: 0.0090
Epoch [35/50], Class Loss: 0.0020, Discrepancy Loss: 0.0060
Epoch [36/50], Class Loss: 0.0025, Discrepancy Loss: 0.0077
Epoch [37/50], Class Loss: 0.0038, Discrepancy Loss: 0.0071
Epoch [38/50], Class Loss: 0.0026, Discrepancy Loss: 0.0074
Epoch [39/50], Class Loss: 0.0022, Discrepancy Loss: 0.0091
Epoch [40/50], Class Loss: 0.0030, Discrepancy Loss: 0.0083
Epoch [41/50], Class Loss: 0.0035, Discrepancy Loss: 0.0107
Epoch [42/50], Class Loss: 0.0020, Discrepancy Loss: 0.0064
Epoch [43/50], Class Loss: 0.0019, Discrepancy Loss: 0.0080
Epoch [44/50], Class Loss: 0.0023, Discrepancy Loss: 0.0070
Epoch [45/50], Class Loss: 0.0039, Discrepancy Loss: 0.0074
Epoch [46/50], Class Loss: 0.0029, Discrepancy Loss: 0.0084
Epoch [47/50], Class Loss: 0.0024, Discrepancy Loss: 0.0083
Epoch [48/50], Class Loss: 0.0027, Discrepancy Loss: 0.0073
Epoch [49/50], Class Loss: 0.0022, Discrepancy Loss: 0.0087
Epoch [50/50], Class Loss: 0.0024, Discrepancy Loss: 0.0069
Source Domain Performance - Accuracy: 56.18%, Precision: 59.02%, Recall: 56.98%, F1 Score: 49.17%
Target Domain Performance - Accuracy: 51.56%, Precision: 59.58%, Recall: 51.83%, F1 Score: 49.93%

Run 5/10
Epoch [1/50], Class Loss: 1.5600, Discrepancy Loss: 0.1165
Epoch [2/50], Class Loss: 1.0134, Discrepancy Loss: 0.1044
Epoch [3/50], Class Loss: 0.6178, Discrepancy Loss: 0.0667
Epoch [4/50], Class Loss: 0.3824, Discrepancy Loss: 0.0627
Epoch [5/50], Class Loss: 0.1756, Discrepancy Loss: 0.0595
Epoch [6/50], Class Loss: 0.0611, Discrepancy Loss: 0.0387
Epoch [7/50], Class Loss: 0.2115, Discrepancy Loss: 0.0463
Epoch [8/50], Class Loss: 0.0319, Discrepancy Loss: 0.0218
Epoch [9/50], Class Loss: 0.0511, Discrepancy Loss: 0.0332
Epoch [10/50], Class Loss: 0.0135, Discrepancy Loss: 0.0117
Epoch [11/50], Class Loss: 0.0045, Discrepancy Loss: 0.0116
Epoch [12/50], Class Loss: 0.0032, Discrepancy Loss: 0.0119
Epoch [13/50], Class Loss: 0.0022, Discrepancy Loss: 0.0093
Epoch [14/50], Class Loss: 0.0059, Discrepancy Loss: 0.0109
Epoch [15/50], Class Loss: 0.0059, Discrepancy Loss: 0.0157
Epoch [16/50], Class Loss: 0.0059, Discrepancy Loss: 0.0184
Epoch [17/50], Class Loss: 0.0038, Discrepancy Loss: 0.0164
Epoch [18/50], Class Loss: 0.0077, Discrepancy Loss: 0.0111
Epoch [19/50], Class Loss: 0.0044, Discrepancy Loss: 0.0096
Epoch [20/50], Class Loss: 0.0012, Discrepancy Loss: 0.0080
Epoch [21/50], Class Loss: 0.0019, Discrepancy Loss: 0.0051
Epoch [22/50], Class Loss: 0.0039, Discrepancy Loss: 0.0065
Epoch [23/50], Class Loss: 0.0015, Discrepancy Loss: 0.0057
Epoch [24/50], Class Loss: 0.0034, Discrepancy Loss: 0.0054
Epoch [25/50], Class Loss: 0.0012, Discrepancy Loss: 0.0069
Epoch [26/50], Class Loss: 0.0008, Discrepancy Loss: 0.0062
Epoch [27/50], Class Loss: 0.0047, Discrepancy Loss: 0.0065
Epoch [28/50], Class Loss: 0.0027, Discrepancy Loss: 0.0061
Epoch [29/50], Class Loss: 0.0011, Discrepancy Loss: 0.0071
Epoch [30/50], Class Loss: 0.0005, Discrepancy Loss: 0.0058
Epoch [31/50], Class Loss: 0.0226, Discrepancy Loss: 0.0058
Epoch [32/50], Class Loss: 0.0069, Discrepancy Loss: 0.0058
Epoch [33/50], Class Loss: 0.0006, Discrepancy Loss: 0.0074
Epoch [34/50], Class Loss: 0.0119, Discrepancy Loss: 0.0061
Epoch [35/50], Class Loss: 0.0020, Discrepancy Loss: 0.0063
Epoch [36/50], Class Loss: 0.0014, Discrepancy Loss: 0.0047
Epoch [37/50], Class Loss: 0.0145, Discrepancy Loss: 0.0062
Epoch [38/50], Class Loss: 0.0012, Discrepancy Loss: 0.0066
Epoch [39/50], Class Loss: 0.0006, Discrepancy Loss: 0.0063
Epoch [40/50], Class Loss: 0.0065, Discrepancy Loss: 0.0064
Epoch [41/50], Class Loss: 0.0012, Discrepancy Loss: 0.0064
Epoch [42/50], Class Loss: 0.0008, Discrepancy Loss: 0.0061
Epoch [43/50], Class Loss: 0.0005, Discrepancy Loss: 0.0068
Epoch [44/50], Class Loss: 0.0011, Discrepancy Loss: 0.0064
Epoch [45/50], Class Loss: 0.0007, Discrepancy Loss: 0.0064
Epoch [46/50], Class Loss: 0.0011, Discrepancy Loss: 0.0049
Epoch [47/50], Class Loss: 0.0010, Discrepancy Loss: 0.0043
Epoch [48/50], Class Loss: 0.0005, Discrepancy Loss: 0.0045
Epoch [49/50], Class Loss: 0.0010, Discrepancy Loss: 0.0064
Epoch [50/50], Class Loss: 0.0012, Discrepancy Loss: 0.0056
Source Domain Performance - Accuracy: 48.92%, Precision: 58.15%, Recall: 50.17%, F1 Score: 37.63%
Target Domain Performance - Accuracy: 57.19%, Precision: 53.02%, Recall: 56.23%, F1 Score: 54.40%

Run 6/10
Epoch [1/50], Class Loss: 1.6611, Discrepancy Loss: 0.0982
Epoch [2/50], Class Loss: 0.6940, Discrepancy Loss: 0.0874
Epoch [3/50], Class Loss: 0.3024, Discrepancy Loss: 0.0528
Epoch [4/50], Class Loss: 0.4850, Discrepancy Loss: 0.0640
Epoch [5/50], Class Loss: 0.1144, Discrepancy Loss: 0.0318
Epoch [6/50], Class Loss: 0.1429, Discrepancy Loss: 0.0260
Epoch [7/50], Class Loss: 0.1166, Discrepancy Loss: 0.0407
Epoch [8/50], Class Loss: 0.0278, Discrepancy Loss: 0.0392
Epoch [9/50], Class Loss: 0.0189, Discrepancy Loss: 0.0185
Epoch [10/50], Class Loss: 0.0335, Discrepancy Loss: 0.0296
Epoch [11/50], Class Loss: 0.0048, Discrepancy Loss: 0.0165
Epoch [12/50], Class Loss: 0.0006, Discrepancy Loss: 0.0127
Epoch [13/50], Class Loss: 0.0004, Discrepancy Loss: 0.0081
Epoch [14/50], Class Loss: 0.0816, Discrepancy Loss: 0.0074
Epoch [15/50], Class Loss: 0.0025, Discrepancy Loss: 0.0072
Epoch [16/50], Class Loss: 0.0015, Discrepancy Loss: 0.0074
Epoch [17/50], Class Loss: 0.0011, Discrepancy Loss: 0.0086
Epoch [18/50], Class Loss: 0.1161, Discrepancy Loss: 0.0062
Epoch [19/50], Class Loss: 0.0007, Discrepancy Loss: 0.0102
Epoch [20/50], Class Loss: 0.0052, Discrepancy Loss: 0.0088
Epoch [21/50], Class Loss: 0.0058, Discrepancy Loss: 0.0056
Epoch [22/50], Class Loss: 0.0170, Discrepancy Loss: 0.0060
Epoch [23/50], Class Loss: 0.0093, Discrepancy Loss: 0.0057
Epoch [24/50], Class Loss: 0.0004, Discrepancy Loss: 0.0064
Epoch [25/50], Class Loss: 0.0020, Discrepancy Loss: 0.0072
Epoch [26/50], Class Loss: 0.0006, Discrepancy Loss: 0.0051
Epoch [27/50], Class Loss: 0.0006, Discrepancy Loss: 0.0052
Epoch [28/50], Class Loss: 0.0014, Discrepancy Loss: 0.0058
Epoch [29/50], Class Loss: 0.0560, Discrepancy Loss: 0.0068
Epoch [30/50], Class Loss: 0.0008, Discrepancy Loss: 0.0060
Epoch [31/50], Class Loss: 0.0037, Discrepancy Loss: 0.0070
Epoch [32/50], Class Loss: 0.0028, Discrepancy Loss: 0.0059
Epoch [33/50], Class Loss: 0.0143, Discrepancy Loss: 0.0060
Epoch [34/50], Class Loss: 0.0005, Discrepancy Loss: 0.0074
Epoch [35/50], Class Loss: 0.0006, Discrepancy Loss: 0.0078
Epoch [36/50], Class Loss: 0.0133, Discrepancy Loss: 0.0077
Epoch [37/50], Class Loss: 0.0026, Discrepancy Loss: 0.0076
Epoch [38/50], Class Loss: 0.0009, Discrepancy Loss: 0.0063
Epoch [39/50], Class Loss: 0.0011, Discrepancy Loss: 0.0069
Epoch [40/50], Class Loss: 0.0018, Discrepancy Loss: 0.0082
Epoch [41/50], Class Loss: 0.0040, Discrepancy Loss: 0.0070
Epoch [42/50], Class Loss: 0.0166, Discrepancy Loss: 0.0060
Epoch [43/50], Class Loss: 0.0010, Discrepancy Loss: 0.0063
Epoch [44/50], Class Loss: 0.0008, Discrepancy Loss: 0.0076
Epoch [45/50], Class Loss: 0.0029, Discrepancy Loss: 0.0057
Epoch [46/50], Class Loss: 0.0119, Discrepancy Loss: 0.0064
Epoch [47/50], Class Loss: 0.0017, Discrepancy Loss: 0.0062
Epoch [48/50], Class Loss: 0.0006, Discrepancy Loss: 0.0063
Epoch [49/50], Class Loss: 0.0027, Discrepancy Loss: 0.0084
Epoch [50/50], Class Loss: 0.0072, Discrepancy Loss: 0.0064
Source Domain Performance - Accuracy: 48.86%, Precision: 58.14%, Recall: 50.11%, F1 Score: 37.51%
Target Domain Performance - Accuracy: 58.15%, Precision: 49.96%, Recall: 57.09%, F1 Score: 53.08%

Run 7/10
Epoch [1/50], Class Loss: 1.5868, Discrepancy Loss: 0.1362
Epoch [2/50], Class Loss: 0.7649, Discrepancy Loss: 0.0944
Epoch [3/50], Class Loss: 0.5605, Discrepancy Loss: 0.0735
Epoch [4/50], Class Loss: 0.4339, Discrepancy Loss: 0.0814
Epoch [5/50], Class Loss: 0.1243, Discrepancy Loss: 0.0285
Epoch [6/50], Class Loss: 0.0637, Discrepancy Loss: 0.0251
Epoch [7/50], Class Loss: 0.0267, Discrepancy Loss: 0.0164
Epoch [8/50], Class Loss: 0.0221, Discrepancy Loss: 0.0140
Epoch [9/50], Class Loss: 0.0788, Discrepancy Loss: 0.0254
Epoch [10/50], Class Loss: 0.0163, Discrepancy Loss: 0.0144
Epoch [11/50], Class Loss: 0.0101, Discrepancy Loss: 0.0103
Epoch [12/50], Class Loss: 0.0055, Discrepancy Loss: 0.0090
Epoch [13/50], Class Loss: 0.0047, Discrepancy Loss: 0.0077
Epoch [14/50], Class Loss: 0.0037, Discrepancy Loss: 0.0070
Epoch [15/50], Class Loss: 0.0054, Discrepancy Loss: 0.0067
Epoch [16/50], Class Loss: 0.0049, Discrepancy Loss: 0.0085
Epoch [17/50], Class Loss: 0.0024, Discrepancy Loss: 0.0069
Epoch [18/50], Class Loss: 0.0095, Discrepancy Loss: 0.0069
Epoch [19/50], Class Loss: 0.0094, Discrepancy Loss: 0.0071
Epoch [20/50], Class Loss: 0.0138, Discrepancy Loss: 0.0063
Epoch [21/50], Class Loss: 0.0021, Discrepancy Loss: 0.0068
Epoch [22/50], Class Loss: 0.0020, Discrepancy Loss: 0.0058
Epoch [23/50], Class Loss: 0.0019, Discrepancy Loss: 0.0062
Epoch [24/50], Class Loss: 0.0025, Discrepancy Loss: 0.0075
Epoch [25/50], Class Loss: 0.0023, Discrepancy Loss: 0.0053
Epoch [26/50], Class Loss: 0.0017, Discrepancy Loss: 0.0073
Epoch [27/50], Class Loss: 0.0012, Discrepancy Loss: 0.0050
Epoch [28/50], Class Loss: 0.0543, Discrepancy Loss: 0.0060
Epoch [29/50], Class Loss: 0.0029, Discrepancy Loss: 0.0058
Epoch [30/50], Class Loss: 0.0086, Discrepancy Loss: 0.0049
Epoch [31/50], Class Loss: 0.0020, Discrepancy Loss: 0.0050
Epoch [32/50], Class Loss: 0.0021, Discrepancy Loss: 0.0048
Epoch [33/50], Class Loss: 0.0021, Discrepancy Loss: 0.0046
Epoch [34/50], Class Loss: 0.1330, Discrepancy Loss: 0.0073
Epoch [35/50], Class Loss: 0.0025, Discrepancy Loss: 0.0055
Epoch [36/50], Class Loss: 0.0037, Discrepancy Loss: 0.0053
Epoch [37/50], Class Loss: 0.0681, Discrepancy Loss: 0.0050
Epoch [38/50], Class Loss: 0.0019, Discrepancy Loss: 0.0059
Epoch [39/50], Class Loss: 0.0031, Discrepancy Loss: 0.0052
Epoch [40/50], Class Loss: 0.0805, Discrepancy Loss: 0.0069
Epoch [41/50], Class Loss: 0.0538, Discrepancy Loss: 0.0055
Epoch [42/50], Class Loss: 0.0289, Discrepancy Loss: 0.0050
Epoch [43/50], Class Loss: 0.0370, Discrepancy Loss: 0.0063
Epoch [44/50], Class Loss: 0.0047, Discrepancy Loss: 0.0054
Epoch [45/50], Class Loss: 0.0014, Discrepancy Loss: 0.0055
Epoch [46/50], Class Loss: 0.0023, Discrepancy Loss: 0.0054
Epoch [47/50], Class Loss: 0.0023, Discrepancy Loss: 0.0060
Epoch [48/50], Class Loss: 0.0022, Discrepancy Loss: 0.0060
Epoch [49/50], Class Loss: 0.0016, Discrepancy Loss: 0.0047
Epoch [50/50], Class Loss: 0.0026, Discrepancy Loss: 0.0048
Source Domain Performance - Accuracy: 48.74%, Precision: 33.13%, Recall: 50.00%, F1 Score: 37.27%
Target Domain Performance - Accuracy: 52.34%, Precision: 45.71%, Recall: 51.44%, F1 Score: 48.20%

Run 8/10
Epoch [1/50], Class Loss: 1.7441, Discrepancy Loss: 0.1219
Epoch [2/50], Class Loss: 0.9503, Discrepancy Loss: 0.0840
Epoch [3/50], Class Loss: 0.6774, Discrepancy Loss: 0.0768
Epoch [4/50], Class Loss: 0.3353, Discrepancy Loss: 0.0579
Epoch [5/50], Class Loss: 0.3422, Discrepancy Loss: 0.0707
Epoch [6/50], Class Loss: 0.2839, Discrepancy Loss: 0.0440
Epoch [7/50], Class Loss: 0.1169, Discrepancy Loss: 0.0271
Epoch [8/50], Class Loss: 0.1866, Discrepancy Loss: 0.0239
Epoch [9/50], Class Loss: 0.1198, Discrepancy Loss: 0.0330
Epoch [10/50], Class Loss: 0.0672, Discrepancy Loss: 0.0297
Epoch [11/50], Class Loss: 0.0225, Discrepancy Loss: 0.0176
Epoch [12/50], Class Loss: 0.0125, Discrepancy Loss: 0.0186
Epoch [13/50], Class Loss: 0.0089, Discrepancy Loss: 0.0143
Epoch [14/50], Class Loss: 0.0057, Discrepancy Loss: 0.0137
Epoch [15/50], Class Loss: 0.0176, Discrepancy Loss: 0.0128
Epoch [16/50], Class Loss: 0.0122, Discrepancy Loss: 0.0148
Epoch [17/50], Class Loss: 0.0048, Discrepancy Loss: 0.0132
Epoch [18/50], Class Loss: 0.0035, Discrepancy Loss: 0.0118
Epoch [19/50], Class Loss: 0.0064, Discrepancy Loss: 0.0103
Epoch [20/50], Class Loss: 0.0051, Discrepancy Loss: 0.0117
Epoch [21/50], Class Loss: 0.0040, Discrepancy Loss: 0.0132
Epoch [22/50], Class Loss: 0.0092, Discrepancy Loss: 0.0111
Epoch [23/50], Class Loss: 0.0032, Discrepancy Loss: 0.0096
Epoch [24/50], Class Loss: 0.0028, Discrepancy Loss: 0.0084
Epoch [25/50], Class Loss: 0.0029, Discrepancy Loss: 0.0104
Epoch [26/50], Class Loss: 0.0143, Discrepancy Loss: 0.0102
Epoch [27/50], Class Loss: 0.0045, Discrepancy Loss: 0.0108
Epoch [28/50], Class Loss: 0.1319, Discrepancy Loss: 0.0113
Epoch [29/50], Class Loss: 0.0030, Discrepancy Loss: 0.0094
Epoch [30/50], Class Loss: 0.0221, Discrepancy Loss: 0.0115
Epoch [31/50], Class Loss: 0.0183, Discrepancy Loss: 0.0090
Epoch [32/50], Class Loss: 0.1020, Discrepancy Loss: 0.0098
Epoch [33/50], Class Loss: 0.0040, Discrepancy Loss: 0.0089
Epoch [34/50], Class Loss: 0.0507, Discrepancy Loss: 0.0088
Epoch [35/50], Class Loss: 0.0032, Discrepancy Loss: 0.0105
Epoch [36/50], Class Loss: 0.0040, Discrepancy Loss: 0.0080
Epoch [37/50], Class Loss: 0.1039, Discrepancy Loss: 0.0085
Epoch [38/50], Class Loss: 0.0047, Discrepancy Loss: 0.0097
Epoch [39/50], Class Loss: 0.0022, Discrepancy Loss: 0.0113
Epoch [40/50], Class Loss: 0.0038, Discrepancy Loss: 0.0096
Epoch [41/50], Class Loss: 0.0014, Discrepancy Loss: 0.0102
Epoch [42/50], Class Loss: 0.0025, Discrepancy Loss: 0.0093
Epoch [43/50], Class Loss: 0.0044, Discrepancy Loss: 0.0095
Epoch [44/50], Class Loss: 0.0024, Discrepancy Loss: 0.0107
Epoch [45/50], Class Loss: 0.0028, Discrepancy Loss: 0.0096
Epoch [46/50], Class Loss: 0.0035, Discrepancy Loss: 0.0104
Epoch [47/50], Class Loss: 0.0031, Discrepancy Loss: 0.0093
Epoch [48/50], Class Loss: 0.0031, Discrepancy Loss: 0.0079
Epoch [49/50], Class Loss: 0.1339, Discrepancy Loss: 0.0087
Epoch [50/50], Class Loss: 0.0050, Discrepancy Loss: 0.0105
Source Domain Performance - Accuracy: 59.41%, Precision: 59.47%, Recall: 60.02%, F1 Score: 53.04%
Target Domain Performance - Accuracy: 56.24%, Precision: 55.86%, Recall: 56.30%, F1 Score: 52.66%

Run 9/10
Epoch [1/50], Class Loss: 1.7392, Discrepancy Loss: 0.1249
Epoch [2/50], Class Loss: 0.8169, Discrepancy Loss: 0.0833
Epoch [3/50], Class Loss: 0.5849, Discrepancy Loss: 0.0669
Epoch [4/50], Class Loss: 0.3004, Discrepancy Loss: 0.0580
Epoch [5/50], Class Loss: 0.1759, Discrepancy Loss: 0.0580
Epoch [6/50], Class Loss: 0.0791, Discrepancy Loss: 0.0419
Epoch [7/50], Class Loss: 0.0430, Discrepancy Loss: 0.0412
Epoch [8/50], Class Loss: 0.0721, Discrepancy Loss: 0.0192
Epoch [9/50], Class Loss: 0.6558, Discrepancy Loss: 0.0707
Epoch [10/50], Class Loss: 0.1863, Discrepancy Loss: 0.0451
Epoch [11/50], Class Loss: 0.1557, Discrepancy Loss: 0.0452
Epoch [12/50], Class Loss: 0.0286, Discrepancy Loss: 0.0500
Epoch [13/50], Class Loss: 0.0304, Discrepancy Loss: 0.0409
Epoch [14/50], Class Loss: 0.0458, Discrepancy Loss: 0.0364
Epoch [15/50], Class Loss: 0.0184, Discrepancy Loss: 0.0414
Epoch [16/50], Class Loss: 0.0450, Discrepancy Loss: 0.0313
Epoch [17/50], Class Loss: 0.0212, Discrepancy Loss: 0.0298
Epoch [18/50], Class Loss: 0.0079, Discrepancy Loss: 0.0293
Epoch [19/50], Class Loss: 0.0051, Discrepancy Loss: 0.0315
Epoch [20/50], Class Loss: 0.0149, Discrepancy Loss: 0.0238
Epoch [21/50], Class Loss: 0.0061, Discrepancy Loss: 0.0195
Epoch [22/50], Class Loss: 0.0070, Discrepancy Loss: 0.0221
Epoch [23/50], Class Loss: 0.0087, Discrepancy Loss: 0.0209
Epoch [24/50], Class Loss: 0.0186, Discrepancy Loss: 0.0171
Epoch [25/50], Class Loss: 0.0056, Discrepancy Loss: 0.0206
Epoch [26/50], Class Loss: 0.0079, Discrepancy Loss: 0.0205
Epoch [27/50], Class Loss: 0.0052, Discrepancy Loss: 0.0185
Epoch [28/50], Class Loss: 0.0075, Discrepancy Loss: 0.0207
Epoch [29/50], Class Loss: 0.0034, Discrepancy Loss: 0.0196
Epoch [30/50], Class Loss: 0.0044, Discrepancy Loss: 0.0191
Epoch [31/50], Class Loss: 0.0084, Discrepancy Loss: 0.0203
Epoch [32/50], Class Loss: 0.0084, Discrepancy Loss: 0.0186
Epoch [33/50], Class Loss: 0.0062, Discrepancy Loss: 0.0208
Epoch [34/50], Class Loss: 0.0258, Discrepancy Loss: 0.0194
Epoch [35/50], Class Loss: 0.0056, Discrepancy Loss: 0.0190
Epoch [36/50], Class Loss: 0.1131, Discrepancy Loss: 0.0176
Epoch [37/50], Class Loss: 0.0028, Discrepancy Loss: 0.0223
Epoch [38/50], Class Loss: 0.0056, Discrepancy Loss: 0.0179
Epoch [39/50], Class Loss: 0.0070, Discrepancy Loss: 0.0200
Epoch [40/50], Class Loss: 0.0029, Discrepancy Loss: 0.0191
Epoch [41/50], Class Loss: 0.0087, Discrepancy Loss: 0.0201
Epoch [42/50], Class Loss: 0.0200, Discrepancy Loss: 0.0231
Epoch [43/50], Class Loss: 0.0947, Discrepancy Loss: 0.0217
Epoch [44/50], Class Loss: 0.0045, Discrepancy Loss: 0.0188
Epoch [45/50], Class Loss: 0.0048, Discrepancy Loss: 0.0194
Epoch [46/50], Class Loss: 0.0035, Discrepancy Loss: 0.0188
Epoch [47/50], Class Loss: 0.0243, Discrepancy Loss: 0.0207
Epoch [48/50], Class Loss: 0.0039, Discrepancy Loss: 0.0194
Epoch [49/50], Class Loss: 0.0239, Discrepancy Loss: 0.0186
Epoch [50/50], Class Loss: 0.0065, Discrepancy Loss: 0.0195
Source Domain Performance - Accuracy: 55.64%, Precision: 72.88%, Recall: 56.50%, F1 Score: 45.40%
Target Domain Performance - Accuracy: 42.57%, Precision: 53.55%, Recall: 42.88%, F1 Score: 42.11%

Run 10/10
Epoch [1/50], Class Loss: 1.5362, Discrepancy Loss: 0.1243
Epoch [2/50], Class Loss: 0.9462, Discrepancy Loss: 0.1042
Epoch [3/50], Class Loss: 0.3966, Discrepancy Loss: 0.0955
Epoch [4/50], Class Loss: 0.2338, Discrepancy Loss: 0.0532
Epoch [5/50], Class Loss: 0.1487, Discrepancy Loss: 0.0356
Epoch [6/50], Class Loss: 0.0554, Discrepancy Loss: 0.0229
Epoch [7/50], Class Loss: 0.0676, Discrepancy Loss: 0.0214
Epoch [8/50], Class Loss: 0.0460, Discrepancy Loss: 0.0161
Epoch [9/50], Class Loss: 0.0474, Discrepancy Loss: 0.0224
Epoch [10/50], Class Loss: 0.0061, Discrepancy Loss: 0.0121
Epoch [11/50], Class Loss: 0.0020, Discrepancy Loss: 0.0059
Epoch [12/50], Class Loss: 0.0023, Discrepancy Loss: 0.0060
Epoch [13/50], Class Loss: 0.0014, Discrepancy Loss: 0.0061
Epoch [14/50], Class Loss: 0.0058, Discrepancy Loss: 0.0075
Epoch [15/50], Class Loss: 0.0048, Discrepancy Loss: 0.0079
Epoch [16/50], Class Loss: 0.0033, Discrepancy Loss: 0.0050
Epoch [17/50], Class Loss: 0.0021, Discrepancy Loss: 0.0053
Epoch [18/50], Class Loss: 0.0018, Discrepancy Loss: 0.0052
Epoch [19/50], Class Loss: 0.0009, Discrepancy Loss: 0.0065
Epoch [20/50], Class Loss: 0.0068, Discrepancy Loss: 0.0062
Epoch [21/50], Class Loss: 0.0005, Discrepancy Loss: 0.0041
Epoch [22/50], Class Loss: 0.0690, Discrepancy Loss: 0.0039
Epoch [23/50], Class Loss: 0.0005, Discrepancy Loss: 0.0039
Epoch [24/50], Class Loss: 0.0021, Discrepancy Loss: 0.0037
Epoch [25/50], Class Loss: 0.0007, Discrepancy Loss: 0.0044
Epoch [26/50], Class Loss: 0.0246, Discrepancy Loss: 0.0039
Epoch [27/50], Class Loss: 0.0247, Discrepancy Loss: 0.0059
Epoch [28/50], Class Loss: 0.0008, Discrepancy Loss: 0.0051
Epoch [29/50], Class Loss: 0.0017, Discrepancy Loss: 0.0045
Epoch [30/50], Class Loss: 0.0023, Discrepancy Loss: 0.0041
Epoch [31/50], Class Loss: 0.0539, Discrepancy Loss: 0.0058
Epoch [32/50], Class Loss: 0.0013, Discrepancy Loss: 0.0036
Epoch [33/50], Class Loss: 0.0006, Discrepancy Loss: 0.0052
Epoch [34/50], Class Loss: 0.0016, Discrepancy Loss: 0.0046
Epoch [35/50], Class Loss: 0.0105, Discrepancy Loss: 0.0041
Epoch [36/50], Class Loss: 0.0006, Discrepancy Loss: 0.0049
Epoch [37/50], Class Loss: 0.0003, Discrepancy Loss: 0.0048
Epoch [38/50], Class Loss: 0.0013, Discrepancy Loss: 0.0046
Epoch [39/50], Class Loss: 0.0036, Discrepancy Loss: 0.0052
Epoch [40/50], Class Loss: 0.0003, Discrepancy Loss: 0.0037
Epoch [41/50], Class Loss: 0.0049, Discrepancy Loss: 0.0050
Epoch [42/50], Class Loss: 0.0067, Discrepancy Loss: 0.0053
Epoch [43/50], Class Loss: 0.0005, Discrepancy Loss: 0.0049
Epoch [44/50], Class Loss: 0.0006, Discrepancy Loss: 0.0060
Epoch [45/50], Class Loss: 0.0005, Discrepancy Loss: 0.0045
Epoch [46/50], Class Loss: 0.0003, Discrepancy Loss: 0.0054
Epoch [47/50], Class Loss: 0.0044, Discrepancy Loss: 0.0049
Epoch [48/50], Class Loss: 0.0209, Discrepancy Loss: 0.0048
Epoch [49/50], Class Loss: 0.0008, Discrepancy Loss: 0.0042
Epoch [50/50], Class Loss: 0.0143, Discrepancy Loss: 0.0053
Source Domain Performance - Accuracy: 49.52%, Precision: 58.21%, Recall: 50.73%, F1 Score: 38.79%
Target Domain Performance - Accuracy: 53.72%, Precision: 51.68%, Recall: 53.12%, F1 Score: 51.75%

Source performance: 51.40% 57.37% 52.50% 41.19%
Target performance: 52.59% 51.90% 52.13% 50.13%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 1.00%
16qam: 65.61%
8apsk: 41.92%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.6816, Discrepancy Loss: 0.0426
Validation Loss: 19.2656
Epoch [2/50], Class Loss: 0.3552, Discrepancy Loss: 0.0253
Validation Loss: 0.1018
Epoch [3/50], Class Loss: 0.1927, Discrepancy Loss: 0.1055
Validation Loss: 0.5087
Epoch [4/50], Class Loss: 0.1112, Discrepancy Loss: 0.0226
Validation Loss: 2.3441
Epoch [5/50], Class Loss: 0.0775, Discrepancy Loss: 0.0771
Validation Loss: 13.1022
Epoch [6/50], Class Loss: 0.0937, Discrepancy Loss: 0.0091
Validation Loss: 0.0226
Epoch [7/50], Class Loss: 0.1416, Discrepancy Loss: 0.0571
Validation Loss: 39.0617
Epoch [8/50], Class Loss: 0.1659, Discrepancy Loss: 0.0305
Validation Loss: 0.7398
Epoch [9/50], Class Loss: 0.0181, Discrepancy Loss: 0.0484
Validation Loss: 4.9110
Epoch [10/50], Class Loss: 0.0326, Discrepancy Loss: 0.0590
Validation Loss: 1.8179
Epoch [11/50], Class Loss: 0.0106, Discrepancy Loss: 0.0649
Validation Loss: 0.0234
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 36.45%, Precision: 19.35%, Recall: 37.19%, F1 Score: 24.14%

Run 2/10
Epoch [1/50], Class Loss: 0.8056, Discrepancy Loss: 0.0561
Validation Loss: 3.5964
Epoch [2/50], Class Loss: 0.3097, Discrepancy Loss: 0.1146
Validation Loss: 0.2925
Epoch [3/50], Class Loss: 0.2528, Discrepancy Loss: 0.0931
Validation Loss: 2.9279
Epoch [4/50], Class Loss: 0.1530, Discrepancy Loss: 0.0231
Validation Loss: 0.1850
Epoch [5/50], Class Loss: 0.0544, Discrepancy Loss: 0.0474
Validation Loss: 1.7276
Epoch [6/50], Class Loss: 0.0267, Discrepancy Loss: 0.0439
Validation Loss: 3.6873
Epoch [7/50], Class Loss: 0.0519, Discrepancy Loss: 0.0457
Validation Loss: 4.0979
Epoch [8/50], Class Loss: 0.0360, Discrepancy Loss: 0.1525
Validation Loss: 0.0137
Epoch [9/50], Class Loss: 0.0221, Discrepancy Loss: 0.1617
Validation Loss: 2.6883
Epoch [10/50], Class Loss: 0.0173, Discrepancy Loss: 0.1530
Validation Loss: 10.8590
Epoch [11/50], Class Loss: 0.0127, Discrepancy Loss: 0.0017
Validation Loss: 0.0634
Epoch [12/50], Class Loss: 0.0003, Discrepancy Loss: 0.0006
Validation Loss: 0.0100
Epoch [13/50], Class Loss: 0.0001, Discrepancy Loss: 0.0006
Validation Loss: 0.0165
Epoch [14/50], Class Loss: 0.0045, Discrepancy Loss: 0.0007
Validation Loss: 0.0169
Epoch [15/50], Class Loss: 0.0005, Discrepancy Loss: 0.0005
Validation Loss: 0.0243
Epoch [16/50], Class Loss: 0.0008, Discrepancy Loss: 0.0008
Validation Loss: 0.0040
Epoch [17/50], Class Loss: 0.0001, Discrepancy Loss: 0.0003
Validation Loss: 0.0066
Epoch [18/50], Class Loss: 0.0001, Discrepancy Loss: 0.0003
Validation Loss: 0.0105
Epoch [19/50], Class Loss: 0.0000, Discrepancy Loss: 0.0009
Validation Loss: 0.0108
Epoch [20/50], Class Loss: 0.0000, Discrepancy Loss: 0.0005
Validation Loss: 0.0157
Epoch [21/50], Class Loss: 0.0068, Discrepancy Loss: 0.0002
Validation Loss: 0.0201
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 48.80%, Precision: 29.48%, Recall: 50.00%, F1 Score: 35.53%

Run 3/10
Epoch [1/50], Class Loss: 0.7576, Discrepancy Loss: 0.0747
Validation Loss: 3.7113
Epoch [2/50], Class Loss: 0.3382, Discrepancy Loss: 0.1085
Validation Loss: 2.0569
Epoch [3/50], Class Loss: 0.2502, Discrepancy Loss: 0.0628
Validation Loss: 0.8861
Epoch [4/50], Class Loss: 0.1013, Discrepancy Loss: 0.0835
Validation Loss: 6.3807
Epoch [5/50], Class Loss: 0.1051, Discrepancy Loss: 0.0289
Validation Loss: 7.9970
Epoch [6/50], Class Loss: 0.0363, Discrepancy Loss: 0.0521
Validation Loss: 10.6128
Epoch [7/50], Class Loss: 0.0949, Discrepancy Loss: 0.1999
Validation Loss: 0.9802
Epoch [8/50], Class Loss: 0.0296, Discrepancy Loss: 0.1404
Validation Loss: 0.2988
Epoch [9/50], Class Loss: 0.0139, Discrepancy Loss: 0.1742
Validation Loss: 81.0897
Epoch [10/50], Class Loss: 0.0688, Discrepancy Loss: 0.0275
Validation Loss: 0.0052
Epoch [11/50], Class Loss: 0.0020, Discrepancy Loss: 0.0211
Validation Loss: 0.0024
Epoch [12/50], Class Loss: 0.0005, Discrepancy Loss: 0.0233
Validation Loss: 0.0063
Epoch [13/50], Class Loss: 0.0044, Discrepancy Loss: 0.0749
Validation Loss: 0.0104
Epoch [14/50], Class Loss: 0.0100, Discrepancy Loss: 0.1319
Validation Loss: 0.0147
Epoch [15/50], Class Loss: 0.0205, Discrepancy Loss: 0.2178
Validation Loss: 0.3715
Epoch [16/50], Class Loss: 0.0358, Discrepancy Loss: 0.1327
Validation Loss: 0.0039
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 57.13%, Precision: 51.55%, Recall: 56.91%, F1 Score: 50.39%

Run 4/10
Epoch [1/50], Class Loss: 0.7010, Discrepancy Loss: 0.0744
Validation Loss: 1.1676
Epoch [2/50], Class Loss: 0.3556, Discrepancy Loss: 0.0635
Validation Loss: 14.7081
Epoch [3/50], Class Loss: 0.2318, Discrepancy Loss: 0.0995
Validation Loss: 1.4994
Epoch [4/50], Class Loss: 0.0677, Discrepancy Loss: 0.0460
Validation Loss: 5.1149
Epoch [5/50], Class Loss: 0.0254, Discrepancy Loss: 0.0006
Validation Loss: 18.5252
Epoch [6/50], Class Loss: 0.0223, Discrepancy Loss: 0.0005
Validation Loss: 3.7593
Early stopping!
Source Domain Performance - Accuracy: 78.96%, Precision: 88.92%, Recall: 78.65%, F1 Score: 74.27%
Target Domain Performance - Accuracy: 38.25%, Precision: 34.18%, Recall: 38.31%, F1 Score: 35.67%

Run 5/10
Epoch [1/50], Class Loss: 0.8084, Discrepancy Loss: 0.0508
Validation Loss: 1.1598
Epoch [2/50], Class Loss: 0.3981, Discrepancy Loss: 0.0214
Validation Loss: 0.7391
Epoch [3/50], Class Loss: 0.3058, Discrepancy Loss: 0.0148
Validation Loss: 2.5087
Epoch [4/50], Class Loss: 0.1410, Discrepancy Loss: 0.1304
Validation Loss: 15.3408
Epoch [5/50], Class Loss: 0.1033, Discrepancy Loss: 0.1885
Validation Loss: 16.6160
Epoch [6/50], Class Loss: 0.1349, Discrepancy Loss: 0.0127
Validation Loss: 25.0241
Epoch [7/50], Class Loss: 0.0548, Discrepancy Loss: 0.0066
Validation Loss: 0.5010
Epoch [8/50], Class Loss: 0.0344, Discrepancy Loss: 0.0438
Validation Loss: 11.7040
Epoch [9/50], Class Loss: 0.0260, Discrepancy Loss: 0.1778
Validation Loss: 1.8887
Epoch [10/50], Class Loss: 0.0940, Discrepancy Loss: 0.2186
Validation Loss: 14.0642
Epoch [11/50], Class Loss: 0.2309, Discrepancy Loss: 0.1878
Validation Loss: 0.0004
Epoch [12/50], Class Loss: 0.0053, Discrepancy Loss: 0.0649
Validation Loss: 0.0177
Epoch [13/50], Class Loss: 0.0157, Discrepancy Loss: 0.1665
Validation Loss: 0.0862
Epoch [14/50], Class Loss: 0.0282, Discrepancy Loss: 0.1927
Validation Loss: 0.0399
Epoch [15/50], Class Loss: 0.0197, Discrepancy Loss: 0.1927
Validation Loss: 0.0062
Epoch [16/50], Class Loss: 0.0036, Discrepancy Loss: 0.1138
Validation Loss: 0.0133
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.89%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 48.32%, Precision: 29.54%, Recall: 49.50%, F1 Score: 34.47%

Run 6/10
Epoch [1/50], Class Loss: 0.6909, Discrepancy Loss: 0.0649
Validation Loss: 5.4515
Epoch [2/50], Class Loss: 0.2241, Discrepancy Loss: 0.0723
Validation Loss: 0.4660
Epoch [3/50], Class Loss: 0.1134, Discrepancy Loss: 0.0774
Validation Loss: 6.6781
Epoch [4/50], Class Loss: 0.0928, Discrepancy Loss: 0.0296
Validation Loss: 0.2534
Epoch [5/50], Class Loss: 0.0895, Discrepancy Loss: 0.0436
Validation Loss: 21.5524
Epoch [6/50], Class Loss: 0.0961, Discrepancy Loss: 0.0817
Validation Loss: 0.1949
Epoch [7/50], Class Loss: 0.0223, Discrepancy Loss: 0.0163
Validation Loss: 12.0777
Epoch [8/50], Class Loss: 0.0400, Discrepancy Loss: 0.0178
Validation Loss: 5.5270
Epoch [9/50], Class Loss: 0.1433, Discrepancy Loss: 0.0696
Validation Loss: 11.9902
Epoch [10/50], Class Loss: 0.0412, Discrepancy Loss: 0.1026
Validation Loss: 8.4556
Epoch [11/50], Class Loss: 0.0100, Discrepancy Loss: 0.0141
Validation Loss: 0.0042
Epoch [12/50], Class Loss: 0.0016, Discrepancy Loss: 0.0099
Validation Loss: 0.0025
Epoch [13/50], Class Loss: 0.0019, Discrepancy Loss: 0.0092
Validation Loss: 0.0017
Epoch [14/50], Class Loss: 0.0011, Discrepancy Loss: 0.0098
Validation Loss: 0.0020
Epoch [15/50], Class Loss: 0.0010, Discrepancy Loss: 0.0134
Validation Loss: 0.0019
Epoch [16/50], Class Loss: 0.0032, Discrepancy Loss: 0.0186
Validation Loss: 0.0057
Epoch [17/50], Class Loss: 0.0027, Discrepancy Loss: 0.0247
Validation Loss: 0.0045
Epoch [18/50], Class Loss: 0.0032, Discrepancy Loss: 0.0304
Validation Loss: 0.0061
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 65.59%, Precision: 56.14%, Recall: 64.92%, F1 Score: 58.42%

Run 7/10
Epoch [1/50], Class Loss: 0.7468, Discrepancy Loss: 0.0406
Validation Loss: 19.0177
Epoch [2/50], Class Loss: 0.3953, Discrepancy Loss: 0.0389
Validation Loss: 1.2130
Epoch [3/50], Class Loss: 0.2170, Discrepancy Loss: 0.0475
Validation Loss: 0.1036
Epoch [4/50], Class Loss: 0.1782, Discrepancy Loss: 0.0444
Validation Loss: 2.2786
Epoch [5/50], Class Loss: 0.0692, Discrepancy Loss: 0.1381
Validation Loss: 27.2497
Epoch [6/50], Class Loss: 0.0837, Discrepancy Loss: 0.1576
Validation Loss: 0.6222
Epoch [7/50], Class Loss: 0.0272, Discrepancy Loss: 0.0010
Validation Loss: 2.7084
Epoch [8/50], Class Loss: 0.5041, Discrepancy Loss: 0.0399
Validation Loss: 0.2858
Early stopping!
Source Domain Performance - Accuracy: 96.76%, Precision: 97.24%, Recall: 96.72%, F1 Score: 96.81%
Target Domain Performance - Accuracy: 48.32%, Precision: 25.21%, Recall: 49.50%, F1 Score: 33.09%

Run 8/10
Epoch [1/50], Class Loss: 0.7530, Discrepancy Loss: 0.0405
Validation Loss: 1.7415
Epoch [2/50], Class Loss: 0.2744, Discrepancy Loss: 0.1492
Validation Loss: 6.6483
Epoch [3/50], Class Loss: 0.2298, Discrepancy Loss: 0.1199
Validation Loss: 0.1506
Epoch [4/50], Class Loss: 0.2138, Discrepancy Loss: 0.1256
Validation Loss: 5.9730
Epoch [5/50], Class Loss: 0.0508, Discrepancy Loss: 0.0760
Validation Loss: 0.2232
Epoch [6/50], Class Loss: 0.0696, Discrepancy Loss: 0.0860
Validation Loss: 6.7358
Epoch [7/50], Class Loss: 0.1373, Discrepancy Loss: 0.0909
Validation Loss: 3.1199
Epoch [8/50], Class Loss: 0.1273, Discrepancy Loss: 0.0676
Validation Loss: 0.2261
Early stopping!
Source Domain Performance - Accuracy: 98.38%, Precision: 98.44%, Recall: 98.37%, F1 Score: 98.37%
Target Domain Performance - Accuracy: 48.80%, Precision: 31.83%, Recall: 50.00%, F1 Score: 36.61%

Run 9/10
Epoch [1/50], Class Loss: 0.7793, Discrepancy Loss: 0.0739
Validation Loss: 3.4779
Epoch [2/50], Class Loss: 0.3469, Discrepancy Loss: 0.0570
Validation Loss: 79.9122
Epoch [3/50], Class Loss: 0.1799, Discrepancy Loss: 0.0892
Validation Loss: 0.4867
Epoch [4/50], Class Loss: 0.1607, Discrepancy Loss: 0.0581
Validation Loss: 3.0768
Epoch [5/50], Class Loss: 0.0639, Discrepancy Loss: 0.0123
Validation Loss: 0.4253
Epoch [6/50], Class Loss: 0.0572, Discrepancy Loss: 0.0146
Validation Loss: 0.0514
Epoch [7/50], Class Loss: 0.1376, Discrepancy Loss: 0.0182
Validation Loss: 17.9647
Epoch [8/50], Class Loss: 0.0874, Discrepancy Loss: 0.0798
Validation Loss: 0.5720
Epoch [9/50], Class Loss: 0.1863, Discrepancy Loss: 0.0795
Validation Loss: 6.2213
Epoch [10/50], Class Loss: 0.1734, Discrepancy Loss: 0.0124
Validation Loss: 0.1222
Epoch [11/50], Class Loss: 0.0036, Discrepancy Loss: 0.0354
Validation Loss: 0.0001
Epoch [12/50], Class Loss: 0.0019, Discrepancy Loss: 0.0542
Validation Loss: 0.0118
Epoch [13/50], Class Loss: 0.0064, Discrepancy Loss: 0.1521
Validation Loss: 0.0043
Epoch [14/50], Class Loss: 0.0190, Discrepancy Loss: 0.1766
Validation Loss: 0.0371
Epoch [15/50], Class Loss: 0.0207, Discrepancy Loss: 0.1801
Validation Loss: 0.3005
Epoch [16/50], Class Loss: 0.0143, Discrepancy Loss: 0.1152
Validation Loss: 0.0120
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 45.44%, Precision: 32.22%, Recall: 45.08%, F1 Score: 37.33%

Run 10/10
Epoch [1/50], Class Loss: 0.8401, Discrepancy Loss: 0.0717
Validation Loss: 20.8227
Epoch [2/50], Class Loss: 0.3867, Discrepancy Loss: 0.0832
Validation Loss: 12.0710
Epoch [3/50], Class Loss: 0.2465, Discrepancy Loss: 0.0354
Validation Loss: 2.2167
Epoch [4/50], Class Loss: 0.1120, Discrepancy Loss: 0.1271
Validation Loss: 0.1045
Epoch [5/50], Class Loss: 0.1512, Discrepancy Loss: 0.1701
Validation Loss: 10.3269
Epoch [6/50], Class Loss: 0.0855, Discrepancy Loss: 0.0413
Validation Loss: 9.2914
Epoch [7/50], Class Loss: 0.0396, Discrepancy Loss: 0.0659
Validation Loss: 9.4792
Epoch [8/50], Class Loss: 0.0361, Discrepancy Loss: 0.0714
Validation Loss: 0.0322
Epoch [9/50], Class Loss: 0.0661, Discrepancy Loss: 0.1334
Validation Loss: 0.1788
Epoch [10/50], Class Loss: 0.2886, Discrepancy Loss: 0.1026
Validation Loss: 1.0448
Epoch [11/50], Class Loss: 0.0010, Discrepancy Loss: 0.0312
Validation Loss: 0.0001
Epoch [12/50], Class Loss: 0.0040, Discrepancy Loss: 0.0208
Validation Loss: 0.0016
Epoch [13/50], Class Loss: 0.0052, Discrepancy Loss: 0.0610
Validation Loss: 0.0082
Epoch [14/50], Class Loss: 0.0107, Discrepancy Loss: 0.1185
Validation Loss: 0.0334
Epoch [15/50], Class Loss: 0.0154, Discrepancy Loss: 0.1519
Validation Loss: 0.0335
Epoch [16/50], Class Loss: 0.0317, Discrepancy Loss: 0.2102
Validation Loss: 0.0538
Early stopping!
Source Domain Performance - Accuracy: 99.40%, Precision: 99.41%, Recall: 99.43%, F1 Score: 99.41%
Target Domain Performance - Accuracy: 48.80%, Precision: 24.48%, Recall: 50.00%, F1 Score: 32.84%

Source performance: 97.30% 98.35% 97.27% 96.84%
Target performance: 48.59% 33.40% 49.14% 37.85%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 0.00%
16qam: 23.45%
8apsk: 73.11%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.4193, JMMD Loss: 0.2023
Validation Loss: 0.2708
Epoch [2/50], Class Loss: 0.2341, JMMD Loss: 0.1402
Validation Loss: 0.3370
Epoch [3/50], Class Loss: 0.2034, JMMD Loss: 0.1176
Validation Loss: 0.6496
Epoch [4/50], Class Loss: 0.1321, JMMD Loss: 0.0969
Validation Loss: 0.7078
Epoch [5/50], Class Loss: 0.0610, JMMD Loss: 0.0974
Validation Loss: 1.7641
Epoch [6/50], Class Loss: 0.0488, JMMD Loss: 0.1071
Validation Loss: 3.4851
Early stopping!
Source Domain Performance - Accuracy: 48.80%, Precision: 35.88%, Recall: 50.06%, F1 Score: 40.15%
Target Domain Performance - Accuracy: 42.03%, Precision: 39.48%, Recall: 42.06%, F1 Score: 40.67%

Run 2/10
Epoch [1/50], Class Loss: 0.3786, JMMD Loss: 0.1958
Validation Loss: 0.7944
Epoch [2/50], Class Loss: 0.1764, JMMD Loss: 0.1549
Validation Loss: 2.5379
Epoch [3/50], Class Loss: 0.1092, JMMD Loss: 0.1238
Validation Loss: 3.4919
Epoch [4/50], Class Loss: 0.0719, JMMD Loss: 0.1136
Validation Loss: 3.9963
Epoch [5/50], Class Loss: 0.0721, JMMD Loss: 0.1276
Validation Loss: 2.5909
Epoch [6/50], Class Loss: 0.0687, JMMD Loss: 0.0882
Validation Loss: 1.2199
Early stopping!
Source Domain Performance - Accuracy: 81.41%, Precision: 81.55%, Recall: 81.25%, F1 Score: 80.17%
Target Domain Performance - Accuracy: 59.17%, Precision: 49.09%, Recall: 58.20%, F1 Score: 51.08%

Run 3/10
Epoch [1/50], Class Loss: 0.4411, JMMD Loss: 0.1635
Validation Loss: 0.4382
Epoch [2/50], Class Loss: 0.2125, JMMD Loss: 0.1644
Validation Loss: 0.3474
Epoch [3/50], Class Loss: 0.1019, JMMD Loss: 0.1037
Validation Loss: 1.4370
Epoch [4/50], Class Loss: 0.0837, JMMD Loss: 0.0841
Validation Loss: 2.4834
Epoch [5/50], Class Loss: 0.1125, JMMD Loss: 0.1121
Validation Loss: 1.0105
Epoch [6/50], Class Loss: 0.0770, JMMD Loss: 0.0795
Validation Loss: 0.0589
Epoch [7/50], Class Loss: 0.1740, JMMD Loss: 0.1156
Validation Loss: 0.5511
Epoch [8/50], Class Loss: 0.1598, JMMD Loss: 0.2068
Validation Loss: 2.3437
Epoch [9/50], Class Loss: 0.0478, JMMD Loss: 0.1038
Validation Loss: 2.4389
Epoch [10/50], Class Loss: 0.1127, JMMD Loss: 0.1275
Validation Loss: 1.0767
Epoch [11/50], Class Loss: 0.0490, JMMD Loss: 0.1072
Validation Loss: 2.5970
Early stopping!
Source Domain Performance - Accuracy: 51.32%, Precision: 64.74%, Recall: 52.57%, F1 Score: 44.96%
Target Domain Performance - Accuracy: 73.44%, Precision: 69.77%, Recall: 72.99%, F1 Score: 65.37%

Run 4/10
Epoch [1/50], Class Loss: 0.4105, JMMD Loss: 0.2036
Validation Loss: 0.1710
Epoch [2/50], Class Loss: 0.2209, JMMD Loss: 0.1497
Validation Loss: 0.8121
Epoch [3/50], Class Loss: 0.1717, JMMD Loss: 0.1548
Validation Loss: 0.2390
Epoch [4/50], Class Loss: 0.1052, JMMD Loss: 0.1535
Validation Loss: 1.6653
Epoch [5/50], Class Loss: 0.0739, JMMD Loss: 0.1277
Validation Loss: 1.6027
Epoch [6/50], Class Loss: 0.0388, JMMD Loss: 0.0770
Validation Loss: 6.0588
Early stopping!
Source Domain Performance - Accuracy: 49.94%, Precision: 59.45%, Recall: 51.14%, F1 Score: 40.31%
Target Domain Performance - Accuracy: 66.13%, Precision: 61.64%, Recall: 65.25%, F1 Score: 60.03%

Run 5/10
Epoch [1/50], Class Loss: 0.3787, JMMD Loss: 0.2111
Validation Loss: 0.4805
Epoch [2/50], Class Loss: 0.1860, JMMD Loss: 0.1894
Validation Loss: 2.1857
Epoch [3/50], Class Loss: 0.1089, JMMD Loss: 0.1615
Validation Loss: 1.4826
Epoch [4/50], Class Loss: 0.0858, JMMD Loss: 0.1634
Validation Loss: 2.2908
Epoch [5/50], Class Loss: 0.0223, JMMD Loss: 0.0655
Validation Loss: 6.6242
Epoch [6/50], Class Loss: 0.2443, JMMD Loss: 0.1982
Validation Loss: 0.7831
Early stopping!
Source Domain Performance - Accuracy: 80.58%, Precision: 86.14%, Recall: 80.29%, F1 Score: 77.87%
Target Domain Performance - Accuracy: 49.82%, Precision: 52.47%, Recall: 49.26%, F1 Score: 45.69%

Run 6/10
Epoch [1/50], Class Loss: 0.4295, JMMD Loss: 0.1884
Validation Loss: 0.2809
Epoch [2/50], Class Loss: 0.1819, JMMD Loss: 0.1217
Validation Loss: 2.9757
Epoch [3/50], Class Loss: 0.1174, JMMD Loss: 0.0925
Validation Loss: 0.1687
Epoch [4/50], Class Loss: 0.0827, JMMD Loss: 0.1118
Validation Loss: 7.1107
Epoch [5/50], Class Loss: 0.0981, JMMD Loss: 0.1046
Validation Loss: 3.1687
Epoch [6/50], Class Loss: 0.0858, JMMD Loss: 0.0754
Validation Loss: 1.6941
Epoch [7/50], Class Loss: 0.2066, JMMD Loss: 0.2079
Validation Loss: 28.0748
Epoch [8/50], Class Loss: 0.1887, JMMD Loss: 0.1828
Validation Loss: 0.2869
Early stopping!
Source Domain Performance - Accuracy: 89.21%, Precision: 90.17%, Recall: 89.39%, F1 Score: 89.54%
Target Domain Performance - Accuracy: 59.95%, Precision: 48.11%, Recall: 59.81%, F1 Score: 52.61%

Run 7/10
Epoch [1/50], Class Loss: 0.4099, JMMD Loss: 0.2063
Validation Loss: 1.2863
Epoch [2/50], Class Loss: 0.1803, JMMD Loss: 0.1408
Validation Loss: 1.2474
Epoch [3/50], Class Loss: 0.1368, JMMD Loss: 0.1466
Validation Loss: 3.2728
Epoch [4/50], Class Loss: 0.0815, JMMD Loss: 0.0864
Validation Loss: 0.3242
Epoch [5/50], Class Loss: 0.0891, JMMD Loss: 0.1253
Validation Loss: 3.2240
Epoch [6/50], Class Loss: 0.0405, JMMD Loss: 0.1192
Validation Loss: 3.7174
Epoch [7/50], Class Loss: 0.0285, JMMD Loss: 0.0845
Validation Loss: 0.0515
Epoch [8/50], Class Loss: 0.0337, JMMD Loss: 0.1041
Validation Loss: 3.5339
Epoch [9/50], Class Loss: 0.0262, JMMD Loss: 0.0807
Validation Loss: 8.5305
Epoch [10/50], Class Loss: 0.0103, JMMD Loss: 0.0715
Validation Loss: 6.2637
Epoch [11/50], Class Loss: 0.0130, JMMD Loss: 0.0780
Validation Loss: 4.0221
Epoch [12/50], Class Loss: 0.0035, JMMD Loss: 0.0625
Validation Loss: 4.2983
Early stopping!
Source Domain Performance - Accuracy: 48.98%, Precision: 60.57%, Recall: 50.23%, F1 Score: 40.31%
Target Domain Performance - Accuracy: 70.68%, Precision: 56.86%, Recall: 70.32%, F1 Score: 62.33%

Run 8/10
Epoch [1/50], Class Loss: 0.3547, JMMD Loss: 0.1538
Validation Loss: 0.2654
Epoch [2/50], Class Loss: 0.1811, JMMD Loss: 0.1357
Validation Loss: 4.8952
Epoch [3/50], Class Loss: 0.1212, JMMD Loss: 0.1343
Validation Loss: 2.1490
Epoch [4/50], Class Loss: 0.0915, JMMD Loss: 0.1025
Validation Loss: 2.7605
Epoch [5/50], Class Loss: 0.0558, JMMD Loss: 0.0951
Validation Loss: 4.8960
Epoch [6/50], Class Loss: 0.0390, JMMD Loss: 0.0924
Validation Loss: 0.0918
Epoch [7/50], Class Loss: 0.0437, JMMD Loss: 0.0699
Validation Loss: 6.2854
Epoch [8/50], Class Loss: 0.1518, JMMD Loss: 0.1626
Validation Loss: 10.8160
Epoch [9/50], Class Loss: 0.0176, JMMD Loss: 0.1016
Validation Loss: 2.2152
Epoch [10/50], Class Loss: 0.0161, JMMD Loss: 0.0793
Validation Loss: 1.6006
Epoch [11/50], Class Loss: 0.0112, JMMD Loss: 0.0815
Validation Loss: 3.2811
Early stopping!
Source Domain Performance - Accuracy: 50.90%, Precision: 62.22%, Recall: 52.05%, F1 Score: 44.79%
Target Domain Performance - Accuracy: 63.73%, Precision: 52.20%, Recall: 63.47%, F1 Score: 57.01%

Run 9/10
Epoch [1/50], Class Loss: 0.4131, JMMD Loss: 0.1700
Validation Loss: 1.9667
Epoch [2/50], Class Loss: 0.1675, JMMD Loss: 0.1365
Validation Loss: 4.4010
Epoch [3/50], Class Loss: 0.1079, JMMD Loss: 0.0872
Validation Loss: 0.5729
Epoch [4/50], Class Loss: 0.1352, JMMD Loss: 0.0869
Validation Loss: 4.8717
Epoch [5/50], Class Loss: 0.1029, JMMD Loss: 0.1153
Validation Loss: 1.9751
Epoch [6/50], Class Loss: 0.0450, JMMD Loss: 0.1102
Validation Loss: 4.2362
Epoch [7/50], Class Loss: 0.0821, JMMD Loss: 0.0558
Validation Loss: 57.9892
Epoch [8/50], Class Loss: 0.2757, JMMD Loss: 0.1840
Validation Loss: 0.0760
Epoch [9/50], Class Loss: 0.0569, JMMD Loss: 0.1921
Validation Loss: 0.3167
Epoch [10/50], Class Loss: 0.0229, JMMD Loss: 0.2099
Validation Loss: 2.0570
Epoch [11/50], Class Loss: 0.0118, JMMD Loss: 0.1923
Validation Loss: 1.2416
Epoch [12/50], Class Loss: 0.0090, JMMD Loss: 0.1769
Validation Loss: 1.2827
Epoch [13/50], Class Loss: 0.0062, JMMD Loss: 0.1721
Validation Loss: 1.2691
Early stopping!
Source Domain Performance - Accuracy: 67.87%, Precision: 67.26%, Recall: 68.08%, F1 Score: 63.76%
Target Domain Performance - Accuracy: 56.18%, Precision: 71.34%, Recall: 57.18%, F1 Score: 49.99%

Run 10/10
Epoch [1/50], Class Loss: 0.3658, JMMD Loss: 0.1832
Validation Loss: 1.8969
Epoch [2/50], Class Loss: 0.1610, JMMD Loss: 0.1604
Validation Loss: 3.7222
Epoch [3/50], Class Loss: 0.1254, JMMD Loss: 0.1366
Validation Loss: 2.6477
Epoch [4/50], Class Loss: 0.0881, JMMD Loss: 0.0969
Validation Loss: 2.1803
Epoch [5/50], Class Loss: 0.0481, JMMD Loss: 0.1001
Validation Loss: 1.8431
Epoch [6/50], Class Loss: 0.0257, JMMD Loss: 0.0999
Validation Loss: 2.4605
Epoch [7/50], Class Loss: 0.0284, JMMD Loss: 0.0727
Validation Loss: 0.8323
Epoch [8/50], Class Loss: 0.0390, JMMD Loss: 0.1684
Validation Loss: 5.2312
Epoch [9/50], Class Loss: 0.0275, JMMD Loss: 0.1129
Validation Loss: 0.1187
Epoch [10/50], Class Loss: 0.0682, JMMD Loss: 0.1881
Validation Loss: 3.4871
Epoch [11/50], Class Loss: 0.0214, JMMD Loss: 0.1427
Validation Loss: 2.7273
Epoch [12/50], Class Loss: 0.0069, JMMD Loss: 0.1358
Validation Loss: 3.2123
Epoch [13/50], Class Loss: 0.0062, JMMD Loss: 0.1142
Validation Loss: 3.0197
Epoch [14/50], Class Loss: 0.0051, JMMD Loss: 0.0921
Validation Loss: 2.7670
Early stopping!
Source Domain Performance - Accuracy: 53.90%, Precision: 65.64%, Recall: 54.98%, F1 Score: 47.15%
Target Domain Performance - Accuracy: 56.77%, Precision: 44.58%, Recall: 57.38%, F1 Score: 49.54%

Source performance: 62.29% 67.36% 63.00% 56.90%
Target performance: 59.79% 54.55% 59.59% 53.43%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 99.98%
  Class 1: 3.06%
  Class 2: 65.56%
  Class 3: 69.78%
Base

Run 1/10
Epoch 1/50, Train Loss: 0.4351, Train Acc: 0.8305, Val Loss: 9.6429, Val Acc: 0.2500
Epoch 2/50, Train Loss: 0.1137, Train Acc: 0.9616, Val Loss: 3.7749, Val Acc: 0.5108
Epoch 3/50, Train Loss: 0.0570, Train Acc: 0.9811, Val Loss: 0.6465, Val Acc: 0.7932
Epoch 4/50, Train Loss: 0.0334, Train Acc: 0.9894, Val Loss: 1.1607, Val Acc: 0.7878
Epoch 5/50, Train Loss: 0.0140, Train Acc: 0.9963, Val Loss: 0.0294, Val Acc: 0.9946
Epoch 6/50, Train Loss: 0.0092, Train Acc: 0.9970, Val Loss: 5.8060, Val Acc: 0.5264
Epoch 7/50, Train Loss: 0.0066, Train Acc: 0.9978, Val Loss: 0.0191, Val Acc: 0.9940
Epoch 8/50, Train Loss: 0.0056, Train Acc: 0.9988, Val Loss: 0.9671, Val Acc: 0.8100
Epoch 9/50, Train Loss: 0.0022, Train Acc: 0.9993, Val Loss: 1.2532, Val Acc: 0.7830
Epoch 10/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.2468, Val Acc: 0.9227
Epoch 11/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Early stopping!

Run 2/10
Epoch 1/50, Train Loss: 0.4313, Train Acc: 0.8256, Val Loss: 5.0888, Val Acc: 0.5096
Epoch 2/50, Train Loss: 0.1201, Train Acc: 0.9553, Val Loss: 0.2633, Val Acc: 0.9173
Epoch 3/50, Train Loss: 0.0479, Train Acc: 0.9844, Val Loss: 0.6930, Val Acc: 0.8159
Epoch 4/50, Train Loss: 0.0249, Train Acc: 0.9922, Val Loss: 0.1585, Val Acc: 0.9448
Epoch 5/50, Train Loss: 0.0255, Train Acc: 0.9922, Val Loss: 2.8059, Val Acc: 0.7392
Epoch 6/50, Train Loss: 0.0051, Train Acc: 0.9991, Val Loss: 0.0997, Val Acc: 0.9562
Epoch 7/50, Train Loss: 0.0043, Train Acc: 0.9987, Val Loss: 0.0016, Val Acc: 1.0000
Epoch 8/50, Train Loss: 0.0047, Train Acc: 0.9991, Val Loss: 0.5371, Val Acc: 0.8034
Epoch 9/50, Train Loss: 0.0036, Train Acc: 0.9994, Val Loss: 0.0035, Val Acc: 0.9988
Epoch 10/50, Train Loss: 0.0030, Train Acc: 0.9996, Val Loss: 0.0065, Val Acc: 0.9994
Epoch 11/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Early stopping!

Run 3/10
Epoch 1/50, Train Loss: 0.4445, Train Acc: 0.8097, Val Loss: 5.8457, Val Acc: 0.5168
Epoch 2/50, Train Loss: 0.1237, Train Acc: 0.9517, Val Loss: 1.0462, Val Acc: 0.7584
Epoch 3/50, Train Loss: 0.0698, Train Acc: 0.9780, Val Loss: 0.0519, Val Acc: 0.9820
Epoch 4/50, Train Loss: 0.0653, Train Acc: 0.9792, Val Loss: 1.5002, Val Acc: 0.7494
Epoch 5/50, Train Loss: 0.0146, Train Acc: 0.9958, Val Loss: 0.0341, Val Acc: 0.9868
Epoch 6/50, Train Loss: 0.0075, Train Acc: 0.9982, Val Loss: 0.0316, Val Acc: 0.9904
Epoch 7/50, Train Loss: 0.0083, Train Acc: 0.9981, Val Loss: 0.0057, Val Acc: 0.9982
Epoch 8/50, Train Loss: 0.0129, Train Acc: 0.9967, Val Loss: 0.8102, Val Acc: 0.8219
Epoch 9/50, Train Loss: 0.0097, Train Acc: 0.9970, Val Loss: 1.1967, Val Acc: 0.7830
Epoch 10/50, Train Loss: 0.0091, Train Acc: 0.9972, Val Loss: 1.0910, Val Acc: 0.7692
Epoch 11/50, Train Loss: 0.0021, Train Acc: 0.9999, Val Loss: 0.0030, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0032, Train Acc: 0.9996, Val Loss: 0.0038, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0018, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0020, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0033, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0028, Train Acc: 0.9994, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0018, Train Acc: 0.9997, Val Loss: 0.0014, Val Acc: 0.9994
Early stopping!

Run 4/10
Epoch 1/50, Train Loss: 0.4402, Train Acc: 0.8193, Val Loss: 4.0767, Val Acc: 0.6007
Epoch 2/50, Train Loss: 0.1011, Train Acc: 0.9616, Val Loss: 1.3675, Val Acc: 0.7560
Epoch 3/50, Train Loss: 0.0533, Train Acc: 0.9829, Val Loss: 1.2313, Val Acc: 0.6505
Epoch 4/50, Train Loss: 0.0268, Train Acc: 0.9904, Val Loss: 0.0490, Val Acc: 0.9796
Epoch 5/50, Train Loss: 0.0173, Train Acc: 0.9951, Val Loss: 3.0751, Val Acc: 0.7296
Epoch 6/50, Train Loss: 0.0250, Train Acc: 0.9925, Val Loss: 0.9436, Val Acc: 0.7986
Epoch 7/50, Train Loss: 0.0172, Train Acc: 0.9948, Val Loss: 0.0035, Val Acc: 0.9994
Epoch 8/50, Train Loss: 0.0086, Train Acc: 0.9981, Val Loss: 0.0398, Val Acc: 0.9862
Epoch 9/50, Train Loss: 0.0074, Train Acc: 0.9973, Val Loss: 0.0143, Val Acc: 0.9934
Epoch 10/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9994
Epoch 11/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0041, Train Acc: 0.9991, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0035, Train Acc: 0.9994, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 5/10
Epoch 1/50, Train Loss: 0.4116, Train Acc: 0.8376, Val Loss: 2.4558, Val Acc: 0.7560
Epoch 2/50, Train Loss: 0.1318, Train Acc: 0.9516, Val Loss: 0.4922, Val Acc: 0.8435
Epoch 3/50, Train Loss: 0.0568, Train Acc: 0.9816, Val Loss: 0.7646, Val Acc: 0.6631
Epoch 4/50, Train Loss: 0.0358, Train Acc: 0.9873, Val Loss: 0.0415, Val Acc: 0.9874
Epoch 5/50, Train Loss: 0.0135, Train Acc: 0.9963, Val Loss: 0.2246, Val Acc: 0.9359
Epoch 6/50, Train Loss: 0.0090, Train Acc: 0.9979, Val Loss: 10.3579, Val Acc: 0.6900
Epoch 7/50, Train Loss: 0.0025, Train Acc: 0.9997, Val Loss: 0.0493, Val Acc: 0.9814
Epoch 8/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0056, Val Acc: 0.9994
Epoch 9/50, Train Loss: 0.0035, Train Acc: 0.9990, Val Loss: 0.2955, Val Acc: 0.9197
Epoch 10/50, Train Loss: 0.0110, Train Acc: 0.9970, Val Loss: 1.0861, Val Acc: 0.8189
Epoch 11/50, Train Loss: 0.0059, Train Acc: 0.9987, Val Loss: 0.0012, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0020, Train Acc: 0.9997, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0022, Train Acc: 0.9996, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0025, Train Acc: 0.9997, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0016, Train Acc: 0.9996, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0016, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0008, Train Acc: 0.9997, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 32/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 33/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 34/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Early stopping!

Run 6/10
Epoch 1/50, Train Loss: 0.4179, Train Acc: 0.8292, Val Loss: 2.3429, Val Acc: 0.7560
Epoch 2/50, Train Loss: 0.1238, Train Acc: 0.9553, Val Loss: 0.6822, Val Acc: 0.8040
Epoch 3/50, Train Loss: 0.0612, Train Acc: 0.9790, Val Loss: 0.0451, Val Acc: 0.9844
Epoch 4/50, Train Loss: 0.0457, Train Acc: 0.9849, Val Loss: 1.4961, Val Acc: 0.6283
Epoch 5/50, Train Loss: 0.0155, Train Acc: 0.9948, Val Loss: 0.2377, Val Acc: 0.9287
Epoch 6/50, Train Loss: 0.0119, Train Acc: 0.9957, Val Loss: 0.0120, Val Acc: 0.9970
Epoch 7/50, Train Loss: 0.0129, Train Acc: 0.9981, Val Loss: 0.0161, Val Acc: 0.9934
Epoch 8/50, Train Loss: 0.0024, Train Acc: 0.9994, Val Loss: 0.0219, Val Acc: 0.9910
Epoch 9/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0018, Val Acc: 0.9994
Epoch 10/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0217, Val Acc: 0.9922
Epoch 11/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0026, Train Acc: 0.9996, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 1.0000
Early stopping!

Run 7/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4174, Train Acc: 0.8311, Val Loss: 3.4097, Val Acc: 0.7554
Epoch 2/50, Train Loss: 0.1369, Train Acc: 0.9486, Val Loss: 0.5632, Val Acc: 0.8100
Epoch 3/50, Train Loss: 0.0772, Train Acc: 0.9714, Val Loss: 0.3007, Val Acc: 0.8813
Epoch 4/50, Train Loss: 0.0444, Train Acc: 0.9862, Val Loss: 0.3440, Val Acc: 0.8921
Epoch 5/50, Train Loss: 0.0295, Train Acc: 0.9903, Val Loss: 0.5836, Val Acc: 0.8076
Epoch 6/50, Train Loss: 0.0217, Train Acc: 0.9930, Val Loss: 1.1536, Val Acc: 0.7962
Epoch 7/50, Train Loss: 0.0140, Train Acc: 0.9955, Val Loss: 5.7833, Val Acc: 0.5414
Epoch 8/50, Train Loss: 0.0051, Train Acc: 0.9988, Val Loss: 0.0156, Val Acc: 0.9952
Epoch 9/50, Train Loss: 0.0039, Train Acc: 0.9988, Val Loss: 0.0657, Val Acc: 0.9820
Epoch 10/50, Train Loss: 0.0039, Train Acc: 0.9988, Val Loss: 0.0918, Val Acc: 0.9610
Epoch 11/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0021, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0029, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0037, Train Acc: 0.9997, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0019, Train Acc: 0.9997, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0023, Train Acc: 0.9999, Val Loss: 0.0021, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 24/50, Train Loss: 0.0019, Train Acc: 0.9997, Val Loss: 0.0030, Val Acc: 0.9994
Early stopping!

Run 8/10
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.3936, Train Acc: 0.8449, Val Loss: 2.6337, Val Acc: 0.7584
Epoch 2/50, Train Loss: 0.0949, Train Acc: 0.9652, Val Loss: 3.0741, Val Acc: 0.7614
Epoch 3/50, Train Loss: 0.0555, Train Acc: 0.9810, Val Loss: 3.1664, Val Acc: 0.6894
Epoch 4/50, Train Loss: 0.0252, Train Acc: 0.9909, Val Loss: 1.6064, Val Acc: 0.6912
Epoch 5/50, Train Loss: 0.0101, Train Acc: 0.9970, Val Loss: 0.0329, Val Acc: 0.9868
Epoch 6/50, Train Loss: 0.0134, Train Acc: 0.9963, Val Loss: 0.3790, Val Acc: 0.9023
Epoch 7/50, Train Loss: 0.0129, Train Acc: 0.9967, Val Loss: 0.2399, Val Acc: 0.9239
Epoch 8/50, Train Loss: 0.0055, Train Acc: 0.9981, Val Loss: 0.2070, Val Acc: 0.9376
Epoch 9/50, Train Loss: 0.0018, Train Acc: 0.9996, Val Loss: 0.0091, Val Acc: 0.9970
Epoch 10/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0039, Val Acc: 0.9982
Epoch 11/50, Train Loss: 0.0028, Train Acc: 0.9999, Val Loss: 0.0021, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0013, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0014, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9988
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9988
Early stopping!

Run 9/10
Epoch 1/50, Train Loss: 0.4488, Train Acc: 0.8118, Val Loss: 1.9780, Val Acc: 0.6613
Epoch 2/50, Train Loss: 0.1073, Train Acc: 0.9604, Val Loss: 10.2401, Val Acc: 0.5474
Epoch 3/50, Train Loss: 0.0762, Train Acc: 0.9754, Val Loss: 0.2472, Val Acc: 0.8681
Epoch 4/50, Train Loss: 0.0479, Train Acc: 0.9838, Val Loss: 0.7475, Val Acc: 0.8129
Epoch 5/50, Train Loss: 0.0162, Train Acc: 0.9951, Val Loss: 0.0279, Val Acc: 0.9904
Epoch 6/50, Train Loss: 0.0048, Train Acc: 0.9994, Val Loss: 0.0752, Val Acc: 0.9730
Epoch 7/50, Train Loss: 0.0027, Train Acc: 0.9996, Val Loss: 0.0119, Val Acc: 0.9958
Epoch 8/50, Train Loss: 0.0032, Train Acc: 0.9994, Val Loss: 0.0037, Val Acc: 0.9988
Epoch 9/50, Train Loss: 0.0138, Train Acc: 0.9972, Val Loss: 0.0087, Val Acc: 0.9958
Epoch 10/50, Train Loss: 0.0084, Train Acc: 0.9975, Val Loss: 0.1907, Val Acc: 0.9311
Epoch 11/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0006, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0021, Train Acc: 0.9996, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0027, Train Acc: 0.9993, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0032, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Early stopping!

Run 10/10
Epoch 1/50, Train Loss: 0.3947, Train Acc: 0.8473, Val Loss: 5.1156, Val Acc: 0.6169
Epoch 2/50, Train Loss: 0.1095, Train Acc: 0.9592, Val Loss: 5.7802, Val Acc: 0.5342
Epoch 3/50, Train Loss: 0.0668, Train Acc: 0.9744, Val Loss: 0.2259, Val Acc: 0.9215
Epoch 4/50, Train Loss: 0.0421, Train Acc: 0.9862, Val Loss: 0.2300, Val Acc: 0.9329
Epoch 5/50, Train Loss: 0.0236, Train Acc: 0.9924, Val Loss: 0.3195, Val Acc: 0.9149
Epoch 6/50, Train Loss: 0.0219, Train Acc: 0.9934, Val Loss: 0.3947, Val Acc: 0.8231
Epoch 7/50, Train Loss: 0.0185, Train Acc: 0.9943, Val Loss: 1.4490, Val Acc: 0.7536
Epoch 8/50, Train Loss: 0.0118, Train Acc: 0.9967, Val Loss: 0.5845, Val Acc: 0.8693
Early stopping!

Source performance: 98.67 99.09 98.73 98.61
Target performance: 69.00 79.04 69.12 61.84

bpsk: 100.00
qpsk: 3.76
16qam: 72.78
8apsk: 99.95
DANN
Epoch 1/50, Loss: 2.3761, Domain Loss: 1.3696, Class Loss: 1.0065
Epoch 2/50, Loss: 1.7471, Domain Loss: 1.3247, Class Loss: 0.4223
Epoch 3/50, Loss: 1.6533, Domain Loss: 1.3354, Class Loss: 0.3179
Epoch 4/50, Loss: 2.1111, Domain Loss: 1.8474, Class Loss: 0.2637
Epoch 5/50, Loss: 8.5284, Domain Loss: 8.1288, Class Loss: 0.3997
Epoch 6/50, Loss: 5.8341, Domain Loss: 5.4236, Class Loss: 0.4105
Epoch 7/50, Loss: 8.9517, Domain Loss: 8.2679, Class Loss: 0.6838
Epoch 8/50, Loss: 9.8416, Domain Loss: 9.1133, Class Loss: 0.7283
Epoch 9/50, Loss: 11.8048, Domain Loss: 11.2554, Class Loss: 0.5493
Epoch 10/50, Loss: 4.3870, Domain Loss: 3.7743, Class Loss: 0.6127
Epoch 11/50, Loss: 3.6857, Domain Loss: 2.9800, Class Loss: 0.7057
Epoch 12/50, Loss: 2.3677, Domain Loss: 1.8395, Class Loss: 0.5282
Epoch 13/50, Loss: 2.9176, Domain Loss: 2.3581, Class Loss: 0.5596
Epoch 14/50, Loss: 4.9454, Domain Loss: 4.2063, Class Loss: 0.7391
Epoch 15/50, Loss: 4.4719, Domain Loss: 3.6747, Class Loss: 0.7972
Epoch 16/50, Loss: 4.0202, Domain Loss: 3.2073, Class Loss: 0.8129
Epoch 17/50, Loss: 3.6351, Domain Loss: 2.8624, Class Loss: 0.7728
Epoch 18/50, Loss: 3.1941, Domain Loss: 2.4750, Class Loss: 0.7191
Epoch 19/50, Loss: 2.6527, Domain Loss: 2.1107, Class Loss: 0.5421
Epoch 20/50, Loss: 2.1593, Domain Loss: 1.6359, Class Loss: 0.5234
Epoch 21/50, Loss: 2.5090, Domain Loss: 1.9375, Class Loss: 0.5715
Epoch 22/50, Loss: 2.1510, Domain Loss: 1.7065, Class Loss: 0.4445
Epoch 23/50, Loss: 2.0740, Domain Loss: 1.5172, Class Loss: 0.5568
Epoch 24/50, Loss: 1.9981, Domain Loss: 1.5388, Class Loss: 0.4593
Epoch 25/50, Loss: 3.0688, Domain Loss: 1.7134, Class Loss: 1.3554
Epoch 26/50, Loss: 2.1060, Domain Loss: 1.4920, Class Loss: 0.6140
Epoch 27/50, Loss: 1.7792, Domain Loss: 1.3315, Class Loss: 0.4478
Epoch 28/50, Loss: 1.8665, Domain Loss: 1.3967, Class Loss: 0.4699
Epoch 29/50, Loss: 2.0844, Domain Loss: 1.5916, Class Loss: 0.4928
Epoch 30/50, Loss: 2.0126, Domain Loss: 1.5670, Class Loss: 0.4456
Epoch 31/50, Loss: 1.9260, Domain Loss: 1.5342, Class Loss: 0.3918
Epoch 32/50, Loss: 2.1873, Domain Loss: 1.7716, Class Loss: 0.4157
Epoch 33/50, Loss: 1.9655, Domain Loss: 1.5256, Class Loss: 0.4399
Epoch 34/50, Loss: 1.9369, Domain Loss: 1.5655, Class Loss: 0.3713
Epoch 35/50, Loss: 1.7163, Domain Loss: 1.4448, Class Loss: 0.2715
Epoch 36/50, Loss: 1.6611, Domain Loss: 1.4472, Class Loss: 0.2139
Epoch 37/50, Loss: 1.7047, Domain Loss: 1.5058, Class Loss: 0.1989
Epoch 38/50, Loss: 1.6699, Domain Loss: 1.5414, Class Loss: 0.1285
Epoch 39/50, Loss: 1.6709, Domain Loss: 1.5603, Class Loss: 0.1105
Epoch 40/50, Loss: 1.5844, Domain Loss: 1.5049, Class Loss: 0.0794
Epoch 41/50, Loss: 1.5077, Domain Loss: 1.4291, Class Loss: 0.0786
Epoch 42/50, Loss: 1.5782, Domain Loss: 1.4889, Class Loss: 0.0893
Epoch 43/50, Loss: 1.5575, Domain Loss: 1.4728, Class Loss: 0.0848
Epoch 44/50, Loss: 1.4732, Domain Loss: 1.4143, Class Loss: 0.0589
Epoch 45/50, Loss: 1.5307, Domain Loss: 1.4298, Class Loss: 0.1009
Epoch 46/50, Loss: 1.7739, Domain Loss: 1.6778, Class Loss: 0.0961
Epoch 47/50, Loss: 1.7045, Domain Loss: 1.6310, Class Loss: 0.0735
Epoch 48/50, Loss: 1.6462, Domain Loss: 1.5914, Class Loss: 0.0548
Epoch 49/50, Loss: 1.4625, Domain Loss: 1.4094, Class Loss: 0.0532
Epoch 50/50, Loss: 1.7846, Domain Loss: 1.6826, Class Loss: 0.1020
32.49


Epoch 1/50, Loss: 2.3558, Domain Loss: 1.3795, Class Loss: 0.9763
Epoch 2/50, Loss: 1.7479, Domain Loss: 1.3055, Class Loss: 0.4424
Epoch 3/50, Loss: 1.7334, Domain Loss: 1.3469, Class Loss: 0.3864
Epoch 4/50, Loss: 1.8045, Domain Loss: 1.4567, Class Loss: 0.3479
Epoch 5/50, Loss: 3.8053, Domain Loss: 3.4969, Class Loss: 0.3085
Epoch 6/50, Loss: 11.1436, Domain Loss: 10.3800, Class Loss: 0.7635
Epoch 7/50, Loss: 6.9538, Domain Loss: 6.2819, Class Loss: 0.6719
Epoch 8/50, Loss: 5.0719, Domain Loss: 4.5011, Class Loss: 0.5708
Epoch 9/50, Loss: 15.2864, Domain Loss: 14.4147, Class Loss: 0.8718
Epoch 10/50, Loss: 11.1464, Domain Loss: 10.2031, Class Loss: 0.9433
Epoch 11/50, Loss: 6.0358, Domain Loss: 5.1198, Class Loss: 0.9160
Epoch 12/50, Loss: 8.3016, Domain Loss: 7.2208, Class Loss: 1.0808
Epoch 13/50, Loss: 5.6212, Domain Loss: 4.4771, Class Loss: 1.1441
Epoch 14/50, Loss: 4.2082, Domain Loss: 3.3953, Class Loss: 0.8129
Epoch 15/50, Loss: 4.7221, Domain Loss: 3.8297, Class Loss: 0.8924
Epoch 16/50, Loss: 3.1225, Domain Loss: 2.3258, Class Loss: 0.7967
Epoch 17/50, Loss: 2.4791, Domain Loss: 1.7846, Class Loss: 0.6945
Epoch 18/50, Loss: 2.6270, Domain Loss: 1.9995, Class Loss: 0.6275
Epoch 19/50, Loss: 2.6133, Domain Loss: 2.0821, Class Loss: 0.5311
Epoch 20/50, Loss: 3.1241, Domain Loss: 2.6018, Class Loss: 0.5223
Epoch 21/50, Loss: 5.2560, Domain Loss: 4.5501, Class Loss: 0.7059
Epoch 22/50, Loss: 10.6082, Domain Loss: 9.8766, Class Loss: 0.7316
Epoch 23/50, Loss: 4.9596, Domain Loss: 4.3575, Class Loss: 0.6021
Epoch 24/50, Loss: 2.6316, Domain Loss: 2.0619, Class Loss: 0.5697
Epoch 25/50, Loss: 2.3671, Domain Loss: 1.8060, Class Loss: 0.5612
Epoch 26/50, Loss: 1.9386, Domain Loss: 1.4767, Class Loss: 0.4619
Epoch 27/50, Loss: 2.0314, Domain Loss: 1.5308, Class Loss: 0.5006
Epoch 28/50, Loss: 3.8071, Domain Loss: 2.8565, Class Loss: 0.9506
Epoch 29/50, Loss: 2.3550, Domain Loss: 1.8383, Class Loss: 0.5167
Epoch 30/50, Loss: 1.8694, Domain Loss: 1.4254, Class Loss: 0.4440
Epoch 31/50, Loss: 1.7390, Domain Loss: 1.3253, Class Loss: 0.4137
Epoch 32/50, Loss: 1.7889, Domain Loss: 1.3567, Class Loss: 0.4322
Epoch 33/50, Loss: 1.7479, Domain Loss: 1.3279, Class Loss: 0.4200
Epoch 34/50, Loss: 1.7512, Domain Loss: 1.3156, Class Loss: 0.4356
Epoch 35/50, Loss: 1.8347, Domain Loss: 1.3760, Class Loss: 0.4587
Epoch 36/50, Loss: 1.8128, Domain Loss: 1.3907, Class Loss: 0.4221
Epoch 37/50, Loss: 1.6815, Domain Loss: 1.3025, Class Loss: 0.3789
Epoch 38/50, Loss: 1.6982, Domain Loss: 1.3106, Class Loss: 0.3876
Epoch 39/50, Loss: 1.6946, Domain Loss: 1.3342, Class Loss: 0.3604
Epoch 40/50, Loss: 1.7183, Domain Loss: 1.3638, Class Loss: 0.3545
Epoch 41/50, Loss: 1.7835, Domain Loss: 1.4418, Class Loss: 0.3417
Epoch 42/50, Loss: 1.8365, Domain Loss: 1.4760, Class Loss: 0.3605
Epoch 43/50, Loss: 1.8016, Domain Loss: 1.4518, Class Loss: 0.3498
Epoch 44/50, Loss: 1.7088, Domain Loss: 1.3882, Class Loss: 0.3206
Epoch 45/50, Loss: 1.7622, Domain Loss: 1.4290, Class Loss: 0.3332
Epoch 46/50, Loss: 1.7283, Domain Loss: 1.4377, Class Loss: 0.2906
Epoch 47/50, Loss: 1.7883, Domain Loss: 1.5075, Class Loss: 0.2809
Epoch 48/50, Loss: 1.8431, Domain Loss: 1.5952, Class Loss: 0.2478
Epoch 49/50, Loss: 1.8311, Domain Loss: 1.5165, Class Loss: 0.3146
Epoch 50/50, Loss: 1.8977, Domain Loss: 1.5989, Class Loss: 0.2988
72.00


Epoch 1/50, Loss: 2.3084, Domain Loss: 1.3807, Class Loss: 0.9277
Epoch 2/50, Loss: 1.7363, Domain Loss: 1.3315, Class Loss: 0.4048
Epoch 3/50, Loss: 1.6498, Domain Loss: 1.2855, Class Loss: 0.3643
Epoch 4/50, Loss: 1.6575, Domain Loss: 1.3344, Class Loss: 0.3231
Epoch 5/50, Loss: 3.8744, Domain Loss: 3.5690, Class Loss: 0.3054
Epoch 6/50, Loss: 15.7765, Domain Loss: 15.1274, Class Loss: 0.6492
Epoch 7/50, Loss: 9.4766, Domain Loss: 8.9681, Class Loss: 0.5085
Epoch 8/50, Loss: 9.3966, Domain Loss: 8.9155, Class Loss: 0.4810
Epoch 9/50, Loss: 17.1324, Domain Loss: 16.5270, Class Loss: 0.6054
Epoch 10/50, Loss: 12.2802, Domain Loss: 11.8199, Class Loss: 0.4603
Epoch 11/50, Loss: 19.6418, Domain Loss: 19.0551, Class Loss: 0.5867
Epoch 12/50, Loss: 10.2761, Domain Loss: 9.5805, Class Loss: 0.6956
Epoch 13/50, Loss: 7.2390, Domain Loss: 6.6148, Class Loss: 0.6242
Epoch 14/50, Loss: 4.6577, Domain Loss: 4.1601, Class Loss: 0.4976
Epoch 15/50, Loss: 3.7717, Domain Loss: 3.0307, Class Loss: 0.7410
Epoch 16/50, Loss: 2.6536, Domain Loss: 2.1484, Class Loss: 0.5052
Epoch 17/50, Loss: 2.3161, Domain Loss: 1.8307, Class Loss: 0.4854
Epoch 18/50, Loss: 2.1812, Domain Loss: 1.6136, Class Loss: 0.5677
Epoch 19/50, Loss: 2.1726, Domain Loss: 1.6756, Class Loss: 0.4971
Epoch 20/50, Loss: 2.6296, Domain Loss: 1.9783, Class Loss: 0.6513
Epoch 21/50, Loss: 2.3362, Domain Loss: 1.7938, Class Loss: 0.5423
Epoch 22/50, Loss: 1.9453, Domain Loss: 1.4805, Class Loss: 0.4648
Epoch 23/50, Loss: 1.8404, Domain Loss: 1.4111, Class Loss: 0.4293
Epoch 24/50, Loss: 1.9974, Domain Loss: 1.5466, Class Loss: 0.4509
Epoch 25/50, Loss: 1.7731, Domain Loss: 1.3673, Class Loss: 0.4058
Epoch 26/50, Loss: 1.6862, Domain Loss: 1.3336, Class Loss: 0.3526
Epoch 27/50, Loss: 1.7106, Domain Loss: 1.3318, Class Loss: 0.3788
Epoch 28/50, Loss: 1.6300, Domain Loss: 1.3216, Class Loss: 0.3085
Epoch 29/50, Loss: 1.6955, Domain Loss: 1.4141, Class Loss: 0.2814
Epoch 30/50, Loss: 1.6572, Domain Loss: 1.4267, Class Loss: 0.2305
Epoch 31/50, Loss: 1.8445, Domain Loss: 1.5354, Class Loss: 0.3091
Epoch 32/50, Loss: 1.9119, Domain Loss: 1.6018, Class Loss: 0.3100
Epoch 33/50, Loss: 1.7059, Domain Loss: 1.4430, Class Loss: 0.2629
Epoch 34/50, Loss: 1.9573, Domain Loss: 1.6266, Class Loss: 0.3307
Epoch 35/50, Loss: 1.8732, Domain Loss: 1.6177, Class Loss: 0.2555
Epoch 36/50, Loss: 2.1570, Domain Loss: 1.8611, Class Loss: 0.2959
Epoch 37/50, Loss: 2.6991, Domain Loss: 2.2895, Class Loss: 0.4096
Epoch 38/50, Loss: 3.1126, Domain Loss: 2.6935, Class Loss: 0.4190
Epoch 39/50, Loss: 3.8823, Domain Loss: 3.1883, Class Loss: 0.6940
Epoch 40/50, Loss: 2.1889, Domain Loss: 1.7079, Class Loss: 0.4810
Epoch 41/50, Loss: 2.2095, Domain Loss: 1.8194, Class Loss: 0.3902
Epoch 42/50, Loss: 2.2226, Domain Loss: 1.8841, Class Loss: 0.3385
Epoch 43/50, Loss: 2.0057, Domain Loss: 1.6508, Class Loss: 0.3548
Epoch 44/50, Loss: 1.8424, Domain Loss: 1.4532, Class Loss: 0.3893
Epoch 45/50, Loss: 1.8893, Domain Loss: 1.5467, Class Loss: 0.3427
Epoch 46/50, Loss: 1.6976, Domain Loss: 1.4294, Class Loss: 0.2683
Epoch 47/50, Loss: 1.6607, Domain Loss: 1.4194, Class Loss: 0.2413
Epoch 48/50, Loss: 1.6601, Domain Loss: 1.3676, Class Loss: 0.2925
Epoch 49/50, Loss: 1.6062, Domain Loss: 1.3755, Class Loss: 0.2307
Epoch 50/50, Loss: 1.6389, Domain Loss: 1.3972, Class Loss: 0.2417
72.90


Epoch 1/50, Loss: 2.2964, Domain Loss: 1.3940, Class Loss: 0.9024
Epoch 2/50, Loss: 1.7073, Domain Loss: 1.3156, Class Loss: 0.3917
Epoch 3/50, Loss: 1.6375, Domain Loss: 1.3418, Class Loss: 0.2957
Epoch 4/50, Loss: 2.6644, Domain Loss: 2.4353, Class Loss: 0.2291
Epoch 5/50, Loss: 5.1419, Domain Loss: 4.8898, Class Loss: 0.2521
Epoch 6/50, Loss: 8.5238, Domain Loss: 8.1401, Class Loss: 0.3838
Epoch 7/50, Loss: 15.7629, Domain Loss: 14.9137, Class Loss: 0.8493
Epoch 8/50, Loss: 6.1191, Domain Loss: 5.5337, Class Loss: 0.5854
Epoch 9/50, Loss: 3.3894, Domain Loss: 2.7860, Class Loss: 0.6034
Epoch 10/50, Loss: 2.2713, Domain Loss: 1.7810, Class Loss: 0.4903
Epoch 11/50, Loss: 2.1580, Domain Loss: 1.7458, Class Loss: 0.4122
Epoch 12/50, Loss: 2.6394, Domain Loss: 2.1914, Class Loss: 0.4481
Epoch 13/50, Loss: 4.1537, Domain Loss: 3.4845, Class Loss: 0.6691
Epoch 14/50, Loss: 4.7805, Domain Loss: 4.0865, Class Loss: 0.6939
Epoch 15/50, Loss: 3.4017, Domain Loss: 2.9139, Class Loss: 0.4878
Epoch 16/50, Loss: 2.6143, Domain Loss: 2.1389, Class Loss: 0.4754
Epoch 17/50, Loss: 2.1520, Domain Loss: 1.7110, Class Loss: 0.4409
Epoch 18/50, Loss: 2.0875, Domain Loss: 1.6583, Class Loss: 0.4292
Epoch 19/50, Loss: 2.0498, Domain Loss: 1.6225, Class Loss: 0.4274
Epoch 20/50, Loss: 2.0155, Domain Loss: 1.6058, Class Loss: 0.4097
Epoch 21/50, Loss: 1.9296, Domain Loss: 1.4875, Class Loss: 0.4421
Epoch 22/50, Loss: 1.8071, Domain Loss: 1.4391, Class Loss: 0.3680
Epoch 23/50, Loss: 1.8689, Domain Loss: 1.5237, Class Loss: 0.3452
Epoch 24/50, Loss: 1.9123, Domain Loss: 1.5805, Class Loss: 0.3317
Epoch 25/50, Loss: 1.8072, Domain Loss: 1.4785, Class Loss: 0.3287
Epoch 26/50, Loss: 1.7694, Domain Loss: 1.4525, Class Loss: 0.3169
Epoch 27/50, Loss: 1.9310, Domain Loss: 1.6207, Class Loss: 0.3103
Epoch 28/50, Loss: 2.2167, Domain Loss: 1.9121, Class Loss: 0.3046
Epoch 29/50, Loss: 2.9766, Domain Loss: 2.5562, Class Loss: 0.4204
Epoch 30/50, Loss: 2.0953, Domain Loss: 1.7274, Class Loss: 0.3679
Epoch 31/50, Loss: 2.1762, Domain Loss: 1.7438, Class Loss: 0.4324
Epoch 32/50, Loss: 2.0402, Domain Loss: 1.7033, Class Loss: 0.3369
Epoch 33/50, Loss: 1.8441, Domain Loss: 1.4996, Class Loss: 0.3445
Epoch 34/50, Loss: 2.0144, Domain Loss: 1.6630, Class Loss: 0.3514
Epoch 35/50, Loss: 1.7602, Domain Loss: 1.4494, Class Loss: 0.3107
Epoch 36/50, Loss: 1.6406, Domain Loss: 1.4073, Class Loss: 0.2333
Epoch 37/50, Loss: 1.6478, Domain Loss: 1.3940, Class Loss: 0.2538
Epoch 38/50, Loss: 1.9135, Domain Loss: 1.5877, Class Loss: 0.3258
Epoch 39/50, Loss: 1.5899, Domain Loss: 1.3041, Class Loss: 0.2858
Epoch 40/50, Loss: 1.7652, Domain Loss: 1.4597, Class Loss: 0.3055
Epoch 41/50, Loss: 1.7373, Domain Loss: 1.4535, Class Loss: 0.2838
Epoch 42/50, Loss: 1.6829, Domain Loss: 1.4399, Class Loss: 0.2430
Epoch 43/50, Loss: 1.6667, Domain Loss: 1.4237, Class Loss: 0.2430
Epoch 44/50, Loss: 1.6222, Domain Loss: 1.3712, Class Loss: 0.2510
Epoch 45/50, Loss: 1.5974, Domain Loss: 1.3719, Class Loss: 0.2255
Epoch 46/50, Loss: 1.5835, Domain Loss: 1.3790, Class Loss: 0.2044
Epoch 47/50, Loss: 1.5610, Domain Loss: 1.3845, Class Loss: 0.1765
Epoch 48/50, Loss: 1.5546, Domain Loss: 1.3825, Class Loss: 0.1721
Epoch 49/50, Loss: 1.5163, Domain Loss: 1.3913, Class Loss: 0.1250
Epoch 50/50, Loss: 1.5492, Domain Loss: 1.4102, Class Loss: 0.1390
70.62


Epoch 1/50, Loss: 2.3760, Domain Loss: 1.3711, Class Loss: 1.0049
Epoch 2/50, Loss: 1.7018, Domain Loss: 1.2649, Class Loss: 0.4370
Epoch 3/50, Loss: 1.7064, Domain Loss: 1.3439, Class Loss: 0.3625
Epoch 4/50, Loss: 3.0280, Domain Loss: 2.6876, Class Loss: 0.3404
Epoch 5/50, Loss: 6.7939, Domain Loss: 6.3265, Class Loss: 0.4673
Epoch 6/50, Loss: 14.2844, Domain Loss: 13.6802, Class Loss: 0.6042
Epoch 7/50, Loss: 16.5158, Domain Loss: 15.9212, Class Loss: 0.5946
Epoch 8/50, Loss: 7.2120, Domain Loss: 6.7664, Class Loss: 0.4456
Epoch 9/50, Loss: 3.2980, Domain Loss: 2.8704, Class Loss: 0.4276
Epoch 10/50, Loss: 2.8816, Domain Loss: 2.3049, Class Loss: 0.5767
Epoch 11/50, Loss: 2.0527, Domain Loss: 1.6627, Class Loss: 0.3900
Epoch 12/50, Loss: 1.6244, Domain Loss: 1.2841, Class Loss: 0.3402
Epoch 13/50, Loss: 2.9561, Domain Loss: 2.3638, Class Loss: 0.5923
Epoch 14/50, Loss: 6.1392, Domain Loss: 5.6598, Class Loss: 0.4795
Epoch 15/50, Loss: 11.3191, Domain Loss: 10.6314, Class Loss: 0.6877
Epoch 16/50, Loss: 20.3051, Domain Loss: 19.2331, Class Loss: 1.0720
Epoch 17/50, Loss: 42.8585, Domain Loss: 41.8018, Class Loss: 1.0567
Epoch 18/50, Loss: 20.7393, Domain Loss: 19.7208, Class Loss: 1.0185
Epoch 19/50, Loss: 7.4904, Domain Loss: 6.7416, Class Loss: 0.7488
Epoch 20/50, Loss: 3.5653, Domain Loss: 2.9390, Class Loss: 0.6264
Epoch 21/50, Loss: 4.6787, Domain Loss: 3.9066, Class Loss: 0.7721
Epoch 22/50, Loss: 4.8219, Domain Loss: 4.0611, Class Loss: 0.7608
Epoch 23/50, Loss: 4.5523, Domain Loss: 3.9816, Class Loss: 0.5707
Epoch 24/50, Loss: 2.5477, Domain Loss: 2.0472, Class Loss: 0.5005
Epoch 25/50, Loss: 2.7650, Domain Loss: 1.9439, Class Loss: 0.8211
Epoch 26/50, Loss: 2.2660, Domain Loss: 1.7792, Class Loss: 0.4868
Epoch 27/50, Loss: 2.0384, Domain Loss: 1.6396, Class Loss: 0.3988
Epoch 28/50, Loss: 1.9511, Domain Loss: 1.5721, Class Loss: 0.3790
Epoch 29/50, Loss: 2.1186, Domain Loss: 1.7422, Class Loss: 0.3764
Epoch 30/50, Loss: 2.3632, Domain Loss: 1.9489, Class Loss: 0.4142
Epoch 31/50, Loss: 2.1731, Domain Loss: 1.7785, Class Loss: 0.3946
Epoch 32/50, Loss: 2.1153, Domain Loss: 1.7653, Class Loss: 0.3500
Epoch 33/50, Loss: 1.9583, Domain Loss: 1.5868, Class Loss: 0.3714
Epoch 34/50, Loss: 1.8132, Domain Loss: 1.4124, Class Loss: 0.4009
Epoch 35/50, Loss: 1.7460, Domain Loss: 1.4155, Class Loss: 0.3305
Epoch 36/50, Loss: 1.8131, Domain Loss: 1.4578, Class Loss: 0.3553
Epoch 37/50, Loss: 1.7828, Domain Loss: 1.4271, Class Loss: 0.3557
Epoch 38/50, Loss: 1.7290, Domain Loss: 1.3956, Class Loss: 0.3334
Epoch 39/50, Loss: 1.7084, Domain Loss: 1.3769, Class Loss: 0.3315
Epoch 40/50, Loss: 1.6895, Domain Loss: 1.3699, Class Loss: 0.3196
Epoch 41/50, Loss: 1.7095, Domain Loss: 1.3689, Class Loss: 0.3406
Epoch 42/50, Loss: 1.6653, Domain Loss: 1.3670, Class Loss: 0.2983
Epoch 43/50, Loss: 1.6904, Domain Loss: 1.3755, Class Loss: 0.3148
Epoch 44/50, Loss: 1.6544, Domain Loss: 1.3563, Class Loss: 0.2981
Epoch 45/50, Loss: 1.6542, Domain Loss: 1.3548, Class Loss: 0.2995
Epoch 46/50, Loss: 1.6300, Domain Loss: 1.3441, Class Loss: 0.2859
Epoch 47/50, Loss: 1.6763, Domain Loss: 1.3653, Class Loss: 0.3110
Epoch 48/50, Loss: 1.6629, Domain Loss: 1.3933, Class Loss: 0.2697
Epoch 49/50, Loss: 1.7534, Domain Loss: 1.4328, Class Loss: 0.3206
Epoch 50/50, Loss: 1.7771, Domain Loss: 1.4955, Class Loss: 0.2816
70.26


Epoch 1/50, Loss: 2.3475, Domain Loss: 1.3888, Class Loss: 0.9587
Epoch 2/50, Loss: 1.7187, Domain Loss: 1.2740, Class Loss: 0.4447
Epoch 3/50, Loss: 1.5341, Domain Loss: 1.1735, Class Loss: 0.3606
Epoch 4/50, Loss: 1.8642, Domain Loss: 1.5945, Class Loss: 0.2697
Epoch 5/50, Loss: 3.7008, Domain Loss: 3.4260, Class Loss: 0.2748
Epoch 6/50, Loss: 16.7419, Domain Loss: 16.2494, Class Loss: 0.4925
Epoch 7/50, Loss: 16.3088, Domain Loss: 15.8334, Class Loss: 0.4754
Epoch 8/50, Loss: 11.0931, Domain Loss: 10.5162, Class Loss: 0.5769
Epoch 9/50, Loss: 7.9222, Domain Loss: 6.8300, Class Loss: 1.0921
Epoch 10/50, Loss: 9.8442, Domain Loss: 8.8332, Class Loss: 1.0109
Epoch 11/50, Loss: 13.1806, Domain Loss: 9.8723, Class Loss: 3.3082
Epoch 12/50, Loss: 7.0252, Domain Loss: 5.9044, Class Loss: 1.1208
Epoch 13/50, Loss: 4.4190, Domain Loss: 3.4997, Class Loss: 0.9193
Epoch 14/50, Loss: 3.1292, Domain Loss: 2.2702, Class Loss: 0.8590
Epoch 15/50, Loss: 2.5025, Domain Loss: 1.7220, Class Loss: 0.7805
Epoch 16/50, Loss: 2.1995, Domain Loss: 1.5386, Class Loss: 0.6608
Epoch 17/50, Loss: 2.1843, Domain Loss: 1.5140, Class Loss: 0.6703
Epoch 18/50, Loss: 1.8896, Domain Loss: 1.3286, Class Loss: 0.5610
Epoch 19/50, Loss: 1.8809, Domain Loss: 1.3219, Class Loss: 0.5590
Epoch 20/50, Loss: 2.0107, Domain Loss: 1.4273, Class Loss: 0.5834
Epoch 21/50, Loss: 1.9407, Domain Loss: 1.4208, Class Loss: 0.5200
Epoch 22/50, Loss: 1.8963, Domain Loss: 1.4096, Class Loss: 0.4867
Epoch 23/50, Loss: 1.8966, Domain Loss: 1.4036, Class Loss: 0.4930
Epoch 24/50, Loss: 1.9156, Domain Loss: 1.4469, Class Loss: 0.4687
Epoch 25/50, Loss: 1.8632, Domain Loss: 1.4496, Class Loss: 0.4136
Epoch 26/50, Loss: 2.5192, Domain Loss: 1.9345, Class Loss: 0.5847
Epoch 27/50, Loss: 2.6793, Domain Loss: 1.9757, Class Loss: 0.7036
Epoch 28/50, Loss: 2.1352, Domain Loss: 1.5800, Class Loss: 0.5553
Epoch 29/50, Loss: 1.8132, Domain Loss: 1.3985, Class Loss: 0.4147
Epoch 30/50, Loss: 1.7584, Domain Loss: 1.3762, Class Loss: 0.3823
Epoch 31/50, Loss: 1.7393, Domain Loss: 1.3616, Class Loss: 0.3777
Epoch 32/50, Loss: 1.8427, Domain Loss: 1.4782, Class Loss: 0.3645
Epoch 33/50, Loss: 1.8356, Domain Loss: 1.4779, Class Loss: 0.3577
Epoch 34/50, Loss: 3.2952, Domain Loss: 1.5845, Class Loss: 1.7107
Epoch 35/50, Loss: 2.0739, Domain Loss: 1.4101, Class Loss: 0.6638
Epoch 36/50, Loss: 1.8401, Domain Loss: 1.2702, Class Loss: 0.5699
Epoch 37/50, Loss: 2.5641, Domain Loss: 1.7241, Class Loss: 0.8400
Epoch 38/50, Loss: 2.2033, Domain Loss: 1.6348, Class Loss: 0.5685
Epoch 39/50, Loss: 2.8480, Domain Loss: 2.0713, Class Loss: 0.7767
Epoch 40/50, Loss: 2.4818, Domain Loss: 1.9740, Class Loss: 0.5079
Epoch 41/50, Loss: 3.2852, Domain Loss: 2.4488, Class Loss: 0.8364
Epoch 42/50, Loss: 4.6214, Domain Loss: 3.5417, Class Loss: 1.0797
Epoch 43/50, Loss: 3.3115, Domain Loss: 2.4943, Class Loss: 0.8171
Epoch 44/50, Loss: 3.2731, Domain Loss: 2.4541, Class Loss: 0.8189
Epoch 45/50, Loss: 2.9594, Domain Loss: 2.1958, Class Loss: 0.7636
Epoch 46/50, Loss: 2.2317, Domain Loss: 1.5197, Class Loss: 0.7120
Epoch 47/50, Loss: 2.0799, Domain Loss: 1.4213, Class Loss: 0.6585
Epoch 48/50, Loss: 2.0712, Domain Loss: 1.4702, Class Loss: 0.6010
Epoch 49/50, Loss: 2.0233, Domain Loss: 1.5175, Class Loss: 0.5058
Epoch 50/50, Loss: 1.9880, Domain Loss: 1.5410, Class Loss: 0.4470
65.59


Epoch 1/50, Loss: 2.3019, Domain Loss: 1.3969, Class Loss: 0.9050
Epoch 2/50, Loss: 1.7034, Domain Loss: 1.2958, Class Loss: 0.4075
Epoch 3/50, Loss: 1.6496, Domain Loss: 1.3005, Class Loss: 0.3492
Epoch 4/50, Loss: 1.7368, Domain Loss: 1.4438, Class Loss: 0.2930
Epoch 5/50, Loss: 3.9081, Domain Loss: 3.6454, Class Loss: 0.2627
Epoch 6/50, Loss: 4.3390, Domain Loss: 3.9905, Class Loss: 0.3485
Epoch 7/50, Loss: 4.6304, Domain Loss: 4.2494, Class Loss: 0.3811
Epoch 8/50, Loss: 9.0544, Domain Loss: 8.5069, Class Loss: 0.5475
Epoch 9/50, Loss: 5.5730, Domain Loss: 4.9184, Class Loss: 0.6546
Epoch 10/50, Loss: 4.0782, Domain Loss: 3.4067, Class Loss: 0.6715
Epoch 11/50, Loss: 2.1982, Domain Loss: 1.8029, Class Loss: 0.3953
Epoch 12/50, Loss: 1.9895, Domain Loss: 1.5596, Class Loss: 0.4300
Epoch 13/50, Loss: 1.8018, Domain Loss: 1.4307, Class Loss: 0.3710
Epoch 14/50, Loss: 1.8220, Domain Loss: 1.4702, Class Loss: 0.3518
Epoch 15/50, Loss: 2.0798, Domain Loss: 1.6725, Class Loss: 0.4073
Epoch 16/50, Loss: 1.8531, Domain Loss: 1.4960, Class Loss: 0.3572
Epoch 17/50, Loss: 2.4580, Domain Loss: 1.9327, Class Loss: 0.5253
Epoch 18/50, Loss: 3.2821, Domain Loss: 2.5570, Class Loss: 0.7252
Epoch 19/50, Loss: 3.3674, Domain Loss: 2.6509, Class Loss: 0.7165
Epoch 20/50, Loss: 3.8580, Domain Loss: 2.6312, Class Loss: 1.2268
Epoch 21/50, Loss: 2.6193, Domain Loss: 1.9379, Class Loss: 0.6815
Epoch 22/50, Loss: 3.4020, Domain Loss: 2.6464, Class Loss: 0.7556
Epoch 23/50, Loss: 3.2165, Domain Loss: 2.3066, Class Loss: 0.9100
Epoch 24/50, Loss: 2.7322, Domain Loss: 1.9197, Class Loss: 0.8124
Epoch 25/50, Loss: 2.3406, Domain Loss: 1.5622, Class Loss: 0.7784
Epoch 26/50, Loss: 2.2413, Domain Loss: 1.4991, Class Loss: 0.7422
Epoch 27/50, Loss: 2.1898, Domain Loss: 1.5733, Class Loss: 0.6166
Epoch 28/50, Loss: 2.2350, Domain Loss: 1.7297, Class Loss: 0.5053
Epoch 29/50, Loss: 2.3510, Domain Loss: 1.8332, Class Loss: 0.5178
Epoch 30/50, Loss: 2.0816, Domain Loss: 1.5855, Class Loss: 0.4960
Epoch 31/50, Loss: 1.9071, Domain Loss: 1.4992, Class Loss: 0.4080
Epoch 32/50, Loss: 1.9823, Domain Loss: 1.5869, Class Loss: 0.3954
Epoch 33/50, Loss: 2.2672, Domain Loss: 1.8884, Class Loss: 0.3788
Epoch 34/50, Loss: 2.3633, Domain Loss: 1.8018, Class Loss: 0.5615
Epoch 35/50, Loss: 2.1754, Domain Loss: 1.7834, Class Loss: 0.3920
Epoch 36/50, Loss: 2.0816, Domain Loss: 1.7386, Class Loss: 0.3429
Epoch 37/50, Loss: 2.5624, Domain Loss: 2.1497, Class Loss: 0.4128
Epoch 38/50, Loss: 2.5371, Domain Loss: 2.1628, Class Loss: 0.3743
Epoch 39/50, Loss: 2.6923, Domain Loss: 2.3271, Class Loss: 0.3653
Epoch 40/50, Loss: 2.9215, Domain Loss: 2.4584, Class Loss: 0.4631
Epoch 41/50, Loss: 4.3804, Domain Loss: 3.7705, Class Loss: 0.6099
Epoch 42/50, Loss: 2.9929, Domain Loss: 2.4689, Class Loss: 0.5240
Epoch 43/50, Loss: 2.2573, Domain Loss: 1.7769, Class Loss: 0.4804
Epoch 44/50, Loss: 1.8731, Domain Loss: 1.4055, Class Loss: 0.4675
Epoch 45/50, Loss: 2.1408, Domain Loss: 1.6137, Class Loss: 0.5271
Epoch 46/50, Loss: 1.9145, Domain Loss: 1.4498, Class Loss: 0.4647
Epoch 47/50, Loss: 1.9991, Domain Loss: 1.5561, Class Loss: 0.4431
Epoch 48/50, Loss: 1.9741, Domain Loss: 1.4885, Class Loss: 0.4855
Epoch 49/50, Loss: 1.8637, Domain Loss: 1.4287, Class Loss: 0.4350
Epoch 50/50, Loss: 1.7675, Domain Loss: 1.3781, Class Loss: 0.3894
69.60


Epoch 1/50, Loss: 2.2875, Domain Loss: 1.3784, Class Loss: 0.9091
Epoch 2/50, Loss: 1.7107, Domain Loss: 1.3000, Class Loss: 0.4107
Epoch 3/50, Loss: 1.6486, Domain Loss: 1.3302, Class Loss: 0.3184
Epoch 4/50, Loss: 1.9965, Domain Loss: 1.6972, Class Loss: 0.2993
Epoch 5/50, Loss: 7.3822, Domain Loss: 7.0685, Class Loss: 0.3137
Epoch 6/50, Loss: 10.4803, Domain Loss: 9.9911, Class Loss: 0.4892
Epoch 7/50, Loss: 8.3374, Domain Loss: 7.6349, Class Loss: 0.7025
Epoch 8/50, Loss: 7.2097, Domain Loss: 6.3835, Class Loss: 0.8262
Epoch 9/50, Loss: 6.6453, Domain Loss: 5.8693, Class Loss: 0.7760
Epoch 10/50, Loss: 30.0305, Domain Loss: 29.2187, Class Loss: 0.8118
Epoch 11/50, Loss: 12.5554, Domain Loss: 11.9980, Class Loss: 0.5574
Epoch 12/50, Loss: 11.1118, Domain Loss: 10.6134, Class Loss: 0.4985
Epoch 13/50, Loss: 15.7390, Domain Loss: 15.0815, Class Loss: 0.6574
Epoch 14/50, Loss: 14.4153, Domain Loss: 13.6969, Class Loss: 0.7184
Epoch 15/50, Loss: 10.4775, Domain Loss: 9.6888, Class Loss: 0.7886
Epoch 16/50, Loss: 7.1555, Domain Loss: 6.4652, Class Loss: 0.6903
Epoch 17/50, Loss: 4.7108, Domain Loss: 4.0904, Class Loss: 0.6204
Epoch 18/50, Loss: 4.4837, Domain Loss: 3.8090, Class Loss: 0.6747
Epoch 19/50, Loss: 5.9039, Domain Loss: 5.2159, Class Loss: 0.6880
Epoch 20/50, Loss: 4.4746, Domain Loss: 3.9574, Class Loss: 0.5172
Epoch 21/50, Loss: 4.3947, Domain Loss: 3.8657, Class Loss: 0.5290
Epoch 22/50, Loss: 2.9846, Domain Loss: 2.4312, Class Loss: 0.5534
Epoch 23/50, Loss: 2.4397, Domain Loss: 1.9848, Class Loss: 0.4549
Epoch 24/50, Loss: 2.6768, Domain Loss: 2.2337, Class Loss: 0.4431
Epoch 25/50, Loss: 3.2598, Domain Loss: 2.7599, Class Loss: 0.4999
Epoch 26/50, Loss: 3.7941, Domain Loss: 3.0406, Class Loss: 0.7535
Epoch 27/50, Loss: 3.4373, Domain Loss: 2.6373, Class Loss: 0.8000
Epoch 28/50, Loss: 3.0218, Domain Loss: 2.4109, Class Loss: 0.6110
Epoch 29/50, Loss: 2.6354, Domain Loss: 1.8529, Class Loss: 0.7824
Epoch 30/50, Loss: 2.3362, Domain Loss: 1.6459, Class Loss: 0.6903
Epoch 31/50, Loss: 2.2255, Domain Loss: 1.7108, Class Loss: 0.5147
Epoch 32/50, Loss: 2.5318, Domain Loss: 2.0524, Class Loss: 0.4794
Epoch 33/50, Loss: 2.1835, Domain Loss: 1.6938, Class Loss: 0.4897
Epoch 34/50, Loss: 2.3132, Domain Loss: 1.9317, Class Loss: 0.3815
Epoch 35/50, Loss: 2.3285, Domain Loss: 1.9460, Class Loss: 0.3825
Epoch 36/50, Loss: 3.0617, Domain Loss: 2.5943, Class Loss: 0.4673
Epoch 37/50, Loss: 2.5421, Domain Loss: 2.0167, Class Loss: 0.5254
Epoch 38/50, Loss: 2.9712, Domain Loss: 2.4719, Class Loss: 0.4994
Epoch 39/50, Loss: 3.3688, Domain Loss: 2.8813, Class Loss: 0.4875
Epoch 40/50, Loss: 2.4537, Domain Loss: 2.0223, Class Loss: 0.4314
Epoch 41/50, Loss: 2.4588, Domain Loss: 1.9975, Class Loss: 0.4613
Epoch 42/50, Loss: 1.9641, Domain Loss: 1.6303, Class Loss: 0.3338
Epoch 43/50, Loss: 2.1985, Domain Loss: 1.8377, Class Loss: 0.3608
Epoch 44/50, Loss: 2.4884, Domain Loss: 2.0675, Class Loss: 0.4209
Epoch 45/50, Loss: 2.3983, Domain Loss: 1.9956, Class Loss: 0.4027
Epoch 46/50, Loss: 2.5657, Domain Loss: 2.1062, Class Loss: 0.4594
Epoch 47/50, Loss: 2.4273, Domain Loss: 2.0796, Class Loss: 0.3477
Epoch 48/50, Loss: 2.5328, Domain Loss: 2.1530, Class Loss: 0.3798
Epoch 49/50, Loss: 2.5631, Domain Loss: 2.1771, Class Loss: 0.3860
Epoch 50/50, Loss: 2.9586, Domain Loss: 2.5561, Class Loss: 0.4025
24.34


Epoch 1/50, Loss: 2.2337, Domain Loss: 1.3875, Class Loss: 0.8462
Epoch 2/50, Loss: 1.6590, Domain Loss: 1.2758, Class Loss: 0.3832
Epoch 3/50, Loss: 1.6420, Domain Loss: 1.3166, Class Loss: 0.3254
Epoch 4/50, Loss: 2.0740, Domain Loss: 1.7636, Class Loss: 0.3104
Epoch 5/50, Loss: 4.6193, Domain Loss: 4.3280, Class Loss: 0.2913
Epoch 6/50, Loss: 8.1095, Domain Loss: 7.7540, Class Loss: 0.3554
Epoch 7/50, Loss: 13.7222, Domain Loss: 13.0291, Class Loss: 0.6932
Epoch 8/50, Loss: 16.9289, Domain Loss: 15.8941, Class Loss: 1.0349
Epoch 9/50, Loss: 12.2057, Domain Loss: 11.1312, Class Loss: 1.0745
Epoch 10/50, Loss: 9.5945, Domain Loss: 8.4226, Class Loss: 1.1719
Epoch 11/50, Loss: 10.7659, Domain Loss: 9.7188, Class Loss: 1.0471
Epoch 12/50, Loss: 8.0944, Domain Loss: 7.1003, Class Loss: 0.9941
Epoch 13/50, Loss: 4.3937, Domain Loss: 3.5667, Class Loss: 0.8270
Epoch 14/50, Loss: 3.4251, Domain Loss: 2.6564, Class Loss: 0.7687
Epoch 15/50, Loss: 4.0487, Domain Loss: 3.2133, Class Loss: 0.8355
Epoch 16/50, Loss: 4.7866, Domain Loss: 3.8327, Class Loss: 0.9539
Epoch 17/50, Loss: 3.9999, Domain Loss: 3.3427, Class Loss: 0.6571
Epoch 18/50, Loss: 2.4446, Domain Loss: 1.8633, Class Loss: 0.5813
Epoch 19/50, Loss: 2.1364, Domain Loss: 1.5682, Class Loss: 0.5681
Epoch 20/50, Loss: 2.5547, Domain Loss: 1.9671, Class Loss: 0.5877
Epoch 21/50, Loss: 1.8678, Domain Loss: 1.3778, Class Loss: 0.4901
Epoch 22/50, Loss: 2.4895, Domain Loss: 1.7052, Class Loss: 0.7843
Epoch 23/50, Loss: 3.0842, Domain Loss: 2.3136, Class Loss: 0.7706
Epoch 24/50, Loss: 2.4466, Domain Loss: 1.7192, Class Loss: 0.7275
Epoch 25/50, Loss: 2.0920, Domain Loss: 1.5797, Class Loss: 0.5124
Epoch 26/50, Loss: 3.3021, Domain Loss: 2.2705, Class Loss: 1.0317
Epoch 27/50, Loss: 2.3682, Domain Loss: 1.6482, Class Loss: 0.7200
Epoch 28/50, Loss: 2.3829, Domain Loss: 1.6451, Class Loss: 0.7378
Epoch 29/50, Loss: 2.3275, Domain Loss: 1.6512, Class Loss: 0.6763
Epoch 30/50, Loss: 2.2949, Domain Loss: 1.7418, Class Loss: 0.5530
Epoch 31/50, Loss: 2.3347, Domain Loss: 1.8087, Class Loss: 0.5260
Epoch 32/50, Loss: 1.9780, Domain Loss: 1.4852, Class Loss: 0.4928
Epoch 33/50, Loss: 2.1911, Domain Loss: 1.6563, Class Loss: 0.5348
Epoch 34/50, Loss: 2.8078, Domain Loss: 2.0882, Class Loss: 0.7196
Epoch 35/50, Loss: 3.1649, Domain Loss: 2.5068, Class Loss: 0.6581
Epoch 36/50, Loss: 3.7617, Domain Loss: 3.2279, Class Loss: 0.5339
Epoch 37/50, Loss: 3.2558, Domain Loss: 2.8265, Class Loss: 0.4293
Epoch 38/50, Loss: 3.6035, Domain Loss: 2.9042, Class Loss: 0.6993
Epoch 39/50, Loss: 2.9514, Domain Loss: 2.2017, Class Loss: 0.7497
Epoch 40/50, Loss: 2.8460, Domain Loss: 2.2108, Class Loss: 0.6352
Epoch 41/50, Loss: 2.7021, Domain Loss: 2.1270, Class Loss: 0.5751
Epoch 42/50, Loss: 2.7490, Domain Loss: 2.2424, Class Loss: 0.5066
Epoch 43/50, Loss: 2.6562, Domain Loss: 2.1895, Class Loss: 0.4667
Epoch 44/50, Loss: 2.5502, Domain Loss: 2.0819, Class Loss: 0.4683
Epoch 45/50, Loss: 2.5333, Domain Loss: 2.0610, Class Loss: 0.4723
Epoch 46/50, Loss: 2.6185, Domain Loss: 2.1684, Class Loss: 0.4501
Epoch 47/50, Loss: 1.9202, Domain Loss: 1.5167, Class Loss: 0.4035
Epoch 48/50, Loss: 1.7551, Domain Loss: 1.3946, Class Loss: 0.3604
Epoch 49/50, Loss: 1.7588, Domain Loss: 1.3878, Class Loss: 0.3710
Epoch 50/50, Loss: 1.9189, Domain Loss: 1.5320, Class Loss: 0.3870
50.90


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2469, Domain Loss: 1.3732, Class Loss: 0.8737
Epoch 2/50, Loss: 1.7247, Domain Loss: 1.2746, Class Loss: 0.4500
Epoch 3/50, Loss: 1.5767, Domain Loss: 1.2348, Class Loss: 0.3419
Epoch 4/50, Loss: 3.5687, Domain Loss: 3.2334, Class Loss: 0.3353
Epoch 5/50, Loss: 4.9612, Domain Loss: 4.6256, Class Loss: 0.3356
Epoch 6/50, Loss: 12.3418, Domain Loss: 11.6365, Class Loss: 0.7053
Epoch 7/50, Loss: 17.9398, Domain Loss: 17.3525, Class Loss: 0.5873
Epoch 8/50, Loss: 9.6603, Domain Loss: 9.1464, Class Loss: 0.5139
Epoch 9/50, Loss: 4.7483, Domain Loss: 4.3298, Class Loss: 0.4185
Epoch 10/50, Loss: 2.4990, Domain Loss: 2.0317, Class Loss: 0.4673
Epoch 11/50, Loss: 2.4283, Domain Loss: 1.9879, Class Loss: 0.4404
Epoch 12/50, Loss: 2.2159, Domain Loss: 1.7860, Class Loss: 0.4299
Epoch 13/50, Loss: 3.4347, Domain Loss: 3.0194, Class Loss: 0.4153
Epoch 14/50, Loss: 11.2623, Domain Loss: 10.6179, Class Loss: 0.6444
Epoch 15/50, Loss: 7.9464, Domain Loss: 7.4958, Class Loss: 0.4506
Epoch 16/50, Loss: 3.6317, Domain Loss: 3.2653, Class Loss: 0.3664
Epoch 17/50, Loss: 2.1323, Domain Loss: 1.7680, Class Loss: 0.3643
Epoch 18/50, Loss: 2.1298, Domain Loss: 1.6821, Class Loss: 0.4477
Epoch 19/50, Loss: 2.0864, Domain Loss: 1.6354, Class Loss: 0.4510
Epoch 20/50, Loss: 1.9681, Domain Loss: 1.5601, Class Loss: 0.4080
Epoch 21/50, Loss: 2.2234, Domain Loss: 1.8053, Class Loss: 0.4181
Epoch 22/50, Loss: 2.2181, Domain Loss: 1.7578, Class Loss: 0.4603
Epoch 23/50, Loss: 1.8845, Domain Loss: 1.5243, Class Loss: 0.3602
Epoch 24/50, Loss: 1.8487, Domain Loss: 1.5147, Class Loss: 0.3340
Epoch 25/50, Loss: 1.8181, Domain Loss: 1.4941, Class Loss: 0.3240
Epoch 26/50, Loss: 1.6606, Domain Loss: 1.3742, Class Loss: 0.2863
Epoch 27/50, Loss: 1.7769, Domain Loss: 1.4977, Class Loss: 0.2791
Epoch 28/50, Loss: 1.6023, Domain Loss: 1.3740, Class Loss: 0.2283
Epoch 29/50, Loss: 1.7456, Domain Loss: 1.5012, Class Loss: 0.2443
Epoch 30/50, Loss: 1.8522, Domain Loss: 1.5978, Class Loss: 0.2543
Epoch 31/50, Loss: 1.6943, Domain Loss: 1.5178, Class Loss: 0.1765
Epoch 32/50, Loss: 1.7869, Domain Loss: 1.6352, Class Loss: 0.1516
Epoch 33/50, Loss: 1.6691, Domain Loss: 1.5462, Class Loss: 0.1229
Epoch 34/50, Loss: 1.6506, Domain Loss: 1.5523, Class Loss: 0.0983
Epoch 35/50, Loss: 1.5852, Domain Loss: 1.4700, Class Loss: 0.1152
Epoch 36/50, Loss: 1.5227, Domain Loss: 1.4279, Class Loss: 0.0948
Epoch 37/50, Loss: 1.5429, Domain Loss: 1.4436, Class Loss: 0.0993
Epoch 38/50, Loss: 1.5066, Domain Loss: 1.4357, Class Loss: 0.0709
Epoch 39/50, Loss: 1.5532, Domain Loss: 1.4907, Class Loss: 0.0626
Epoch 40/50, Loss: 1.6944, Domain Loss: 1.6038, Class Loss: 0.0905
Epoch 41/50, Loss: 1.4839, Domain Loss: 1.4423, Class Loss: 0.0416
Epoch 42/50, Loss: 1.4309, Domain Loss: 1.3997, Class Loss: 0.0312
Epoch 43/50, Loss: 1.4286, Domain Loss: 1.4143, Class Loss: 0.0143
Epoch 44/50, Loss: 1.4064, Domain Loss: 1.3962, Class Loss: 0.0101
Epoch 45/50, Loss: 1.4126, Domain Loss: 1.3958, Class Loss: 0.0168
Epoch 46/50, Loss: 1.4111, Domain Loss: 1.3972, Class Loss: 0.0139
Epoch 47/50, Loss: 1.4502, Domain Loss: 1.4291, Class Loss: 0.0211
Epoch 48/50, Loss: 1.4184, Domain Loss: 1.3683, Class Loss: 0.0501
Epoch 49/50, Loss: 1.5080, Domain Loss: 1.3792, Class Loss: 0.1288
Epoch 50/50, Loss: 1.6721, Domain Loss: 1.4603, Class Loss: 0.2118
52.16


Source performance:
68.47 73.27 68.49 64.36 
Target performance:
58.09 61.54 57.79 50.21 

Per-class target performance: 90.77 6.79 76.65 56.93 Deep CORAL
Deep CORAL Run 1/10
Epoch 1: Source Val Acc = 0.7194, Target Val Acc = 0.7116
Epoch 2: Source Val Acc = 0.9634, Target Val Acc = 0.7452
Epoch 3: Source Val Acc = 0.9448, Target Val Acc = 0.7134
Epoch 4: Source Val Acc = 0.9826, Target Val Acc = 0.7242
Epoch 5: Source Val Acc = 0.9532, Target Val Acc = 0.7758
Epoch 6: Source Val Acc = 0.9934, Target Val Acc = 0.8375
Epoch 7: Source Val Acc = 0.9664, Target Val Acc = 0.7170
Epoch 8: Source Val Acc = 0.9802, Target Val Acc = 0.7290
Epoch 9: Source Val Acc = 0.9844, Target Val Acc = 0.7746
Epoch 10: Source Val Acc = 0.9802, Target Val Acc = 0.5558
Epoch 11: Source Val Acc = 0.9994, Target Val Acc = 0.6799
Epoch 12: Source Val Acc = 0.9910, Target Val Acc = 0.8471
Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 0.9023
Epoch 14: Source Val Acc = 0.9988, Target Val Acc = 0.9143
Epoch 15: Source Val Acc = 0.9988, Target Val Acc = 0.9137
Epoch 16: Source Val Acc = 0.9934, Target Val Acc = 0.9251
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.9934, Target Val Acc = 0.9251

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.7956, Target Val Acc = 0.5252
Epoch 2: Source Val Acc = 0.9430, Target Val Acc = 0.7224
Epoch 3: Source Val Acc = 0.8195, Target Val Acc = 0.5845
Epoch 4: Source Val Acc = 0.9143, Target Val Acc = 0.6996
Epoch 5: Source Val Acc = 0.5941, Target Val Acc = 0.3405
Epoch 6: Source Val Acc = 0.9850, Target Val Acc = 0.8363
Epoch 7: Source Val Acc = 0.9940, Target Val Acc = 0.9281
Epoch 8: Source Val Acc = 0.9922, Target Val Acc = 0.9167
Epoch 9: Source Val Acc = 0.9856, Target Val Acc = 0.8309
Epoch 10: Source Val Acc = 0.9826, Target Val Acc = 0.9047
Epoch 11: Source Val Acc = 0.9976, Target Val Acc = 0.8519
Epoch 12: Source Val Acc = 0.9568, Target Val Acc = 0.6996
Epoch 13: Source Val Acc = 0.9856, Target Val Acc = 0.9155
Epoch 14: Source Val Acc = 0.9982, Target Val Acc = 0.8711
Epoch 15: Source Val Acc = 0.9922, Target Val Acc = 0.8999
Epoch 16: Source Val Acc = 0.9964, Target Val Acc = 0.7416
Epoch 17: Source Val Acc = 0.9406, Target Val Acc = 0.7242
Epoch 18: Source Val Acc = 0.9898, Target Val Acc = 0.7860
Epoch 19: Source Val Acc = 0.9886, Target Val Acc = 0.7848
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9886, Target Val Acc = 0.7848

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.7320, Target Val Acc = 0.5725
Epoch 2: Source Val Acc = 0.8453, Target Val Acc = 0.8165
Epoch 3: Source Val Acc = 0.9227, Target Val Acc = 0.6697
Epoch 4: Source Val Acc = 0.8819, Target Val Acc = 0.6469
Epoch 5: Source Val Acc = 0.9706, Target Val Acc = 0.8171
Epoch 6: Source Val Acc = 0.9682, Target Val Acc = 0.7980
Epoch 7: Source Val Acc = 0.9688, Target Val Acc = 0.6493
Epoch 8: Source Val Acc = 0.2536, Target Val Acc = 0.2494
Epoch 9: Source Val Acc = 0.9688, Target Val Acc = 0.5641
Epoch 10: Source Val Acc = 0.9934, Target Val Acc = 0.9430
Epoch 11: Source Val Acc = 0.9814, Target Val Acc = 0.9454
Epoch 12: Source Val Acc = 0.9970, Target Val Acc = 0.8819
Epoch 13: Source Val Acc = 0.5534, Target Val Acc = 0.3453
Epoch 14: Source Val Acc = 0.9712, Target Val Acc = 0.7782
Epoch 15: Source Val Acc = 1.0000, Target Val Acc = 0.6481
Epoch 16: Source Val Acc = 0.7230, Target Val Acc = 0.5312
Epoch 17: Source Val Acc = 0.7614, Target Val Acc = 0.7806
Epoch 18: Source Val Acc = 0.9850, Target Val Acc = 0.8561
Epoch 19: Source Val Acc = 0.9988, Target Val Acc = 0.7758
Epoch 20: Source Val Acc = 0.9928, Target Val Acc = 0.9107
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.9928, Target Val Acc = 0.9107

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.8010, Target Val Acc = 0.6235
Epoch 2: Source Val Acc = 0.7572, Target Val Acc = 0.7686
Epoch 3: Source Val Acc = 0.9730, Target Val Acc = 0.7998
Epoch 4: Source Val Acc = 0.6871, Target Val Acc = 0.4928
Epoch 5: Source Val Acc = 0.7794, Target Val Acc = 0.7080
Epoch 6: Source Val Acc = 0.9868, Target Val Acc = 0.9125
Epoch 7: Source Val Acc = 0.9970, Target Val Acc = 0.8969
Epoch 8: Source Val Acc = 0.9934, Target Val Acc = 0.8237
Epoch 9: Source Val Acc = 0.9964, Target Val Acc = 0.8747
Epoch 10: Source Val Acc = 0.9958, Target Val Acc = 0.8303
Epoch 11: Source Val Acc = 0.9958, Target Val Acc = 0.8357
Epoch 12: Source Val Acc = 0.9964, Target Val Acc = 0.8699
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.9964, Target Val Acc = 0.8699

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.7590, Target Val Acc = 0.6169
Epoch 2: Source Val Acc = 0.9598, Target Val Acc = 0.7308
Epoch 3: Source Val Acc = 0.8927, Target Val Acc = 0.7740
Epoch 4: Source Val Acc = 0.9970, Target Val Acc = 0.7614
Epoch 5: Source Val Acc = 0.9976, Target Val Acc = 0.9568
Epoch 6: Source Val Acc = 0.9958, Target Val Acc = 0.9191
Epoch 7: Source Val Acc = 0.7476, Target Val Acc = 0.4712
Epoch 8: Source Val Acc = 0.9934, Target Val Acc = 0.7584
Epoch 9: Source Val Acc = 0.9970, Target Val Acc = 0.7872
Epoch 10: Source Val Acc = 0.9311, Target Val Acc = 0.9329
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.9311, Target Val Acc = 0.9329

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.7674, Target Val Acc = 0.5582
Epoch 2: Source Val Acc = 0.7956, Target Val Acc = 0.6403
Epoch 3: Source Val Acc = 0.9125, Target Val Acc = 0.7644
Epoch 4: Source Val Acc = 0.7380, Target Val Acc = 0.7080
Epoch 5: Source Val Acc = 0.9682, Target Val Acc = 0.7332
Epoch 6: Source Val Acc = 0.8735, Target Val Acc = 0.6439
Epoch 7: Source Val Acc = 0.9628, Target Val Acc = 0.7872
Epoch 8: Source Val Acc = 0.9916, Target Val Acc = 0.6421
Epoch 9: Source Val Acc = 0.8879, Target Val Acc = 0.7140
Epoch 10: Source Val Acc = 0.9880, Target Val Acc = 0.7818
Epoch 11: Source Val Acc = 0.9964, Target Val Acc = 0.7248
Epoch 12: Source Val Acc = 0.9970, Target Val Acc = 0.7446
Epoch 13: Source Val Acc = 0.9904, Target Val Acc = 0.8513
Epoch 14: Source Val Acc = 0.6397, Target Val Acc = 0.4341
Epoch 15: Source Val Acc = 0.9275, Target Val Acc = 0.8429
Epoch 16: Source Val Acc = 0.9748, Target Val Acc = 0.7404
Epoch 17: Source Val Acc = 0.9311, Target Val Acc = 0.9077
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 0.9311, Target Val Acc = 0.9077

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.6211, Target Val Acc = 0.5851
Epoch 2: Source Val Acc = 0.9287, Target Val Acc = 0.8609
Epoch 3: Source Val Acc = 0.8387, Target Val Acc = 0.5629
Epoch 4: Source Val Acc = 0.7740, Target Val Acc = 0.5210
Epoch 5: Source Val Acc = 0.9946, Target Val Acc = 0.7680
Epoch 6: Source Val Acc = 0.9964, Target Val Acc = 0.7044
Epoch 7: Source Val Acc = 0.9958, Target Val Acc = 0.9694
Epoch 8: Source Val Acc = 0.7752, Target Val Acc = 0.8255
Epoch 9: Source Val Acc = 0.9772, Target Val Acc = 0.7140
Epoch 10: Source Val Acc = 0.9532, Target Val Acc = 0.8207
Epoch 11: Source Val Acc = 1.0000, Target Val Acc = 0.7308
Epoch 12: Source Val Acc = 0.9982, Target Val Acc = 0.9005
Epoch 13: Source Val Acc = 1.0000, Target Val Acc = 0.8813
Epoch 14: Source Val Acc = 0.9988, Target Val Acc = 0.7950
Epoch 15: Source Val Acc = 1.0000, Target Val Acc = 0.9095
Epoch 16: Source Val Acc = 0.9994, Target Val Acc = 0.8735
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.9994, Target Val Acc = 0.8735

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.0156, Target Val Acc = 0.1547
Epoch 2: Source Val Acc = 0.6613, Target Val Acc = 0.6025
Epoch 3: Source Val Acc = 0.9634, Target Val Acc = 0.7176
Epoch 4: Source Val Acc = 0.6505, Target Val Acc = 0.5108
Epoch 5: Source Val Acc = 0.9424, Target Val Acc = 0.7026
Epoch 6: Source Val Acc = 0.9814, Target Val Acc = 0.7182
Epoch 7: Source Val Acc = 0.5857, Target Val Acc = 0.5012
Epoch 8: Source Val Acc = 0.7662, Target Val Acc = 0.5198
Epoch 9: Source Val Acc = 0.9688, Target Val Acc = 0.8597
Epoch 10: Source Val Acc = 0.9970, Target Val Acc = 0.7698
Epoch 11: Source Val Acc = 0.6745, Target Val Acc = 0.4988
Epoch 12: Source Val Acc = 0.9910, Target Val Acc = 0.9538
Epoch 13: Source Val Acc = 0.9994, Target Val Acc = 0.9263
Epoch 14: Source Val Acc = 0.9958, Target Val Acc = 0.7932
Epoch 15: Source Val Acc = 1.0000, Target Val Acc = 0.7356
Epoch 16: Source Val Acc = 0.9994, Target Val Acc = 0.9137
Epoch 17: Source Val Acc = 0.0042, Target Val Acc = 0.1265
Epoch 18: Source Val Acc = 0.9628, Target Val Acc = 0.8597
Epoch 19: Source Val Acc = 0.9886, Target Val Acc = 0.7914
Epoch 20: Source Val Acc = 0.9850, Target Val Acc = 0.9017
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.9850, Target Val Acc = 0.9017

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.9412, Target Val Acc = 0.7068
Epoch 2: Source Val Acc = 0.9706, Target Val Acc = 0.7086
Epoch 3: Source Val Acc = 0.7224, Target Val Acc = 0.5132
Epoch 4: Source Val Acc = 0.8627, Target Val Acc = 0.5821
Epoch 5: Source Val Acc = 0.9976, Target Val Acc = 0.8435
Epoch 6: Source Val Acc = 0.5240, Target Val Acc = 0.5012
Epoch 7: Source Val Acc = 0.9934, Target Val Acc = 0.8747
Epoch 8: Source Val Acc = 0.9928, Target Val Acc = 0.8429
Epoch 9: Source Val Acc = 0.9940, Target Val Acc = 0.6121
Epoch 10: Source Val Acc = 1.0000, Target Val Acc = 0.8771
Epoch 11: Source Val Acc = 1.0000, Target Val Acc = 0.8477
Epoch 12: Source Val Acc = 0.9982, Target Val Acc = 0.5821
Epoch 13: Source Val Acc = 1.0000, Target Val Acc = 0.9514
Epoch 14: Source Val Acc = 1.0000, Target Val Acc = 0.8094
Epoch 15: Source Val Acc = 1.0000, Target Val Acc = 0.8369
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 1.0000, Target Val Acc = 0.8369

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.7674, Target Val Acc = 0.4976
Epoch 2: Source Val Acc = 0.6343, Target Val Acc = 0.5965
Epoch 3: Source Val Acc = 0.9299, Target Val Acc = 0.6253
Epoch 4: Source Val Acc = 0.8645, Target Val Acc = 0.9053
Epoch 5: Source Val Acc = 0.9652, Target Val Acc = 0.6871
Epoch 6: Source Val Acc = 0.9910, Target Val Acc = 0.6103
Epoch 7: Source Val Acc = 0.9838, Target Val Acc = 0.6271
Epoch 8: Source Val Acc = 0.9928, Target Val Acc = 0.8879
Epoch 9: Source Val Acc = 0.9970, Target Val Acc = 0.8106
Epoch 10: Source Val Acc = 0.9682, Target Val Acc = 0.8369
Epoch 11: Source Val Acc = 0.9958, Target Val Acc = 0.7770
Epoch 12: Source Val Acc = 0.9886, Target Val Acc = 0.7032
Epoch 13: Source Val Acc = 0.9964, Target Val Acc = 0.7806
Epoch 14: Source Val Acc = 0.9982, Target Val Acc = 0.9083
Epoch 15: Source Val Acc = 0.9388, Target Val Acc = 0.8699
Epoch 16: Source Val Acc = 0.9844, Target Val Acc = 0.8699
Epoch 17: Source Val Acc = 0.9922, Target Val Acc = 0.8411
Epoch 18: Source Val Acc = 0.9988, Target Val Acc = 0.9412
Epoch 19: Source Val Acc = 0.9616, Target Val Acc = 0.8495
Epoch 20: Source Val Acc = 0.9922, Target Val Acc = 0.6739
Epoch 21: Source Val Acc = 0.9874, Target Val Acc = 0.7092
Epoch 22: Source Val Acc = 0.9622, Target Val Acc = 0.8699
Epoch 23: Source Val Acc = 0.9970, Target Val Acc = 0.9359
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.9970, Target Val Acc = 0.9359

Deep CORAL: Average Source Val Acc = 0.9815, Average Target Val Acc = 0.8879
STAR

Run 1/10
Epoch [1/50], Class Loss: 1.5626, Discrepancy Loss: 0.1204
Epoch [2/50], Class Loss: 0.8124, Discrepancy Loss: 0.0876
Epoch [3/50], Class Loss: 0.1969, Discrepancy Loss: 0.0683
Epoch [4/50], Class Loss: 0.0662, Discrepancy Loss: 0.0229
Epoch [5/50], Class Loss: 0.1212, Discrepancy Loss: 0.0300
Epoch [6/50], Class Loss: 0.0308, Discrepancy Loss: 0.0110
Epoch [7/50], Class Loss: 0.0110, Discrepancy Loss: 0.0077
Epoch [8/50], Class Loss: 0.0166, Discrepancy Loss: 0.0076
Epoch [9/50], Class Loss: 0.0232, Discrepancy Loss: 0.0063
Epoch [10/50], Class Loss: 0.1147, Discrepancy Loss: 0.0253
Epoch [11/50], Class Loss: 0.0094, Discrepancy Loss: 0.0099
Epoch [12/50], Class Loss: 0.0088, Discrepancy Loss: 0.0086
Epoch [13/50], Class Loss: 0.0060, Discrepancy Loss: 0.0078
Epoch [14/50], Class Loss: 0.0089, Discrepancy Loss: 0.0099
Epoch [15/50], Class Loss: 0.0061, Discrepancy Loss: 0.0123
Epoch [16/50], Class Loss: 0.0054, Discrepancy Loss: 0.0078
Epoch [17/50], Class Loss: 0.0055, Discrepancy Loss: 0.0071
Epoch [18/50], Class Loss: 0.0863, Discrepancy Loss: 0.0091
Epoch [19/50], Class Loss: 0.0094, Discrepancy Loss: 0.0179
Epoch [20/50], Class Loss: 0.0171, Discrepancy Loss: 0.0117
Epoch [21/50], Class Loss: 0.0048, Discrepancy Loss: 0.0078
Epoch [22/50], Class Loss: 0.0939, Discrepancy Loss: 0.0081
Epoch [23/50], Class Loss: 0.1296, Discrepancy Loss: 0.0094
Epoch [24/50], Class Loss: 0.0043, Discrepancy Loss: 0.0107
Epoch [25/50], Class Loss: 0.0747, Discrepancy Loss: 0.0098
Epoch [26/50], Class Loss: 0.0030, Discrepancy Loss: 0.0086
Epoch [27/50], Class Loss: 0.0104, Discrepancy Loss: 0.0082
Epoch [28/50], Class Loss: 0.0226, Discrepancy Loss: 0.0062
Epoch [29/50], Class Loss: 0.0096, Discrepancy Loss: 0.0071
Epoch [30/50], Class Loss: 0.0089, Discrepancy Loss: 0.0073
Epoch [31/50], Class Loss: 0.0032, Discrepancy Loss: 0.0079
Epoch [32/50], Class Loss: 0.0202, Discrepancy Loss: 0.0064
Epoch [33/50], Class Loss: 0.0052, Discrepancy Loss: 0.0065
Epoch [34/50], Class Loss: 0.0053, Discrepancy Loss: 0.0085
Epoch [35/50], Class Loss: 0.0026, Discrepancy Loss: 0.0064
Epoch [36/50], Class Loss: 0.0041, Discrepancy Loss: 0.0065
Epoch [37/50], Class Loss: 0.0690, Discrepancy Loss: 0.0068
Epoch [38/50], Class Loss: 0.0072, Discrepancy Loss: 0.0062
Epoch [39/50], Class Loss: 0.0026, Discrepancy Loss: 0.0079
Epoch [40/50], Class Loss: 0.0050, Discrepancy Loss: 0.0090
Epoch [41/50], Class Loss: 0.0045, Discrepancy Loss: 0.0067
Epoch [42/50], Class Loss: 0.0069, Discrepancy Loss: 0.0063
Epoch [43/50], Class Loss: 0.0063, Discrepancy Loss: 0.0075
Epoch [44/50], Class Loss: 0.0023, Discrepancy Loss: 0.0069
Epoch [45/50], Class Loss: 0.1225, Discrepancy Loss: 0.0063
Epoch [46/50], Class Loss: 0.0021, Discrepancy Loss: 0.0066
Epoch [47/50], Class Loss: 0.0057, Discrepancy Loss: 0.0063
Epoch [48/50], Class Loss: 0.0040, Discrepancy Loss: 0.0066
Epoch [49/50], Class Loss: 0.0151, Discrepancy Loss: 0.0060
Epoch [50/50], Class Loss: 0.0956, Discrepancy Loss: 0.0070
Source Domain Performance - Accuracy: 90.59%, Precision: 93.12%, Recall: 90.96%, F1 Score: 90.67%
Target Domain Performance - Accuracy: 98.62%, Precision: 98.61%, Recall: 98.60%, F1 Score: 98.60%

Run 2/10
Epoch [1/50], Class Loss: 2.1363, Discrepancy Loss: 0.1248
Epoch [2/50], Class Loss: 1.0199, Discrepancy Loss: 0.1076
Epoch [3/50], Class Loss: 0.7016, Discrepancy Loss: 0.1020
Epoch [4/50], Class Loss: 0.5972, Discrepancy Loss: 0.0847
Epoch [5/50], Class Loss: 0.4107, Discrepancy Loss: 0.0727
Epoch [6/50], Class Loss: 0.1849, Discrepancy Loss: 0.0491
Epoch [7/50], Class Loss: 0.0358, Discrepancy Loss: 0.0187
Epoch [8/50], Class Loss: 0.0267, Discrepancy Loss: 0.0183
Epoch [9/50], Class Loss: 0.0461, Discrepancy Loss: 0.0247
Epoch [10/50], Class Loss: 0.0182, Discrepancy Loss: 0.0100
Epoch [11/50], Class Loss: 0.0080, Discrepancy Loss: 0.0094
Epoch [12/50], Class Loss: 0.0057, Discrepancy Loss: 0.0090
Epoch [13/50], Class Loss: 0.0535, Discrepancy Loss: 0.0067
Epoch [14/50], Class Loss: 0.0030, Discrepancy Loss: 0.0084
Epoch [15/50], Class Loss: 0.0246, Discrepancy Loss: 0.0069
Epoch [16/50], Class Loss: 0.0087, Discrepancy Loss: 0.0104
Epoch [17/50], Class Loss: 0.0048, Discrepancy Loss: 0.0098
Epoch [18/50], Class Loss: 0.0273, Discrepancy Loss: 0.0058
Epoch [19/50], Class Loss: 0.0503, Discrepancy Loss: 0.0119
Epoch [20/50], Class Loss: 0.0061, Discrepancy Loss: 0.0058
Epoch [21/50], Class Loss: 0.0024, Discrepancy Loss: 0.0058
Epoch [22/50], Class Loss: 0.0144, Discrepancy Loss: 0.0062
Epoch [23/50], Class Loss: 0.0024, Discrepancy Loss: 0.0062
Epoch [24/50], Class Loss: 0.0017, Discrepancy Loss: 0.0046
Epoch [25/50], Class Loss: 0.0024, Discrepancy Loss: 0.0047
Epoch [26/50], Class Loss: 0.0117, Discrepancy Loss: 0.0060
Epoch [27/50], Class Loss: 0.0069, Discrepancy Loss: 0.0035
Epoch [28/50], Class Loss: 0.0032, Discrepancy Loss: 0.0058
Epoch [29/50], Class Loss: 0.0018, Discrepancy Loss: 0.0051
Epoch [30/50], Class Loss: 0.0014, Discrepancy Loss: 0.0044
Epoch [31/50], Class Loss: 0.0021, Discrepancy Loss: 0.0054
Epoch [32/50], Class Loss: 0.0013, Discrepancy Loss: 0.0053
Epoch [33/50], Class Loss: 0.0016, Discrepancy Loss: 0.0046
Epoch [34/50], Class Loss: 0.0023, Discrepancy Loss: 0.0051
Epoch [35/50], Class Loss: 0.0014, Discrepancy Loss: 0.0053
Epoch [36/50], Class Loss: 0.0012, Discrepancy Loss: 0.0049
Epoch [37/50], Class Loss: 0.0037, Discrepancy Loss: 0.0065
Epoch [38/50], Class Loss: 0.0013, Discrepancy Loss: 0.0049
Epoch [39/50], Class Loss: 0.0011, Discrepancy Loss: 0.0040
Epoch [40/50], Class Loss: 0.0158, Discrepancy Loss: 0.0055
Epoch [41/50], Class Loss: 0.0016, Discrepancy Loss: 0.0041
Epoch [42/50], Class Loss: 0.0067, Discrepancy Loss: 0.0050
Epoch [43/50], Class Loss: 0.0010, Discrepancy Loss: 0.0056
Epoch [44/50], Class Loss: 0.0012, Discrepancy Loss: 0.0048
Epoch [45/50], Class Loss: 0.0030, Discrepancy Loss: 0.0049
Epoch [46/50], Class Loss: 0.0018, Discrepancy Loss: 0.0047
Epoch [47/50], Class Loss: 0.0021, Discrepancy Loss: 0.0052
Epoch [48/50], Class Loss: 0.0073, Discrepancy Loss: 0.0042
Epoch [49/50], Class Loss: 0.0034, Discrepancy Loss: 0.0040
Epoch [50/50], Class Loss: 0.0021, Discrepancy Loss: 0.0077
Source Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.77%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 98.20%, Precision: 98.19%, Recall: 98.20%, F1 Score: 98.17%

Run 3/10
Epoch [1/50], Class Loss: 1.7304, Discrepancy Loss: 0.1260
Epoch [2/50], Class Loss: 0.7108, Discrepancy Loss: 0.0811
Epoch [3/50], Class Loss: 0.4208, Discrepancy Loss: 0.0614
Epoch [4/50], Class Loss: 0.2028, Discrepancy Loss: 0.0516
Epoch [5/50], Class Loss: 0.1666, Discrepancy Loss: 0.0362
Epoch [6/50], Class Loss: 0.2074, Discrepancy Loss: 0.0348
Epoch [7/50], Class Loss: 0.0779, Discrepancy Loss: 0.0284
Epoch [8/50], Class Loss: 0.0495, Discrepancy Loss: 0.0199
Epoch [9/50], Class Loss: 0.0210, Discrepancy Loss: 0.0127
Epoch [10/50], Class Loss: 0.0062, Discrepancy Loss: 0.0073
Epoch [11/50], Class Loss: 0.0032, Discrepancy Loss: 0.0070
Epoch [12/50], Class Loss: 0.0017, Discrepancy Loss: 0.0061
Epoch [13/50], Class Loss: 0.0022, Discrepancy Loss: 0.0048
Epoch [14/50], Class Loss: 0.0011, Discrepancy Loss: 0.0059
Epoch [15/50], Class Loss: 0.0014, Discrepancy Loss: 0.0053
Epoch [16/50], Class Loss: 0.0011, Discrepancy Loss: 0.0062
Epoch [17/50], Class Loss: 0.0021, Discrepancy Loss: 0.0042
Epoch [18/50], Class Loss: 0.0003, Discrepancy Loss: 0.0039
Epoch [19/50], Class Loss: 0.0012, Discrepancy Loss: 0.0048
Epoch [20/50], Class Loss: 0.0015, Discrepancy Loss: 0.0048
Epoch [21/50], Class Loss: 0.2306, Discrepancy Loss: 0.0049
Epoch [22/50], Class Loss: 0.0007, Discrepancy Loss: 0.0036
Epoch [23/50], Class Loss: 0.0016, Discrepancy Loss: 0.0045
Epoch [24/50], Class Loss: 0.0014, Discrepancy Loss: 0.0040
Epoch [25/50], Class Loss: 0.0003, Discrepancy Loss: 0.0043
Epoch [26/50], Class Loss: 0.0005, Discrepancy Loss: 0.0043
Epoch [27/50], Class Loss: 0.0040, Discrepancy Loss: 0.0041
Epoch [28/50], Class Loss: 0.0082, Discrepancy Loss: 0.0036
Epoch [29/50], Class Loss: 0.0007, Discrepancy Loss: 0.0052
Epoch [30/50], Class Loss: 0.0004, Discrepancy Loss: 0.0049
Epoch [31/50], Class Loss: 0.0081, Discrepancy Loss: 0.0034
Epoch [32/50], Class Loss: 0.0008, Discrepancy Loss: 0.0057
Epoch [33/50], Class Loss: 0.0002, Discrepancy Loss: 0.0042
Epoch [34/50], Class Loss: 0.0005, Discrepancy Loss: 0.0050
Epoch [35/50], Class Loss: 0.0009, Discrepancy Loss: 0.0031
Epoch [36/50], Class Loss: 0.0007, Discrepancy Loss: 0.0044
Epoch [37/50], Class Loss: 0.0017, Discrepancy Loss: 0.0044
Epoch [38/50], Class Loss: 0.1581, Discrepancy Loss: 0.0043
Epoch [39/50], Class Loss: 0.0003, Discrepancy Loss: 0.0038
Epoch [40/50], Class Loss: 0.0004, Discrepancy Loss: 0.0039
Epoch [41/50], Class Loss: 0.0003, Discrepancy Loss: 0.0054
Epoch [42/50], Class Loss: 0.0021, Discrepancy Loss: 0.0045
Epoch [43/50], Class Loss: 0.0004, Discrepancy Loss: 0.0045
Epoch [44/50], Class Loss: 0.0010, Discrepancy Loss: 0.0045
Epoch [45/50], Class Loss: 0.0007, Discrepancy Loss: 0.0039
Epoch [46/50], Class Loss: 0.0003, Discrepancy Loss: 0.0046
Epoch [47/50], Class Loss: 0.0004, Discrepancy Loss: 0.0042
Epoch [48/50], Class Loss: 0.0035, Discrepancy Loss: 0.0037
Epoch [49/50], Class Loss: 0.0004, Discrepancy Loss: 0.0039
Epoch [50/50], Class Loss: 0.0262, Discrepancy Loss: 0.0049
Source Domain Performance - Accuracy: 99.28%, Precision: 99.27%, Recall: 99.30%, F1 Score: 99.28%
Target Domain Performance - Accuracy: 99.22%, Precision: 99.20%, Recall: 99.21%, F1 Score: 99.21%

Run 4/10
Epoch [1/50], Class Loss: 1.5332, Discrepancy Loss: 0.1328
Epoch [2/50], Class Loss: 0.8947, Discrepancy Loss: 0.1040
Epoch [3/50], Class Loss: 0.5792, Discrepancy Loss: 0.0800
Epoch [4/50], Class Loss: 0.2822, Discrepancy Loss: 0.0646
Epoch [5/50], Class Loss: 0.4471, Discrepancy Loss: 0.0504
Epoch [6/50], Class Loss: 0.3645, Discrepancy Loss: 0.0871
Epoch [7/50], Class Loss: 0.1640, Discrepancy Loss: 0.0400
Epoch [8/50], Class Loss: 0.1804, Discrepancy Loss: 0.0359
Epoch [9/50], Class Loss: 0.2068, Discrepancy Loss: 0.0235
Epoch [10/50], Class Loss: 0.0892, Discrepancy Loss: 0.0316
Epoch [11/50], Class Loss: 0.0373, Discrepancy Loss: 0.0208
Epoch [12/50], Class Loss: 0.0203, Discrepancy Loss: 0.0160
Epoch [13/50], Class Loss: 0.0087, Discrepancy Loss: 0.0164
Epoch [14/50], Class Loss: 0.0257, Discrepancy Loss: 0.0148
Epoch [15/50], Class Loss: 0.0272, Discrepancy Loss: 0.0148
Epoch [16/50], Class Loss: 0.0216, Discrepancy Loss: 0.0116
Epoch [17/50], Class Loss: 0.0100, Discrepancy Loss: 0.0097
Epoch [18/50], Class Loss: 0.0103, Discrepancy Loss: 0.0102
Epoch [19/50], Class Loss: 0.0075, Discrepancy Loss: 0.0121
Epoch [20/50], Class Loss: 0.0070, Discrepancy Loss: 0.0127
Epoch [21/50], Class Loss: 0.0035, Discrepancy Loss: 0.0105
Epoch [22/50], Class Loss: 0.0490, Discrepancy Loss: 0.0108
Epoch [23/50], Class Loss: 0.0067, Discrepancy Loss: 0.0137
Epoch [24/50], Class Loss: 0.0054, Discrepancy Loss: 0.0113
Epoch [25/50], Class Loss: 0.0047, Discrepancy Loss: 0.0133
Epoch [26/50], Class Loss: 0.0049, Discrepancy Loss: 0.0150
Epoch [27/50], Class Loss: 0.0051, Discrepancy Loss: 0.0111
Epoch [28/50], Class Loss: 0.0069, Discrepancy Loss: 0.0093
Epoch [29/50], Class Loss: 0.0094, Discrepancy Loss: 0.0125
Epoch [30/50], Class Loss: 0.0079, Discrepancy Loss: 0.0134
Epoch [31/50], Class Loss: 0.0049, Discrepancy Loss: 0.0106
Epoch [32/50], Class Loss: 0.0072, Discrepancy Loss: 0.0114
Epoch [33/50], Class Loss: 0.0055, Discrepancy Loss: 0.0120
Epoch [34/50], Class Loss: 0.0056, Discrepancy Loss: 0.0121
Epoch [35/50], Class Loss: 0.0427, Discrepancy Loss: 0.0115
Epoch [36/50], Class Loss: 0.0031, Discrepancy Loss: 0.0104
Epoch [37/50], Class Loss: 0.0058, Discrepancy Loss: 0.0129
Epoch [38/50], Class Loss: 0.0193, Discrepancy Loss: 0.0121
Epoch [39/50], Class Loss: 0.0039, Discrepancy Loss: 0.0099
Epoch [40/50], Class Loss: 0.0043, Discrepancy Loss: 0.0085
Epoch [41/50], Class Loss: 0.0055, Discrepancy Loss: 0.0102
Epoch [42/50], Class Loss: 0.0062, Discrepancy Loss: 0.0100
Epoch [43/50], Class Loss: 0.0042, Discrepancy Loss: 0.0091
Epoch [44/50], Class Loss: 0.0037, Discrepancy Loss: 0.0098
Epoch [45/50], Class Loss: 0.0075, Discrepancy Loss: 0.0110
Epoch [46/50], Class Loss: 0.0036, Discrepancy Loss: 0.0105
Epoch [47/50], Class Loss: 0.0059, Discrepancy Loss: 0.0123
Epoch [48/50], Class Loss: 0.0048, Discrepancy Loss: 0.0113
Epoch [49/50], Class Loss: 0.0054, Discrepancy Loss: 0.0112
Epoch [50/50], Class Loss: 0.0059, Discrepancy Loss: 0.0118
Source Domain Performance - Accuracy: 99.28%, Precision: 99.28%, Recall: 99.29%, F1 Score: 99.28%
Target Domain Performance - Accuracy: 96.82%, Precision: 96.75%, Recall: 96.77%, F1 Score: 96.76%

Run 5/10
Epoch [1/50], Class Loss: 1.7814, Discrepancy Loss: 0.1159
Epoch [2/50], Class Loss: 0.8412, Discrepancy Loss: 0.0965
Epoch [3/50], Class Loss: 0.6945, Discrepancy Loss: 0.0843
Epoch [4/50], Class Loss: 0.4414, Discrepancy Loss: 0.0787
Epoch [5/50], Class Loss: 0.1897, Discrepancy Loss: 0.0575
Epoch [6/50], Class Loss: 0.0463, Discrepancy Loss: 0.0332
Epoch [7/50], Class Loss: 0.0183, Discrepancy Loss: 0.0113
Epoch [8/50], Class Loss: 0.0199, Discrepancy Loss: 0.0119
Epoch [9/50], Class Loss: 0.0159, Discrepancy Loss: 0.0110
Epoch [10/50], Class Loss: 0.0438, Discrepancy Loss: 0.0186
Epoch [11/50], Class Loss: 0.0145, Discrepancy Loss: 0.0069
Epoch [12/50], Class Loss: 0.0080, Discrepancy Loss: 0.0061
Epoch [13/50], Class Loss: 0.0809, Discrepancy Loss: 0.0051
Epoch [14/50], Class Loss: 0.0121, Discrepancy Loss: 0.0049
Epoch [15/50], Class Loss: 0.0041, Discrepancy Loss: 0.0064
Epoch [16/50], Class Loss: 0.0065, Discrepancy Loss: 0.0043
Epoch [17/50], Class Loss: 0.0041, Discrepancy Loss: 0.0041
Epoch [18/50], Class Loss: 0.0091, Discrepancy Loss: 0.0036
Epoch [19/50], Class Loss: 0.0035, Discrepancy Loss: 0.0042
Epoch [20/50], Class Loss: 0.0076, Discrepancy Loss: 0.0043
Epoch [21/50], Class Loss: 0.0540, Discrepancy Loss: 0.0031
Epoch [22/50], Class Loss: 0.0025, Discrepancy Loss: 0.0035
Epoch [23/50], Class Loss: 0.0472, Discrepancy Loss: 0.0028
Epoch [24/50], Class Loss: 0.0014, Discrepancy Loss: 0.0030
Epoch [25/50], Class Loss: 0.0016, Discrepancy Loss: 0.0026
Epoch [26/50], Class Loss: 0.0012, Discrepancy Loss: 0.0032
Epoch [27/50], Class Loss: 0.0019, Discrepancy Loss: 0.0023
Epoch [28/50], Class Loss: 0.0020, Discrepancy Loss: 0.0029
Epoch [29/50], Class Loss: 0.0010, Discrepancy Loss: 0.0028
Epoch [30/50], Class Loss: 0.0026, Discrepancy Loss: 0.0024
Epoch [31/50], Class Loss: 0.0013, Discrepancy Loss: 0.0036
Epoch [32/50], Class Loss: 0.0682, Discrepancy Loss: 0.0027
Epoch [33/50], Class Loss: 0.0008, Discrepancy Loss: 0.0027
Epoch [34/50], Class Loss: 0.0170, Discrepancy Loss: 0.0047
Epoch [35/50], Class Loss: 0.0010, Discrepancy Loss: 0.0028
Epoch [36/50], Class Loss: 0.0007, Discrepancy Loss: 0.0034
Epoch [37/50], Class Loss: 0.0008, Discrepancy Loss: 0.0031
Epoch [38/50], Class Loss: 0.0012, Discrepancy Loss: 0.0029
Epoch [39/50], Class Loss: 0.0014, Discrepancy Loss: 0.0040
Epoch [40/50], Class Loss: 0.0012, Discrepancy Loss: 0.0033
Epoch [41/50], Class Loss: 0.0012, Discrepancy Loss: 0.0031
Epoch [42/50], Class Loss: 0.0007, Discrepancy Loss: 0.0031
Epoch [43/50], Class Loss: 0.0014, Discrepancy Loss: 0.0035
Epoch [44/50], Class Loss: 0.0010, Discrepancy Loss: 0.0032
Epoch [45/50], Class Loss: 0.0010, Discrepancy Loss: 0.0045
Epoch [46/50], Class Loss: 0.0012, Discrepancy Loss: 0.0035
Epoch [47/50], Class Loss: 0.0012, Discrepancy Loss: 0.0030
Epoch [48/50], Class Loss: 0.0023, Discrepancy Loss: 0.0028
Epoch [49/50], Class Loss: 0.0017, Discrepancy Loss: 0.0028
Epoch [50/50], Class Loss: 0.0040, Discrepancy Loss: 0.0029
Source Domain Performance - Accuracy: 99.58%, Precision: 99.58%, Recall: 99.57%, F1 Score: 99.58%
Target Domain Performance - Accuracy: 99.28%, Precision: 99.28%, Recall: 99.27%, F1 Score: 99.27%

Run 6/10
Epoch [1/50], Class Loss: 2.2825, Discrepancy Loss: 0.1483
Epoch [2/50], Class Loss: 0.5569, Discrepancy Loss: 0.0871
Epoch [3/50], Class Loss: 0.4032, Discrepancy Loss: 0.0797
Epoch [4/50], Class Loss: 0.2976, Discrepancy Loss: 0.0628
Epoch [5/50], Class Loss: 0.4004, Discrepancy Loss: 0.0622
Epoch [6/50], Class Loss: 0.4042, Discrepancy Loss: 0.0797
Epoch [7/50], Class Loss: 0.3031, Discrepancy Loss: 0.0507
Epoch [8/50], Class Loss: 0.2795, Discrepancy Loss: 0.0513
Epoch [9/50], Class Loss: 0.1200, Discrepancy Loss: 0.0456
Epoch [10/50], Class Loss: 0.1096, Discrepancy Loss: 0.0204
Epoch [11/50], Class Loss: 0.1801, Discrepancy Loss: 0.0489
Epoch [12/50], Class Loss: 0.0281, Discrepancy Loss: 0.0323
Epoch [13/50], Class Loss: 0.0381, Discrepancy Loss: 0.0218
Epoch [14/50], Class Loss: 0.0152, Discrepancy Loss: 0.0167
Epoch [15/50], Class Loss: 0.0076, Discrepancy Loss: 0.0130
Epoch [16/50], Class Loss: 0.0091, Discrepancy Loss: 0.0144
Epoch [17/50], Class Loss: 0.0061, Discrepancy Loss: 0.0125
Epoch [18/50], Class Loss: 0.0057, Discrepancy Loss: 0.0123
Epoch [19/50], Class Loss: 0.0120, Discrepancy Loss: 0.0102
Epoch [20/50], Class Loss: 0.0025, Discrepancy Loss: 0.0103
Epoch [21/50], Class Loss: 0.0112, Discrepancy Loss: 0.0077
Epoch [22/50], Class Loss: 0.0021, Discrepancy Loss: 0.0078
Epoch [23/50], Class Loss: 0.0026, Discrepancy Loss: 0.0070
Epoch [24/50], Class Loss: 0.0067, Discrepancy Loss: 0.0095
Epoch [25/50], Class Loss: 0.0053, Discrepancy Loss: 0.0075
Epoch [26/50], Class Loss: 0.0022, Discrepancy Loss: 0.0075
Epoch [27/50], Class Loss: 0.0027, Discrepancy Loss: 0.0076
Epoch [28/50], Class Loss: 0.0051, Discrepancy Loss: 0.0085
Epoch [29/50], Class Loss: 0.0034, Discrepancy Loss: 0.0078
Epoch [30/50], Class Loss: 0.0043, Discrepancy Loss: 0.0080
Epoch [31/50], Class Loss: 0.0040, Discrepancy Loss: 0.0067
Epoch [32/50], Class Loss: 0.0060, Discrepancy Loss: 0.0082
Epoch [33/50], Class Loss: 0.0022, Discrepancy Loss: 0.0077
Epoch [34/50], Class Loss: 0.0012, Discrepancy Loss: 0.0077
Epoch [35/50], Class Loss: 0.0031, Discrepancy Loss: 0.0065
Epoch [36/50], Class Loss: 0.0031, Discrepancy Loss: 0.0074
Epoch [37/50], Class Loss: 0.0019, Discrepancy Loss: 0.0074
Epoch [38/50], Class Loss: 0.0017, Discrepancy Loss: 0.0111
Epoch [39/50], Class Loss: 0.0037, Discrepancy Loss: 0.0066
Epoch [40/50], Class Loss: 0.0208, Discrepancy Loss: 0.0084
Epoch [41/50], Class Loss: 0.0175, Discrepancy Loss: 0.0072
Epoch [42/50], Class Loss: 0.0017, Discrepancy Loss: 0.0092
Epoch [43/50], Class Loss: 0.0012, Discrepancy Loss: 0.0094
Epoch [44/50], Class Loss: 0.0017, Discrepancy Loss: 0.0083
Epoch [45/50], Class Loss: 0.0016, Discrepancy Loss: 0.0068
Epoch [46/50], Class Loss: 0.0014, Discrepancy Loss: 0.0099
Epoch [47/50], Class Loss: 0.0022, Discrepancy Loss: 0.0074
Epoch [48/50], Class Loss: 0.0851, Discrepancy Loss: 0.0073
Epoch [49/50], Class Loss: 0.0018, Discrepancy Loss: 0.0070
Epoch [50/50], Class Loss: 0.0346, Discrepancy Loss: 0.0101
Source Domain Performance - Accuracy: 99.46%, Precision: 99.45%, Recall: 99.47%, F1 Score: 99.46%
Target Domain Performance - Accuracy: 97.60%, Precision: 97.59%, Recall: 97.59%, F1 Score: 97.56%

Run 7/10
Epoch [1/50], Class Loss: 2.1275, Discrepancy Loss: 0.1377
Epoch [2/50], Class Loss: 0.6847, Discrepancy Loss: 0.0844
Epoch [3/50], Class Loss: 0.3875, Discrepancy Loss: 0.0747
Epoch [4/50], Class Loss: 0.4614, Discrepancy Loss: 0.0461
Epoch [5/50], Class Loss: 0.4304, Discrepancy Loss: 0.0744
Epoch [6/50], Class Loss: 0.3860, Discrepancy Loss: 0.0775
Epoch [7/50], Class Loss: 0.1568, Discrepancy Loss: 0.0406
Epoch [8/50], Class Loss: 0.1212, Discrepancy Loss: 0.0252
Epoch [9/50], Class Loss: 0.0710, Discrepancy Loss: 0.0181
Epoch [10/50], Class Loss: 0.1031, Discrepancy Loss: 0.0290
Epoch [11/50], Class Loss: 0.0714, Discrepancy Loss: 0.0362
Epoch [12/50], Class Loss: 0.1002, Discrepancy Loss: 0.0300
Epoch [13/50], Class Loss: 0.0193, Discrepancy Loss: 0.0212
Epoch [14/50], Class Loss: 0.0785, Discrepancy Loss: 0.0162
Epoch [15/50], Class Loss: 0.0458, Discrepancy Loss: 0.0223
Epoch [16/50], Class Loss: 0.0231, Discrepancy Loss: 0.0273
Epoch [17/50], Class Loss: 0.0124, Discrepancy Loss: 0.0143
Epoch [18/50], Class Loss: 0.0207, Discrepancy Loss: 0.0159
Epoch [19/50], Class Loss: 0.0822, Discrepancy Loss: 0.0125
Epoch [20/50], Class Loss: 0.0127, Discrepancy Loss: 0.0144
Epoch [21/50], Class Loss: 0.0103, Discrepancy Loss: 0.0112
Epoch [22/50], Class Loss: 0.0077, Discrepancy Loss: 0.0117
Epoch [23/50], Class Loss: 0.0051, Discrepancy Loss: 0.0098
Epoch [24/50], Class Loss: 0.0089, Discrepancy Loss: 0.0110
Epoch [25/50], Class Loss: 0.1058, Discrepancy Loss: 0.0169
Epoch [26/50], Class Loss: 0.0142, Discrepancy Loss: 0.0132
Epoch [27/50], Class Loss: 0.0054, Discrepancy Loss: 0.0146
Epoch [28/50], Class Loss: 0.0048, Discrepancy Loss: 0.0109
Epoch [29/50], Class Loss: 0.0047, Discrepancy Loss: 0.0148
Epoch [30/50], Class Loss: 0.0039, Discrepancy Loss: 0.0093
Epoch [31/50], Class Loss: 0.0046, Discrepancy Loss: 0.0095
Epoch [32/50], Class Loss: 0.0072, Discrepancy Loss: 0.0112
Epoch [33/50], Class Loss: 0.0040, Discrepancy Loss: 0.0101
Epoch [34/50], Class Loss: 0.0051, Discrepancy Loss: 0.0119
Epoch [35/50], Class Loss: 0.0051, Discrepancy Loss: 0.0097
Epoch [36/50], Class Loss: 0.0060, Discrepancy Loss: 0.0097
Epoch [37/50], Class Loss: 0.0065, Discrepancy Loss: 0.0146
Epoch [38/50], Class Loss: 0.0165, Discrepancy Loss: 0.0115
Epoch [39/50], Class Loss: 0.0100, Discrepancy Loss: 0.0122
Epoch [40/50], Class Loss: 0.0095, Discrepancy Loss: 0.0094
Epoch [41/50], Class Loss: 0.0479, Discrepancy Loss: 0.0123
Epoch [42/50], Class Loss: 0.0102, Discrepancy Loss: 0.0091
Epoch [43/50], Class Loss: 0.0593, Discrepancy Loss: 0.0119
Epoch [44/50], Class Loss: 0.0036, Discrepancy Loss: 0.0111
Epoch [45/50], Class Loss: 0.0040, Discrepancy Loss: 0.0103
Epoch [46/50], Class Loss: 0.0069, Discrepancy Loss: 0.0100
Epoch [47/50], Class Loss: 0.0061, Discrepancy Loss: 0.0116
Epoch [48/50], Class Loss: 0.0026, Discrepancy Loss: 0.0101
Epoch [49/50], Class Loss: 0.0031, Discrepancy Loss: 0.0092
Epoch [50/50], Class Loss: 0.0029, Discrepancy Loss: 0.0107
Source Domain Performance - Accuracy: 97.12%, Precision: 97.20%, Recall: 97.21%, F1 Score: 97.12%
Target Domain Performance - Accuracy: 98.14%, Precision: 98.12%, Recall: 98.12%, F1 Score: 98.11%

Run 8/10
Epoch [1/50], Class Loss: 1.5584, Discrepancy Loss: 0.1320
Epoch [2/50], Class Loss: 0.7965, Discrepancy Loss: 0.0816
Epoch [3/50], Class Loss: 0.1941, Discrepancy Loss: 0.0367
Epoch [4/50], Class Loss: 0.0466, Discrepancy Loss: 0.0183
Epoch [5/50], Class Loss: 0.0402, Discrepancy Loss: 0.0109
Epoch [6/50], Class Loss: 0.0321, Discrepancy Loss: 0.0117
Epoch [7/50], Class Loss: 0.0171, Discrepancy Loss: 0.0142
Epoch [8/50], Class Loss: 0.0483, Discrepancy Loss: 0.0169
Epoch [9/50], Class Loss: 0.0792, Discrepancy Loss: 0.0164
Epoch [10/50], Class Loss: 0.0465, Discrepancy Loss: 0.0077
Epoch [11/50], Class Loss: 0.0315, Discrepancy Loss: 0.0068
Epoch [12/50], Class Loss: 0.0054, Discrepancy Loss: 0.0063
Epoch [13/50], Class Loss: 0.0026, Discrepancy Loss: 0.0049
Epoch [14/50], Class Loss: 0.0009, Discrepancy Loss: 0.0044
Epoch [15/50], Class Loss: 0.0019, Discrepancy Loss: 0.0071
Epoch [16/50], Class Loss: 0.0026, Discrepancy Loss: 0.0039
Epoch [17/50], Class Loss: 0.0040, Discrepancy Loss: 0.0036
Epoch [18/50], Class Loss: 0.0008, Discrepancy Loss: 0.0032
Epoch [19/50], Class Loss: 0.0010, Discrepancy Loss: 0.0052
Epoch [20/50], Class Loss: 0.0061, Discrepancy Loss: 0.0046
Epoch [21/50], Class Loss: 0.0229, Discrepancy Loss: 0.0034
Epoch [22/50], Class Loss: 0.0009, Discrepancy Loss: 0.0050
Epoch [23/50], Class Loss: 0.0154, Discrepancy Loss: 0.0043
Epoch [24/50], Class Loss: 0.1505, Discrepancy Loss: 0.0032
Epoch [25/50], Class Loss: 0.0018, Discrepancy Loss: 0.0025
Epoch [26/50], Class Loss: 0.0011, Discrepancy Loss: 0.0027
Epoch [27/50], Class Loss: 0.0005, Discrepancy Loss: 0.0039
Epoch [28/50], Class Loss: 0.0006, Discrepancy Loss: 0.0030
Epoch [29/50], Class Loss: 0.0013, Discrepancy Loss: 0.0026
Epoch [30/50], Class Loss: 0.0072, Discrepancy Loss: 0.0031
Epoch [31/50], Class Loss: 0.0004, Discrepancy Loss: 0.0027
Epoch [32/50], Class Loss: 0.0026, Discrepancy Loss: 0.0039
Epoch [33/50], Class Loss: 0.0007, Discrepancy Loss: 0.0028
Epoch [34/50], Class Loss: 0.0003, Discrepancy Loss: 0.0042
Epoch [35/50], Class Loss: 0.0010, Discrepancy Loss: 0.0026
Epoch [36/50], Class Loss: 0.0302, Discrepancy Loss: 0.0039
Epoch [37/50], Class Loss: 0.0027, Discrepancy Loss: 0.0029
Epoch [38/50], Class Loss: 0.0006, Discrepancy Loss: 0.0033
Epoch [39/50], Class Loss: 0.0004, Discrepancy Loss: 0.0032
Epoch [40/50], Class Loss: 0.0025, Discrepancy Loss: 0.0029
Epoch [41/50], Class Loss: 0.0004, Discrepancy Loss: 0.0031
Epoch [42/50], Class Loss: 0.0009, Discrepancy Loss: 0.0023
Epoch [43/50], Class Loss: 0.0010, Discrepancy Loss: 0.0036
Epoch [44/50], Class Loss: 0.0016, Discrepancy Loss: 0.0033
Epoch [45/50], Class Loss: 0.0004, Discrepancy Loss: 0.0034
Epoch [46/50], Class Loss: 0.0003, Discrepancy Loss: 0.0041
Epoch [47/50], Class Loss: 0.0057, Discrepancy Loss: 0.0028
Epoch [48/50], Class Loss: 0.0007, Discrepancy Loss: 0.0032
Epoch [49/50], Class Loss: 0.0005, Discrepancy Loss: 0.0034
Epoch [50/50], Class Loss: 0.0004, Discrepancy Loss: 0.0037
Source Domain Performance - Accuracy: 98.98%, Precision: 99.01%, Recall: 98.95%, F1 Score: 98.97%
Target Domain Performance - Accuracy: 98.62%, Precision: 98.63%, Recall: 98.58%, F1 Score: 98.59%

Run 9/10
Epoch [1/50], Class Loss: 1.5145, Discrepancy Loss: 0.1241
Epoch [2/50], Class Loss: 0.7338, Discrepancy Loss: 0.1059
Epoch [3/50], Class Loss: 0.3600, Discrepancy Loss: 0.0540
Epoch [4/50], Class Loss: 0.2560, Discrepancy Loss: 0.0478
Epoch [5/50], Class Loss: 0.2117, Discrepancy Loss: 0.0542
Epoch [6/50], Class Loss: 0.0616, Discrepancy Loss: 0.0293
Epoch [7/50], Class Loss: 0.0340, Discrepancy Loss: 0.0213
Epoch [8/50], Class Loss: 0.0230, Discrepancy Loss: 0.0136
Epoch [9/50], Class Loss: 0.0709, Discrepancy Loss: 0.0325
Epoch [10/50], Class Loss: 0.0241, Discrepancy Loss: 0.0161
Epoch [11/50], Class Loss: 0.0078, Discrepancy Loss: 0.0095
Epoch [12/50], Class Loss: 0.0043, Discrepancy Loss: 0.0081
Epoch [13/50], Class Loss: 0.0026, Discrepancy Loss: 0.0076
Epoch [14/50], Class Loss: 0.0031, Discrepancy Loss: 0.0070
Epoch [15/50], Class Loss: 0.0012, Discrepancy Loss: 0.0070
Epoch [16/50], Class Loss: 0.0105, Discrepancy Loss: 0.0066
Epoch [17/50], Class Loss: 0.0276, Discrepancy Loss: 0.0248
Epoch [18/50], Class Loss: 0.0032, Discrepancy Loss: 0.0145
Epoch [19/50], Class Loss: 0.0033, Discrepancy Loss: 0.0124
Epoch [20/50], Class Loss: 0.0072, Discrepancy Loss: 0.0075
Epoch [21/50], Class Loss: 0.0028, Discrepancy Loss: 0.0087
Epoch [22/50], Class Loss: 0.0054, Discrepancy Loss: 0.0088
Epoch [23/50], Class Loss: 0.0020, Discrepancy Loss: 0.0090
Epoch [24/50], Class Loss: 0.0024, Discrepancy Loss: 0.0087
Epoch [25/50], Class Loss: 0.0022, Discrepancy Loss: 0.0092
Epoch [26/50], Class Loss: 0.0042, Discrepancy Loss: 0.0074
Epoch [27/50], Class Loss: 0.0026, Discrepancy Loss: 0.0078
Epoch [28/50], Class Loss: 0.0071, Discrepancy Loss: 0.0082
Epoch [29/50], Class Loss: 0.1497, Discrepancy Loss: 0.0077
Epoch [30/50], Class Loss: 0.0019, Discrepancy Loss: 0.0073
Epoch [31/50], Class Loss: 0.0023, Discrepancy Loss: 0.0068
Epoch [32/50], Class Loss: 0.0018, Discrepancy Loss: 0.0080
Epoch [33/50], Class Loss: 0.0114, Discrepancy Loss: 0.0089
Epoch [34/50], Class Loss: 0.0021, Discrepancy Loss: 0.0073
Epoch [35/50], Class Loss: 0.0043, Discrepancy Loss: 0.0069
Epoch [36/50], Class Loss: 0.0030, Discrepancy Loss: 0.0057
Epoch [37/50], Class Loss: 0.0016, Discrepancy Loss: 0.0064
Epoch [38/50], Class Loss: 0.1713, Discrepancy Loss: 0.0074
Epoch [39/50], Class Loss: 0.0011, Discrepancy Loss: 0.0074
Epoch [40/50], Class Loss: 0.0042, Discrepancy Loss: 0.0065
Epoch [41/50], Class Loss: 0.1068, Discrepancy Loss: 0.0060
Epoch [42/50], Class Loss: 0.0012, Discrepancy Loss: 0.0064
Epoch [43/50], Class Loss: 0.0008, Discrepancy Loss: 0.0072
Epoch [44/50], Class Loss: 0.0013, Discrepancy Loss: 0.0079
Epoch [45/50], Class Loss: 0.0280, Discrepancy Loss: 0.0074
Epoch [46/50], Class Loss: 0.0018, Discrepancy Loss: 0.0073
Epoch [47/50], Class Loss: 0.0046, Discrepancy Loss: 0.0072
Epoch [48/50], Class Loss: 0.0109, Discrepancy Loss: 0.0077
Epoch [49/50], Class Loss: 0.0112, Discrepancy Loss: 0.0067
Epoch [50/50], Class Loss: 0.0021, Discrepancy Loss: 0.0078
Source Domain Performance - Accuracy: 99.52%, Precision: 99.51%, Recall: 99.53%, F1 Score: 99.52%
Target Domain Performance - Accuracy: 98.20%, Precision: 98.18%, Recall: 98.19%, F1 Score: 98.17%

Run 10/10
Epoch [1/50], Class Loss: 1.3615, Discrepancy Loss: 0.1208
Epoch [2/50], Class Loss: 0.4899, Discrepancy Loss: 0.0891
Epoch [3/50], Class Loss: 0.2059, Discrepancy Loss: 0.0541
Epoch [4/50], Class Loss: 0.1620, Discrepancy Loss: 0.0413
Epoch [5/50], Class Loss: 0.0364, Discrepancy Loss: 0.0173
Epoch [6/50], Class Loss: 0.0190, Discrepancy Loss: 0.0153
Epoch [7/50], Class Loss: 0.0093, Discrepancy Loss: 0.0064
Epoch [8/50], Class Loss: 0.1246, Discrepancy Loss: 0.0279
Epoch [9/50], Class Loss: 0.0808, Discrepancy Loss: 0.0248
Epoch [10/50], Class Loss: 0.0174, Discrepancy Loss: 0.0108
Epoch [11/50], Class Loss: 0.0081, Discrepancy Loss: 0.0103
Epoch [12/50], Class Loss: 0.0029, Discrepancy Loss: 0.0092
Epoch [13/50], Class Loss: 0.0028, Discrepancy Loss: 0.0061
Epoch [14/50], Class Loss: 0.0030, Discrepancy Loss: 0.0059
Epoch [15/50], Class Loss: 0.0036, Discrepancy Loss: 0.0050
Epoch [16/50], Class Loss: 0.0012, Discrepancy Loss: 0.0053
Epoch [17/50], Class Loss: 0.0016, Discrepancy Loss: 0.0053
Epoch [18/50], Class Loss: 0.0560, Discrepancy Loss: 0.0044
Epoch [19/50], Class Loss: 0.0144, Discrepancy Loss: 0.0361
Epoch [20/50], Class Loss: 0.0152, Discrepancy Loss: 0.0182
Epoch [21/50], Class Loss: 0.0049, Discrepancy Loss: 0.0056
Epoch [22/50], Class Loss: 0.0022, Discrepancy Loss: 0.0057
Epoch [23/50], Class Loss: 0.0028, Discrepancy Loss: 0.0051
Epoch [24/50], Class Loss: 0.0035, Discrepancy Loss: 0.0051
Epoch [25/50], Class Loss: 0.0024, Discrepancy Loss: 0.0051
Epoch [26/50], Class Loss: 0.0028, Discrepancy Loss: 0.0047
Epoch [27/50], Class Loss: 0.0094, Discrepancy Loss: 0.0055
Epoch [28/50], Class Loss: 0.0021, Discrepancy Loss: 0.0068
Epoch [29/50], Class Loss: 0.0026, Discrepancy Loss: 0.0073
Epoch [30/50], Class Loss: 0.0097, Discrepancy Loss: 0.0069
Epoch [31/50], Class Loss: 0.0013, Discrepancy Loss: 0.0085
Epoch [32/50], Class Loss: 0.0013, Discrepancy Loss: 0.0073
Epoch [33/50], Class Loss: 0.0107, Discrepancy Loss: 0.0056
Epoch [34/50], Class Loss: 0.0069, Discrepancy Loss: 0.0055
Epoch [35/50], Class Loss: 0.0028, Discrepancy Loss: 0.0079
Epoch [36/50], Class Loss: 0.0042, Discrepancy Loss: 0.0060
Epoch [37/50], Class Loss: 0.0016, Discrepancy Loss: 0.0069
Epoch [38/50], Class Loss: 0.0009, Discrepancy Loss: 0.0063
Epoch [39/50], Class Loss: 0.0047, Discrepancy Loss: 0.0060
Epoch [40/50], Class Loss: 0.0025, Discrepancy Loss: 0.0063
Epoch [41/50], Class Loss: 0.0022, Discrepancy Loss: 0.0070
Epoch [42/50], Class Loss: 0.0019, Discrepancy Loss: 0.0055
Epoch [43/50], Class Loss: 0.0065, Discrepancy Loss: 0.0057
Epoch [44/50], Class Loss: 0.0019, Discrepancy Loss: 0.0066
Epoch [45/50], Class Loss: 0.0677, Discrepancy Loss: 0.0064
Epoch [46/50], Class Loss: 0.0015, Discrepancy Loss: 0.0056
Epoch [47/50], Class Loss: 0.0110, Discrepancy Loss: 0.0064
Epoch [48/50], Class Loss: 0.0019, Discrepancy Loss: 0.0065
Epoch [49/50], Class Loss: 0.0063, Discrepancy Loss: 0.0069
Epoch [50/50], Class Loss: 0.0016, Discrepancy Loss: 0.0078
Source Domain Performance - Accuracy: 99.40%, Precision: 99.42%, Recall: 99.37%, F1 Score: 99.39%
Target Domain Performance - Accuracy: 98.62%, Precision: 98.66%, Recall: 98.57%, F1 Score: 98.59%

Source performance: 98.30% 98.56% 98.34% 98.30%
Target performance: 98.33% 98.32% 98.31% 98.30%

Per-Class Accuracy on Target Domain:
bpsk: 99.98%
qpsk: 99.11%
16qam: 96.89%
8apsk: 97.26%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.7219, Discrepancy Loss: 0.0722
Validation Loss: 3.0445
Epoch [2/50], Class Loss: 0.2432, Discrepancy Loss: 0.0325
Validation Loss: 51.4953
Epoch [3/50], Class Loss: 0.1620, Discrepancy Loss: 0.0237
Validation Loss: 0.0754
Epoch [4/50], Class Loss: 0.1482, Discrepancy Loss: 0.0310
Validation Loss: 0.4448
Epoch [5/50], Class Loss: 0.0663, Discrepancy Loss: 0.0398
Validation Loss: 1.7643
Epoch [6/50], Class Loss: 0.0812, Discrepancy Loss: 0.0790
Validation Loss: 0.7112
Epoch [7/50], Class Loss: 0.0479, Discrepancy Loss: 0.0255
Validation Loss: 1.2088
Epoch [8/50], Class Loss: 0.0596, Discrepancy Loss: 0.0539
Validation Loss: 3.7348
Early stopping!
Source Domain Performance - Accuracy: 80.04%, Precision: 89.11%, Recall: 79.08%, F1 Score: 75.09%
Target Domain Performance - Accuracy: 54.80%, Precision: 75.90%, Recall: 54.08%, F1 Score: 44.97%

Run 2/10
Epoch [1/50], Class Loss: 0.6752, Discrepancy Loss: 0.0639
Validation Loss: 15.9228
Epoch [2/50], Class Loss: 0.4324, Discrepancy Loss: 0.0203
Validation Loss: 1.8059
Epoch [3/50], Class Loss: 0.1376, Discrepancy Loss: 0.0357
Validation Loss: 2.5086
Epoch [4/50], Class Loss: 0.1330, Discrepancy Loss: 0.0292
Validation Loss: 9.8865
Epoch [5/50], Class Loss: 0.0826, Discrepancy Loss: 0.0179
Validation Loss: 0.4948
Epoch [6/50], Class Loss: 0.0537, Discrepancy Loss: 0.0209
Validation Loss: 0.2604
Epoch [7/50], Class Loss: 0.0272, Discrepancy Loss: 0.0730
Validation Loss: 6.3169
Epoch [8/50], Class Loss: 0.0539, Discrepancy Loss: 0.0610
Validation Loss: 5.4932
Epoch [9/50], Class Loss: 0.0599, Discrepancy Loss: 0.0634
Validation Loss: 0.1261
Epoch [10/50], Class Loss: 0.0318, Discrepancy Loss: 0.0876
Validation Loss: 0.0820
Epoch [11/50], Class Loss: 0.0202, Discrepancy Loss: 0.0261
Validation Loss: 0.0012
Epoch [12/50], Class Loss: 0.0117, Discrepancy Loss: 0.0184
Validation Loss: 0.0022
Epoch [13/50], Class Loss: 0.0065, Discrepancy Loss: 0.0154
Validation Loss: 0.0013
Epoch [14/50], Class Loss: 0.0074, Discrepancy Loss: 0.0214
Validation Loss: 0.0043
Epoch [15/50], Class Loss: 0.0462, Discrepancy Loss: 0.0792
Validation Loss: 0.0150
Epoch [16/50], Class Loss: 0.0104, Discrepancy Loss: 0.0840
Validation Loss: 0.0093
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 55.46%, Precision: 69.79%, Recall: 55.70%, F1 Score: 46.74%

Run 3/10
Epoch [1/50], Class Loss: 0.6829, Discrepancy Loss: 0.0455
Validation Loss: 1.5203
Epoch [2/50], Class Loss: 0.1811, Discrepancy Loss: 0.0249
Validation Loss: 0.7891
Epoch [3/50], Class Loss: 0.1184, Discrepancy Loss: 0.0209
Validation Loss: 0.7097
Epoch [4/50], Class Loss: 0.1219, Discrepancy Loss: 0.0440
Validation Loss: 4.2712
Epoch [5/50], Class Loss: 0.0990, Discrepancy Loss: 0.0567
Validation Loss: 8.6833
Epoch [6/50], Class Loss: 0.0734, Discrepancy Loss: 0.0427
Validation Loss: 0.7145
Epoch [7/50], Class Loss: 0.0633, Discrepancy Loss: 0.0296
Validation Loss: 1.1143
Epoch [8/50], Class Loss: 0.0211, Discrepancy Loss: 0.0474
Validation Loss: 0.7169
Early stopping!
Source Domain Performance - Accuracy: 92.69%, Precision: 94.54%, Recall: 92.34%, F1 Score: 92.41%
Target Domain Performance - Accuracy: 81.41%, Precision: 84.49%, Recall: 81.26%, F1 Score: 81.73%

Run 4/10
Epoch [1/50], Class Loss: 0.6792, Discrepancy Loss: 0.0459
Validation Loss: 3.6179
Epoch [2/50], Class Loss: 0.2844, Discrepancy Loss: 0.0381
Validation Loss: 0.8880
Epoch [3/50], Class Loss: 0.2128, Discrepancy Loss: 0.0352
Validation Loss: 1.2661
Epoch [4/50], Class Loss: 0.0877, Discrepancy Loss: 0.0169
Validation Loss: 0.1390
Epoch [5/50], Class Loss: 0.0388, Discrepancy Loss: 0.0557
Validation Loss: 3.4844
Epoch [6/50], Class Loss: 0.0503, Discrepancy Loss: 0.0577
Validation Loss: 0.0715
Epoch [7/50], Class Loss: 0.0391, Discrepancy Loss: 0.0350
Validation Loss: 0.1871
Epoch [8/50], Class Loss: 0.0520, Discrepancy Loss: 0.0774
Validation Loss: 12.1955
Epoch [9/50], Class Loss: 0.0753, Discrepancy Loss: 0.0243
Validation Loss: 0.0237
Epoch [10/50], Class Loss: 0.0278, Discrepancy Loss: 0.0268
Validation Loss: 0.1177
Epoch [11/50], Class Loss: 0.0053, Discrepancy Loss: 0.0169
Validation Loss: 0.0011
Epoch [12/50], Class Loss: 0.0010, Discrepancy Loss: 0.0123
Validation Loss: 0.0080
Epoch [13/50], Class Loss: 0.0087, Discrepancy Loss: 0.0229
Validation Loss: 0.0003
Epoch [14/50], Class Loss: 0.0011, Discrepancy Loss: 0.0356
Validation Loss: 0.0052
Epoch [15/50], Class Loss: 0.0164, Discrepancy Loss: 0.0873
Validation Loss: 0.0713
Epoch [16/50], Class Loss: 0.0145, Discrepancy Loss: 0.0629
Validation Loss: 0.0228
Epoch [17/50], Class Loss: 0.0067, Discrepancy Loss: 0.0380
Validation Loss: 0.0084
Epoch [18/50], Class Loss: 0.0044, Discrepancy Loss: 0.0276
Validation Loss: 0.0032
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 57.37%, Precision: 77.39%, Recall: 57.60%, F1 Score: 48.17%

Run 5/10
Epoch [1/50], Class Loss: 0.6911, Discrepancy Loss: 0.0577
Validation Loss: 9.3203
Epoch [2/50], Class Loss: 0.3646, Discrepancy Loss: 0.0260
Validation Loss: 0.8566
Epoch [3/50], Class Loss: 0.2436, Discrepancy Loss: 0.0328
Validation Loss: 0.1230
Epoch [4/50], Class Loss: 0.1149, Discrepancy Loss: 0.0762
Validation Loss: 2.4710
Epoch [5/50], Class Loss: 0.0907, Discrepancy Loss: 0.0271
Validation Loss: 4.6216
Epoch [6/50], Class Loss: 0.0801, Discrepancy Loss: 0.0204
Validation Loss: 1.3498
Epoch [7/50], Class Loss: 0.0572, Discrepancy Loss: 0.0170
Validation Loss: 0.4170
Epoch [8/50], Class Loss: 0.0334, Discrepancy Loss: 0.0243
Validation Loss: 1.2039
Early stopping!
Source Domain Performance - Accuracy: 87.65%, Precision: 91.99%, Recall: 87.06%, F1 Score: 86.50%
Target Domain Performance - Accuracy: 62.83%, Precision: 80.58%, Recall: 62.29%, F1 Score: 55.67%

Run 6/10
Epoch [1/50], Class Loss: 0.7803, Discrepancy Loss: 0.0413
Validation Loss: 3.8973
Epoch [2/50], Class Loss: 0.2585, Discrepancy Loss: 0.0333
Validation Loss: 3.1018
Epoch [3/50], Class Loss: 0.2279, Discrepancy Loss: 0.0364
Validation Loss: 1.2600
Epoch [4/50], Class Loss: 0.1886, Discrepancy Loss: 0.0291
Validation Loss: 2.7687
Epoch [5/50], Class Loss: 0.0965, Discrepancy Loss: 0.0201
Validation Loss: 0.1554
Epoch [6/50], Class Loss: 0.0453, Discrepancy Loss: 0.0150
Validation Loss: 6.2255
Epoch [7/50], Class Loss: 0.0716, Discrepancy Loss: 0.0247
Validation Loss: 0.1181
Epoch [8/50], Class Loss: 0.0657, Discrepancy Loss: 0.0248
Validation Loss: 0.1755
Epoch [9/50], Class Loss: 0.0304, Discrepancy Loss: 0.0111
Validation Loss: 4.4987
Epoch [10/50], Class Loss: 0.0600, Discrepancy Loss: 0.0473
Validation Loss: 15.3824
Epoch [11/50], Class Loss: 0.0225, Discrepancy Loss: 0.0363
Validation Loss: 0.0013
Epoch [12/50], Class Loss: 0.0019, Discrepancy Loss: 0.0056
Validation Loss: 0.0222
Epoch [13/50], Class Loss: 0.0003, Discrepancy Loss: 0.0040
Validation Loss: 0.0038
Epoch [14/50], Class Loss: 0.0005, Discrepancy Loss: 0.0078
Validation Loss: 0.0100
Epoch [15/50], Class Loss: 0.0015, Discrepancy Loss: 0.0164
Validation Loss: 0.0039
Epoch [16/50], Class Loss: 0.0027, Discrepancy Loss: 0.0168
Validation Loss: 0.0116
Early stopping!
Source Domain Performance - Accuracy: 99.76%, Precision: 99.75%, Recall: 99.77%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 63.85%, Precision: 76.85%, Recall: 64.01%, F1 Score: 55.61%

Run 7/10
Epoch [1/50], Class Loss: 0.6610, Discrepancy Loss: 0.0528
Validation Loss: 5.7988
Epoch [2/50], Class Loss: 0.3917, Discrepancy Loss: 0.0591
Validation Loss: 2.4036
Epoch [3/50], Class Loss: 0.1562, Discrepancy Loss: 0.0393
Validation Loss: 0.1631
Epoch [4/50], Class Loss: 0.1473, Discrepancy Loss: 0.0554
Validation Loss: 18.1280
Epoch [5/50], Class Loss: 0.0681, Discrepancy Loss: 0.0503
Validation Loss: 5.0633
Epoch [6/50], Class Loss: 0.0448, Discrepancy Loss: 0.0677
Validation Loss: 13.1561
Epoch [7/50], Class Loss: 0.0304, Discrepancy Loss: 0.0765
Validation Loss: 0.9208
Epoch [8/50], Class Loss: 0.0471, Discrepancy Loss: 0.1453
Validation Loss: 0.0479
Epoch [9/50], Class Loss: 0.0483, Discrepancy Loss: 0.0373
Validation Loss: 1.6937
Epoch [10/50], Class Loss: 0.0165, Discrepancy Loss: 0.0455
Validation Loss: 0.0129
Epoch [11/50], Class Loss: 0.0094, Discrepancy Loss: 0.0227
Validation Loss: 0.0065
Epoch [12/50], Class Loss: 0.0018, Discrepancy Loss: 0.0304
Validation Loss: 0.0040
Epoch [13/50], Class Loss: 0.0011, Discrepancy Loss: 0.0301
Validation Loss: 0.0153
Epoch [14/50], Class Loss: 0.0025, Discrepancy Loss: 0.0331
Validation Loss: 0.0342
Epoch [15/50], Class Loss: 0.0027, Discrepancy Loss: 0.0288
Validation Loss: 0.0081
Epoch [16/50], Class Loss: 0.0032, Discrepancy Loss: 0.0475
Validation Loss: 0.0111
Epoch [17/50], Class Loss: 0.0070, Discrepancy Loss: 0.0594
Validation Loss: 0.0078
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 61.27%, Precision: 49.19%, Recall: 61.46%, F1 Score: 52.89%

Run 8/10
Epoch [1/50], Class Loss: 0.6580, Discrepancy Loss: 0.0513
Validation Loss: 2.1265
Epoch [2/50], Class Loss: 0.3890, Discrepancy Loss: 0.0323
Validation Loss: 1.8862
Epoch [3/50], Class Loss: 0.1335, Discrepancy Loss: 0.0333
Validation Loss: 0.2345
Epoch [4/50], Class Loss: 0.0891, Discrepancy Loss: 0.0599
Validation Loss: 6.7428
Epoch [5/50], Class Loss: 0.1367, Discrepancy Loss: 0.0415
Validation Loss: 15.0469
Epoch [6/50], Class Loss: 0.0591, Discrepancy Loss: 0.0328
Validation Loss: 1.6301
Epoch [7/50], Class Loss: 0.0296, Discrepancy Loss: 0.0703
Validation Loss: 2.2205
Epoch [8/50], Class Loss: 0.0250, Discrepancy Loss: 0.0677
Validation Loss: 26.6308
Early stopping!
Source Domain Performance - Accuracy: 48.74%, Precision: 57.94%, Recall: 50.06%, F1 Score: 37.17%
Target Domain Performance - Accuracy: 49.70%, Precision: 33.04%, Recall: 50.00%, F1 Score: 37.17%

Run 9/10
Epoch [1/50], Class Loss: 0.6186, Discrepancy Loss: 0.0435
Validation Loss: 4.4844
Epoch [2/50], Class Loss: 0.4349, Discrepancy Loss: 0.0386
Validation Loss: 7.1271
Epoch [3/50], Class Loss: 0.1520, Discrepancy Loss: 0.0264
Validation Loss: 0.8176
Epoch [4/50], Class Loss: 0.2102, Discrepancy Loss: 0.0466
Validation Loss: 0.3707
Epoch [5/50], Class Loss: 0.1032, Discrepancy Loss: 0.0323
Validation Loss: 0.1989
Epoch [6/50], Class Loss: 0.0503, Discrepancy Loss: 0.0287
Validation Loss: 0.5819
Epoch [7/50], Class Loss: 0.0667, Discrepancy Loss: 0.0408
Validation Loss: 0.1394
Epoch [8/50], Class Loss: 0.0659, Discrepancy Loss: 0.0575
Validation Loss: 3.1494
Epoch [9/50], Class Loss: 0.0633, Discrepancy Loss: 0.0892
Validation Loss: 2.9747
Epoch [10/50], Class Loss: 0.0701, Discrepancy Loss: 0.1198
Validation Loss: 0.2944
Epoch [11/50], Class Loss: 0.0315, Discrepancy Loss: 0.1123
Validation Loss: 0.1604
Epoch [12/50], Class Loss: 0.0168, Discrepancy Loss: 0.1085
Validation Loss: 0.0505
Epoch [13/50], Class Loss: 0.0104, Discrepancy Loss: 0.1423
Validation Loss: 0.3105
Epoch [14/50], Class Loss: 0.0209, Discrepancy Loss: 0.1743
Validation Loss: 0.8392
Epoch [15/50], Class Loss: 0.0344, Discrepancy Loss: 0.1754
Validation Loss: 0.0276
Epoch [16/50], Class Loss: 0.0298, Discrepancy Loss: 0.1998
Validation Loss: 0.0486
Epoch [17/50], Class Loss: 0.0639, Discrepancy Loss: 0.2202
Validation Loss: 0.0427
Epoch [18/50], Class Loss: 0.0287, Discrepancy Loss: 0.2219
Validation Loss: 0.0218
Epoch [19/50], Class Loss: 0.0245, Discrepancy Loss: 0.1631
Validation Loss: 0.0122
Epoch [20/50], Class Loss: 0.0298, Discrepancy Loss: 0.1039
Validation Loss: 0.1465
Epoch [21/50], Class Loss: 0.0353, Discrepancy Loss: 0.1758
Validation Loss: 0.0349
Epoch [22/50], Class Loss: 0.0213, Discrepancy Loss: 0.1837
Validation Loss: 0.0249
Epoch [23/50], Class Loss: 0.0214, Discrepancy Loss: 0.1899
Validation Loss: 0.0294
Epoch [24/50], Class Loss: 0.0183, Discrepancy Loss: 0.1883
Validation Loss: 0.0278
Early stopping!
Source Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.76%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 59.77%, Precision: 49.12%, Recall: 59.88%, F1 Score: 52.07%

Run 10/10
Epoch [1/50], Class Loss: 0.6709, Discrepancy Loss: 0.0507
Validation Loss: 1.8266
Epoch [2/50], Class Loss: 0.2751, Discrepancy Loss: 0.0463
Validation Loss: 3.8208
Epoch [3/50], Class Loss: 0.1956, Discrepancy Loss: 0.0625
Validation Loss: 0.2104
Epoch [4/50], Class Loss: 0.0967, Discrepancy Loss: 0.0257
Validation Loss: 17.0333
Epoch [5/50], Class Loss: 0.0933, Discrepancy Loss: 0.0110
Validation Loss: 2.3945
Epoch [6/50], Class Loss: 0.0802, Discrepancy Loss: 0.0369
Validation Loss: 19.0446
Epoch [7/50], Class Loss: 0.0765, Discrepancy Loss: 0.0944
Validation Loss: 21.0874
Epoch [8/50], Class Loss: 0.0390, Discrepancy Loss: 0.0985
Validation Loss: 3.7902
Early stopping!
Source Domain Performance - Accuracy: 76.98%, Precision: 87.66%, Recall: 78.03%, F1 Score: 72.32%
Target Domain Performance - Accuracy: 59.59%, Precision: 83.90%, Recall: 59.85%, F1 Score: 54.20%

Source performance: 88.54% 92.05% 88.59% 86.28%
Target performance: 60.61% 68.03% 60.61% 52.92%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 11.29%
16qam: 46.44%
8apsk: 84.72%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.4616, JMMD Loss: 0.2211
Validation Loss: 0.5301
Epoch [2/50], Class Loss: 0.1918, JMMD Loss: 0.1219
Validation Loss: 1.4393
Epoch [3/50], Class Loss: 0.1905, JMMD Loss: 0.1387
Validation Loss: 0.3345
Epoch [4/50], Class Loss: 0.0710, JMMD Loss: 0.1042
Validation Loss: 5.4032
Epoch [5/50], Class Loss: 0.0449, JMMD Loss: 0.1108
Validation Loss: 0.1056
Epoch [6/50], Class Loss: 0.0258, JMMD Loss: 0.0947
Validation Loss: 0.1922
Epoch [7/50], Class Loss: 0.0277, JMMD Loss: 0.1159
Validation Loss: 0.0731
Epoch [8/50], Class Loss: 0.0175, JMMD Loss: 0.1206
Validation Loss: 0.5074
Epoch [9/50], Class Loss: 0.0288, JMMD Loss: 0.1262
Validation Loss: 0.0086
Epoch [10/50], Class Loss: 0.0087, JMMD Loss: 0.1171
Validation Loss: 0.0208
Epoch [11/50], Class Loss: 0.0023, JMMD Loss: 0.1206
Validation Loss: 0.0038
Epoch [12/50], Class Loss: 0.0113, JMMD Loss: 0.0884
Validation Loss: 0.0042
Epoch [13/50], Class Loss: 0.0052, JMMD Loss: 0.0928
Validation Loss: 0.0060
Epoch [14/50], Class Loss: 0.0037, JMMD Loss: 0.0903
Validation Loss: 0.0068
Epoch [15/50], Class Loss: 0.0038, JMMD Loss: 0.0869
Validation Loss: 0.0053
Epoch [16/50], Class Loss: 0.0020, JMMD Loss: 0.0790
Validation Loss: 0.0086
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.89%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 98.14%, Precision: 98.14%, Recall: 98.14%, F1 Score: 98.12%

Run 2/10
Epoch [1/50], Class Loss: 0.3865, JMMD Loss: 0.2176
Validation Loss: 0.1455
Epoch [2/50], Class Loss: 0.1917, JMMD Loss: 0.1563
Validation Loss: 0.0870
Epoch [3/50], Class Loss: 0.1323, JMMD Loss: 0.1421
Validation Loss: 0.1187
Epoch [4/50], Class Loss: 0.0875, JMMD Loss: 0.1010
Validation Loss: 0.2934
Epoch [5/50], Class Loss: 0.0962, JMMD Loss: 0.1307
Validation Loss: 0.1102
Epoch [6/50], Class Loss: 0.0359, JMMD Loss: 0.0890
Validation Loss: 0.0882
Epoch [7/50], Class Loss: 0.0286, JMMD Loss: 0.0721
Validation Loss: 0.7428
Early stopping!
Source Domain Performance - Accuracy: 86.21%, Precision: 90.79%, Recall: 86.83%, F1 Score: 85.52%
Target Domain Performance - Accuracy: 78.84%, Precision: 88.08%, Recall: 78.97%, F1 Score: 78.95%

Run 3/10
Epoch [1/50], Class Loss: 0.3917, JMMD Loss: 0.1855
Validation Loss: 1.5220
Epoch [2/50], Class Loss: 0.1452, JMMD Loss: 0.1088
Validation Loss: 2.7816
Epoch [3/50], Class Loss: 0.2193, JMMD Loss: 0.1541
Validation Loss: 3.9225
Epoch [4/50], Class Loss: 0.0911, JMMD Loss: 0.1340
Validation Loss: 0.2364
Epoch [5/50], Class Loss: 0.0843, JMMD Loss: 0.1002
Validation Loss: 0.8850
Epoch [6/50], Class Loss: 0.1449, JMMD Loss: 0.1216
Validation Loss: 0.1371
Epoch [7/50], Class Loss: 0.0436, JMMD Loss: 0.1002
Validation Loss: 0.1222
Epoch [8/50], Class Loss: 0.0147, JMMD Loss: 0.0807
Validation Loss: 0.1138
Epoch [9/50], Class Loss: 0.0303, JMMD Loss: 0.0951
Validation Loss: 14.6978
Epoch [10/50], Class Loss: 0.0938, JMMD Loss: 0.1300
Validation Loss: 0.3754
Epoch [11/50], Class Loss: 0.0254, JMMD Loss: 0.0926
Validation Loss: 0.0292
Epoch [12/50], Class Loss: 0.0213, JMMD Loss: 0.0856
Validation Loss: 0.0287
Epoch [13/50], Class Loss: 0.0244, JMMD Loss: 0.0799
Validation Loss: 0.0351
Epoch [14/50], Class Loss: 0.0124, JMMD Loss: 0.0785
Validation Loss: 0.0310
Epoch [15/50], Class Loss: 0.0103, JMMD Loss: 0.0824
Validation Loss: 0.0473
Epoch [16/50], Class Loss: 0.0147, JMMD Loss: 0.0684
Validation Loss: 0.0284
Epoch [17/50], Class Loss: 0.0140, JMMD Loss: 0.0839
Validation Loss: 0.0382
Epoch [18/50], Class Loss: 0.0254, JMMD Loss: 0.0832
Validation Loss: 0.0412
Epoch [19/50], Class Loss: 0.0148, JMMD Loss: 0.0717
Validation Loss: 0.0530
Epoch [20/50], Class Loss: 0.0210, JMMD Loss: 0.0738
Validation Loss: 0.0197
Epoch [21/50], Class Loss: 0.0068, JMMD Loss: 0.0713
Validation Loss: 0.0294
Epoch [22/50], Class Loss: 0.0153, JMMD Loss: 0.0659
Validation Loss: 0.0228
Epoch [23/50], Class Loss: 0.0061, JMMD Loss: 0.0754
Validation Loss: 0.0383
Epoch [24/50], Class Loss: 0.0163, JMMD Loss: 0.0785
Validation Loss: 0.0242
Epoch [25/50], Class Loss: 0.0091, JMMD Loss: 0.0769
Validation Loss: 0.0308
Early stopping!
Source Domain Performance - Accuracy: 98.80%, Precision: 98.79%, Recall: 98.83%, F1 Score: 98.80%
Target Domain Performance - Accuracy: 90.65%, Precision: 92.75%, Recall: 90.68%, F1 Score: 90.68%

Run 4/10
Epoch [1/50], Class Loss: 0.4619, JMMD Loss: 0.2336
Validation Loss: 7.2468
Epoch [2/50], Class Loss: 0.2551, JMMD Loss: 0.1383
Validation Loss: 0.1870
Epoch [3/50], Class Loss: 0.2161, JMMD Loss: 0.1401
Validation Loss: 0.4005
Epoch [4/50], Class Loss: 0.1439, JMMD Loss: 0.1276
Validation Loss: 0.1882
Epoch [5/50], Class Loss: 0.0796, JMMD Loss: 0.1148
Validation Loss: 1.1902
Epoch [6/50], Class Loss: 0.0885, JMMD Loss: 0.1174
Validation Loss: 1.2531
Epoch [7/50], Class Loss: 0.0389, JMMD Loss: 0.1371
Validation Loss: 0.0127
Epoch [8/50], Class Loss: 0.0172, JMMD Loss: 0.1198
Validation Loss: 1.5022
Epoch [9/50], Class Loss: 0.0071, JMMD Loss: 0.0918
Validation Loss: 0.1212
Epoch [10/50], Class Loss: 0.0060, JMMD Loss: 0.0838
Validation Loss: 0.7740
Epoch [11/50], Class Loss: 0.0050, JMMD Loss: 0.0926
Validation Loss: 0.0080
Epoch [12/50], Class Loss: 0.0027, JMMD Loss: 0.0832
Validation Loss: 0.0062
Epoch [13/50], Class Loss: 0.0017, JMMD Loss: 0.0806
Validation Loss: 0.0147
Epoch [14/50], Class Loss: 0.0133, JMMD Loss: 0.0762
Validation Loss: 0.0081
Epoch [15/50], Class Loss: 0.0037, JMMD Loss: 0.0723
Validation Loss: 0.0097
Epoch [16/50], Class Loss: 0.0030, JMMD Loss: 0.0714
Validation Loss: 0.0099
Epoch [17/50], Class Loss: 0.0058, JMMD Loss: 0.0805
Validation Loss: 0.0088
Early stopping!
Source Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.77%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 97.78%, Precision: 97.79%, Recall: 97.77%, F1 Score: 97.75%

Run 5/10
Epoch [1/50], Class Loss: 0.3884, JMMD Loss: 0.1635
Validation Loss: 0.5030
Epoch [2/50], Class Loss: 0.1783, JMMD Loss: 0.1321
Validation Loss: 0.2278
Epoch [3/50], Class Loss: 0.1097, JMMD Loss: 0.1425
Validation Loss: 0.1136
Epoch [4/50], Class Loss: 0.0870, JMMD Loss: 0.1355
Validation Loss: 0.0674
Epoch [5/50], Class Loss: 0.0373, JMMD Loss: 0.0872
Validation Loss: 0.1441
Epoch [6/50], Class Loss: 0.0337, JMMD Loss: 0.0856
Validation Loss: 0.0874
Epoch [7/50], Class Loss: 0.0256, JMMD Loss: 0.1578
Validation Loss: 1.8758
Epoch [8/50], Class Loss: 0.0506, JMMD Loss: 0.1324
Validation Loss: 0.0736
Epoch [9/50], Class Loss: 0.0152, JMMD Loss: 0.1162
Validation Loss: 0.0358
Epoch [10/50], Class Loss: 0.0062, JMMD Loss: 0.0844
Validation Loss: 0.0183
Epoch [11/50], Class Loss: 0.0035, JMMD Loss: 0.0920
Validation Loss: 0.0037
Epoch [12/50], Class Loss: 0.0022, JMMD Loss: 0.0865
Validation Loss: 0.0059
Epoch [13/50], Class Loss: 0.0018, JMMD Loss: 0.0693
Validation Loss: 0.0094
Epoch [14/50], Class Loss: 0.0019, JMMD Loss: 0.0659
Validation Loss: 0.0308
Epoch [15/50], Class Loss: 0.0023, JMMD Loss: 0.0613
Validation Loss: 0.0258
Epoch [16/50], Class Loss: 0.0046, JMMD Loss: 0.0659
Validation Loss: 0.0234
Early stopping!
Source Domain Performance - Accuracy: 99.52%, Precision: 99.51%, Recall: 99.52%, F1 Score: 99.51%
Target Domain Performance - Accuracy: 98.38%, Precision: 98.36%, Recall: 98.37%, F1 Score: 98.35%

Run 6/10
Epoch [1/50], Class Loss: 0.4208, JMMD Loss: 0.1971
Validation Loss: 0.4814
Epoch [2/50], Class Loss: 0.1429, JMMD Loss: 0.1637
Validation Loss: 1.4954
Epoch [3/50], Class Loss: 0.0946, JMMD Loss: 0.1319
Validation Loss: 0.1076
Epoch [4/50], Class Loss: 0.0778, JMMD Loss: 0.1205
Validation Loss: 0.0597
Epoch [5/50], Class Loss: 0.0481, JMMD Loss: 0.0932
Validation Loss: 0.0592
Epoch [6/50], Class Loss: 0.1201, JMMD Loss: 0.1082
Validation Loss: 2.0019
Epoch [7/50], Class Loss: 0.2197, JMMD Loss: 0.1788
Validation Loss: 0.2616
Epoch [8/50], Class Loss: 0.0604, JMMD Loss: 0.1331
Validation Loss: 0.3372
Epoch [9/50], Class Loss: 0.0547, JMMD Loss: 0.1410
Validation Loss: 0.0219
Epoch [10/50], Class Loss: 0.0435, JMMD Loss: 0.1157
Validation Loss: 0.0451
Epoch [11/50], Class Loss: 0.0236, JMMD Loss: 0.1275
Validation Loss: 0.0278
Epoch [12/50], Class Loss: 0.0173, JMMD Loss: 0.1091
Validation Loss: 0.0481
Epoch [13/50], Class Loss: 0.0520, JMMD Loss: 0.1161
Validation Loss: 0.0520
Epoch [14/50], Class Loss: 0.0599, JMMD Loss: 0.1412
Validation Loss: 0.0803
Early stopping!
Source Domain Performance - Accuracy: 97.12%, Precision: 97.23%, Recall: 97.22%, F1 Score: 97.12%
Target Domain Performance - Accuracy: 73.68%, Precision: 81.74%, Recall: 73.75%, F1 Score: 65.79%

Run 7/10
Epoch [1/50], Class Loss: 0.4147, JMMD Loss: 0.1615
Validation Loss: 4.2347
Epoch [2/50], Class Loss: 0.1668, JMMD Loss: 0.1711
Validation Loss: 2.5482
Epoch [3/50], Class Loss: 0.1172, JMMD Loss: 0.1452
Validation Loss: 0.1802
Epoch [4/50], Class Loss: 0.0823, JMMD Loss: 0.1208
Validation Loss: 0.3688
Epoch [5/50], Class Loss: 0.0544, JMMD Loss: 0.1063
Validation Loss: 2.1725
Epoch [6/50], Class Loss: 0.0552, JMMD Loss: 0.1190
Validation Loss: 0.0520
Epoch [7/50], Class Loss: 0.0201, JMMD Loss: 0.0939
Validation Loss: 0.1165
Epoch [8/50], Class Loss: 0.0427, JMMD Loss: 0.1006
Validation Loss: 0.3100
Epoch [9/50], Class Loss: 0.0504, JMMD Loss: 0.1393
Validation Loss: 0.0771
Epoch [10/50], Class Loss: 0.1136, JMMD Loss: 0.1224
Validation Loss: 1.9537
Epoch [11/50], Class Loss: 0.0265, JMMD Loss: 0.1188
Validation Loss: 0.0309
Epoch [12/50], Class Loss: 0.0186, JMMD Loss: 0.1166
Validation Loss: 0.0289
Epoch [13/50], Class Loss: 0.0093, JMMD Loss: 0.1054
Validation Loss: 0.0264
Epoch [14/50], Class Loss: 0.0076, JMMD Loss: 0.1026
Validation Loss: 0.0254
Epoch [15/50], Class Loss: 0.0065, JMMD Loss: 0.1028
Validation Loss: 0.0279
Epoch [16/50], Class Loss: 0.0050, JMMD Loss: 0.0930
Validation Loss: 0.0318
Epoch [17/50], Class Loss: 0.0056, JMMD Loss: 0.0920
Validation Loss: 0.0256
Epoch [18/50], Class Loss: 0.0048, JMMD Loss: 0.0971
Validation Loss: 0.0213
Epoch [19/50], Class Loss: 0.0067, JMMD Loss: 0.0945
Validation Loss: 0.0164
Epoch [20/50], Class Loss: 0.0077, JMMD Loss: 0.0943
Validation Loss: 0.0467
Epoch [21/50], Class Loss: 0.0027, JMMD Loss: 0.0918
Validation Loss: 0.0215
Epoch [22/50], Class Loss: 0.0039, JMMD Loss: 0.0834
Validation Loss: 0.0319
Epoch [23/50], Class Loss: 0.0599, JMMD Loss: 0.0984
Validation Loss: 0.0172
Epoch [24/50], Class Loss: 0.0033, JMMD Loss: 0.0848
Validation Loss: 0.0289
Early stopping!
Source Domain Performance - Accuracy: 98.86%, Precision: 98.85%, Recall: 98.90%, F1 Score: 98.86%
Target Domain Performance - Accuracy: 87.53%, Precision: 91.26%, Recall: 87.57%, F1 Score: 87.32%

Run 8/10
Epoch [1/50], Class Loss: 0.3737, JMMD Loss: 0.1824
Validation Loss: 6.3160
Epoch [2/50], Class Loss: 0.1504, JMMD Loss: 0.1140
Validation Loss: 5.2697
Epoch [3/50], Class Loss: 0.1627, JMMD Loss: 0.1346
Validation Loss: 0.0990
Epoch [4/50], Class Loss: 0.0887, JMMD Loss: 0.0973
Validation Loss: 0.1287
Epoch [5/50], Class Loss: 0.0698, JMMD Loss: 0.0940
Validation Loss: 1.5680
Epoch [6/50], Class Loss: 0.0813, JMMD Loss: 0.1066
Validation Loss: 0.1517
Epoch [7/50], Class Loss: 0.1058, JMMD Loss: 0.1248
Validation Loss: 0.1048
Epoch [8/50], Class Loss: 0.0688, JMMD Loss: 0.2019
Validation Loss: 0.2281
Early stopping!
Source Domain Performance - Accuracy: 93.11%, Precision: 94.29%, Recall: 92.81%, F1 Score: 92.92%
Target Domain Performance - Accuracy: 69.42%, Precision: 78.92%, Recall: 69.10%, F1 Score: 63.42%

Run 9/10
Epoch [1/50], Class Loss: 0.4402, JMMD Loss: 0.1445
Validation Loss: 0.3682
Epoch [2/50], Class Loss: 0.2352, JMMD Loss: 0.1422
Validation Loss: 4.2432
Epoch [3/50], Class Loss: 0.1031, JMMD Loss: 0.1200
Validation Loss: 0.1280
Epoch [4/50], Class Loss: 0.0913, JMMD Loss: 0.1086
Validation Loss: 1.0661
Epoch [5/50], Class Loss: 0.2251, JMMD Loss: 0.1362
Validation Loss: 0.1306
Epoch [6/50], Class Loss: 0.0999, JMMD Loss: 0.1261
Validation Loss: 0.3168
Epoch [7/50], Class Loss: 0.0446, JMMD Loss: 0.1261
Validation Loss: 0.2985
Epoch [8/50], Class Loss: 0.0368, JMMD Loss: 0.1183
Validation Loss: 0.0195
Epoch [9/50], Class Loss: 0.0256, JMMD Loss: 0.1382
Validation Loss: 0.1295
Epoch [10/50], Class Loss: 0.0096, JMMD Loss: 0.1214
Validation Loss: 0.0200
Epoch [11/50], Class Loss: 0.0102, JMMD Loss: 0.0898
Validation Loss: 0.0347
Epoch [12/50], Class Loss: 0.0036, JMMD Loss: 0.0996
Validation Loss: 0.0085
Epoch [13/50], Class Loss: 0.0029, JMMD Loss: 0.0834
Validation Loss: 0.0179
Epoch [14/50], Class Loss: 0.0037, JMMD Loss: 0.0795
Validation Loss: 0.0150
Epoch [15/50], Class Loss: 0.0034, JMMD Loss: 0.0831
Validation Loss: 0.0100
Epoch [16/50], Class Loss: 0.0038, JMMD Loss: 0.0880
Validation Loss: 0.0137
Epoch [17/50], Class Loss: 0.0032, JMMD Loss: 0.0846
Validation Loss: 0.0070
Epoch [18/50], Class Loss: 0.0029, JMMD Loss: 0.0840
Validation Loss: 0.0294
Epoch [19/50], Class Loss: 0.0028, JMMD Loss: 0.0741
Validation Loss: 0.0176
Epoch [20/50], Class Loss: 0.0050, JMMD Loss: 0.0734
Validation Loss: 0.0095
Epoch [21/50], Class Loss: 0.0089, JMMD Loss: 0.0801
Validation Loss: 0.0173
Epoch [22/50], Class Loss: 0.0025, JMMD Loss: 0.0731
Validation Loss: 0.0121
Early stopping!
Source Domain Performance - Accuracy: 99.64%, Precision: 99.63%, Recall: 99.66%, F1 Score: 99.64%
Target Domain Performance - Accuracy: 92.21%, Precision: 93.78%, Recall: 92.23%, F1 Score: 92.15%

Run 10/10
Epoch [1/50], Class Loss: 0.4607, JMMD Loss: 0.1379
Validation Loss: 5.0014
Epoch [2/50], Class Loss: 0.2407, JMMD Loss: 0.1528
Validation Loss: 4.2345
Epoch [3/50], Class Loss: 0.2116, JMMD Loss: 0.1138
Validation Loss: 1.9279
Epoch [4/50], Class Loss: 0.1200, JMMD Loss: 0.1048
Validation Loss: 0.6314
Epoch [5/50], Class Loss: 0.0538, JMMD Loss: 0.0831
Validation Loss: 0.1244
Epoch [6/50], Class Loss: 0.0529, JMMD Loss: 0.0817
Validation Loss: 0.0372
Epoch [7/50], Class Loss: 0.0417, JMMD Loss: 0.1035
Validation Loss: 1.3052
Epoch [8/50], Class Loss: 0.0530, JMMD Loss: 0.1300
Validation Loss: 0.0547
Epoch [9/50], Class Loss: 0.0495, JMMD Loss: 0.1344
Validation Loss: 0.0648
Epoch [10/50], Class Loss: 0.0212, JMMD Loss: 0.1477
Validation Loss: 0.1917
Epoch [11/50], Class Loss: 0.0025, JMMD Loss: 0.1563
Validation Loss: 0.0089
Epoch [12/50], Class Loss: 0.0246, JMMD Loss: 0.1116
Validation Loss: 0.0179
Epoch [13/50], Class Loss: 0.0126, JMMD Loss: 0.0971
Validation Loss: 0.0203
Epoch [14/50], Class Loss: 0.0043, JMMD Loss: 0.0961
Validation Loss: 0.0071
Epoch [15/50], Class Loss: 0.0076, JMMD Loss: 0.0900
Validation Loss: 0.0137
Epoch [16/50], Class Loss: 0.0032, JMMD Loss: 0.0969
Validation Loss: 0.0101
Epoch [17/50], Class Loss: 0.0029, JMMD Loss: 0.0883
Validation Loss: 0.0129
Epoch [18/50], Class Loss: 0.0249, JMMD Loss: 0.0853
Validation Loss: 0.0085
Epoch [19/50], Class Loss: 0.0113, JMMD Loss: 0.0935
Validation Loss: 0.0178
Early stopping!
Source Domain Performance - Accuracy: 99.46%, Precision: 99.45%, Recall: 99.49%, F1 Score: 99.46%
Target Domain Performance - Accuracy: 97.54%, Precision: 97.64%, Recall: 97.56%, F1 Score: 97.51%

Source performance: 97.24% 97.82% 97.29% 97.15%
Target performance: 88.42% 91.85% 88.41% 87.00%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 100.00%
  Class 1: 67.25%
  Class 2: 90.50%
  Class 3: 95.90%
Base

Run 1/10
Epoch 1/50, Train Loss: 0.4112, Train Acc: 0.8290, Val Loss: 10.6710, Val Acc: 0.4940
Epoch 2/50, Train Loss: 0.1334, Train Acc: 0.9502, Val Loss: 0.0707, Val Acc: 0.9784
Epoch 3/50, Train Loss: 0.0579, Train Acc: 0.9790, Val Loss: 3.7466, Val Acc: 0.3165
Epoch 4/50, Train Loss: 0.0387, Train Acc: 0.9867, Val Loss: 2.0150, Val Acc: 0.7470
Epoch 5/50, Train Loss: 0.0225, Train Acc: 0.9942, Val Loss: 0.0300, Val Acc: 0.9880
Epoch 6/50, Train Loss: 0.0058, Train Acc: 0.9984, Val Loss: 0.0050, Val Acc: 0.9994
Epoch 7/50, Train Loss: 0.0047, Train Acc: 0.9985, Val Loss: 4.3000, Val Acc: 0.7230
Epoch 8/50, Train Loss: 0.0076, Train Acc: 0.9981, Val Loss: 1.5069, Val Acc: 0.7470
Epoch 9/50, Train Loss: 0.0046, Train Acc: 0.9988, Val Loss: 0.0259, Val Acc: 0.9916
Epoch 10/50, Train Loss: 0.0018, Train Acc: 0.9996, Val Loss: 0.1783, Val Acc: 0.9347
Epoch 11/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0011, Val Acc: 0.9994
Early stopping!

Run 2/10
Epoch 1/50, Train Loss: 0.4381, Train Acc: 0.8155, Val Loss: 1.7768, Val Acc: 0.7368
Epoch 2/50, Train Loss: 0.1103, Train Acc: 0.9592, Val Loss: 2.7407, Val Acc: 0.7440
Epoch 3/50, Train Loss: 0.0672, Train Acc: 0.9769, Val Loss: 1.4158, Val Acc: 0.6139
Epoch 4/50, Train Loss: 0.0520, Train Acc: 0.9829, Val Loss: 0.0302, Val Acc: 0.9880
Epoch 5/50, Train Loss: 0.0206, Train Acc: 0.9939, Val Loss: 1.5206, Val Acc: 0.6367
Epoch 6/50, Train Loss: 0.0161, Train Acc: 0.9957, Val Loss: 0.0334, Val Acc: 0.9904
Epoch 7/50, Train Loss: 0.0071, Train Acc: 0.9982, Val Loss: 2.2334, Val Acc: 0.7614
Epoch 8/50, Train Loss: 0.0116, Train Acc: 0.9976, Val Loss: 2.1909, Val Acc: 0.7602
Epoch 9/50, Train Loss: 0.0088, Train Acc: 0.9981, Val Loss: 2.7786, Val Acc: 0.7464
Early stopping!

Run 3/10
Epoch 1/50, Train Loss: 0.4076, Train Acc: 0.8413, Val Loss: 2.5795, Val Acc: 0.5234
Epoch 2/50, Train Loss: 0.0957, Train Acc: 0.9669, Val Loss: 0.6618, Val Acc: 0.8022
Epoch 3/50, Train Loss: 0.0417, Train Acc: 0.9882, Val Loss: 0.0114, Val Acc: 0.9964
Epoch 4/50, Train Loss: 0.0107, Train Acc: 0.9984, Val Loss: 0.0039, Val Acc: 0.9982
Epoch 5/50, Train Loss: 0.0101, Train Acc: 0.9973, Val Loss: 2.9337, Val Acc: 0.6469
Epoch 6/50, Train Loss: 0.0040, Train Acc: 0.9993, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 7/50, Train Loss: 0.0102, Train Acc: 0.9969, Val Loss: 0.0018, Val Acc: 1.0000
Epoch 8/50, Train Loss: 0.0017, Train Acc: 0.9996, Val Loss: 0.0205, Val Acc: 0.9910
Epoch 9/50, Train Loss: 0.0019, Train Acc: 0.9999, Val Loss: 0.5954, Val Acc: 0.8453
Epoch 10/50, Train Loss: 0.0034, Train Acc: 0.9991, Val Loss: 0.0021, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0023, Train Acc: 0.9997, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0008, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0055, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Early stopping!

Run 4/10
Epoch 1/50, Train Loss: 0.4146, Train Acc: 0.8389, Val Loss: 4.1092, Val Acc: 0.5695
Epoch 2/50, Train Loss: 0.1187, Train Acc: 0.9553, Val Loss: 5.1985, Val Acc: 0.6109
Epoch 3/50, Train Loss: 0.0597, Train Acc: 0.9787, Val Loss: 0.9824, Val Acc: 0.7422
Epoch 4/50, Train Loss: 0.0475, Train Acc: 0.9840, Val Loss: 0.0440, Val Acc: 0.9844
Epoch 5/50, Train Loss: 0.0391, Train Acc: 0.9877, Val Loss: 2.0640, Val Acc: 0.7224
Epoch 6/50, Train Loss: 0.0231, Train Acc: 0.9933, Val Loss: 0.0424, Val Acc: 0.9868
Epoch 7/50, Train Loss: 0.0147, Train Acc: 0.9952, Val Loss: 0.0196, Val Acc: 0.9928
Epoch 8/50, Train Loss: 0.0066, Train Acc: 0.9979, Val Loss: 0.0303, Val Acc: 0.9904
Epoch 9/50, Train Loss: 0.0065, Train Acc: 0.9981, Val Loss: 0.4508, Val Acc: 0.9059
Epoch 10/50, Train Loss: 0.0017, Train Acc: 0.9999, Val Loss: 0.0148, Val Acc: 0.9952
Epoch 11/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0061, Val Acc: 0.9982
Epoch 12/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0057, Val Acc: 0.9982
Epoch 13/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0030, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0054, Val Acc: 0.9982
Epoch 15/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0040, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0041, Val Acc: 0.9982
Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0076, Val Acc: 0.9976
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0032, Val Acc: 0.9988
Early stopping!

Run 5/10
Epoch 1/50, Train Loss: 0.4222, Train Acc: 0.8329, Val Loss: 2.1011, Val Acc: 0.7422
Epoch 2/50, Train Loss: 0.1151, Train Acc: 0.9597, Val Loss: 0.4192, Val Acc: 0.8591
Epoch 3/50, Train Loss: 0.0686, Train Acc: 0.9763, Val Loss: 1.3633, Val Acc: 0.6900
Epoch 4/50, Train Loss: 0.0243, Train Acc: 0.9918, Val Loss: 19.2916, Val Acc: 0.5036
Epoch 5/50, Train Loss: 0.0050, Train Acc: 0.9988, Val Loss: 0.0907, Val Acc: 0.9640
Epoch 6/50, Train Loss: 0.0024, Train Acc: 0.9997, Val Loss: 1.9201, Val Acc: 0.7554
Epoch 7/50, Train Loss: 0.0072, Train Acc: 0.9982, Val Loss: 2.4810, Val Acc: 0.7446
Epoch 8/50, Train Loss: 0.0160, Train Acc: 0.9955, Val Loss: 0.0065, Val Acc: 0.9964
Epoch 9/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 10/50, Train Loss: 0.0022, Train Acc: 0.9996, Val Loss: 0.0231, Val Acc: 0.9898
Epoch 11/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 30/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 31/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 32/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Early stopping!

Run 6/10
Epoch 1/50, Train Loss: 0.4290, Train Acc: 0.8230, Val Loss: 1.6651, Val Acc: 0.6775
Epoch 2/50, Train Loss: 0.1197, Train Acc: 0.9547, Val Loss: 1.2869, Val Acc: 0.6703
Epoch 3/50, Train Loss: 0.0685, Train Acc: 0.9735, Val Loss: 0.5182, Val Acc: 0.8357
Epoch 4/50, Train Loss: 0.0410, Train Acc: 0.9871, Val Loss: 1.3305, Val Acc: 0.6823
Epoch 5/50, Train Loss: 0.0167, Train Acc: 0.9940, Val Loss: 2.6666, Val Acc: 0.7434
Epoch 6/50, Train Loss: 0.0162, Train Acc: 0.9951, Val Loss: 0.0663, Val Acc: 0.9784
Epoch 7/50, Train Loss: 0.0132, Train Acc: 0.9964, Val Loss: 0.5563, Val Acc: 0.8567
Epoch 8/50, Train Loss: 0.0120, Train Acc: 0.9967, Val Loss: 2.2997, Val Acc: 0.5839
Epoch 9/50, Train Loss: 0.0156, Train Acc: 0.9966, Val Loss: 0.0072, Val Acc: 0.9976
Epoch 10/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0091, Val Acc: 0.9976
Epoch 11/50, Train Loss: 0.0019, Train Acc: 0.9997, Val Loss: 0.0035, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0059, Val Acc: 0.9976
Epoch 13/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0038, Val Acc: 0.9982
Epoch 14/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0038, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0026, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0035, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0033, Val Acc: 0.9988
Epoch 19/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0048, Val Acc: 0.9976
Epoch 20/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 24/50, Train Loss: 0.0025, Train Acc: 0.9997, Val Loss: 0.0033, Val Acc: 0.9988
Epoch 25/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0020, Val Acc: 0.9994
Epoch 26/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0021, Val Acc: 0.9994
Epoch 27/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9994
Epoch 28/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 29/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9994
Epoch 30/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0024, Val Acc: 0.9994
Early stopping!

Run 7/10
Epoch 1/50, Train Loss: 0.4434, Train Acc: 0.8274, Val Loss: 7.2568, Val Acc: 0.4898
Epoch 2/50, Train Loss: 0.1259, Train Acc: 0.9525, Val Loss: 3.0414, Val Acc: 0.6211
Epoch 3/50, Train Loss: 0.0571, Train Acc: 0.9801, Val Loss: 1.8938, Val Acc: 0.6337
Epoch 4/50, Train Loss: 0.0418, Train Acc: 0.9873, Val Loss: 0.2622, Val Acc: 0.9209
Epoch 5/50, Train Loss: 0.0126, Train Acc: 0.9966, Val Loss: 0.2582, Val Acc: 0.8939
Epoch 6/50, Train Loss: 0.0096, Train Acc: 0.9978, Val Loss: 1.7055, Val Acc: 0.7470
Epoch 7/50, Train Loss: 0.0064, Train Acc: 0.9985, Val Loss: 1.2581, Val Acc: 0.6643
Epoch 8/50, Train Loss: 0.0043, Train Acc: 0.9994, Val Loss: 0.0031, Val Acc: 0.9994
Epoch 9/50, Train Loss: 0.0072, Train Acc: 0.9975, Val Loss: 0.4986, Val Acc: 0.8279
Epoch 10/50, Train Loss: 0.0070, Train Acc: 0.9984, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 11/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0004, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994
Early stopping!

Run 8/10
Epoch 1/50, Train Loss: 0.4033, Train Acc: 0.8409, Val Loss: 5.2297, Val Acc: 0.6595
Epoch 2/50, Train Loss: 0.1104, Train Acc: 0.9619, Val Loss: 0.0759, Val Acc: 0.9742
Epoch 3/50, Train Loss: 0.0446, Train Acc: 0.9844, Val Loss: 1.5556, Val Acc: 0.7608
Epoch 4/50, Train Loss: 0.0543, Train Acc: 0.9832, Val Loss: 0.1472, Val Acc: 0.9610
Epoch 5/50, Train Loss: 0.0230, Train Acc: 0.9918, Val Loss: 0.0810, Val Acc: 0.9736
Epoch 6/50, Train Loss: 0.0117, Train Acc: 0.9970, Val Loss: 0.1239, Val Acc: 0.9592
Epoch 7/50, Train Loss: 0.0050, Train Acc: 0.9988, Val Loss: 0.0604, Val Acc: 0.9790
Epoch 8/50, Train Loss: 0.0050, Train Acc: 0.9982, Val Loss: 0.0433, Val Acc: 0.9844
Epoch 9/50, Train Loss: 0.0067, Train Acc: 0.9976, Val Loss: 0.0317, Val Acc: 0.9886
Epoch 10/50, Train Loss: 0.0456, Train Acc: 0.9933, Val Loss: 0.0601, Val Acc: 0.9790
Epoch 11/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0030, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0014, Train Acc: 0.9997, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0027, Train Acc: 0.9997, Val Loss: 0.0007, Val Acc: 1.0000
Early stopping!

Run 9/10
Epoch 1/50, Train Loss: 0.4594, Train Acc: 0.8163, Val Loss: 0.4712, Val Acc: 0.8411
Epoch 2/50, Train Loss: 0.0566, Train Acc: 0.9802, Val Loss: 0.5636, Val Acc: 0.8783
Epoch 3/50, Train Loss: 0.0233, Train Acc: 0.9928, Val Loss: 1.8945, Val Acc: 0.7632
Epoch 4/50, Train Loss: 0.0142, Train Acc: 0.9958, Val Loss: 0.0419, Val Acc: 0.9820
Epoch 5/50, Train Loss: 0.0113, Train Acc: 0.9978, Val Loss: 0.0272, Val Acc: 0.9934
Epoch 6/50, Train Loss: 0.0099, Train Acc: 0.9982, Val Loss: 0.0083, Val Acc: 0.9964
Epoch 7/50, Train Loss: 0.0018, Train Acc: 0.9996, Val Loss: 0.0850, Val Acc: 0.9676
Epoch 8/50, Train Loss: 0.0042, Train Acc: 0.9990, Val Loss: 0.2123, Val Acc: 0.9023
Epoch 9/50, Train Loss: 0.0034, Train Acc: 0.9990, Val Loss: 0.0399, Val Acc: 0.9874
Epoch 10/50, Train Loss: 0.0070, Train Acc: 0.9981, Val Loss: 0.0057, Val Acc: 0.9976
Epoch 11/50, Train Loss: 0.0067, Train Acc: 0.9988, Val Loss: 0.0058, Val Acc: 0.9982
Epoch 12/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0040, Val Acc: 0.9976
Epoch 13/50, Train Loss: 0.0027, Train Acc: 0.9997, Val Loss: 0.0073, Val Acc: 0.9976
Epoch 14/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0031, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 24/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9994
Epoch 25/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9994
Early stopping!

Run 10/10
Epoch 1/50, Train Loss: 0.4050, Train Acc: 0.8335, Val Loss: 2.3293, Val Acc: 0.7416
Epoch 2/50, Train Loss: 0.0915, Train Acc: 0.9660, Val Loss: 3.5571, Val Acc: 0.5522
Epoch 3/50, Train Loss: 0.0619, Train Acc: 0.9807, Val Loss: 0.0319, Val Acc: 0.9886
Epoch 4/50, Train Loss: 0.0213, Train Acc: 0.9948, Val Loss: 0.0483, Val Acc: 0.9868
Epoch 5/50, Train Loss: 0.0205, Train Acc: 0.9946, Val Loss: 1.2836, Val Acc: 0.8010
Epoch 6/50, Train Loss: 0.0073, Train Acc: 0.9979, Val Loss: 2.7108, Val Acc: 0.6097
Epoch 7/50, Train Loss: 0.0048, Train Acc: 0.9988, Val Loss: 0.0123, Val Acc: 0.9952
Epoch 8/50, Train Loss: 0.0069, Train Acc: 0.9981, Val Loss: 0.2678, Val Acc: 0.9251
Epoch 9/50, Train Loss: 0.0099, Train Acc: 0.9975, Val Loss: 0.1079, Val Acc: 0.9676
Epoch 10/50, Train Loss: 0.0035, Train Acc: 0.9993, Val Loss: 0.6852, Val Acc: 0.8447
Epoch 11/50, Train Loss: 0.0020, Train Acc: 0.9994, Val Loss: 0.0092, Val Acc: 0.9970
Epoch 12/50, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0033, Val Acc: 0.9982
Epoch 13/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0042, Val Acc: 0.9982
Epoch 14/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0040, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0035, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0037, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0012, Train Acc: 0.9997, Val Loss: 0.0031, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0045, Val Acc: 0.9988
Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0035, Val Acc: 0.9988
Epoch 20/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0030, Val Acc: 0.9982
Epoch 21/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0035, Val Acc: 0.9988
Epoch 22/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0053, Val Acc: 0.9988
Epoch 23/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0045, Val Acc: 0.9988
Epoch 24/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0048, Val Acc: 0.9988
Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0041, Val Acc: 0.9988
Early stopping!

Source performance: 97.42 98.67 97.47 96.64
Target performance: 97.58 98.75 97.51 96.73

bpsk: 100.00
qpsk: 100.00
16qam: 99.98
8apsk: 90.07
DANN
Epoch 1/50, Loss: 2.3506, Domain Loss: 1.3983, Class Loss: 0.9524
Epoch 2/50, Loss: 1.8058, Domain Loss: 1.3952, Class Loss: 0.4106
Epoch 3/50, Loss: 1.7041, Domain Loss: 1.3893, Class Loss: 0.3148
Epoch 4/50, Loss: 1.5492, Domain Loss: 1.3877, Class Loss: 0.1615
Epoch 5/50, Loss: 1.5538, Domain Loss: 1.3934, Class Loss: 0.1604
Epoch 6/50, Loss: 1.5427, Domain Loss: 1.3874, Class Loss: 0.1552
Epoch 7/50, Loss: 1.5216, Domain Loss: 1.3923, Class Loss: 0.1293
Epoch 8/50, Loss: 2.0856, Domain Loss: 1.9533, Class Loss: 0.1324
Epoch 9/50, Loss: 2.0147, Domain Loss: 1.8135, Class Loss: 0.2011
Epoch 10/50, Loss: 3.3313, Domain Loss: 3.1645, Class Loss: 0.1668
Epoch 11/50, Loss: 2.8558, Domain Loss: 2.5718, Class Loss: 0.2840
Epoch 12/50, Loss: 1.6984, Domain Loss: 1.5171, Class Loss: 0.1813
Epoch 13/50, Loss: 1.5036, Domain Loss: 1.4199, Class Loss: 0.0837
Epoch 14/50, Loss: 1.9644, Domain Loss: 1.6054, Class Loss: 0.3590
Epoch 15/50, Loss: 1.9128, Domain Loss: 1.6919, Class Loss: 0.2208
Epoch 16/50, Loss: 1.6238, Domain Loss: 1.4871, Class Loss: 0.1367
Epoch 17/50, Loss: 1.5033, Domain Loss: 1.4280, Class Loss: 0.0753
Epoch 18/50, Loss: 1.5389, Domain Loss: 1.4047, Class Loss: 0.1342
Epoch 19/50, Loss: 1.4751, Domain Loss: 1.4110, Class Loss: 0.0642
Epoch 20/50, Loss: 1.4850, Domain Loss: 1.4510, Class Loss: 0.0340
Epoch 21/50, Loss: 1.5687, Domain Loss: 1.5271, Class Loss: 0.0416
Epoch 22/50, Loss: 1.6370, Domain Loss: 1.5879, Class Loss: 0.0491
Epoch 23/50, Loss: 1.6834, Domain Loss: 1.6153, Class Loss: 0.0681
Epoch 24/50, Loss: 1.8778, Domain Loss: 1.8286, Class Loss: 0.0492
Epoch 25/50, Loss: 1.6615, Domain Loss: 1.6047, Class Loss: 0.0569
Epoch 26/50, Loss: 1.6304, Domain Loss: 1.5853, Class Loss: 0.0451
Epoch 27/50, Loss: 1.5467, Domain Loss: 1.5302, Class Loss: 0.0165
Epoch 28/50, Loss: 1.5023, Domain Loss: 1.4729, Class Loss: 0.0294
Epoch 29/50, Loss: 1.4625, Domain Loss: 1.4510, Class Loss: 0.0116
Epoch 30/50, Loss: 1.4451, Domain Loss: 1.4282, Class Loss: 0.0170
Epoch 31/50, Loss: 1.4247, Domain Loss: 1.4198, Class Loss: 0.0049
Epoch 32/50, Loss: 1.4305, Domain Loss: 1.4179, Class Loss: 0.0126
Epoch 33/50, Loss: 1.4492, Domain Loss: 1.4343, Class Loss: 0.0150
Epoch 34/50, Loss: 1.4110, Domain Loss: 1.4037, Class Loss: 0.0073
Epoch 35/50, Loss: 1.4362, Domain Loss: 1.4273, Class Loss: 0.0089
Epoch 36/50, Loss: 1.5080, Domain Loss: 1.5043, Class Loss: 0.0037
Epoch 37/50, Loss: 1.4876, Domain Loss: 1.4761, Class Loss: 0.0116
Epoch 38/50, Loss: 1.6027, Domain Loss: 1.5762, Class Loss: 0.0265
Epoch 39/50, Loss: 1.6263, Domain Loss: 1.5496, Class Loss: 0.0767
Epoch 40/50, Loss: 1.5390, Domain Loss: 1.5189, Class Loss: 0.0200
Epoch 41/50, Loss: 1.5366, Domain Loss: 1.5293, Class Loss: 0.0074
Epoch 42/50, Loss: 1.5222, Domain Loss: 1.5106, Class Loss: 0.0116
Epoch 43/50, Loss: 1.5377, Domain Loss: 1.5108, Class Loss: 0.0269
Epoch 44/50, Loss: 1.4971, Domain Loss: 1.4844, Class Loss: 0.0127
Epoch 45/50, Loss: 1.4816, Domain Loss: 1.4654, Class Loss: 0.0163
Epoch 46/50, Loss: 1.4290, Domain Loss: 1.4244, Class Loss: 0.0046
Epoch 47/50, Loss: 1.4300, Domain Loss: 1.4270, Class Loss: 0.0031
Epoch 48/50, Loss: 1.4479, Domain Loss: 1.4416, Class Loss: 0.0064
Epoch 49/50, Loss: 1.4524, Domain Loss: 1.4372, Class Loss: 0.0151
Epoch 50/50, Loss: 1.4464, Domain Loss: 1.4415, Class Loss: 0.0048
99.34


Epoch 1/50, Loss: 2.3474, Domain Loss: 1.3967, Class Loss: 0.9506
Epoch 2/50, Loss: 1.8652, Domain Loss: 1.3893, Class Loss: 0.4759
Epoch 3/50, Loss: 1.7310, Domain Loss: 1.3896, Class Loss: 0.3414
Epoch 4/50, Loss: 1.6704, Domain Loss: 1.3901, Class Loss: 0.2803
Epoch 5/50, Loss: 1.6081, Domain Loss: 1.3884, Class Loss: 0.2197
Epoch 6/50, Loss: 1.5513, Domain Loss: 1.3918, Class Loss: 0.1596
Epoch 7/50, Loss: 1.5407, Domain Loss: 1.3933, Class Loss: 0.1474
Epoch 8/50, Loss: 1.5287, Domain Loss: 1.4239, Class Loss: 0.1048
Epoch 9/50, Loss: 2.9211, Domain Loss: 2.8228, Class Loss: 0.0982
Epoch 10/50, Loss: 7.2697, Domain Loss: 7.1042, Class Loss: 0.1654
Epoch 11/50, Loss: 5.1173, Domain Loss: 4.8911, Class Loss: 0.2262
Epoch 12/50, Loss: 2.3469, Domain Loss: 2.0652, Class Loss: 0.2816
Epoch 13/50, Loss: 1.7184, Domain Loss: 1.4482, Class Loss: 0.2702
Epoch 14/50, Loss: 1.5749, Domain Loss: 1.4306, Class Loss: 0.1443
Epoch 15/50, Loss: 1.5176, Domain Loss: 1.4132, Class Loss: 0.1044
Epoch 16/50, Loss: 1.4804, Domain Loss: 1.3992, Class Loss: 0.0812
Epoch 17/50, Loss: 1.5063, Domain Loss: 1.4083, Class Loss: 0.0980
Epoch 18/50, Loss: 1.4440, Domain Loss: 1.3906, Class Loss: 0.0533
Epoch 19/50, Loss: 1.4576, Domain Loss: 1.4042, Class Loss: 0.0534
Epoch 20/50, Loss: 1.4383, Domain Loss: 1.3917, Class Loss: 0.0466
Epoch 21/50, Loss: 1.4316, Domain Loss: 1.3940, Class Loss: 0.0376
Epoch 22/50, Loss: 1.4386, Domain Loss: 1.3921, Class Loss: 0.0465
Epoch 23/50, Loss: 1.4333, Domain Loss: 1.3910, Class Loss: 0.0423
Epoch 24/50, Loss: 1.4448, Domain Loss: 1.3906, Class Loss: 0.0542
Epoch 25/50, Loss: 1.4271, Domain Loss: 1.3850, Class Loss: 0.0420
Epoch 26/50, Loss: 1.4135, Domain Loss: 1.3869, Class Loss: 0.0267
Epoch 27/50, Loss: 1.4271, Domain Loss: 1.3940, Class Loss: 0.0331
Epoch 28/50, Loss: 1.4137, Domain Loss: 1.3916, Class Loss: 0.0221
Epoch 29/50, Loss: 1.4013, Domain Loss: 1.3895, Class Loss: 0.0119
Epoch 30/50, Loss: 1.4065, Domain Loss: 1.3912, Class Loss: 0.0153
Epoch 31/50, Loss: 1.4041, Domain Loss: 1.3904, Class Loss: 0.0137
Epoch 32/50, Loss: 1.4023, Domain Loss: 1.3912, Class Loss: 0.0111
Epoch 33/50, Loss: 1.4024, Domain Loss: 1.3885, Class Loss: 0.0139
Epoch 34/50, Loss: 1.4046, Domain Loss: 1.3879, Class Loss: 0.0167
Epoch 35/50, Loss: 1.3943, Domain Loss: 1.3885, Class Loss: 0.0058
Epoch 36/50, Loss: 1.3975, Domain Loss: 1.3881, Class Loss: 0.0094
Epoch 37/50, Loss: 1.4006, Domain Loss: 1.3927, Class Loss: 0.0079
Epoch 38/50, Loss: 1.4187, Domain Loss: 1.3909, Class Loss: 0.0278
Epoch 39/50, Loss: 1.4140, Domain Loss: 1.3932, Class Loss: 0.0207
Epoch 40/50, Loss: 1.4064, Domain Loss: 1.3863, Class Loss: 0.0201
Epoch 41/50, Loss: 1.4186, Domain Loss: 1.3894, Class Loss: 0.0292
Epoch 42/50, Loss: 1.4080, Domain Loss: 1.3862, Class Loss: 0.0218
Epoch 43/50, Loss: 1.4021, Domain Loss: 1.3879, Class Loss: 0.0141
Epoch 44/50, Loss: 1.3941, Domain Loss: 1.3888, Class Loss: 0.0052
Epoch 45/50, Loss: 1.4004, Domain Loss: 1.3889, Class Loss: 0.0115
Epoch 46/50, Loss: 1.3964, Domain Loss: 1.3910, Class Loss: 0.0054
Epoch 47/50, Loss: 1.3944, Domain Loss: 1.3893, Class Loss: 0.0051
Epoch 48/50, Loss: 1.3940, Domain Loss: 1.3883, Class Loss: 0.0057
Epoch 49/50, Loss: 1.3924, Domain Loss: 1.3908, Class Loss: 0.0016
Epoch 50/50, Loss: 1.3978, Domain Loss: 1.3896, Class Loss: 0.0082
98.74


Epoch 1/50, Loss: 2.4281, Domain Loss: 1.3955, Class Loss: 1.0326
Epoch 2/50, Loss: 1.8080, Domain Loss: 1.3910, Class Loss: 0.4170
Epoch 3/50, Loss: 1.6958, Domain Loss: 1.3895, Class Loss: 0.3063
Epoch 4/50, Loss: 1.6135, Domain Loss: 1.3907, Class Loss: 0.2227
Epoch 5/50, Loss: 1.5843, Domain Loss: 1.3867, Class Loss: 0.1976
Epoch 6/50, Loss: 1.5195, Domain Loss: 1.3912, Class Loss: 0.1283
Epoch 7/50, Loss: 1.4930, Domain Loss: 1.3858, Class Loss: 0.1072
Epoch 8/50, Loss: 1.5147, Domain Loss: 1.3910, Class Loss: 0.1237
Epoch 9/50, Loss: 1.5142, Domain Loss: 1.4284, Class Loss: 0.0858
Epoch 10/50, Loss: 3.0765, Domain Loss: 2.9540, Class Loss: 0.1225
Epoch 11/50, Loss: 5.0276, Domain Loss: 4.7945, Class Loss: 0.2331
Epoch 12/50, Loss: 2.8778, Domain Loss: 2.4478, Class Loss: 0.4300
Epoch 13/50, Loss: 7.6335, Domain Loss: 6.2980, Class Loss: 1.3355
Epoch 14/50, Loss: 14.2883, Domain Loss: 12.9619, Class Loss: 1.3264
Epoch 15/50, Loss: 5.0849, Domain Loss: 4.6423, Class Loss: 0.4426
Epoch 16/50, Loss: 3.0291, Domain Loss: 2.3213, Class Loss: 0.7078
Epoch 17/50, Loss: 3.0179, Domain Loss: 2.5364, Class Loss: 0.4815
Epoch 18/50, Loss: 6.3667, Domain Loss: 5.4190, Class Loss: 0.9477
Epoch 19/50, Loss: 4.9134, Domain Loss: 4.0447, Class Loss: 0.8687
Epoch 20/50, Loss: 4.2361, Domain Loss: 3.5837, Class Loss: 0.6524
Epoch 21/50, Loss: 3.9956, Domain Loss: 3.5558, Class Loss: 0.4398
Epoch 22/50, Loss: 3.3697, Domain Loss: 2.8244, Class Loss: 0.5454
Epoch 23/50, Loss: 2.9491, Domain Loss: 2.4733, Class Loss: 0.4758
Epoch 24/50, Loss: 2.3096, Domain Loss: 1.8873, Class Loss: 0.4222
Epoch 25/50, Loss: 1.9677, Domain Loss: 1.5755, Class Loss: 0.3922
Epoch 26/50, Loss: 1.8887, Domain Loss: 1.5136, Class Loss: 0.3752
Epoch 27/50, Loss: 1.8176, Domain Loss: 1.4593, Class Loss: 0.3582
Epoch 28/50, Loss: 1.7489, Domain Loss: 1.4182, Class Loss: 0.3307
Epoch 29/50, Loss: 1.7592, Domain Loss: 1.4169, Class Loss: 0.3423
Epoch 30/50, Loss: 1.7362, Domain Loss: 1.4144, Class Loss: 0.3218
Epoch 31/50, Loss: 1.7277, Domain Loss: 1.4076, Class Loss: 0.3201
Epoch 32/50, Loss: 1.6837, Domain Loss: 1.4002, Class Loss: 0.2835
Epoch 33/50, Loss: 1.6564, Domain Loss: 1.3864, Class Loss: 0.2700
Epoch 34/50, Loss: 1.6484, Domain Loss: 1.4022, Class Loss: 0.2462
Epoch 35/50, Loss: 1.6771, Domain Loss: 1.4227, Class Loss: 0.2544
Epoch 36/50, Loss: 1.6040, Domain Loss: 1.3916, Class Loss: 0.2124
Epoch 37/50, Loss: 1.6307, Domain Loss: 1.4021, Class Loss: 0.2286
Epoch 38/50, Loss: 1.5758, Domain Loss: 1.3999, Class Loss: 0.1759
Epoch 39/50, Loss: 1.6007, Domain Loss: 1.4092, Class Loss: 0.1915
Epoch 40/50, Loss: 1.5786, Domain Loss: 1.4144, Class Loss: 0.1641
Epoch 41/50, Loss: 1.5784, Domain Loss: 1.4122, Class Loss: 0.1662
Epoch 42/50, Loss: 1.5580, Domain Loss: 1.3996, Class Loss: 0.1584
Epoch 43/50, Loss: 1.5047, Domain Loss: 1.4044, Class Loss: 0.1003
Epoch 44/50, Loss: 1.5400, Domain Loss: 1.4118, Class Loss: 0.1283
Epoch 45/50, Loss: 1.5470, Domain Loss: 1.4477, Class Loss: 0.0993
Epoch 46/50, Loss: 1.6998, Domain Loss: 1.5490, Class Loss: 0.1508
Epoch 47/50, Loss: 1.6579, Domain Loss: 1.5625, Class Loss: 0.0954
Epoch 48/50, Loss: 1.5500, Domain Loss: 1.4270, Class Loss: 0.1231
Epoch 49/50, Loss: 1.4731, Domain Loss: 1.4034, Class Loss: 0.0696
Epoch 50/50, Loss: 1.4346, Domain Loss: 1.3892, Class Loss: 0.0454
96.88


Epoch 1/50, Loss: 2.3694, Domain Loss: 1.4027, Class Loss: 0.9667
Epoch 2/50, Loss: 1.8034, Domain Loss: 1.3927, Class Loss: 0.4107
Epoch 3/50, Loss: 1.6709, Domain Loss: 1.3952, Class Loss: 0.2757
Epoch 4/50, Loss: 1.5747, Domain Loss: 1.3891, Class Loss: 0.1856
Epoch 5/50, Loss: 1.5224, Domain Loss: 1.3885, Class Loss: 0.1340
Epoch 6/50, Loss: 1.5135, Domain Loss: 1.3889, Class Loss: 0.1246
Epoch 7/50, Loss: 1.5381, Domain Loss: 1.3915, Class Loss: 0.1466
Epoch 8/50, Loss: 2.8202, Domain Loss: 2.6367, Class Loss: 0.1835
Epoch 9/50, Loss: 3.9336, Domain Loss: 3.7632, Class Loss: 0.1704
Epoch 10/50, Loss: 2.6456, Domain Loss: 2.4932, Class Loss: 0.1524
Epoch 11/50, Loss: 1.6640, Domain Loss: 1.5768, Class Loss: 0.0871
Epoch 12/50, Loss: 1.5063, Domain Loss: 1.4494, Class Loss: 0.0569
Epoch 13/50, Loss: 1.4557, Domain Loss: 1.4007, Class Loss: 0.0550
Epoch 14/50, Loss: 1.4674, Domain Loss: 1.3931, Class Loss: 0.0743
Epoch 15/50, Loss: 1.4409, Domain Loss: 1.3878, Class Loss: 0.0531
Epoch 16/50, Loss: 1.4239, Domain Loss: 1.3963, Class Loss: 0.0276
Epoch 17/50, Loss: 1.4625, Domain Loss: 1.4303, Class Loss: 0.0322
Epoch 18/50, Loss: 2.1425, Domain Loss: 1.9995, Class Loss: 0.1430
Epoch 19/50, Loss: 3.8703, Domain Loss: 3.6061, Class Loss: 0.2642
Epoch 20/50, Loss: 2.6920, Domain Loss: 2.4367, Class Loss: 0.2553
Epoch 21/50, Loss: 3.5497, Domain Loss: 2.8946, Class Loss: 0.6551
Epoch 22/50, Loss: 2.5908, Domain Loss: 2.1750, Class Loss: 0.4157
Epoch 23/50, Loss: 2.9046, Domain Loss: 2.4371, Class Loss: 0.4675
Epoch 24/50, Loss: 2.0147, Domain Loss: 1.7405, Class Loss: 0.2742
Epoch 25/50, Loss: 1.6674, Domain Loss: 1.5075, Class Loss: 0.1600
Epoch 26/50, Loss: 1.7800, Domain Loss: 1.6091, Class Loss: 0.1709
Epoch 27/50, Loss: 1.8783, Domain Loss: 1.5829, Class Loss: 0.2954
Epoch 28/50, Loss: 1.6599, Domain Loss: 1.5101, Class Loss: 0.1498
Epoch 29/50, Loss: 1.5198, Domain Loss: 1.4564, Class Loss: 0.0634
Epoch 30/50, Loss: 1.4818, Domain Loss: 1.4409, Class Loss: 0.0409
Epoch 31/50, Loss: 1.4771, Domain Loss: 1.4493, Class Loss: 0.0277
Epoch 32/50, Loss: 1.5019, Domain Loss: 1.4648, Class Loss: 0.0371
Epoch 33/50, Loss: 1.4848, Domain Loss: 1.4597, Class Loss: 0.0252
Epoch 34/50, Loss: 1.4980, Domain Loss: 1.4760, Class Loss: 0.0220
Epoch 35/50, Loss: 1.4976, Domain Loss: 1.4687, Class Loss: 0.0289
Epoch 36/50, Loss: 1.4680, Domain Loss: 1.4412, Class Loss: 0.0268
Epoch 37/50, Loss: 1.4885, Domain Loss: 1.4539, Class Loss: 0.0346
Epoch 38/50, Loss: 1.4953, Domain Loss: 1.4643, Class Loss: 0.0310
Epoch 39/50, Loss: 1.4390, Domain Loss: 1.4214, Class Loss: 0.0176
Epoch 40/50, Loss: 1.4308, Domain Loss: 1.4173, Class Loss: 0.0135
Epoch 41/50, Loss: 1.4195, Domain Loss: 1.4068, Class Loss: 0.0127
Epoch 42/50, Loss: 1.4186, Domain Loss: 1.4074, Class Loss: 0.0112
Epoch 43/50, Loss: 1.4013, Domain Loss: 1.3943, Class Loss: 0.0070
Epoch 44/50, Loss: 1.4037, Domain Loss: 1.3948, Class Loss: 0.0090
Epoch 45/50, Loss: 1.3993, Domain Loss: 1.3942, Class Loss: 0.0051
Epoch 46/50, Loss: 1.4006, Domain Loss: 1.3952, Class Loss: 0.0054
Epoch 47/50, Loss: 1.4016, Domain Loss: 1.3977, Class Loss: 0.0039
Epoch 48/50, Loss: 1.3991, Domain Loss: 1.3937, Class Loss: 0.0054
Epoch 49/50, Loss: 1.4003, Domain Loss: 1.3968, Class Loss: 0.0035
Epoch 50/50, Loss: 1.4041, Domain Loss: 1.3982, Class Loss: 0.0058
99.40


Epoch 1/50, Loss: 2.3438, Domain Loss: 1.4040, Class Loss: 0.9399
Epoch 2/50, Loss: 1.8219, Domain Loss: 1.3914, Class Loss: 0.4305
Epoch 3/50, Loss: 1.7053, Domain Loss: 1.3869, Class Loss: 0.3184
Epoch 4/50, Loss: 1.6091, Domain Loss: 1.3864, Class Loss: 0.2228
Epoch 5/50, Loss: 1.5487, Domain Loss: 1.3879, Class Loss: 0.1608
Epoch 6/50, Loss: 1.5104, Domain Loss: 1.3873, Class Loss: 0.1231
Epoch 7/50, Loss: 1.4744, Domain Loss: 1.3880, Class Loss: 0.0863
Epoch 8/50, Loss: 1.4473, Domain Loss: 1.3875, Class Loss: 0.0599
Epoch 9/50, Loss: 1.4857, Domain Loss: 1.4422, Class Loss: 0.0436
Epoch 10/50, Loss: 3.4145, Domain Loss: 3.2845, Class Loss: 0.1300
Epoch 11/50, Loss: 4.5351, Domain Loss: 4.2926, Class Loss: 0.2425
Epoch 12/50, Loss: 4.8314, Domain Loss: 4.5533, Class Loss: 0.2781
Epoch 13/50, Loss: 2.2043, Domain Loss: 2.0361, Class Loss: 0.1682
Epoch 14/50, Loss: 2.0119, Domain Loss: 1.8279, Class Loss: 0.1840
Epoch 15/50, Loss: 3.0946, Domain Loss: 2.9337, Class Loss: 0.1609
Epoch 16/50, Loss: 3.3377, Domain Loss: 3.1565, Class Loss: 0.1812
Epoch 17/50, Loss: 1.9994, Domain Loss: 1.9094, Class Loss: 0.0899
Epoch 18/50, Loss: 1.6274, Domain Loss: 1.4989, Class Loss: 0.1285
Epoch 19/50, Loss: 1.4699, Domain Loss: 1.4416, Class Loss: 0.0284
Epoch 20/50, Loss: 1.4497, Domain Loss: 1.4290, Class Loss: 0.0207
Epoch 21/50, Loss: 1.4671, Domain Loss: 1.4509, Class Loss: 0.0162
Epoch 22/50, Loss: 1.4814, Domain Loss: 1.4568, Class Loss: 0.0246
Epoch 23/50, Loss: 1.4292, Domain Loss: 1.4120, Class Loss: 0.0172
Epoch 24/50, Loss: 1.4199, Domain Loss: 1.4101, Class Loss: 0.0098
Epoch 25/50, Loss: 1.4237, Domain Loss: 1.4134, Class Loss: 0.0102
Epoch 26/50, Loss: 1.4195, Domain Loss: 1.4128, Class Loss: 0.0067
Epoch 27/50, Loss: 1.4096, Domain Loss: 1.4037, Class Loss: 0.0059
Epoch 28/50, Loss: 1.4106, Domain Loss: 1.4023, Class Loss: 0.0083
Epoch 29/50, Loss: 1.4028, Domain Loss: 1.3976, Class Loss: 0.0052
Epoch 30/50, Loss: 1.4023, Domain Loss: 1.3962, Class Loss: 0.0061
Epoch 31/50, Loss: 1.3942, Domain Loss: 1.3906, Class Loss: 0.0035
Epoch 32/50, Loss: 1.3974, Domain Loss: 1.3949, Class Loss: 0.0024
Epoch 33/50, Loss: 1.4126, Domain Loss: 1.4096, Class Loss: 0.0030
Epoch 34/50, Loss: 1.4024, Domain Loss: 1.3990, Class Loss: 0.0034
Epoch 35/50, Loss: 1.4051, Domain Loss: 1.3981, Class Loss: 0.0070
Epoch 36/50, Loss: 1.4072, Domain Loss: 1.4040, Class Loss: 0.0033
Epoch 37/50, Loss: 1.4147, Domain Loss: 1.4081, Class Loss: 0.0067
Epoch 38/50, Loss: 1.4060, Domain Loss: 1.4022, Class Loss: 0.0038
Epoch 39/50, Loss: 1.4026, Domain Loss: 1.3990, Class Loss: 0.0036
Epoch 40/50, Loss: 1.4063, Domain Loss: 1.4032, Class Loss: 0.0031
Epoch 41/50, Loss: 1.4092, Domain Loss: 1.4080, Class Loss: 0.0012
Epoch 42/50, Loss: 1.4112, Domain Loss: 1.4054, Class Loss: 0.0057
Epoch 43/50, Loss: 1.4074, Domain Loss: 1.4006, Class Loss: 0.0068
Epoch 44/50, Loss: 1.4044, Domain Loss: 1.4035, Class Loss: 0.0009
Epoch 45/50, Loss: 1.4141, Domain Loss: 1.4099, Class Loss: 0.0041
Epoch 46/50, Loss: 1.4173, Domain Loss: 1.4163, Class Loss: 0.0010
Epoch 47/50, Loss: 1.4584, Domain Loss: 1.4544, Class Loss: 0.0040
Epoch 48/50, Loss: 1.6547, Domain Loss: 1.6431, Class Loss: 0.0116
Epoch 49/50, Loss: 1.4881, Domain Loss: 1.4835, Class Loss: 0.0046
Epoch 50/50, Loss: 1.5132, Domain Loss: 1.5077, Class Loss: 0.0056
99.94


Epoch 1/50, Loss: 2.2389, Domain Loss: 1.4026, Class Loss: 0.8363
Epoch 2/50, Loss: 1.8124, Domain Loss: 1.3896, Class Loss: 0.4228
Epoch 3/50, Loss: 1.6839, Domain Loss: 1.3910, Class Loss: 0.2929
Epoch 4/50, Loss: 1.6095, Domain Loss: 1.3888, Class Loss: 0.2207
Epoch 5/50, Loss: 1.5635, Domain Loss: 1.3859, Class Loss: 0.1775
Epoch 6/50, Loss: 1.4753, Domain Loss: 1.3881, Class Loss: 0.0872
Epoch 7/50, Loss: 1.4759, Domain Loss: 1.3880, Class Loss: 0.0879
Epoch 8/50, Loss: 1.4552, Domain Loss: 1.3902, Class Loss: 0.0649
Epoch 9/50, Loss: 1.4336, Domain Loss: 1.3921, Class Loss: 0.0415
Epoch 10/50, Loss: 1.5079, Domain Loss: 1.4676, Class Loss: 0.0403
Epoch 11/50, Loss: 2.7169, Domain Loss: 2.6094, Class Loss: 0.1075
Epoch 12/50, Loss: 3.8756, Domain Loss: 3.5586, Class Loss: 0.3171
Epoch 13/50, Loss: 7.9326, Domain Loss: 7.2877, Class Loss: 0.6449
Epoch 14/50, Loss: 15.2120, Domain Loss: 13.9400, Class Loss: 1.2720
Epoch 15/50, Loss: 5.7625, Domain Loss: 5.1647, Class Loss: 0.5978
Epoch 16/50, Loss: 2.5639, Domain Loss: 2.1050, Class Loss: 0.4589
Epoch 17/50, Loss: 1.8520, Domain Loss: 1.4765, Class Loss: 0.3755
Epoch 18/50, Loss: 1.7601, Domain Loss: 1.4156, Class Loss: 0.3445
Epoch 19/50, Loss: 1.7388, Domain Loss: 1.4233, Class Loss: 0.3155
Epoch 20/50, Loss: 1.6869, Domain Loss: 1.4174, Class Loss: 0.2696
Epoch 21/50, Loss: 1.7035, Domain Loss: 1.4140, Class Loss: 0.2895
Epoch 22/50, Loss: 1.6606, Domain Loss: 1.4014, Class Loss: 0.2592
Epoch 23/50, Loss: 1.6211, Domain Loss: 1.3939, Class Loss: 0.2272
Epoch 24/50, Loss: 1.5584, Domain Loss: 1.3955, Class Loss: 0.1629
Epoch 25/50, Loss: 1.5312, Domain Loss: 1.3883, Class Loss: 0.1429
Epoch 26/50, Loss: 1.4897, Domain Loss: 1.3881, Class Loss: 0.1016
Epoch 27/50, Loss: 1.4949, Domain Loss: 1.4058, Class Loss: 0.0891
Epoch 28/50, Loss: 1.4957, Domain Loss: 1.3975, Class Loss: 0.0982
Epoch 29/50, Loss: 1.4585, Domain Loss: 1.3948, Class Loss: 0.0637
Epoch 30/50, Loss: 1.4382, Domain Loss: 1.3892, Class Loss: 0.0490
Epoch 31/50, Loss: 1.4242, Domain Loss: 1.3923, Class Loss: 0.0319
Epoch 32/50, Loss: 1.4422, Domain Loss: 1.3977, Class Loss: 0.0445
Epoch 33/50, Loss: 1.4655, Domain Loss: 1.4238, Class Loss: 0.0417
Epoch 34/50, Loss: 1.4795, Domain Loss: 1.4529, Class Loss: 0.0266
Epoch 35/50, Loss: 1.5665, Domain Loss: 1.5328, Class Loss: 0.0337
Epoch 36/50, Loss: 1.8326, Domain Loss: 1.7708, Class Loss: 0.0618
Epoch 37/50, Loss: 1.7819, Domain Loss: 1.6556, Class Loss: 0.1263
Epoch 38/50, Loss: 1.8071, Domain Loss: 1.7581, Class Loss: 0.0490
Epoch 39/50, Loss: 1.4921, Domain Loss: 1.4569, Class Loss: 0.0351
Epoch 40/50, Loss: 1.4670, Domain Loss: 1.4324, Class Loss: 0.0345
Epoch 41/50, Loss: 1.4498, Domain Loss: 1.4306, Class Loss: 0.0192
Epoch 42/50, Loss: 1.4212, Domain Loss: 1.4059, Class Loss: 0.0153
Epoch 43/50, Loss: 1.4104, Domain Loss: 1.3959, Class Loss: 0.0145
Epoch 44/50, Loss: 1.4054, Domain Loss: 1.3931, Class Loss: 0.0124
Epoch 45/50, Loss: 1.4028, Domain Loss: 1.3878, Class Loss: 0.0149
Epoch 46/50, Loss: 1.4102, Domain Loss: 1.3944, Class Loss: 0.0158
Epoch 47/50, Loss: 1.4155, Domain Loss: 1.4006, Class Loss: 0.0149
Epoch 48/50, Loss: 1.4000, Domain Loss: 1.3900, Class Loss: 0.0100
Epoch 49/50, Loss: 1.3940, Domain Loss: 1.3860, Class Loss: 0.0080
Epoch 50/50, Loss: 1.4066, Domain Loss: 1.3917, Class Loss: 0.0150
98.26


Epoch 1/50, Loss: 2.3479, Domain Loss: 1.3964, Class Loss: 0.9516
Epoch 2/50, Loss: 1.8257, Domain Loss: 1.3887, Class Loss: 0.4370
Epoch 3/50, Loss: 1.6934, Domain Loss: 1.3881, Class Loss: 0.3053
Epoch 4/50, Loss: 1.6084, Domain Loss: 1.3854, Class Loss: 0.2229
Epoch 5/50, Loss: 1.5691, Domain Loss: 1.3851, Class Loss: 0.1840
Epoch 6/50, Loss: 1.4886, Domain Loss: 1.3883, Class Loss: 0.1004
Epoch 7/50, Loss: 1.4575, Domain Loss: 1.3870, Class Loss: 0.0705
Epoch 8/50, Loss: 1.4749, Domain Loss: 1.3960, Class Loss: 0.0789
Epoch 9/50, Loss: 2.1308, Domain Loss: 2.0152, Class Loss: 0.1156
Epoch 10/50, Loss: 2.7482, Domain Loss: 2.5949, Class Loss: 0.1534
Epoch 11/50, Loss: 7.6435, Domain Loss: 7.4681, Class Loss: 0.1754
Epoch 12/50, Loss: 14.3449, Domain Loss: 13.9087, Class Loss: 0.4362
Epoch 13/50, Loss: 7.0831, Domain Loss: 6.6744, Class Loss: 0.4087
Epoch 14/50, Loss: 7.0346, Domain Loss: 6.5986, Class Loss: 0.4360
Epoch 15/50, Loss: 9.5575, Domain Loss: 8.5442, Class Loss: 1.0133
Epoch 16/50, Loss: 4.0776, Domain Loss: 3.4719, Class Loss: 0.6057
Epoch 17/50, Loss: 2.4057, Domain Loss: 1.9830, Class Loss: 0.4227
Epoch 18/50, Loss: 2.2037, Domain Loss: 1.8134, Class Loss: 0.3903
Epoch 19/50, Loss: 1.8905, Domain Loss: 1.5616, Class Loss: 0.3288
Epoch 20/50, Loss: 1.6775, Domain Loss: 1.4720, Class Loss: 0.2055
Epoch 21/50, Loss: 1.6276, Domain Loss: 1.5183, Class Loss: 0.1093
Epoch 22/50, Loss: 1.6231, Domain Loss: 1.5325, Class Loss: 0.0907
Epoch 23/50, Loss: 1.5335, Domain Loss: 1.4850, Class Loss: 0.0486
Epoch 24/50, Loss: 1.5409, Domain Loss: 1.4695, Class Loss: 0.0714
Epoch 25/50, Loss: 1.4823, Domain Loss: 1.4461, Class Loss: 0.0362
Epoch 26/50, Loss: 1.4331, Domain Loss: 1.4073, Class Loss: 0.0259
Epoch 27/50, Loss: 1.4712, Domain Loss: 1.4407, Class Loss: 0.0305
Epoch 28/50, Loss: 1.4468, Domain Loss: 1.4088, Class Loss: 0.0380
Epoch 29/50, Loss: 1.4152, Domain Loss: 1.3941, Class Loss: 0.0211
Epoch 30/50, Loss: 1.4319, Domain Loss: 1.4200, Class Loss: 0.0118
Epoch 31/50, Loss: 1.4594, Domain Loss: 1.4398, Class Loss: 0.0195
Epoch 32/50, Loss: 1.4276, Domain Loss: 1.4103, Class Loss: 0.0172
Epoch 33/50, Loss: 1.4353, Domain Loss: 1.4103, Class Loss: 0.0250
Epoch 34/50, Loss: 1.4143, Domain Loss: 1.4004, Class Loss: 0.0138
Epoch 35/50, Loss: 1.4105, Domain Loss: 1.3974, Class Loss: 0.0131
Epoch 36/50, Loss: 1.4121, Domain Loss: 1.4055, Class Loss: 0.0066
Epoch 37/50, Loss: 1.4094, Domain Loss: 1.3977, Class Loss: 0.0117
Epoch 38/50, Loss: 1.5298, Domain Loss: 1.5046, Class Loss: 0.0252
Epoch 39/50, Loss: 1.6633, Domain Loss: 1.6011, Class Loss: 0.0622
Epoch 40/50, Loss: 2.0954, Domain Loss: 1.9594, Class Loss: 0.1360
Epoch 41/50, Loss: 2.4480, Domain Loss: 2.2788, Class Loss: 0.1692
Epoch 42/50, Loss: 1.8320, Domain Loss: 1.7147, Class Loss: 0.1173
Epoch 43/50, Loss: 1.6084, Domain Loss: 1.5424, Class Loss: 0.0660
Epoch 44/50, Loss: 1.4742, Domain Loss: 1.4384, Class Loss: 0.0358
Epoch 45/50, Loss: 1.4827, Domain Loss: 1.4246, Class Loss: 0.0581
Epoch 46/50, Loss: 1.4334, Domain Loss: 1.4122, Class Loss: 0.0212
Epoch 47/50, Loss: 1.4108, Domain Loss: 1.3944, Class Loss: 0.0163
Epoch 48/50, Loss: 1.4058, Domain Loss: 1.4003, Class Loss: 0.0054
Epoch 49/50, Loss: 1.4022, Domain Loss: 1.3918, Class Loss: 0.0104
Epoch 50/50, Loss: 1.4134, Domain Loss: 1.4010, Class Loss: 0.0124
99.94


Epoch 1/50, Loss: 2.2466, Domain Loss: 1.3958, Class Loss: 0.8508
Epoch 2/50, Loss: 1.7918, Domain Loss: 1.3920, Class Loss: 0.3997
Epoch 3/50, Loss: 1.7060, Domain Loss: 1.3944, Class Loss: 0.3116
Epoch 4/50, Loss: 1.6541, Domain Loss: 1.3912, Class Loss: 0.2629
Epoch 5/50, Loss: 1.5811, Domain Loss: 1.3891, Class Loss: 0.1920
Epoch 6/50, Loss: 1.5413, Domain Loss: 1.3894, Class Loss: 0.1518
Epoch 7/50, Loss: 2.3122, Domain Loss: 2.1758, Class Loss: 0.1364
Epoch 8/50, Loss: 5.4533, Domain Loss: 5.2169, Class Loss: 0.2364
Epoch 9/50, Loss: 4.7271, Domain Loss: 4.2548, Class Loss: 0.4722
Epoch 10/50, Loss: 16.2167, Domain Loss: 15.4976, Class Loss: 0.7191
Epoch 11/50, Loss: 10.3774, Domain Loss: 9.8674, Class Loss: 0.5100
Epoch 12/50, Loss: 4.1530, Domain Loss: 3.7301, Class Loss: 0.4229
Epoch 13/50, Loss: 2.3477, Domain Loss: 1.9867, Class Loss: 0.3610
Epoch 14/50, Loss: 1.9522, Domain Loss: 1.5864, Class Loss: 0.3657
Epoch 15/50, Loss: 1.8669, Domain Loss: 1.5296, Class Loss: 0.3372
Epoch 16/50, Loss: 1.8126, Domain Loss: 1.4986, Class Loss: 0.3139
Epoch 17/50, Loss: 4.2996, Domain Loss: 3.6122, Class Loss: 0.6875
Epoch 18/50, Loss: 4.9037, Domain Loss: 4.5294, Class Loss: 0.3743
Epoch 19/50, Loss: 2.5122, Domain Loss: 2.2109, Class Loss: 0.3013
Epoch 20/50, Loss: 3.5392, Domain Loss: 3.0933, Class Loss: 0.4458
Epoch 21/50, Loss: 3.7991, Domain Loss: 3.1027, Class Loss: 0.6964
Epoch 22/50, Loss: 2.9470, Domain Loss: 2.4484, Class Loss: 0.4986
Epoch 23/50, Loss: 2.2521, Domain Loss: 1.8535, Class Loss: 0.3987
Epoch 24/50, Loss: 1.9561, Domain Loss: 1.5646, Class Loss: 0.3915
Epoch 25/50, Loss: 1.7683, Domain Loss: 1.4076, Class Loss: 0.3607
Epoch 26/50, Loss: 1.7307, Domain Loss: 1.4295, Class Loss: 0.3011
Epoch 27/50, Loss: 1.7200, Domain Loss: 1.4037, Class Loss: 0.3164
Epoch 28/50, Loss: 1.6825, Domain Loss: 1.4094, Class Loss: 0.2731
Epoch 29/50, Loss: 1.6146, Domain Loss: 1.3943, Class Loss: 0.2203
Epoch 30/50, Loss: 1.5864, Domain Loss: 1.3883, Class Loss: 0.1981
Epoch 31/50, Loss: 1.5762, Domain Loss: 1.3910, Class Loss: 0.1852
Epoch 32/50, Loss: 1.5522, Domain Loss: 1.3887, Class Loss: 0.1636
Epoch 33/50, Loss: 1.5774, Domain Loss: 1.3990, Class Loss: 0.1785
Epoch 34/50, Loss: 1.5181, Domain Loss: 1.3860, Class Loss: 0.1321
Epoch 35/50, Loss: 1.5155, Domain Loss: 1.4060, Class Loss: 0.1095
Epoch 36/50, Loss: 1.4895, Domain Loss: 1.4070, Class Loss: 0.0825
Epoch 37/50, Loss: 1.4727, Domain Loss: 1.3966, Class Loss: 0.0761
Epoch 38/50, Loss: 1.4630, Domain Loss: 1.3940, Class Loss: 0.0691
Epoch 39/50, Loss: 1.4670, Domain Loss: 1.4026, Class Loss: 0.0643
Epoch 40/50, Loss: 1.4716, Domain Loss: 1.4062, Class Loss: 0.0655
Epoch 41/50, Loss: 1.4825, Domain Loss: 1.4187, Class Loss: 0.0638
Epoch 42/50, Loss: 1.4679, Domain Loss: 1.4064, Class Loss: 0.0615
Epoch 43/50, Loss: 1.4593, Domain Loss: 1.4008, Class Loss: 0.0585
Epoch 44/50, Loss: 1.5125, Domain Loss: 1.3902, Class Loss: 0.1223
Epoch 45/50, Loss: 1.4655, Domain Loss: 1.3877, Class Loss: 0.0779
Epoch 46/50, Loss: 1.4706, Domain Loss: 1.4089, Class Loss: 0.0617
Epoch 47/50, Loss: 1.4440, Domain Loss: 1.4046, Class Loss: 0.0394
Epoch 48/50, Loss: 1.4402, Domain Loss: 1.3973, Class Loss: 0.0429
Epoch 49/50, Loss: 1.4376, Domain Loss: 1.3991, Class Loss: 0.0385
Epoch 50/50, Loss: 1.4143, Domain Loss: 1.3954, Class Loss: 0.0189
99.40


Epoch 1/50, Loss: 2.3157, Domain Loss: 1.3943, Class Loss: 0.9214
Epoch 2/50, Loss: 1.8122, Domain Loss: 1.3875, Class Loss: 0.4247
Epoch 3/50, Loss: 1.7237, Domain Loss: 1.3932, Class Loss: 0.3306
Epoch 4/50, Loss: 1.6475, Domain Loss: 1.3890, Class Loss: 0.2585
Epoch 5/50, Loss: 1.5773, Domain Loss: 1.3893, Class Loss: 0.1880
Epoch 6/50, Loss: 1.5676, Domain Loss: 1.3878, Class Loss: 0.1798
Epoch 7/50, Loss: 1.4783, Domain Loss: 1.3885, Class Loss: 0.0897
Epoch 8/50, Loss: 1.5202, Domain Loss: 1.4629, Class Loss: 0.0573
Epoch 9/50, Loss: 3.4023, Domain Loss: 3.2776, Class Loss: 0.1247
Epoch 10/50, Loss: 4.7649, Domain Loss: 4.5984, Class Loss: 0.1665
Epoch 11/50, Loss: 3.4284, Domain Loss: 3.1751, Class Loss: 0.2533
Epoch 12/50, Loss: 2.2537, Domain Loss: 1.9986, Class Loss: 0.2550
Epoch 13/50, Loss: 1.6841, Domain Loss: 1.5790, Class Loss: 0.1051
Epoch 14/50, Loss: 1.5102, Domain Loss: 1.4347, Class Loss: 0.0755
Epoch 15/50, Loss: 1.6125, Domain Loss: 1.5401, Class Loss: 0.0725
Epoch 16/50, Loss: 2.1610, Domain Loss: 1.9223, Class Loss: 0.2386
Epoch 17/50, Loss: 6.2471, Domain Loss: 6.0724, Class Loss: 0.1747
Epoch 18/50, Loss: 7.7897, Domain Loss: 7.3513, Class Loss: 0.4384
Epoch 19/50, Loss: 3.4100, Domain Loss: 3.2456, Class Loss: 0.1644
Epoch 20/50, Loss: 1.6119, Domain Loss: 1.5267, Class Loss: 0.0853
Epoch 21/50, Loss: 1.4297, Domain Loss: 1.3907, Class Loss: 0.0391
Epoch 22/50, Loss: 1.4360, Domain Loss: 1.4098, Class Loss: 0.0262
Epoch 23/50, Loss: 1.4507, Domain Loss: 1.4340, Class Loss: 0.0166
Epoch 24/50, Loss: 1.5068, Domain Loss: 1.4504, Class Loss: 0.0564
Epoch 25/50, Loss: 1.4642, Domain Loss: 1.4187, Class Loss: 0.0455
Epoch 26/50, Loss: 1.4339, Domain Loss: 1.4091, Class Loss: 0.0248
Epoch 27/50, Loss: 1.4072, Domain Loss: 1.3961, Class Loss: 0.0110
Epoch 28/50, Loss: 1.4117, Domain Loss: 1.3995, Class Loss: 0.0122
Epoch 29/50, Loss: 1.3996, Domain Loss: 1.3912, Class Loss: 0.0083
Epoch 30/50, Loss: 1.3987, Domain Loss: 1.3938, Class Loss: 0.0049
Epoch 31/50, Loss: 1.3945, Domain Loss: 1.3899, Class Loss: 0.0045
Epoch 32/50, Loss: 1.3987, Domain Loss: 1.3938, Class Loss: 0.0049
Epoch 33/50, Loss: 1.3982, Domain Loss: 1.3905, Class Loss: 0.0078
Epoch 34/50, Loss: 1.3931, Domain Loss: 1.3884, Class Loss: 0.0048
Epoch 35/50, Loss: 1.3961, Domain Loss: 1.3873, Class Loss: 0.0088
Epoch 36/50, Loss: 1.3931, Domain Loss: 1.3879, Class Loss: 0.0051
Epoch 37/50, Loss: 1.3923, Domain Loss: 1.3865, Class Loss: 0.0059
Epoch 38/50, Loss: 1.3955, Domain Loss: 1.3885, Class Loss: 0.0070
Epoch 39/50, Loss: 1.3926, Domain Loss: 1.3877, Class Loss: 0.0050
Epoch 40/50, Loss: 1.3975, Domain Loss: 1.3926, Class Loss: 0.0049
Epoch 41/50, Loss: 1.3920, Domain Loss: 1.3900, Class Loss: 0.0020
Epoch 42/50, Loss: 1.3942, Domain Loss: 1.3922, Class Loss: 0.0020
Epoch 43/50, Loss: 1.3897, Domain Loss: 1.3881, Class Loss: 0.0016
Epoch 44/50, Loss: 1.3891, Domain Loss: 1.3879, Class Loss: 0.0013
Epoch 45/50, Loss: 1.3936, Domain Loss: 1.3928, Class Loss: 0.0008
Epoch 46/50, Loss: 1.3891, Domain Loss: 1.3883, Class Loss: 0.0008
Epoch 47/50, Loss: 1.3870, Domain Loss: 1.3851, Class Loss: 0.0019
Epoch 48/50, Loss: 1.3906, Domain Loss: 1.3895, Class Loss: 0.0011
Epoch 49/50, Loss: 1.3940, Domain Loss: 1.3931, Class Loss: 0.0009
Epoch 50/50, Loss: 1.3892, Domain Loss: 1.3879, Class Loss: 0.0013
99.94


Epoch 1/50, Loss: 2.2909, Domain Loss: 1.3947, Class Loss: 0.8962
Epoch 2/50, Loss: 1.7997, Domain Loss: 1.3881, Class Loss: 0.4116
Epoch 3/50, Loss: 1.6566, Domain Loss: 1.3865, Class Loss: 0.2701
Epoch 4/50, Loss: 1.5871, Domain Loss: 1.3895, Class Loss: 0.1977
Epoch 5/50, Loss: 1.5769, Domain Loss: 1.3937, Class Loss: 0.1832
Epoch 6/50, Loss: 1.5455, Domain Loss: 1.3882, Class Loss: 0.1573
Epoch 7/50, Loss: 1.4933, Domain Loss: 1.3887, Class Loss: 0.1045
Epoch 8/50, Loss: 1.4843, Domain Loss: 1.3896, Class Loss: 0.0947
Epoch 9/50, Loss: 1.9480, Domain Loss: 1.8343, Class Loss: 0.1137
Epoch 10/50, Loss: 3.7310, Domain Loss: 3.5694, Class Loss: 0.1616
Epoch 11/50, Loss: 2.7099, Domain Loss: 2.5418, Class Loss: 0.1681
Epoch 12/50, Loss: 1.7516, Domain Loss: 1.6474, Class Loss: 0.1042
Epoch 13/50, Loss: 1.6912, Domain Loss: 1.5569, Class Loss: 0.1344
Epoch 14/50, Loss: 1.6720, Domain Loss: 1.5173, Class Loss: 0.1547
Epoch 15/50, Loss: 1.6167, Domain Loss: 1.4497, Class Loss: 0.1670
Epoch 16/50, Loss: 1.4863, Domain Loss: 1.4142, Class Loss: 0.0721
Epoch 17/50, Loss: 1.5193, Domain Loss: 1.4563, Class Loss: 0.0630
Epoch 18/50, Loss: 1.6601, Domain Loss: 1.5517, Class Loss: 0.1085
Epoch 19/50, Loss: 2.3768, Domain Loss: 1.9192, Class Loss: 0.4576
Epoch 20/50, Loss: 1.7720, Domain Loss: 1.5609, Class Loss: 0.2112
Epoch 21/50, Loss: 2.5688, Domain Loss: 2.2040, Class Loss: 0.3648
Epoch 22/50, Loss: 2.0809, Domain Loss: 1.8540, Class Loss: 0.2269
Epoch 23/50, Loss: 1.6142, Domain Loss: 1.5038, Class Loss: 0.1104
Epoch 24/50, Loss: 1.7731, Domain Loss: 1.6143, Class Loss: 0.1588
Epoch 25/50, Loss: 1.6749, Domain Loss: 1.5838, Class Loss: 0.0911
Epoch 26/50, Loss: 1.5729, Domain Loss: 1.5030, Class Loss: 0.0700
Epoch 27/50, Loss: 1.5100, Domain Loss: 1.4504, Class Loss: 0.0596
Epoch 28/50, Loss: 1.4723, Domain Loss: 1.4459, Class Loss: 0.0264
Epoch 29/50, Loss: 1.6965, Domain Loss: 1.6584, Class Loss: 0.0381
Epoch 30/50, Loss: 1.6994, Domain Loss: 1.6416, Class Loss: 0.0578
Epoch 31/50, Loss: 1.6703, Domain Loss: 1.5906, Class Loss: 0.0796
Epoch 32/50, Loss: 1.5881, Domain Loss: 1.5061, Class Loss: 0.0820
Epoch 33/50, Loss: 1.4401, Domain Loss: 1.4199, Class Loss: 0.0202
Epoch 34/50, Loss: 1.4288, Domain Loss: 1.4144, Class Loss: 0.0144
Epoch 35/50, Loss: 1.4444, Domain Loss: 1.4341, Class Loss: 0.0104
Epoch 36/50, Loss: 1.4531, Domain Loss: 1.4396, Class Loss: 0.0135
Epoch 37/50, Loss: 1.4513, Domain Loss: 1.4427, Class Loss: 0.0086
Epoch 38/50, Loss: 1.4291, Domain Loss: 1.4148, Class Loss: 0.0144
Epoch 39/50, Loss: 1.3999, Domain Loss: 1.3951, Class Loss: 0.0047
Epoch 40/50, Loss: 1.3964, Domain Loss: 1.3915, Class Loss: 0.0049
Epoch 41/50, Loss: 1.4010, Domain Loss: 1.3913, Class Loss: 0.0097
Epoch 42/50, Loss: 1.4063, Domain Loss: 1.3947, Class Loss: 0.0116
Epoch 43/50, Loss: 1.3927, Domain Loss: 1.3867, Class Loss: 0.0059
Epoch 44/50, Loss: 1.3859, Domain Loss: 1.3830, Class Loss: 0.0029
Epoch 45/50, Loss: 1.4004, Domain Loss: 1.3915, Class Loss: 0.0089
Epoch 46/50, Loss: 1.4015, Domain Loss: 1.3987, Class Loss: 0.0028
Epoch 47/50, Loss: 1.3965, Domain Loss: 1.3948, Class Loss: 0.0018
Epoch 48/50, Loss: 1.3923, Domain Loss: 1.3912, Class Loss: 0.0011
Epoch 49/50, Loss: 1.3873, Domain Loss: 1.3858, Class Loss: 0.0015
Epoch 50/50, Loss: 1.3908, Domain Loss: 1.3878, Class Loss: 0.0029
99.40


Source performance:
99.02 99.08 99.03 99.02 
Target performance:
99.12 99.16 99.10 99.11 

Per-class target performance: 100.00 100.00 99.06 97.36 Deep CORAL
Deep CORAL Run 1/10
Epoch 1: Source Val Acc = 0.4766, Target Val Acc = 0.4646
Epoch 2: Source Val Acc = 0.8201, Target Val Acc = 0.8255
Epoch 3: Source Val Acc = 0.7074, Target Val Acc = 0.7098
Epoch 4: Source Val Acc = 0.8681, Target Val Acc = 0.8555
Epoch 5: Source Val Acc = 0.9874, Target Val Acc = 0.9928
Epoch 6: Source Val Acc = 0.8321, Target Val Acc = 0.8273
Epoch 7: Source Val Acc = 0.9826, Target Val Acc = 0.9856
Epoch 8: Source Val Acc = 0.9928, Target Val Acc = 0.9982
Epoch 9: Source Val Acc = 0.4305, Target Val Acc = 0.4203
Epoch 10: Source Val Acc = 0.9233, Target Val Acc = 0.9245
Epoch 11: Source Val Acc = 0.8747, Target Val Acc = 0.8681
Epoch 12: Source Val Acc = 0.7140, Target Val Acc = 0.7278
Epoch 13: Source Val Acc = 0.8783, Target Val Acc = 0.8729
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.8783, Target Val Acc = 0.8729

Deep CORAL Run 2/10
Epoch 1: Source Val Acc = 0.8501, Target Val Acc = 0.8465
Epoch 2: Source Val Acc = 0.7218, Target Val Acc = 0.7284
Epoch 3: Source Val Acc = 0.9424, Target Val Acc = 0.9406
Epoch 4: Source Val Acc = 0.9916, Target Val Acc = 0.9958
Epoch 5: Source Val Acc = 0.2020, Target Val Acc = 0.2098
Epoch 6: Source Val Acc = 0.9934, Target Val Acc = 0.9952
Epoch 7: Source Val Acc = 0.9784, Target Val Acc = 0.9826
Epoch 8: Source Val Acc = 0.9952, Target Val Acc = 0.9988
Epoch 9: Source Val Acc = 0.9970, Target Val Acc = 0.9994
Epoch 10: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 11: Source Val Acc = 0.9988, Target Val Acc = 0.9994
Epoch 12: Source Val Acc = 0.9988, Target Val Acc = 0.9988
Epoch 13: Source Val Acc = 0.9952, Target Val Acc = 0.9976
Epoch 14: Source Val Acc = 0.9958, Target Val Acc = 0.9970
Epoch 15: Source Val Acc = 0.9952, Target Val Acc = 0.9952
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9952, Target Val Acc = 0.9952

Deep CORAL Run 3/10
Epoch 1: Source Val Acc = 0.9257, Target Val Acc = 0.9269
Epoch 2: Source Val Acc = 0.7848, Target Val Acc = 0.7914
Epoch 3: Source Val Acc = 0.7686, Target Val Acc = 0.7674
Epoch 4: Source Val Acc = 0.8405, Target Val Acc = 0.8633
Epoch 5: Source Val Acc = 0.9376, Target Val Acc = 0.9371
Epoch 6: Source Val Acc = 0.7530, Target Val Acc = 0.7668
Epoch 7: Source Val Acc = 0.9736, Target Val Acc = 0.9748
Epoch 8: Source Val Acc = 0.5372, Target Val Acc = 0.5234
Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.9976
Epoch 10: Source Val Acc = 1.0000, Target Val Acc = 0.9994
Epoch 11: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 12: Source Val Acc = 0.9994, Target Val Acc = 0.9988
Epoch 13: Source Val Acc = 0.9994, Target Val Acc = 0.9994
Epoch 14: Source Val Acc = 0.9922, Target Val Acc = 0.9988
Epoch 15: Source Val Acc = 0.9994, Target Val Acc = 0.9994
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.9994, Target Val Acc = 0.9994

Deep CORAL Run 4/10
Epoch 1: Source Val Acc = 0.7506, Target Val Acc = 0.7590
Epoch 2: Source Val Acc = 0.7896, Target Val Acc = 0.8106
Epoch 3: Source Val Acc = 0.6529, Target Val Acc = 0.6463
Epoch 4: Source Val Acc = 0.9742, Target Val Acc = 0.9814
Epoch 5: Source Val Acc = 0.8813, Target Val Acc = 0.8957
Epoch 6: Source Val Acc = 0.9892, Target Val Acc = 0.9940
Epoch 7: Source Val Acc = 0.5384, Target Val Acc = 0.5402
Epoch 8: Source Val Acc = 0.9898, Target Val Acc = 0.9952
Epoch 9: Source Val Acc = 0.7380, Target Val Acc = 0.7368
Epoch 10: Source Val Acc = 0.9940, Target Val Acc = 0.9982
Epoch 11: Source Val Acc = 0.9928, Target Val Acc = 0.9982
Epoch 12: Source Val Acc = 0.9940, Target Val Acc = 0.9952
Epoch 13: Source Val Acc = 0.9826, Target Val Acc = 0.9814
Epoch 14: Source Val Acc = 0.7284, Target Val Acc = 0.7236
Epoch 15: Source Val Acc = 0.9964, Target Val Acc = 0.9976
Epoch 16: Source Val Acc = 0.9976, Target Val Acc = 0.9988
Epoch 17: Source Val Acc = 0.9652, Target Val Acc = 0.9640
Epoch 18: Source Val Acc = 0.9958, Target Val Acc = 0.9964
Epoch 19: Source Val Acc = 0.9988, Target Val Acc = 0.9988
Epoch 20: Source Val Acc = 0.9988, Target Val Acc = 0.9994
Epoch 21: Source Val Acc = 0.9994, Target Val Acc = 0.9994
Epoch 22: Source Val Acc = 0.9988, Target Val Acc = 0.9994
Epoch 23: Source Val Acc = 0.9994, Target Val Acc = 0.9994
Epoch 24: Source Val Acc = 0.9988, Target Val Acc = 0.9994
Epoch 25: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 26: Source Val Acc = 0.9946, Target Val Acc = 0.9964
Epoch 27: Source Val Acc = 0.9946, Target Val Acc = 0.9952
Epoch 28: Source Val Acc = 0.9988, Target Val Acc = 0.9994
Epoch 29: Source Val Acc = 0.9982, Target Val Acc = 0.9988
Epoch 30: Source Val Acc = 0.9976, Target Val Acc = 0.9994
Early stopping triggered.
Run 4 finished: Best Source Val Acc = 0.9976, Target Val Acc = 0.9994

Deep CORAL Run 5/10
Epoch 1: Source Val Acc = 0.7698, Target Val Acc = 0.7776
Epoch 2: Source Val Acc = 0.9017, Target Val Acc = 0.8921
Epoch 3: Source Val Acc = 0.9143, Target Val Acc = 0.9233
Epoch 4: Source Val Acc = 0.9736, Target Val Acc = 0.9754
Epoch 5: Source Val Acc = 0.9922, Target Val Acc = 0.9970
Epoch 6: Source Val Acc = 0.9970, Target Val Acc = 0.9982
Epoch 7: Source Val Acc = 0.7044, Target Val Acc = 0.6966
Epoch 8: Source Val Acc = 0.7290, Target Val Acc = 0.7476
Epoch 9: Source Val Acc = 0.9886, Target Val Acc = 0.9886
Epoch 10: Source Val Acc = 0.9904, Target Val Acc = 0.9922
Epoch 11: Source Val Acc = 0.9790, Target Val Acc = 0.9832
Early stopping triggered.
Run 5 finished: Best Source Val Acc = 0.9790, Target Val Acc = 0.9832

Deep CORAL Run 6/10
Epoch 1: Source Val Acc = 0.6697, Target Val Acc = 0.6709
Epoch 2: Source Val Acc = 0.5198, Target Val Acc = 0.5276
Epoch 3: Source Val Acc = 0.5863, Target Val Acc = 0.5779
Epoch 4: Source Val Acc = 0.9916, Target Val Acc = 0.9952
Epoch 5: Source Val Acc = 0.7626, Target Val Acc = 0.7578
Epoch 6: Source Val Acc = 0.9227, Target Val Acc = 0.9406
Epoch 7: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 8: Source Val Acc = 0.9994, Target Val Acc = 1.0000
Epoch 9: Source Val Acc = 0.9838, Target Val Acc = 0.9838
Epoch 10: Source Val Acc = 0.9934, Target Val Acc = 0.9928
Epoch 11: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 12: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Early stopping triggered.
Run 6 finished: Best Source Val Acc = 1.0000, Target Val Acc = 1.0000

Deep CORAL Run 7/10
Epoch 1: Source Val Acc = 0.7260, Target Val Acc = 0.7392
Epoch 2: Source Val Acc = 0.9670, Target Val Acc = 0.9688
Epoch 3: Source Val Acc = 0.6073, Target Val Acc = 0.6127
Epoch 4: Source Val Acc = 0.8657, Target Val Acc = 0.8855
Epoch 5: Source Val Acc = 0.9239, Target Val Acc = 0.9400
Epoch 6: Source Val Acc = 0.7152, Target Val Acc = 0.7146
Epoch 7: Source Val Acc = 0.8615, Target Val Acc = 0.8537
Early stopping triggered.
Run 7 finished: Best Source Val Acc = 0.8615, Target Val Acc = 0.8537

Deep CORAL Run 8/10
Epoch 1: Source Val Acc = 0.7668, Target Val Acc = 0.7728
Epoch 2: Source Val Acc = 0.5096, Target Val Acc = 0.5114
Epoch 3: Source Val Acc = 0.8561, Target Val Acc = 0.8555
Epoch 4: Source Val Acc = 0.9466, Target Val Acc = 0.9640
Epoch 5: Source Val Acc = 0.9155, Target Val Acc = 0.9329
Epoch 6: Source Val Acc = 0.7758, Target Val Acc = 0.7872
Epoch 7: Source Val Acc = 0.9940, Target Val Acc = 0.9970
Epoch 8: Source Val Acc = 0.9964, Target Val Acc = 0.9976
Epoch 9: Source Val Acc = 0.9970, Target Val Acc = 0.9988
Epoch 10: Source Val Acc = 0.2686, Target Val Acc = 0.2806
Epoch 11: Source Val Acc = 0.9802, Target Val Acc = 0.9826
Epoch 12: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 1.0000
Epoch 14: Source Val Acc = 0.9994, Target Val Acc = 1.0000
Epoch 15: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 16: Source Val Acc = 0.9982, Target Val Acc = 0.9988
Epoch 17: Source Val Acc = 0.9988, Target Val Acc = 0.9994
Early stopping triggered.
Run 8 finished: Best Source Val Acc = 0.9988, Target Val Acc = 0.9994

Deep CORAL Run 9/10
Epoch 1: Source Val Acc = 0.7056, Target Val Acc = 0.7170
Epoch 2: Source Val Acc = 0.8118, Target Val Acc = 0.8237
Epoch 3: Source Val Acc = 0.9209, Target Val Acc = 0.9203
Epoch 4: Source Val Acc = 0.9886, Target Val Acc = 0.9904
Epoch 5: Source Val Acc = 0.9952, Target Val Acc = 0.9970
Epoch 6: Source Val Acc = 0.9964, Target Val Acc = 0.9970
Epoch 7: Source Val Acc = 0.9910, Target Val Acc = 0.9964
Epoch 8: Source Val Acc = 0.8957, Target Val Acc = 0.8957
Epoch 9: Source Val Acc = 0.9976, Target Val Acc = 0.9994
Epoch 10: Source Val Acc = 0.9988, Target Val Acc = 0.9994
Epoch 11: Source Val Acc = 0.5390, Target Val Acc = 0.5432
Epoch 12: Source Val Acc = 0.9814, Target Val Acc = 0.9784
Epoch 13: Source Val Acc = 0.9976, Target Val Acc = 0.9988
Epoch 14: Source Val Acc = 0.9898, Target Val Acc = 0.9928
Epoch 15: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 16: Source Val Acc = 0.9970, Target Val Acc = 0.9994
Epoch 17: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 18: Source Val Acc = 0.9886, Target Val Acc = 0.9868
Epoch 19: Source Val Acc = 0.9994, Target Val Acc = 0.9988
Epoch 20: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Early stopping triggered.
Run 9 finished: Best Source Val Acc = 1.0000, Target Val Acc = 1.0000

Deep CORAL Run 10/10
Epoch 1: Source Val Acc = 0.9251, Target Val Acc = 0.9239
Epoch 2: Source Val Acc = 0.9466, Target Val Acc = 0.9382
Epoch 3: Source Val Acc = 0.9640, Target Val Acc = 0.9676
Epoch 4: Source Val Acc = 0.9940, Target Val Acc = 0.9970
Epoch 5: Source Val Acc = 0.9916, Target Val Acc = 0.9958
Epoch 6: Source Val Acc = 0.9844, Target Val Acc = 0.9940
Epoch 7: Source Val Acc = 0.9982, Target Val Acc = 0.9994
Epoch 8: Source Val Acc = 0.6924, Target Val Acc = 0.6984
Epoch 9: Source Val Acc = 0.9376, Target Val Acc = 0.9442
Epoch 10: Source Val Acc = 0.9814, Target Val Acc = 0.9886
Epoch 11: Source Val Acc = 0.9994, Target Val Acc = 1.0000
Epoch 12: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 13: Source Val Acc = 1.0000, Target Val Acc = 1.0000
Epoch 14: Source Val Acc = 0.7584, Target Val Acc = 0.7536
Epoch 15: Source Val Acc = 0.9988, Target Val Acc = 0.9964
Epoch 16: Source Val Acc = 0.9970, Target Val Acc = 0.9964
Epoch 17: Source Val Acc = 0.9946, Target Val Acc = 0.9970
Early stopping triggered.
Run 10 finished: Best Source Val Acc = 0.9946, Target Val Acc = 0.9970

Deep CORAL: Average Source Val Acc = 0.9704, Average Target Val Acc = 0.9700
STAR

Run 1/10
Epoch [1/50], Class Loss: 1.6635, Discrepancy Loss: 0.1151
Epoch [2/50], Class Loss: 0.7653, Discrepancy Loss: 0.0697
Epoch [3/50], Class Loss: 0.4508, Discrepancy Loss: 0.0482
Epoch [4/50], Class Loss: 0.1956, Discrepancy Loss: 0.0154
Epoch [5/50], Class Loss: 0.0711, Discrepancy Loss: 0.0124
Epoch [6/50], Class Loss: 0.0274, Discrepancy Loss: 0.0030
Epoch [7/50], Class Loss: 0.1810, Discrepancy Loss: 0.0196
Epoch [8/50], Class Loss: 0.0334, Discrepancy Loss: 0.0050
Epoch [9/50], Class Loss: 0.0104, Discrepancy Loss: 0.0019
Epoch [10/50], Class Loss: 0.0206, Discrepancy Loss: 0.0014
Epoch [11/50], Class Loss: 0.0053, Discrepancy Loss: 0.0012
Epoch [12/50], Class Loss: 0.0239, Discrepancy Loss: 0.0014
Epoch [13/50], Class Loss: 0.0088, Discrepancy Loss: 0.0023
Epoch [14/50], Class Loss: 0.0194, Discrepancy Loss: 0.0012
Epoch [15/50], Class Loss: 0.1362, Discrepancy Loss: 0.0022
Epoch [16/50], Class Loss: 0.0055, Discrepancy Loss: 0.0018
Epoch [17/50], Class Loss: 0.0041, Discrepancy Loss: 0.0008
Epoch [18/50], Class Loss: 0.0013, Discrepancy Loss: 0.0005
Epoch [19/50], Class Loss: 0.0010, Discrepancy Loss: 0.0010
Epoch [20/50], Class Loss: 0.0035, Discrepancy Loss: 0.0014
Epoch [21/50], Class Loss: 0.0013, Discrepancy Loss: 0.0010
Epoch [22/50], Class Loss: 0.0011, Discrepancy Loss: 0.0004
Epoch [23/50], Class Loss: 0.0428, Discrepancy Loss: 0.0012
Epoch [24/50], Class Loss: 0.0012, Discrepancy Loss: 0.0004
Epoch [25/50], Class Loss: 0.0022, Discrepancy Loss: 0.0004
Epoch [26/50], Class Loss: 0.0077, Discrepancy Loss: 0.0005
Epoch [27/50], Class Loss: 0.0021, Discrepancy Loss: 0.0003
Epoch [28/50], Class Loss: 0.0020, Discrepancy Loss: 0.0005
Epoch [29/50], Class Loss: 0.0010, Discrepancy Loss: 0.0004
Epoch [30/50], Class Loss: 0.0008, Discrepancy Loss: 0.0004
Epoch [31/50], Class Loss: 0.0008, Discrepancy Loss: 0.0004
Epoch [32/50], Class Loss: 0.0017, Discrepancy Loss: 0.0003
Epoch [33/50], Class Loss: 0.0020, Discrepancy Loss: 0.0010
Epoch [34/50], Class Loss: 0.0006, Discrepancy Loss: 0.0004
Epoch [35/50], Class Loss: 0.0003, Discrepancy Loss: 0.0014
Epoch [36/50], Class Loss: 0.0072, Discrepancy Loss: 0.0027
Epoch [37/50], Class Loss: 0.0012, Discrepancy Loss: 0.0003
Epoch [38/50], Class Loss: 0.0004, Discrepancy Loss: 0.0005
Epoch [39/50], Class Loss: 0.0022, Discrepancy Loss: 0.0007
Epoch [40/50], Class Loss: 0.0010, Discrepancy Loss: 0.0003
Epoch [41/50], Class Loss: 0.0032, Discrepancy Loss: 0.0007
Epoch [42/50], Class Loss: 0.0005, Discrepancy Loss: 0.0004
Epoch [43/50], Class Loss: 0.0011, Discrepancy Loss: 0.0006
Epoch [44/50], Class Loss: 0.0196, Discrepancy Loss: 0.0011
Epoch [45/50], Class Loss: 0.0006, Discrepancy Loss: 0.0003
Epoch [46/50], Class Loss: 0.0009, Discrepancy Loss: 0.0007
Epoch [47/50], Class Loss: 0.0011, Discrepancy Loss: 0.0003
Epoch [48/50], Class Loss: 0.0008, Discrepancy Loss: 0.0006
Epoch [49/50], Class Loss: 0.0010, Discrepancy Loss: 0.0009
Epoch [50/50], Class Loss: 0.0091, Discrepancy Loss: 0.0009
Source Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.76%, F1 Score: 99.76%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 2/10
Epoch [1/50], Class Loss: 1.7424, Discrepancy Loss: 0.1165
Epoch [2/50], Class Loss: 0.8750, Discrepancy Loss: 0.0750
Epoch [3/50], Class Loss: 0.6178, Discrepancy Loss: 0.0500
Epoch [4/50], Class Loss: 0.2906, Discrepancy Loss: 0.0324
Epoch [5/50], Class Loss: 0.2194, Discrepancy Loss: 0.0226
Epoch [6/50], Class Loss: 0.0419, Discrepancy Loss: 0.0077
Epoch [7/50], Class Loss: 0.0925, Discrepancy Loss: 0.0050
Epoch [8/50], Class Loss: 0.0195, Discrepancy Loss: 0.0022
Epoch [9/50], Class Loss: 0.0600, Discrepancy Loss: 0.0092
Epoch [10/50], Class Loss: 0.0093, Discrepancy Loss: 0.0007
Epoch [11/50], Class Loss: 0.0008, Discrepancy Loss: 0.0003
Epoch [12/50], Class Loss: 0.0004, Discrepancy Loss: 0.0002
Epoch [13/50], Class Loss: 0.0043, Discrepancy Loss: 0.0002
Epoch [14/50], Class Loss: 0.0033, Discrepancy Loss: 0.0005
Epoch [15/50], Class Loss: 0.0006, Discrepancy Loss: 0.0001
Epoch [16/50], Class Loss: 0.0010, Discrepancy Loss: 0.0005
Epoch [17/50], Class Loss: 0.0008, Discrepancy Loss: 0.0003
Epoch [18/50], Class Loss: 0.0008, Discrepancy Loss: 0.0001
Epoch [19/50], Class Loss: 0.0008, Discrepancy Loss: 0.0002
Epoch [20/50], Class Loss: 0.0011, Discrepancy Loss: 0.0008
Epoch [21/50], Class Loss: 0.0460, Discrepancy Loss: 0.0009
Epoch [22/50], Class Loss: 0.0014, Discrepancy Loss: 0.0002
Epoch [23/50], Class Loss: 0.0017, Discrepancy Loss: 0.0001
Epoch [24/50], Class Loss: 0.0010, Discrepancy Loss: 0.0001
Epoch [25/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Epoch [26/50], Class Loss: 0.0004, Discrepancy Loss: 0.0004
Epoch [27/50], Class Loss: 0.0003, Discrepancy Loss: 0.0006
Epoch [28/50], Class Loss: 0.0038, Discrepancy Loss: 0.0002
Epoch [29/50], Class Loss: 0.0008, Discrepancy Loss: 0.0001
Epoch [30/50], Class Loss: 0.0004, Discrepancy Loss: 0.0002
Epoch [31/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Epoch [32/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Epoch [33/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Epoch [34/50], Class Loss: 0.0004, Discrepancy Loss: 0.0004
Epoch [35/50], Class Loss: 0.0004, Discrepancy Loss: 0.0001
Epoch [36/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Epoch [37/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Epoch [38/50], Class Loss: 0.0007, Discrepancy Loss: 0.0002
Epoch [39/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Epoch [40/50], Class Loss: 0.0005, Discrepancy Loss: 0.0003
Epoch [41/50], Class Loss: 0.0010, Discrepancy Loss: 0.0001
Epoch [42/50], Class Loss: 0.0006, Discrepancy Loss: 0.0001
Epoch [43/50], Class Loss: 0.0089, Discrepancy Loss: 0.0002
Epoch [44/50], Class Loss: 0.0008, Discrepancy Loss: 0.0004
Epoch [45/50], Class Loss: 0.0009, Discrepancy Loss: 0.0001
Epoch [46/50], Class Loss: 0.0006, Discrepancy Loss: 0.0001
Epoch [47/50], Class Loss: 0.0008, Discrepancy Loss: 0.0001
Epoch [48/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Epoch [49/50], Class Loss: 0.0004, Discrepancy Loss: 0.0001
Epoch [50/50], Class Loss: 0.0005, Discrepancy Loss: 0.0001
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 3/10
Epoch [1/50], Class Loss: 1.9983, Discrepancy Loss: 0.1107
Epoch [2/50], Class Loss: 0.8306, Discrepancy Loss: 0.0733
Epoch [3/50], Class Loss: 0.4224, Discrepancy Loss: 0.0313
Epoch [4/50], Class Loss: 0.2458, Discrepancy Loss: 0.0337
Epoch [5/50], Class Loss: 0.3150, Discrepancy Loss: 0.0160
Epoch [6/50], Class Loss: 0.2829, Discrepancy Loss: 0.0399
Epoch [7/50], Class Loss: 0.1062, Discrepancy Loss: 0.0195
Epoch [8/50], Class Loss: 0.0439, Discrepancy Loss: 0.0081
Epoch [9/50], Class Loss: 0.1592, Discrepancy Loss: 0.0067
Epoch [10/50], Class Loss: 0.3396, Discrepancy Loss: 0.0341
Epoch [11/50], Class Loss: 0.0429, Discrepancy Loss: 0.0086
Epoch [12/50], Class Loss: 0.0242, Discrepancy Loss: 0.0069
Epoch [13/50], Class Loss: 0.0158, Discrepancy Loss: 0.0053
Epoch [14/50], Class Loss: 0.0410, Discrepancy Loss: 0.0043
Epoch [15/50], Class Loss: 0.0125, Discrepancy Loss: 0.0039
Epoch [16/50], Class Loss: 0.0498, Discrepancy Loss: 0.0035
Epoch [17/50], Class Loss: 0.0247, Discrepancy Loss: 0.0039
Epoch [18/50], Class Loss: 0.0071, Discrepancy Loss: 0.0022
Epoch [19/50], Class Loss: 0.0079, Discrepancy Loss: 0.0015
Epoch [20/50], Class Loss: 0.0030, Discrepancy Loss: 0.0015
Epoch [21/50], Class Loss: 0.0256, Discrepancy Loss: 0.0022
Epoch [22/50], Class Loss: 0.0915, Discrepancy Loss: 0.0011
Epoch [23/50], Class Loss: 0.0067, Discrepancy Loss: 0.0009
Epoch [24/50], Class Loss: 0.0029, Discrepancy Loss: 0.0020
Epoch [25/50], Class Loss: 0.0040, Discrepancy Loss: 0.0012
Epoch [26/50], Class Loss: 0.0033, Discrepancy Loss: 0.0008
Epoch [27/50], Class Loss: 0.0019, Discrepancy Loss: 0.0020
Epoch [28/50], Class Loss: 0.0030, Discrepancy Loss: 0.0006
Epoch [29/50], Class Loss: 0.0023, Discrepancy Loss: 0.0013
Epoch [30/50], Class Loss: 0.0023, Discrepancy Loss: 0.0022
Epoch [31/50], Class Loss: 0.0029, Discrepancy Loss: 0.0009
Epoch [32/50], Class Loss: 0.0023, Discrepancy Loss: 0.0014
Epoch [33/50], Class Loss: 0.0038, Discrepancy Loss: 0.0010
Epoch [34/50], Class Loss: 0.0025, Discrepancy Loss: 0.0015
Epoch [35/50], Class Loss: 0.0014, Discrepancy Loss: 0.0009
Epoch [36/50], Class Loss: 0.0876, Discrepancy Loss: 0.0015
Epoch [37/50], Class Loss: 0.0022, Discrepancy Loss: 0.0015
Epoch [38/50], Class Loss: 0.0120, Discrepancy Loss: 0.0016
Epoch [39/50], Class Loss: 0.0027, Discrepancy Loss: 0.0008
Epoch [40/50], Class Loss: 0.0018, Discrepancy Loss: 0.0015
Epoch [41/50], Class Loss: 0.0023, Discrepancy Loss: 0.0016
Epoch [42/50], Class Loss: 0.0018, Discrepancy Loss: 0.0010
Epoch [43/50], Class Loss: 0.0024, Discrepancy Loss: 0.0019
Epoch [44/50], Class Loss: 0.0024, Discrepancy Loss: 0.0009
Epoch [45/50], Class Loss: 0.0043, Discrepancy Loss: 0.0022
Epoch [46/50], Class Loss: 0.0028, Discrepancy Loss: 0.0009
Epoch [47/50], Class Loss: 0.0378, Discrepancy Loss: 0.0006
Epoch [48/50], Class Loss: 0.0079, Discrepancy Loss: 0.0008
Epoch [49/50], Class Loss: 0.0023, Discrepancy Loss: 0.0015
Epoch [50/50], Class Loss: 0.0020, Discrepancy Loss: 0.0028
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 4/10
Epoch [1/50], Class Loss: 1.8867, Discrepancy Loss: 0.1052
Epoch [2/50], Class Loss: 0.8429, Discrepancy Loss: 0.0720
Epoch [3/50], Class Loss: 0.4244, Discrepancy Loss: 0.0460
Epoch [4/50], Class Loss: 0.1295, Discrepancy Loss: 0.0190
Epoch [5/50], Class Loss: 0.0431, Discrepancy Loss: 0.0060
Epoch [6/50], Class Loss: 0.1488, Discrepancy Loss: 0.0112
Epoch [7/50], Class Loss: 0.0485, Discrepancy Loss: 0.0073
Epoch [8/50], Class Loss: 0.0201, Discrepancy Loss: 0.0020
Epoch [9/50], Class Loss: 0.0064, Discrepancy Loss: 0.0013
Epoch [10/50], Class Loss: 0.0280, Discrepancy Loss: 0.0023
Epoch [11/50], Class Loss: 0.0023, Discrepancy Loss: 0.0022
Epoch [12/50], Class Loss: 0.0009, Discrepancy Loss: 0.0006
Epoch [13/50], Class Loss: 0.0038, Discrepancy Loss: 0.0005
Epoch [14/50], Class Loss: 0.0113, Discrepancy Loss: 0.0007
Epoch [15/50], Class Loss: 0.0052, Discrepancy Loss: 0.0028
Epoch [16/50], Class Loss: 0.0447, Discrepancy Loss: 0.0005
Epoch [17/50], Class Loss: 0.0056, Discrepancy Loss: 0.0032
Epoch [18/50], Class Loss: 0.0010, Discrepancy Loss: 0.0005
Epoch [19/50], Class Loss: 0.0012, Discrepancy Loss: 0.0005
Epoch [20/50], Class Loss: 0.0006, Discrepancy Loss: 0.0025
Epoch [21/50], Class Loss: 0.0009, Discrepancy Loss: 0.0005
Epoch [22/50], Class Loss: 0.0003, Discrepancy Loss: 0.0007
Epoch [23/50], Class Loss: 0.0007, Discrepancy Loss: 0.0004
Epoch [24/50], Class Loss: 0.0700, Discrepancy Loss: 0.0004
Epoch [25/50], Class Loss: 0.0058, Discrepancy Loss: 0.0003
Epoch [26/50], Class Loss: 0.0004, Discrepancy Loss: 0.0025
Epoch [27/50], Class Loss: 0.0024, Discrepancy Loss: 0.0005
Epoch [28/50], Class Loss: 0.0007, Discrepancy Loss: 0.0018
Epoch [29/50], Class Loss: 0.0010, Discrepancy Loss: 0.0023
Epoch [30/50], Class Loss: 0.0759, Discrepancy Loss: 0.0010
Epoch [31/50], Class Loss: 0.1428, Discrepancy Loss: 0.0004
Epoch [32/50], Class Loss: 0.0043, Discrepancy Loss: 0.0003
Epoch [33/50], Class Loss: 0.0017, Discrepancy Loss: 0.0005
Epoch [34/50], Class Loss: 0.0029, Discrepancy Loss: 0.0004
Epoch [35/50], Class Loss: 0.0008, Discrepancy Loss: 0.0004
Epoch [36/50], Class Loss: 0.0011, Discrepancy Loss: 0.0010
Epoch [37/50], Class Loss: 0.0006, Discrepancy Loss: 0.0003
Epoch [38/50], Class Loss: 0.0433, Discrepancy Loss: 0.0008
Epoch [39/50], Class Loss: 0.0010, Discrepancy Loss: 0.0009
Epoch [40/50], Class Loss: 0.0006, Discrepancy Loss: 0.0013
Epoch [41/50], Class Loss: 0.0006, Discrepancy Loss: 0.0007
Epoch [42/50], Class Loss: 0.0013, Discrepancy Loss: 0.0006
Epoch [43/50], Class Loss: 0.0201, Discrepancy Loss: 0.0006
Epoch [44/50], Class Loss: 0.0029, Discrepancy Loss: 0.0003
Epoch [45/50], Class Loss: 0.0125, Discrepancy Loss: 0.0005
Epoch [46/50], Class Loss: 0.0009, Discrepancy Loss: 0.0006
Epoch [47/50], Class Loss: 0.0029, Discrepancy Loss: 0.0013
Epoch [48/50], Class Loss: 0.0008, Discrepancy Loss: 0.0010
Epoch [49/50], Class Loss: 0.0005, Discrepancy Loss: 0.0018
Epoch [50/50], Class Loss: 0.0004, Discrepancy Loss: 0.0003
Source Domain Performance - Accuracy: 99.70%, Precision: 99.70%, Recall: 99.71%, F1 Score: 99.70%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 5/10
Epoch [1/50], Class Loss: 1.7595, Discrepancy Loss: 0.1281
Epoch [2/50], Class Loss: 0.8494, Discrepancy Loss: 0.0687
Epoch [3/50], Class Loss: 0.5912, Discrepancy Loss: 0.0531
Epoch [4/50], Class Loss: 0.2712, Discrepancy Loss: 0.0292
Epoch [5/50], Class Loss: 0.3020, Discrepancy Loss: 0.0302
Epoch [6/50], Class Loss: 0.2111, Discrepancy Loss: 0.0170
Epoch [7/50], Class Loss: 0.0250, Discrepancy Loss: 0.0047
Epoch [8/50], Class Loss: 0.0133, Discrepancy Loss: 0.0030
Epoch [9/50], Class Loss: 0.0214, Discrepancy Loss: 0.0028
Epoch [10/50], Class Loss: 0.0177, Discrepancy Loss: 0.0018
Epoch [11/50], Class Loss: 0.0069, Discrepancy Loss: 0.0009
Epoch [12/50], Class Loss: 0.0037, Discrepancy Loss: 0.0006
Epoch [13/50], Class Loss: 0.0560, Discrepancy Loss: 0.0007
Epoch [14/50], Class Loss: 0.0043, Discrepancy Loss: 0.0013
Epoch [15/50], Class Loss: 0.0030, Discrepancy Loss: 0.0025
Epoch [16/50], Class Loss: 0.0027, Discrepancy Loss: 0.0007
Epoch [17/50], Class Loss: 0.0011, Discrepancy Loss: 0.0016
Epoch [18/50], Class Loss: 0.0009, Discrepancy Loss: 0.0005
Epoch [19/50], Class Loss: 0.0084, Discrepancy Loss: 0.0005
Epoch [20/50], Class Loss: 0.0039, Discrepancy Loss: 0.0007
Epoch [21/50], Class Loss: 0.0016, Discrepancy Loss: 0.0029
Epoch [22/50], Class Loss: 0.0398, Discrepancy Loss: 0.0006
Epoch [23/50], Class Loss: 0.0016, Discrepancy Loss: 0.0006
Epoch [24/50], Class Loss: 0.0026, Discrepancy Loss: 0.0006
Epoch [25/50], Class Loss: 0.0015, Discrepancy Loss: 0.0009
Epoch [26/50], Class Loss: 0.0012, Discrepancy Loss: 0.0009
Epoch [27/50], Class Loss: 0.0270, Discrepancy Loss: 0.0004
Epoch [28/50], Class Loss: 0.0004, Discrepancy Loss: 0.0005
Epoch [29/50], Class Loss: 0.0013, Discrepancy Loss: 0.0007
Epoch [30/50], Class Loss: 0.0005, Discrepancy Loss: 0.0006
Epoch [31/50], Class Loss: 0.0008, Discrepancy Loss: 0.0006
Epoch [32/50], Class Loss: 0.0007, Discrepancy Loss: 0.0011
Epoch [33/50], Class Loss: 0.0013, Discrepancy Loss: 0.0005
Epoch [34/50], Class Loss: 0.0019, Discrepancy Loss: 0.0004
Epoch [35/50], Class Loss: 0.0006, Discrepancy Loss: 0.0032
Epoch [36/50], Class Loss: 0.0005, Discrepancy Loss: 0.0006
Epoch [37/50], Class Loss: 0.0006, Discrepancy Loss: 0.0006
Epoch [38/50], Class Loss: 0.0010, Discrepancy Loss: 0.0006
Epoch [39/50], Class Loss: 0.0007, Discrepancy Loss: 0.0002
Epoch [40/50], Class Loss: 0.0011, Discrepancy Loss: 0.0006
Epoch [41/50], Class Loss: 0.0007, Discrepancy Loss: 0.0016
Epoch [42/50], Class Loss: 0.0004, Discrepancy Loss: 0.0005
Epoch [43/50], Class Loss: 0.0180, Discrepancy Loss: 0.0006
Epoch [44/50], Class Loss: 0.0006, Discrepancy Loss: 0.0004
Epoch [45/50], Class Loss: 0.0008, Discrepancy Loss: 0.0007
Epoch [46/50], Class Loss: 0.0011, Discrepancy Loss: 0.0005
Epoch [47/50], Class Loss: 0.0009, Discrepancy Loss: 0.0007
Epoch [48/50], Class Loss: 0.0014, Discrepancy Loss: 0.0004
Epoch [49/50], Class Loss: 0.0082, Discrepancy Loss: 0.0005
Epoch [50/50], Class Loss: 0.0006, Discrepancy Loss: 0.0005
Source Domain Performance - Accuracy: 99.58%, Precision: 99.58%, Recall: 99.59%, F1 Score: 99.58%
Target Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.75%, F1 Score: 99.76%

Run 6/10
Epoch [1/50], Class Loss: 1.6401, Discrepancy Loss: 0.1098
Epoch [2/50], Class Loss: 0.4786, Discrepancy Loss: 0.0595
Epoch [3/50], Class Loss: 0.1761, Discrepancy Loss: 0.0280
Epoch [4/50], Class Loss: 0.0608, Discrepancy Loss: 0.0079
Epoch [5/50], Class Loss: 0.0159, Discrepancy Loss: 0.0039
Epoch [6/50], Class Loss: 0.0049, Discrepancy Loss: 0.0037
Epoch [7/50], Class Loss: 0.1259, Discrepancy Loss: 0.0053
Epoch [8/50], Class Loss: 0.7894, Discrepancy Loss: 0.0806
Epoch [9/50], Class Loss: 0.3661, Discrepancy Loss: 0.0609
Epoch [10/50], Class Loss: 0.1815, Discrepancy Loss: 0.0310
Epoch [11/50], Class Loss: 0.1815, Discrepancy Loss: 0.0266
Epoch [12/50], Class Loss: 0.1143, Discrepancy Loss: 0.0206
Epoch [13/50], Class Loss: 0.1086, Discrepancy Loss: 0.0161
Epoch [14/50], Class Loss: 0.1189, Discrepancy Loss: 0.0201
Epoch [15/50], Class Loss: 0.1165, Discrepancy Loss: 0.0160
Epoch [16/50], Class Loss: 0.0916, Discrepancy Loss: 0.0158
Epoch [17/50], Class Loss: 0.0704, Discrepancy Loss: 0.0215
Epoch [18/50], Class Loss: 0.0857, Discrepancy Loss: 0.0193
Epoch [19/50], Class Loss: 0.0754, Discrepancy Loss: 0.0181
Epoch [20/50], Class Loss: 0.0601, Discrepancy Loss: 0.0134
Epoch [21/50], Class Loss: 0.1103, Discrepancy Loss: 0.0138
Epoch [22/50], Class Loss: 0.1902, Discrepancy Loss: 0.0112
Epoch [23/50], Class Loss: 0.0560, Discrepancy Loss: 0.0105
Epoch [24/50], Class Loss: 0.0441, Discrepancy Loss: 0.0175
Epoch [25/50], Class Loss: 0.0599, Discrepancy Loss: 0.0098
Epoch [26/50], Class Loss: 0.0608, Discrepancy Loss: 0.0129
Epoch [27/50], Class Loss: 0.0449, Discrepancy Loss: 0.0134
Epoch [28/50], Class Loss: 0.0372, Discrepancy Loss: 0.0082
Epoch [29/50], Class Loss: 0.0534, Discrepancy Loss: 0.0127
Epoch [30/50], Class Loss: 0.0358, Discrepancy Loss: 0.0114
Epoch [31/50], Class Loss: 0.0395, Discrepancy Loss: 0.0120
Epoch [32/50], Class Loss: 0.0384, Discrepancy Loss: 0.0109
Epoch [33/50], Class Loss: 0.0643, Discrepancy Loss: 0.0125
Epoch [34/50], Class Loss: 0.0485, Discrepancy Loss: 0.0085
Epoch [35/50], Class Loss: 0.0564, Discrepancy Loss: 0.0093
Epoch [36/50], Class Loss: 0.0439, Discrepancy Loss: 0.0077
Epoch [37/50], Class Loss: 0.0448, Discrepancy Loss: 0.0115
Epoch [38/50], Class Loss: 0.0491, Discrepancy Loss: 0.0086
Epoch [39/50], Class Loss: 0.0525, Discrepancy Loss: 0.0143
Epoch [40/50], Class Loss: 0.0467, Discrepancy Loss: 0.0131
Epoch [41/50], Class Loss: 0.0426, Discrepancy Loss: 0.0086
Epoch [42/50], Class Loss: 0.1309, Discrepancy Loss: 0.0112
Epoch [43/50], Class Loss: 0.0417, Discrepancy Loss: 0.0100
Epoch [44/50], Class Loss: 0.0449, Discrepancy Loss: 0.0084
Epoch [45/50], Class Loss: 0.0413, Discrepancy Loss: 0.0109
Epoch [46/50], Class Loss: 0.0383, Discrepancy Loss: 0.0112
Epoch [47/50], Class Loss: 0.1322, Discrepancy Loss: 0.0100
Epoch [48/50], Class Loss: 0.0632, Discrepancy Loss: 0.0110
Epoch [49/50], Class Loss: 0.0344, Discrepancy Loss: 0.0126
Epoch [50/50], Class Loss: 0.0369, Discrepancy Loss: 0.0128
Source Domain Performance - Accuracy: 97.90%, Precision: 97.94%, Recall: 97.93%, F1 Score: 97.90%
Target Domain Performance - Accuracy: 99.10%, Precision: 99.11%, Recall: 99.08%, F1 Score: 99.08%

Run 7/10
Epoch [1/50], Class Loss: 1.4966, Discrepancy Loss: 0.1118
Epoch [2/50], Class Loss: 0.7572, Discrepancy Loss: 0.0682
Epoch [3/50], Class Loss: 0.5053, Discrepancy Loss: 0.0509
Epoch [4/50], Class Loss: 0.1716, Discrepancy Loss: 0.0221
Epoch [5/50], Class Loss: 0.0533, Discrepancy Loss: 0.0076
Epoch [6/50], Class Loss: 0.0138, Discrepancy Loss: 0.0019
Epoch [7/50], Class Loss: 0.0113, Discrepancy Loss: 0.0033
Epoch [8/50], Class Loss: 0.0571, Discrepancy Loss: 0.0039
Epoch [9/50], Class Loss: 0.0513, Discrepancy Loss: 0.0028
Epoch [10/50], Class Loss: 0.0034, Discrepancy Loss: 0.0020
Epoch [11/50], Class Loss: 0.0008, Discrepancy Loss: 0.0005
Epoch [12/50], Class Loss: 0.0005, Discrepancy Loss: 0.0006
Epoch [13/50], Class Loss: 0.0005, Discrepancy Loss: 0.0003
Epoch [14/50], Class Loss: 0.0008, Discrepancy Loss: 0.0003
Epoch [15/50], Class Loss: 0.0006, Discrepancy Loss: 0.0015
Epoch [16/50], Class Loss: 0.0005, Discrepancy Loss: 0.0002
Epoch [17/50], Class Loss: 0.0009, Discrepancy Loss: 0.0004
Epoch [18/50], Class Loss: 0.0009, Discrepancy Loss: 0.0002
Epoch [19/50], Class Loss: 0.0013, Discrepancy Loss: 0.0004
Epoch [20/50], Class Loss: 0.0019, Discrepancy Loss: 0.0005
Epoch [21/50], Class Loss: 0.0069, Discrepancy Loss: 0.0004
Epoch [22/50], Class Loss: 0.0058, Discrepancy Loss: 0.0001
Epoch [23/50], Class Loss: 0.0005, Discrepancy Loss: 0.0002
Epoch [24/50], Class Loss: 0.0537, Discrepancy Loss: 0.0002
Epoch [25/50], Class Loss: 0.0010, Discrepancy Loss: 0.0002
Epoch [26/50], Class Loss: 0.0009, Discrepancy Loss: 0.0002
Epoch [27/50], Class Loss: 0.0003, Discrepancy Loss: 0.0002
Epoch [28/50], Class Loss: 0.0187, Discrepancy Loss: 0.0001
Epoch [29/50], Class Loss: 0.0007, Discrepancy Loss: 0.0001
Epoch [30/50], Class Loss: 0.0082, Discrepancy Loss: 0.0005
Epoch [31/50], Class Loss: 0.0010, Discrepancy Loss: 0.0001
Epoch [32/50], Class Loss: 0.0004, Discrepancy Loss: 0.0002
Epoch [33/50], Class Loss: 0.0127, Discrepancy Loss: 0.0005
Epoch [34/50], Class Loss: 0.0003, Discrepancy Loss: 0.0003
Epoch [35/50], Class Loss: 0.0033, Discrepancy Loss: 0.0005
Epoch [36/50], Class Loss: 0.0004, Discrepancy Loss: 0.0003
Epoch [37/50], Class Loss: 0.0007, Discrepancy Loss: 0.0001
Epoch [38/50], Class Loss: 0.0006, Discrepancy Loss: 0.0003
Epoch [39/50], Class Loss: 0.0006, Discrepancy Loss: 0.0004
Epoch [40/50], Class Loss: 0.0003, Discrepancy Loss: 0.0002
Epoch [41/50], Class Loss: 0.0004, Discrepancy Loss: 0.0005
Epoch [42/50], Class Loss: 0.0009, Discrepancy Loss: 0.0001
Epoch [43/50], Class Loss: 0.0683, Discrepancy Loss: 0.0002
Epoch [44/50], Class Loss: 0.0006, Discrepancy Loss: 0.0001
Epoch [45/50], Class Loss: 0.0040, Discrepancy Loss: 0.0001
Epoch [46/50], Class Loss: 0.0004, Discrepancy Loss: 0.0001
Epoch [47/50], Class Loss: 0.0004, Discrepancy Loss: 0.0003
Epoch [48/50], Class Loss: 0.0012, Discrepancy Loss: 0.0002
Epoch [49/50], Class Loss: 0.0005, Discrepancy Loss: 0.0003
Epoch [50/50], Class Loss: 0.0017, Discrepancy Loss: 0.0001
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 8/10
Epoch [1/50], Class Loss: 1.6743, Discrepancy Loss: 0.1264
Epoch [2/50], Class Loss: 0.9467, Discrepancy Loss: 0.0747
Epoch [3/50], Class Loss: 0.5493, Discrepancy Loss: 0.0673
Epoch [4/50], Class Loss: 0.1726, Discrepancy Loss: 0.0184
Epoch [5/50], Class Loss: 0.0488, Discrepancy Loss: 0.0073
Epoch [6/50], Class Loss: 0.0718, Discrepancy Loss: 0.0058
Epoch [7/50], Class Loss: 0.7496, Discrepancy Loss: 0.0669
Epoch [8/50], Class Loss: 0.2876, Discrepancy Loss: 0.0151
Epoch [9/50], Class Loss: 0.0942, Discrepancy Loss: 0.0110
Epoch [10/50], Class Loss: 0.0108, Discrepancy Loss: 0.0018
Epoch [11/50], Class Loss: 0.0056, Discrepancy Loss: 0.0014
Epoch [12/50], Class Loss: 0.0052, Discrepancy Loss: 0.0019
Epoch [13/50], Class Loss: 0.0104, Discrepancy Loss: 0.0011
Epoch [14/50], Class Loss: 0.0039, Discrepancy Loss: 0.0014
Epoch [15/50], Class Loss: 0.0028, Discrepancy Loss: 0.0010
Epoch [16/50], Class Loss: 0.0016, Discrepancy Loss: 0.0009
Epoch [17/50], Class Loss: 0.0024, Discrepancy Loss: 0.0010
Epoch [18/50], Class Loss: 0.0024, Discrepancy Loss: 0.0008
Epoch [19/50], Class Loss: 0.0821, Discrepancy Loss: 0.0005
Epoch [20/50], Class Loss: 0.0073, Discrepancy Loss: 0.0022
Epoch [21/50], Class Loss: 0.0020, Discrepancy Loss: 0.0011
Epoch [22/50], Class Loss: 0.0261, Discrepancy Loss: 0.0013
Epoch [23/50], Class Loss: 0.0036, Discrepancy Loss: 0.0019
Epoch [24/50], Class Loss: 0.0022, Discrepancy Loss: 0.0014
Epoch [25/50], Class Loss: 0.0023, Discrepancy Loss: 0.0010
Epoch [26/50], Class Loss: 0.0013, Discrepancy Loss: 0.0010
Epoch [27/50], Class Loss: 0.0037, Discrepancy Loss: 0.0013
Epoch [28/50], Class Loss: 0.0022, Discrepancy Loss: 0.0011
Epoch [29/50], Class Loss: 0.0019, Discrepancy Loss: 0.0013
Epoch [30/50], Class Loss: 0.0028, Discrepancy Loss: 0.0020
Epoch [31/50], Class Loss: 0.0919, Discrepancy Loss: 0.0008
Epoch [32/50], Class Loss: 0.0025, Discrepancy Loss: 0.0008
Epoch [33/50], Class Loss: 0.0113, Discrepancy Loss: 0.0009
Epoch [34/50], Class Loss: 0.0029, Discrepancy Loss: 0.0010
Epoch [35/50], Class Loss: 0.0040, Discrepancy Loss: 0.0011
Epoch [36/50], Class Loss: 0.0417, Discrepancy Loss: 0.0021
Epoch [37/50], Class Loss: 0.0025, Discrepancy Loss: 0.0011
Epoch [38/50], Class Loss: 0.1255, Discrepancy Loss: 0.0011
Epoch [39/50], Class Loss: 0.1469, Discrepancy Loss: 0.0014
Epoch [40/50], Class Loss: 0.0026, Discrepancy Loss: 0.0010
Epoch [41/50], Class Loss: 0.0034, Discrepancy Loss: 0.0015
Epoch [42/50], Class Loss: 0.0032, Discrepancy Loss: 0.0009
Epoch [43/50], Class Loss: 0.0034, Discrepancy Loss: 0.0019
Epoch [44/50], Class Loss: 0.0027, Discrepancy Loss: 0.0008
Epoch [45/50], Class Loss: 0.0031, Discrepancy Loss: 0.0012
Epoch [46/50], Class Loss: 0.0027, Discrepancy Loss: 0.0016
Epoch [47/50], Class Loss: 0.0117, Discrepancy Loss: 0.0011
Epoch [48/50], Class Loss: 0.0019, Discrepancy Loss: 0.0009
Epoch [49/50], Class Loss: 0.0021, Discrepancy Loss: 0.0009
Epoch [50/50], Class Loss: 0.0054, Discrepancy Loss: 0.0017
Source Domain Performance - Accuracy: 99.64%, Precision: 99.64%, Recall: 99.65%, F1 Score: 99.64%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 9/10
Epoch [1/50], Class Loss: 1.7225, Discrepancy Loss: 0.1079
Epoch [2/50], Class Loss: 0.8606, Discrepancy Loss: 0.0694
Epoch [3/50], Class Loss: 0.6122, Discrepancy Loss: 0.0633
Epoch [4/50], Class Loss: 0.3555, Discrepancy Loss: 0.0233
Epoch [5/50], Class Loss: 0.1970, Discrepancy Loss: 0.0273
Epoch [6/50], Class Loss: 0.1014, Discrepancy Loss: 0.0146
Epoch [7/50], Class Loss: 0.0219, Discrepancy Loss: 0.0056
Epoch [8/50], Class Loss: 0.0172, Discrepancy Loss: 0.0025
Epoch [9/50], Class Loss: 0.0705, Discrepancy Loss: 0.0068
Epoch [10/50], Class Loss: 0.0564, Discrepancy Loss: 0.0042
Epoch [11/50], Class Loss: 0.0365, Discrepancy Loss: 0.0048
Epoch [12/50], Class Loss: 0.0092, Discrepancy Loss: 0.0026
Epoch [13/50], Class Loss: 0.0042, Discrepancy Loss: 0.0020
Epoch [14/50], Class Loss: 0.0040, Discrepancy Loss: 0.0016
Epoch [15/50], Class Loss: 0.0056, Discrepancy Loss: 0.0014
Epoch [16/50], Class Loss: 0.0104, Discrepancy Loss: 0.0012
Epoch [17/50], Class Loss: 0.0091, Discrepancy Loss: 0.0012
Epoch [18/50], Class Loss: 0.0261, Discrepancy Loss: 0.0011
Epoch [19/50], Class Loss: 0.0093, Discrepancy Loss: 0.0020
Epoch [20/50], Class Loss: 0.0027, Discrepancy Loss: 0.0008
Epoch [21/50], Class Loss: 0.0028, Discrepancy Loss: 0.0005
Epoch [22/50], Class Loss: 0.0621, Discrepancy Loss: 0.0012
Epoch [23/50], Class Loss: 0.0023, Discrepancy Loss: 0.0018
Epoch [24/50], Class Loss: 0.0023, Discrepancy Loss: 0.0023
Epoch [25/50], Class Loss: 0.0020, Discrepancy Loss: 0.0006
Epoch [26/50], Class Loss: 0.0013, Discrepancy Loss: 0.0010
Epoch [27/50], Class Loss: 0.0017, Discrepancy Loss: 0.0009
Epoch [28/50], Class Loss: 0.0280, Discrepancy Loss: 0.0008
Epoch [29/50], Class Loss: 0.0025, Discrepancy Loss: 0.0024
Epoch [30/50], Class Loss: 0.0020, Discrepancy Loss: 0.0020
Epoch [31/50], Class Loss: 0.0078, Discrepancy Loss: 0.0013
Epoch [32/50], Class Loss: 0.0068, Discrepancy Loss: 0.0019
Epoch [33/50], Class Loss: 0.0125, Discrepancy Loss: 0.0018
Epoch [34/50], Class Loss: 0.0391, Discrepancy Loss: 0.0010
Epoch [35/50], Class Loss: 0.0402, Discrepancy Loss: 0.0013
Epoch [36/50], Class Loss: 0.0033, Discrepancy Loss: 0.0009
Epoch [37/50], Class Loss: 0.0333, Discrepancy Loss: 0.0015
Epoch [38/50], Class Loss: 0.0017, Discrepancy Loss: 0.0009
Epoch [39/50], Class Loss: 0.0068, Discrepancy Loss: 0.0010
Epoch [40/50], Class Loss: 0.0164, Discrepancy Loss: 0.0007
Epoch [41/50], Class Loss: 0.0020, Discrepancy Loss: 0.0010
Epoch [42/50], Class Loss: 0.0023, Discrepancy Loss: 0.0006
Epoch [43/50], Class Loss: 0.0023, Discrepancy Loss: 0.0010
Epoch [44/50], Class Loss: 0.0440, Discrepancy Loss: 0.0007
Epoch [45/50], Class Loss: 0.0020, Discrepancy Loss: 0.0007
Epoch [46/50], Class Loss: 0.0014, Discrepancy Loss: 0.0013
Epoch [47/50], Class Loss: 0.0161, Discrepancy Loss: 0.0007
Epoch [48/50], Class Loss: 0.0202, Discrepancy Loss: 0.0006
Epoch [49/50], Class Loss: 0.0027, Discrepancy Loss: 0.0023
Epoch [50/50], Class Loss: 0.0138, Discrepancy Loss: 0.0006
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 10/10
Epoch [1/50], Class Loss: 1.5770, Discrepancy Loss: 0.1197
Epoch [2/50], Class Loss: 0.6821, Discrepancy Loss: 0.0650
Epoch [3/50], Class Loss: 0.3284, Discrepancy Loss: 0.0507
Epoch [4/50], Class Loss: 0.4001, Discrepancy Loss: 0.0343
Epoch [5/50], Class Loss: 0.2169, Discrepancy Loss: 0.0290
Epoch [6/50], Class Loss: 0.0947, Discrepancy Loss: 0.0162
Epoch [7/50], Class Loss: 0.2353, Discrepancy Loss: 0.0377
Epoch [8/50], Class Loss: 0.0967, Discrepancy Loss: 0.0139
Epoch [9/50], Class Loss: 0.1201, Discrepancy Loss: 0.0167
Epoch [10/50], Class Loss: 0.0603, Discrepancy Loss: 0.0067
Epoch [11/50], Class Loss: 0.0239, Discrepancy Loss: 0.0052
Epoch [12/50], Class Loss: 0.0205, Discrepancy Loss: 0.0042
Epoch [13/50], Class Loss: 0.0127, Discrepancy Loss: 0.0052
Epoch [14/50], Class Loss: 0.1155, Discrepancy Loss: 0.0044
Epoch [15/50], Class Loss: 0.0430, Discrepancy Loss: 0.0044
Epoch [16/50], Class Loss: 0.0129, Discrepancy Loss: 0.0042
Epoch [17/50], Class Loss: 0.0133, Discrepancy Loss: 0.0028
Epoch [18/50], Class Loss: 0.0092, Discrepancy Loss: 0.0033
Epoch [19/50], Class Loss: 0.0076, Discrepancy Loss: 0.0019
Epoch [20/50], Class Loss: 0.0158, Discrepancy Loss: 0.0023
Epoch [21/50], Class Loss: 0.0067, Discrepancy Loss: 0.0018
Epoch [22/50], Class Loss: 0.0057, Discrepancy Loss: 0.0020
Epoch [23/50], Class Loss: 0.0497, Discrepancy Loss: 0.0019
Epoch [24/50], Class Loss: 0.0080, Discrepancy Loss: 0.0017
Epoch [25/50], Class Loss: 0.0107, Discrepancy Loss: 0.0029
Epoch [26/50], Class Loss: 0.0083, Discrepancy Loss: 0.0024
Epoch [27/50], Class Loss: 0.0065, Discrepancy Loss: 0.0022
Epoch [28/50], Class Loss: 0.0066, Discrepancy Loss: 0.0016
Epoch [29/50], Class Loss: 0.0078, Discrepancy Loss: 0.0018
Epoch [30/50], Class Loss: 0.0061, Discrepancy Loss: 0.0020
Epoch [31/50], Class Loss: 0.0058, Discrepancy Loss: 0.0017
Epoch [32/50], Class Loss: 0.0075, Discrepancy Loss: 0.0023
Epoch [33/50], Class Loss: 0.0086, Discrepancy Loss: 0.0013
Epoch [34/50], Class Loss: 0.0689, Discrepancy Loss: 0.0015
Epoch [35/50], Class Loss: 0.0050, Discrepancy Loss: 0.0017
Epoch [36/50], Class Loss: 0.0050, Discrepancy Loss: 0.0018
Epoch [37/50], Class Loss: 0.0261, Discrepancy Loss: 0.0015
Epoch [38/50], Class Loss: 0.0259, Discrepancy Loss: 0.0019
Epoch [39/50], Class Loss: 0.0137, Discrepancy Loss: 0.0019
Epoch [40/50], Class Loss: 0.0059, Discrepancy Loss: 0.0015
Epoch [41/50], Class Loss: 0.0169, Discrepancy Loss: 0.0015
Epoch [42/50], Class Loss: 0.0035, Discrepancy Loss: 0.0016
Epoch [43/50], Class Loss: 0.0054, Discrepancy Loss: 0.0024
Epoch [44/50], Class Loss: 0.0054, Discrepancy Loss: 0.0014
Epoch [45/50], Class Loss: 0.0065, Discrepancy Loss: 0.0019
Epoch [46/50], Class Loss: 0.0046, Discrepancy Loss: 0.0019
Epoch [47/50], Class Loss: 0.0471, Discrepancy Loss: 0.0021
Epoch [48/50], Class Loss: 0.0096, Discrepancy Loss: 0.0030
Epoch [49/50], Class Loss: 0.0035, Discrepancy Loss: 0.0016
Epoch [50/50], Class Loss: 0.0048, Discrepancy Loss: 0.0038
Source Domain Performance - Accuracy: 99.10%, Precision: 99.13%, Recall: 99.08%, F1 Score: 99.10%
Target Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.76%, F1 Score: 99.76%

Source performance: 99.55% 99.56% 99.55% 99.55%
Target performance: 99.81% 99.81% 99.81% 99.81%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 100.00%
16qam: 99.83%
8apsk: 99.41%
MCD

Run 1/10
Epoch [1/50], Class Loss: 0.7849, Discrepancy Loss: 0.0339
Validation Loss: 8.2447
Epoch [2/50], Class Loss: 0.2109, Discrepancy Loss: 0.0063
Validation Loss: 3.5362
Epoch [3/50], Class Loss: 0.2465, Discrepancy Loss: 0.0092
Validation Loss: 21.1081
Epoch [4/50], Class Loss: 0.0972, Discrepancy Loss: 0.0032
Validation Loss: 0.5398
Epoch [5/50], Class Loss: 0.1160, Discrepancy Loss: 0.0038
Validation Loss: 0.1046
Epoch [6/50], Class Loss: 0.0390, Discrepancy Loss: 0.0013
Validation Loss: 0.1400
Epoch [7/50], Class Loss: 0.0346, Discrepancy Loss: 0.0006
Validation Loss: 0.1133
Epoch [8/50], Class Loss: 0.0614, Discrepancy Loss: 0.0006
Validation Loss: 5.0424
Epoch [9/50], Class Loss: 0.0323, Discrepancy Loss: 0.0003
Validation Loss: 0.0029
Epoch [10/50], Class Loss: 0.0201, Discrepancy Loss: 0.0004
Validation Loss: 0.0230
Epoch [11/50], Class Loss: 0.0020, Discrepancy Loss: 0.0001
Validation Loss: 0.0012
Epoch [12/50], Class Loss: 0.0015, Discrepancy Loss: 0.0000
Validation Loss: 0.0018
Epoch [13/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [14/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0008
Epoch [15/50], Class Loss: 0.0002, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [16/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [17/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [18/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [19/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [20/50], Class Loss: 0.0001, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [21/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [22/50], Class Loss: 0.0204, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [23/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [24/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0007
Epoch [25/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [26/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [27/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [28/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [29/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [30/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [31/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [32/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 2/10
Epoch [1/50], Class Loss: 0.6596, Discrepancy Loss: 0.0256
Validation Loss: 4.9507
Epoch [2/50], Class Loss: 0.2222, Discrepancy Loss: 0.0139
Validation Loss: 4.3046
Epoch [3/50], Class Loss: 0.1154, Discrepancy Loss: 0.0052
Validation Loss: 0.7375
Epoch [4/50], Class Loss: 0.1008, Discrepancy Loss: 0.0024
Validation Loss: 0.3490
Epoch [5/50], Class Loss: 0.0340, Discrepancy Loss: 0.0012
Validation Loss: 0.9545
Epoch [6/50], Class Loss: 0.0517, Discrepancy Loss: 0.0007
Validation Loss: 0.4305
Epoch [7/50], Class Loss: 0.0603, Discrepancy Loss: 0.0003
Validation Loss: 0.2036
Epoch [8/50], Class Loss: 0.0067, Discrepancy Loss: 0.0002
Validation Loss: 0.0982
Epoch [9/50], Class Loss: 0.0082, Discrepancy Loss: 0.0003
Validation Loss: 0.0196
Epoch [10/50], Class Loss: 0.0607, Discrepancy Loss: 0.0005
Validation Loss: 0.0527
Epoch [11/50], Class Loss: 0.0015, Discrepancy Loss: 0.0001
Validation Loss: 0.0039
Epoch [12/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0049
Epoch [13/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0045
Epoch [14/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0048
Epoch [15/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0035
Epoch [16/50], Class Loss: 0.0317, Discrepancy Loss: 0.0000
Validation Loss: 0.0033
Epoch [17/50], Class Loss: 0.0055, Discrepancy Loss: 0.0000
Validation Loss: 0.0019
Epoch [18/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0016
Epoch [19/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0013
Epoch [20/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0014
Epoch [21/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0016
Epoch [22/50], Class Loss: 0.0004, Discrepancy Loss: 0.0000
Validation Loss: 0.0026
Epoch [23/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0022
Epoch [24/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0022
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 3/10
Epoch [1/50], Class Loss: 0.7118, Discrepancy Loss: 0.0172
Validation Loss: 5.1211
Epoch [2/50], Class Loss: 0.2903, Discrepancy Loss: 0.0076
Validation Loss: 0.9951
Epoch [3/50], Class Loss: 0.1357, Discrepancy Loss: 0.0046
Validation Loss: 0.5266
Epoch [4/50], Class Loss: 0.0859, Discrepancy Loss: 0.0025
Validation Loss: 8.2219
Epoch [5/50], Class Loss: 0.0521, Discrepancy Loss: 0.0014
Validation Loss: 0.2849
Epoch [6/50], Class Loss: 0.0372, Discrepancy Loss: 0.0009
Validation Loss: 1.1581
Epoch [7/50], Class Loss: 0.1009, Discrepancy Loss: 0.0023
Validation Loss: 0.3911
Epoch [8/50], Class Loss: 0.0303, Discrepancy Loss: 0.0010
Validation Loss: 2.9386
Epoch [9/50], Class Loss: 0.0323, Discrepancy Loss: 0.0007
Validation Loss: 20.4940
Epoch [10/50], Class Loss: 0.0261, Discrepancy Loss: 0.0005
Validation Loss: 0.1826
Epoch [11/50], Class Loss: 0.0105, Discrepancy Loss: 0.0004
Validation Loss: 0.0225
Epoch [12/50], Class Loss: 0.0012, Discrepancy Loss: 0.0001
Validation Loss: 0.0364
Epoch [13/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0071
Epoch [14/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0060
Epoch [15/50], Class Loss: 0.0004, Discrepancy Loss: 0.0000
Validation Loss: 0.0049
Epoch [16/50], Class Loss: 0.0003, Discrepancy Loss: 0.0000
Validation Loss: 0.0062
Epoch [17/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0054
Epoch [18/50], Class Loss: 0.0144, Discrepancy Loss: 0.0000
Validation Loss: 0.0045
Epoch [19/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0050
Epoch [20/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0072
Epoch [21/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0077
Epoch [22/50], Class Loss: 0.0017, Discrepancy Loss: 0.0000
Validation Loss: 0.0084
Epoch [23/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0060
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 4/10
Epoch [1/50], Class Loss: 0.6916, Discrepancy Loss: 0.0249
Validation Loss: 24.3929
Epoch [2/50], Class Loss: 0.1664, Discrepancy Loss: 0.0059
Validation Loss: 32.0569
Epoch [3/50], Class Loss: 0.1593, Discrepancy Loss: 0.0068
Validation Loss: 0.3597
Epoch [4/50], Class Loss: 0.0900, Discrepancy Loss: 0.0043
Validation Loss: 0.7750
Epoch [5/50], Class Loss: 0.0357, Discrepancy Loss: 0.0014
Validation Loss: 0.3171
Epoch [6/50], Class Loss: 0.0495, Discrepancy Loss: 0.0017
Validation Loss: 3.9021
Epoch [7/50], Class Loss: 0.0125, Discrepancy Loss: 0.0007
Validation Loss: 13.8075
Epoch [8/50], Class Loss: 0.0448, Discrepancy Loss: 0.0006
Validation Loss: 0.0073
Epoch [9/50], Class Loss: 0.0083, Discrepancy Loss: 0.0003
Validation Loss: 0.0122
Epoch [10/50], Class Loss: 0.0748, Discrepancy Loss: 0.0005
Validation Loss: 0.0036
Epoch [11/50], Class Loss: 0.0010, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [12/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [13/50], Class Loss: 0.0075, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [14/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [15/50], Class Loss: 0.0125, Discrepancy Loss: 0.0001
Validation Loss: 0.0004
Epoch [16/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0010
Epoch [18/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [19/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0006
Epoch [20/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0005
Epoch [21/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0005
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 5/10
Epoch [1/50], Class Loss: 0.7159, Discrepancy Loss: 0.0247
Validation Loss: 11.8406
Epoch [2/50], Class Loss: 0.2044, Discrepancy Loss: 0.0119
Validation Loss: 4.3155
Epoch [3/50], Class Loss: 0.0985, Discrepancy Loss: 0.0026
Validation Loss: 1.8861
Epoch [4/50], Class Loss: 0.0726, Discrepancy Loss: 0.0031
Validation Loss: 3.6524
Epoch [5/50], Class Loss: 0.0401, Discrepancy Loss: 0.0013
Validation Loss: 2.0029
Epoch [6/50], Class Loss: 0.0281, Discrepancy Loss: 0.0012
Validation Loss: 2.0601
Epoch [7/50], Class Loss: 0.0473, Discrepancy Loss: 0.0011
Validation Loss: 0.0612
Epoch [8/50], Class Loss: 0.0208, Discrepancy Loss: 0.0004
Validation Loss: 0.4038
Epoch [9/50], Class Loss: 0.0196, Discrepancy Loss: 0.0003
Validation Loss: 0.1592
Epoch [10/50], Class Loss: 0.0031, Discrepancy Loss: 0.0002
Validation Loss: 1.8031
Epoch [11/50], Class Loss: 0.0006, Discrepancy Loss: 0.0000
Validation Loss: 0.0014
Epoch [12/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0016
Epoch [13/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0013
Epoch [14/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0013
Epoch [15/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0012
Epoch [16/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0021
Epoch [17/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0009
Epoch [18/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0008
Epoch [19/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0016
Epoch [20/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0013
Epoch [21/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0015
Epoch [22/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0018
Epoch [23/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0014
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 6/10
Epoch [1/50], Class Loss: 0.7866, Discrepancy Loss: 0.0312
Validation Loss: 8.3022
Epoch [2/50], Class Loss: 0.2331, Discrepancy Loss: 0.0144
Validation Loss: 0.3159
Epoch [3/50], Class Loss: 0.1342, Discrepancy Loss: 0.0027
Validation Loss: 0.2848
Epoch [4/50], Class Loss: 0.0756, Discrepancy Loss: 0.0009
Validation Loss: 2.9045
Epoch [5/50], Class Loss: 0.0647, Discrepancy Loss: 0.0008
Validation Loss: 13.2228
Epoch [6/50], Class Loss: 0.0518, Discrepancy Loss: 0.0013
Validation Loss: 0.8532
Epoch [7/50], Class Loss: 0.0965, Discrepancy Loss: 0.0022
Validation Loss: 0.0692
Epoch [8/50], Class Loss: 0.0444, Discrepancy Loss: 0.0008
Validation Loss: 0.3034
Epoch [9/50], Class Loss: 0.0273, Discrepancy Loss: 0.0008
Validation Loss: 0.0552
Epoch [10/50], Class Loss: 0.0142, Discrepancy Loss: 0.0006
Validation Loss: 0.1871
Epoch [11/50], Class Loss: 0.0010, Discrepancy Loss: 0.0000
Validation Loss: 0.0005
Epoch [12/50], Class Loss: 0.0020, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [13/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [14/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [15/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [16/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [17/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [18/50], Class Loss: 0.0005, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [19/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0006
Epoch [20/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [21/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [22/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [23/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [24/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [25/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [26/50], Class Loss: 0.0081, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 7/10
Epoch [1/50], Class Loss: 0.6620, Discrepancy Loss: 0.0264
Validation Loss: 10.1747
Epoch [2/50], Class Loss: 0.2835, Discrepancy Loss: 0.0173
Validation Loss: 4.8228
Epoch [3/50], Class Loss: 0.2253, Discrepancy Loss: 0.0066
Validation Loss: 8.2600
Epoch [4/50], Class Loss: 0.1420, Discrepancy Loss: 0.0037
Validation Loss: 2.2640
Epoch [5/50], Class Loss: 0.0647, Discrepancy Loss: 0.0022
Validation Loss: 1.3774
Epoch [6/50], Class Loss: 0.0683, Discrepancy Loss: 0.0013
Validation Loss: 13.8347
Epoch [7/50], Class Loss: 0.0498, Discrepancy Loss: 0.0014
Validation Loss: 15.9456
Epoch [8/50], Class Loss: 0.0166, Discrepancy Loss: 0.0004
Validation Loss: 1.0212
Epoch [9/50], Class Loss: 0.0103, Discrepancy Loss: 0.0001
Validation Loss: 1.6874
Epoch [10/50], Class Loss: 0.0081, Discrepancy Loss: 0.0001
Validation Loss: 2.1586
Epoch [11/50], Class Loss: 0.0201, Discrepancy Loss: 0.0001
Validation Loss: 0.0030
Epoch [12/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0025
Epoch [13/50], Class Loss: 0.0049, Discrepancy Loss: 0.0000
Validation Loss: 0.0019
Epoch [14/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0025
Epoch [15/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0023
Epoch [16/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0017
Epoch [17/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0019
Epoch [18/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0020
Epoch [19/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0018
Epoch [20/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0021
Epoch [21/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0018
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 8/10
Epoch [1/50], Class Loss: 0.7346, Discrepancy Loss: 0.0309
Validation Loss: 23.4488
Epoch [2/50], Class Loss: 0.2237, Discrepancy Loss: 0.0073
Validation Loss: 0.6813
Epoch [3/50], Class Loss: 0.2078, Discrepancy Loss: 0.0074
Validation Loss: 4.2571
Epoch [4/50], Class Loss: 0.1254, Discrepancy Loss: 0.0051
Validation Loss: 0.3738
Epoch [5/50], Class Loss: 0.0675, Discrepancy Loss: 0.0015
Validation Loss: 2.4343
Epoch [6/50], Class Loss: 0.0345, Discrepancy Loss: 0.0009
Validation Loss: 27.1819
Epoch [7/50], Class Loss: 0.0254, Discrepancy Loss: 0.0006
Validation Loss: 1.5014
Epoch [8/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0003
Epoch [9/50], Class Loss: 0.0056, Discrepancy Loss: 0.0001
Validation Loss: 0.4262
Epoch [10/50], Class Loss: 0.0019, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [11/50], Class Loss: 0.0004, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [12/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [13/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [14/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [15/50], Class Loss: 0.0013, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [16/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [18/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [19/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [20/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [21/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [22/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [23/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 9/10
Epoch [1/50], Class Loss: 0.6103, Discrepancy Loss: 0.0178
Validation Loss: 5.7099
Epoch [2/50], Class Loss: 0.3163, Discrepancy Loss: 0.0100
Validation Loss: 65.6301
Epoch [3/50], Class Loss: 0.1295, Discrepancy Loss: 0.0033
Validation Loss: 1.1147
Epoch [4/50], Class Loss: 0.1206, Discrepancy Loss: 0.0053
Validation Loss: 1.1418
Epoch [5/50], Class Loss: 0.0314, Discrepancy Loss: 0.0017
Validation Loss: 23.6983
Epoch [6/50], Class Loss: 0.0269, Discrepancy Loss: 0.0014
Validation Loss: 0.0072
Epoch [7/50], Class Loss: 0.0210, Discrepancy Loss: 0.0005
Validation Loss: 4.1137
Epoch [8/50], Class Loss: 0.0047, Discrepancy Loss: 0.0002
Validation Loss: 0.1219
Epoch [9/50], Class Loss: 0.0208, Discrepancy Loss: 0.0003
Validation Loss: 3.1699
Epoch [10/50], Class Loss: 0.0144, Discrepancy Loss: 0.0002
Validation Loss: 0.0420
Epoch [11/50], Class Loss: 0.0002, Discrepancy Loss: 0.0000
Validation Loss: 0.0002
Epoch [12/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [13/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [14/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [15/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [16/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [18/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [19/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [20/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [21/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [22/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [23/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [24/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 10/10
Epoch [1/50], Class Loss: 0.6796, Discrepancy Loss: 0.0253
Validation Loss: 4.4974
Epoch [2/50], Class Loss: 0.2697, Discrepancy Loss: 0.0118
Validation Loss: 0.4854
Epoch [3/50], Class Loss: 0.1113, Discrepancy Loss: 0.0035
Validation Loss: 9.2317
Epoch [4/50], Class Loss: 0.1426, Discrepancy Loss: 0.0066
Validation Loss: 6.7734
Epoch [5/50], Class Loss: 0.0364, Discrepancy Loss: 0.0011
Validation Loss: 1.5171
Epoch [6/50], Class Loss: 0.0242, Discrepancy Loss: 0.0006
Validation Loss: 0.0394
Epoch [7/50], Class Loss: 0.0219, Discrepancy Loss: 0.0016
Validation Loss: 0.2966
Epoch [8/50], Class Loss: 0.0101, Discrepancy Loss: 0.0003
Validation Loss: 0.1745
Epoch [9/50], Class Loss: 0.0003, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [10/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0004
Epoch [11/50], Class Loss: 0.0000, Discrepancy Loss: 0.0001
Validation Loss: 0.0001
Epoch [12/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [13/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0000
Epoch [14/50], Class Loss: 0.0001, Discrepancy Loss: 0.0000
Validation Loss: 0.0000
Epoch [15/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [16/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [17/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [18/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0001
Epoch [19/50], Class Loss: 0.0000, Discrepancy Loss: 0.0000
Validation Loss: 0.0000
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Source performance: 99.98% 99.98% 99.98% 99.98%
Target performance: 99.99% 99.99% 99.99% 99.99%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 100.00%
16qam: 99.98%
8apsk: 99.98%
JAN

Run 1/10
Epoch [1/50], Class Loss: 0.4585, JMMD Loss: 0.0524
Validation Loss: 0.4091
Epoch [2/50], Class Loss: 0.1850, JMMD Loss: 0.0408
Validation Loss: 0.4043
Epoch [3/50], Class Loss: 0.1446, JMMD Loss: 0.0371
Validation Loss: 0.0731
Epoch [4/50], Class Loss: 0.1196, JMMD Loss: 0.0363
Validation Loss: 0.1044
Epoch [5/50], Class Loss: 0.0366, JMMD Loss: 0.0405
Validation Loss: 1.1728
Epoch [6/50], Class Loss: 0.0462, JMMD Loss: 0.0391
Validation Loss: 0.2569
Epoch [7/50], Class Loss: 0.0191, JMMD Loss: 0.0412
Validation Loss: 0.0155
Epoch [8/50], Class Loss: 0.0173, JMMD Loss: 0.0438
Validation Loss: 0.5895
Epoch [9/50], Class Loss: 0.1972, JMMD Loss: 0.0657
Validation Loss: 0.3631
Epoch [10/50], Class Loss: 0.1139, JMMD Loss: 0.0462
Validation Loss: 0.0451
Epoch [11/50], Class Loss: 0.0388, JMMD Loss: 0.0502
Validation Loss: 0.0203
Epoch [12/50], Class Loss: 0.0499, JMMD Loss: 0.0494
Validation Loss: 0.0250
Early stopping!
Source Domain Performance - Accuracy: 99.10%, Precision: 99.13%, Recall: 99.08%, F1 Score: 99.10%
Target Domain Performance - Accuracy: 99.28%, Precision: 99.28%, Recall: 99.28%, F1 Score: 99.27%

Run 2/10
Epoch [1/50], Class Loss: 0.4662, JMMD Loss: 0.0525
Validation Loss: 0.3848
Epoch [2/50], Class Loss: 0.2836, JMMD Loss: 0.0387
Validation Loss: 3.2816
Epoch [3/50], Class Loss: 0.1678, JMMD Loss: 0.0555
Validation Loss: 0.3810
Epoch [4/50], Class Loss: 0.0900, JMMD Loss: 0.0374
Validation Loss: 0.2008
Epoch [5/50], Class Loss: 0.0507, JMMD Loss: 0.0421
Validation Loss: 0.0931
Epoch [6/50], Class Loss: 0.0213, JMMD Loss: 0.0364
Validation Loss: 1.4624
Epoch [7/50], Class Loss: 0.0468, JMMD Loss: 0.0444
Validation Loss: 0.2599
Epoch [8/50], Class Loss: 0.0679, JMMD Loss: 0.0505
Validation Loss: 0.3650
Epoch [9/50], Class Loss: 0.0348, JMMD Loss: 0.0305
Validation Loss: 0.6536
Epoch [10/50], Class Loss: 0.0348, JMMD Loss: 0.0539
Validation Loss: 4.7981
Early stopping!
Source Domain Performance - Accuracy: 74.46%, Precision: 62.10%, Recall: 75.00%, F1 Score: 66.35%
Target Domain Performance - Accuracy: 75.72%, Precision: 62.57%, Recall: 75.00%, F1 Score: 66.77%

Run 3/10
Epoch [1/50], Class Loss: 0.3578, JMMD Loss: 0.0512
Validation Loss: 1.3699
Epoch [2/50], Class Loss: 0.1860, JMMD Loss: 0.0441
Validation Loss: 0.1037
Epoch [3/50], Class Loss: 0.0820, JMMD Loss: 0.0348
Validation Loss: 0.1479
Epoch [4/50], Class Loss: 0.0601, JMMD Loss: 0.0450
Validation Loss: 1.0649
Epoch [5/50], Class Loss: 0.0662, JMMD Loss: 0.0394
Validation Loss: 0.2358
Epoch [6/50], Class Loss: 0.0524, JMMD Loss: 0.0525
Validation Loss: 0.3188
Epoch [7/50], Class Loss: 0.0115, JMMD Loss: 0.0399
Validation Loss: 0.3510
Early stopping!
Source Domain Performance - Accuracy: 90.47%, Precision: 92.95%, Recall: 90.67%, F1 Score: 90.16%
Target Domain Performance - Accuracy: 90.71%, Precision: 93.12%, Recall: 90.43%, F1 Score: 90.16%

Run 4/10
Epoch [1/50], Class Loss: 0.3796, JMMD Loss: 0.0412
Validation Loss: 1.4309
Epoch [2/50], Class Loss: 0.1541, JMMD Loss: 0.0449
Validation Loss: 0.5708
Epoch [3/50], Class Loss: 0.1140, JMMD Loss: 0.0393
Validation Loss: 0.1032
Epoch [4/50], Class Loss: 0.1392, JMMD Loss: 0.0361
Validation Loss: 0.5038
Epoch [5/50], Class Loss: 0.0632, JMMD Loss: 0.0351
Validation Loss: 0.0518
Epoch [6/50], Class Loss: 0.1385, JMMD Loss: 0.0343
Validation Loss: 0.0667
Epoch [7/50], Class Loss: 0.0543, JMMD Loss: 0.0355
Validation Loss: 2.4872
Epoch [8/50], Class Loss: 0.0588, JMMD Loss: 0.0414
Validation Loss: 0.1223
Epoch [9/50], Class Loss: 0.0141, JMMD Loss: 0.0422
Validation Loss: 0.0364
Epoch [10/50], Class Loss: 0.0194, JMMD Loss: 0.0322
Validation Loss: 4.0615
Epoch [11/50], Class Loss: 0.0069, JMMD Loss: 0.0437
Validation Loss: 0.0101
Epoch [12/50], Class Loss: 0.0030, JMMD Loss: 0.0426
Validation Loss: 0.0063
Epoch [13/50], Class Loss: 0.0019, JMMD Loss: 0.0311
Validation Loss: 0.0077
Epoch [14/50], Class Loss: 0.0047, JMMD Loss: 0.0408
Validation Loss: 0.0103
Epoch [15/50], Class Loss: 0.0226, JMMD Loss: 0.0432
Validation Loss: 0.0130
Epoch [16/50], Class Loss: 0.0056, JMMD Loss: 0.0415
Validation Loss: 0.0224
Epoch [17/50], Class Loss: 0.0026, JMMD Loss: 0.0386
Validation Loss: 0.0067
Early stopping!
Source Domain Performance - Accuracy: 99.82%, Precision: 99.83%, Recall: 99.81%, F1 Score: 99.82%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 5/10
Epoch [1/50], Class Loss: 0.3676, JMMD Loss: 0.0570
Validation Loss: 0.5831
Epoch [2/50], Class Loss: 0.1963, JMMD Loss: 0.0438
Validation Loss: 0.7888
Epoch [3/50], Class Loss: 0.1185, JMMD Loss: 0.0330
Validation Loss: 0.1892
Epoch [4/50], Class Loss: 0.0876, JMMD Loss: 0.0364
Validation Loss: 0.0818
Epoch [5/50], Class Loss: 0.0504, JMMD Loss: 0.0397
Validation Loss: 0.1024
Epoch [6/50], Class Loss: 0.1175, JMMD Loss: 0.0423
Validation Loss: 1.3737
Epoch [7/50], Class Loss: 0.1912, JMMD Loss: 0.0538
Validation Loss: 0.0644
Epoch [8/50], Class Loss: 0.0593, JMMD Loss: 0.0345
Validation Loss: 0.3247
Epoch [9/50], Class Loss: 0.0357, JMMD Loss: 0.0427
Validation Loss: 0.0567
Epoch [10/50], Class Loss: 0.0159, JMMD Loss: 0.0314
Validation Loss: 0.0632
Epoch [11/50], Class Loss: 0.0398, JMMD Loss: 0.0429
Validation Loss: 0.0115
Epoch [12/50], Class Loss: 0.0224, JMMD Loss: 0.0380
Validation Loss: 0.0096
Epoch [13/50], Class Loss: 0.0205, JMMD Loss: 0.0368
Validation Loss: 0.0135
Epoch [14/50], Class Loss: 0.0090, JMMD Loss: 0.0372
Validation Loss: 0.0129
Epoch [15/50], Class Loss: 0.0104, JMMD Loss: 0.0331
Validation Loss: 0.0082
Epoch [16/50], Class Loss: 0.0089, JMMD Loss: 0.0401
Validation Loss: 0.0149
Epoch [17/50], Class Loss: 0.0058, JMMD Loss: 0.0450
Validation Loss: 0.0148
Epoch [18/50], Class Loss: 0.0063, JMMD Loss: 0.0416
Validation Loss: 0.0099
Epoch [19/50], Class Loss: 0.0096, JMMD Loss: 0.0362
Validation Loss: 0.0386
Epoch [20/50], Class Loss: 0.0058, JMMD Loss: 0.0348
Validation Loss: 0.0158
Early stopping!
Source Domain Performance - Accuracy: 99.52%, Precision: 99.54%, Recall: 99.51%, F1 Score: 99.52%
Target Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%

Run 6/10
Epoch [1/50], Class Loss: 0.4411, JMMD Loss: 0.0457
Validation Loss: 2.0673
Epoch [2/50], Class Loss: 0.1647, JMMD Loss: 0.0669
Validation Loss: 3.9556
Epoch [3/50], Class Loss: 0.1146, JMMD Loss: 0.0413
Validation Loss: 0.7646
Epoch [4/50], Class Loss: 0.3446, JMMD Loss: 0.0452
Validation Loss: 0.1425
Epoch [5/50], Class Loss: 0.1283, JMMD Loss: 0.0377
Validation Loss: 0.1808
Epoch [6/50], Class Loss: 0.0649, JMMD Loss: 0.0333
Validation Loss: 0.1615
Epoch [7/50], Class Loss: 0.0582, JMMD Loss: 0.0371
Validation Loss: 0.0234
Epoch [8/50], Class Loss: 0.0915, JMMD Loss: 0.0325
Validation Loss: 1.3276
Epoch [9/50], Class Loss: 0.0466, JMMD Loss: 0.0335
Validation Loss: 0.0280
Epoch [10/50], Class Loss: 0.0747, JMMD Loss: 0.0417
Validation Loss: 0.0195
Epoch [11/50], Class Loss: 0.0349, JMMD Loss: 0.0401
Validation Loss: 0.0385
Epoch [12/50], Class Loss: 0.0452, JMMD Loss: 0.0402
Validation Loss: 0.0122
Epoch [13/50], Class Loss: 0.0090, JMMD Loss: 0.0415
Validation Loss: 0.0121
Epoch [14/50], Class Loss: 0.0068, JMMD Loss: 0.0359
Validation Loss: 0.0081
Epoch [15/50], Class Loss: 0.0098, JMMD Loss: 0.0396
Validation Loss: 0.0106
Epoch [16/50], Class Loss: 0.0094, JMMD Loss: 0.0425
Validation Loss: 0.0149
Epoch [17/50], Class Loss: 0.0060, JMMD Loss: 0.0448
Validation Loss: 0.0072
Epoch [18/50], Class Loss: 0.0158, JMMD Loss: 0.0437
Validation Loss: 0.0055
Epoch [19/50], Class Loss: 0.0079, JMMD Loss: 0.0414
Validation Loss: 0.0096
Epoch [20/50], Class Loss: 0.0043, JMMD Loss: 0.0516
Validation Loss: 0.0202
Epoch [21/50], Class Loss: 0.0070, JMMD Loss: 0.0393
Validation Loss: 0.0057
Epoch [22/50], Class Loss: 0.0207, JMMD Loss: 0.0430
Validation Loss: 0.0108
Epoch [23/50], Class Loss: 0.0119, JMMD Loss: 0.0457
Validation Loss: 0.0061
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Run 7/10
Epoch [1/50], Class Loss: 0.4464, JMMD Loss: 0.0515
Validation Loss: 0.2069
Epoch [2/50], Class Loss: 0.2256, JMMD Loss: 0.0479
Validation Loss: 0.1799
Epoch [3/50], Class Loss: 0.2055, JMMD Loss: 0.0430
Validation Loss: 0.2099
Epoch [4/50], Class Loss: 0.1073, JMMD Loss: 0.0498
Validation Loss: 0.1711
Epoch [5/50], Class Loss: 0.0743, JMMD Loss: 0.0371
Validation Loss: 0.4303
Epoch [6/50], Class Loss: 0.0415, JMMD Loss: 0.0282
Validation Loss: 0.3629
Epoch [7/50], Class Loss: 0.0803, JMMD Loss: 0.0313
Validation Loss: 0.0526
Epoch [8/50], Class Loss: 0.0265, JMMD Loss: 0.0338
Validation Loss: 0.1265
Epoch [9/50], Class Loss: 0.0082, JMMD Loss: 0.0390
Validation Loss: 0.0201
Epoch [10/50], Class Loss: 0.0053, JMMD Loss: 0.0377
Validation Loss: 0.0177
Epoch [11/50], Class Loss: 0.0050, JMMD Loss: 0.0374
Validation Loss: 0.0098
Epoch [12/50], Class Loss: 0.0012, JMMD Loss: 0.0385
Validation Loss: 0.0086
Epoch [13/50], Class Loss: 0.0012, JMMD Loss: 0.0345
Validation Loss: 0.0128
Epoch [14/50], Class Loss: 0.0017, JMMD Loss: 0.0330
Validation Loss: 0.0090
Epoch [15/50], Class Loss: 0.0021, JMMD Loss: 0.0414
Validation Loss: 0.0100
Epoch [16/50], Class Loss: 0.0013, JMMD Loss: 0.0303
Validation Loss: 0.0079
Epoch [17/50], Class Loss: 0.0019, JMMD Loss: 0.0349
Validation Loss: 0.0078
Epoch [18/50], Class Loss: 0.0023, JMMD Loss: 0.0432
Validation Loss: 0.0105
Epoch [19/50], Class Loss: 0.0034, JMMD Loss: 0.0371
Validation Loss: 0.0077
Epoch [20/50], Class Loss: 0.0008, JMMD Loss: 0.0322
Validation Loss: 0.0070
Epoch [21/50], Class Loss: 0.0012, JMMD Loss: 0.0378
Validation Loss: 0.0076
Epoch [22/50], Class Loss: 0.0116, JMMD Loss: 0.0475
Validation Loss: 0.0067
Epoch [23/50], Class Loss: 0.0013, JMMD Loss: 0.0360
Validation Loss: 0.0085
Epoch [24/50], Class Loss: 0.0018, JMMD Loss: 0.0346
Validation Loss: 0.0096
Epoch [25/50], Class Loss: 0.0023, JMMD Loss: 0.0393
Validation Loss: 0.0078
Epoch [26/50], Class Loss: 0.0013, JMMD Loss: 0.0346
Validation Loss: 0.0085
Epoch [27/50], Class Loss: 0.0007, JMMD Loss: 0.0333
Validation Loss: 0.0068
Early stopping!
Source Domain Performance - Accuracy: 99.70%, Precision: 99.70%, Recall: 99.70%, F1 Score: 99.70%
Target Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%

Run 8/10
Epoch [1/50], Class Loss: 0.3795, JMMD Loss: 0.0369
Validation Loss: 0.2855
Epoch [2/50], Class Loss: 0.1085, JMMD Loss: 0.0399
Validation Loss: 0.1436
Epoch [3/50], Class Loss: 0.1420, JMMD Loss: 0.0308
Validation Loss: 0.4670
Epoch [4/50], Class Loss: 0.0571, JMMD Loss: 0.0303
Validation Loss: 0.0580
Epoch [5/50], Class Loss: 0.0900, JMMD Loss: 0.0411
Validation Loss: 2.5104
Epoch [6/50], Class Loss: 0.1334, JMMD Loss: 0.0347
Validation Loss: 0.0670
Epoch [7/50], Class Loss: 0.0245, JMMD Loss: 0.0383
Validation Loss: 0.0143
Epoch [8/50], Class Loss: 0.0302, JMMD Loss: 0.0361
Validation Loss: 0.2844
Epoch [9/50], Class Loss: 0.0114, JMMD Loss: 0.0448
Validation Loss: 0.0087
Epoch [10/50], Class Loss: 0.0118, JMMD Loss: 0.0344
Validation Loss: 0.5867
Epoch [11/50], Class Loss: 0.0033, JMMD Loss: 0.0373
Validation Loss: 0.0027
Epoch [12/50], Class Loss: 0.0016, JMMD Loss: 0.0319
Validation Loss: 0.0022
Epoch [13/50], Class Loss: 0.0012, JMMD Loss: 0.0363
Validation Loss: 0.0023
Epoch [14/50], Class Loss: 0.0539, JMMD Loss: 0.0393
Validation Loss: 0.0057
Epoch [15/50], Class Loss: 0.0153, JMMD Loss: 0.0373
Validation Loss: 0.0061
Epoch [16/50], Class Loss: 0.0028, JMMD Loss: 0.0467
Validation Loss: 0.0024
Epoch [17/50], Class Loss: 0.0024, JMMD Loss: 0.0376
Validation Loss: 0.0033
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%
Target Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%

Run 9/10
Epoch [1/50], Class Loss: 0.3453, JMMD Loss: 0.0503
Validation Loss: 1.4082
Epoch [2/50], Class Loss: 0.1484, JMMD Loss: 0.0408
Validation Loss: 0.6144
Epoch [3/50], Class Loss: 0.0837, JMMD Loss: 0.0325
Validation Loss: 2.2221
Epoch [4/50], Class Loss: 0.0657, JMMD Loss: 0.0310
Validation Loss: 5.5318
Epoch [5/50], Class Loss: 0.0688, JMMD Loss: 0.0389
Validation Loss: 1.0092
Epoch [6/50], Class Loss: 0.0361, JMMD Loss: 0.0363
Validation Loss: 0.1077
Epoch [7/50], Class Loss: 0.0369, JMMD Loss: 0.0330
Validation Loss: 4.3288
Epoch [8/50], Class Loss: 0.2311, JMMD Loss: 0.0446
Validation Loss: 0.9947
Epoch [9/50], Class Loss: 0.2690, JMMD Loss: 0.0458
Validation Loss: 0.4263
Epoch [10/50], Class Loss: 0.1092, JMMD Loss: 0.0370
Validation Loss: 0.2230
Epoch [11/50], Class Loss: 0.0267, JMMD Loss: 0.0414
Validation Loss: 0.0197
Epoch [12/50], Class Loss: 0.0164, JMMD Loss: 0.0443
Validation Loss: 0.0119
Epoch [13/50], Class Loss: 0.0146, JMMD Loss: 0.0391
Validation Loss: 0.0120
Epoch [14/50], Class Loss: 0.0097, JMMD Loss: 0.0394
Validation Loss: 0.0117
Epoch [15/50], Class Loss: 0.0111, JMMD Loss: 0.0392
Validation Loss: 0.0078
Epoch [16/50], Class Loss: 0.0093, JMMD Loss: 0.0433
Validation Loss: 0.0080
Epoch [17/50], Class Loss: 0.0077, JMMD Loss: 0.0398
Validation Loss: 0.0095
Epoch [18/50], Class Loss: 0.0082, JMMD Loss: 0.0313
Validation Loss: 0.0104
Epoch [19/50], Class Loss: 0.0111, JMMD Loss: 0.0343
Validation Loss: 0.0129
Epoch [20/50], Class Loss: 0.0246, JMMD Loss: 0.0352
Validation Loss: 0.0141
Early stopping!
Source Domain Performance - Accuracy: 99.40%, Precision: 99.40%, Recall: 99.41%, F1 Score: 99.40%
Target Domain Performance - Accuracy: 99.76%, Precision: 99.76%, Recall: 99.75%, F1 Score: 99.76%

Run 10/10
Epoch [1/50], Class Loss: 0.4742, JMMD Loss: 0.0502
Validation Loss: 0.3001
Epoch [2/50], Class Loss: 0.1561, JMMD Loss: 0.0328
Validation Loss: 0.2699
Epoch [3/50], Class Loss: 0.1031, JMMD Loss: 0.0342
Validation Loss: 0.3919
Epoch [4/50], Class Loss: 0.1202, JMMD Loss: 0.0338
Validation Loss: 17.8817
Epoch [5/50], Class Loss: 0.1883, JMMD Loss: 0.0309
Validation Loss: 0.8706
Epoch [6/50], Class Loss: 0.0882, JMMD Loss: 0.0318
Validation Loss: 0.1374
Epoch [7/50], Class Loss: 0.0864, JMMD Loss: 0.0467
Validation Loss: 0.4421
Epoch [8/50], Class Loss: 0.0402, JMMD Loss: 0.0302
Validation Loss: 0.2175
Epoch [9/50], Class Loss: 0.0723, JMMD Loss: 0.0343
Validation Loss: 0.0173
Epoch [10/50], Class Loss: 0.0340, JMMD Loss: 0.0475
Validation Loss: 1.1788
Epoch [11/50], Class Loss: 0.0151, JMMD Loss: 0.0338
Validation Loss: 0.0060
Epoch [12/50], Class Loss: 0.0031, JMMD Loss: 0.0365
Validation Loss: 0.0044
Epoch [13/50], Class Loss: 0.0037, JMMD Loss: 0.0406
Validation Loss: 0.0036
Epoch [14/50], Class Loss: 0.0036, JMMD Loss: 0.0392
Validation Loss: 0.0205
Epoch [15/50], Class Loss: 0.0152, JMMD Loss: 0.0360
Validation Loss: 0.0319
Epoch [16/50], Class Loss: 0.0047, JMMD Loss: 0.0365
Validation Loss: 0.0052
Epoch [17/50], Class Loss: 0.0019, JMMD Loss: 0.0380
Validation Loss: 0.0061
Epoch [18/50], Class Loss: 0.0032, JMMD Loss: 0.0406
Validation Loss: 0.0055
Early stopping!
Source Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%
Target Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%

Source performance: 96.21% 95.22% 96.28% 95.36%
Target performance: 96.50% 95.42% 96.40% 95.55%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 100.00%
  Class 1: 100.00%
  Class 2: 99.54%
  Class 3: 86.05%
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=67fffd7a-3b75-46c4-a8aa-502de54b3650">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [5]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_snr</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_deep_coral_acc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_base_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Base'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_dann_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'DANN'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_star_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Star'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_mcd_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'D'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'MCD'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'v'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'DCORAL'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_jan_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'x'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'JANN'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'SNR (dB)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Acc (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'SIM SNR-to-SNR DA'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>4
4
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3gUVRfA4d/W9E56Agmh995BVJCmUhXBQv8sNEVREBCwIagIUqwUBUSkWiiKKAqE3ntHQhoJ6XXbfH+sLMQkECCd8z5PHnZn7tw5s5Mle/Y2laIoCkIIIYQQQggh8qUu6QCEEEIIIYQQorSTxEkIIYQQQgghbkMSJyGEEEIIIYS4DUmchBBCCCGEEOI2JHESQgghhBBCiNuQxEkIIYQQQgghbkMSJyGEEEIIIYS4DUmchBBCCCGEEOI2JHESQgghhBBCiNuQxEkIIYQQQgghbkMSJyGEKOOOHj1Knz59qFSpEvb29gQGBtKxY0fmzJmTo1xISAiPPvpojm0qlQqVSsXQoUPzrHvChAm2MvHx8YUai0qlYuTIkbnq2Lp1KyqVilWrVtm2LV682BaHSqVCq9USGBjIwIEDiYyMvG1cABs2bGDKlCkFKnunLBYL3377Lc2bN8fT0xMXFxeqVavGc889x65du2zlrl+bSqVi//79ueoZOHAgzs7OOba1b98+x7U7ODhQr149Zs2ahcViKVB8119vlUqFWq3G3d2dunXr8r///Y/du3ff8tgNGzagUqkICAgo8PmEEKI8ksRJCCHKsPDwcJo0acLhw4cZNmwYc+fOZejQoajVambPnl2gOuzt7Vm9ejUGgyHXvuXLl2Nvb19ksXz11VdERUUVqH6At99+myVLlvD555/TpUsXli5dygMPPEBWVtZtj92wYQNTp04t8LnuxKhRoxgwYAD+/v5MmTKF6dOn06VLF3bt2sWmTZvyPOZOkrigoCCWLFnCkiVLmDZtGvb29rzyyitMmjSpwHU0aNCAJUuW8O233zJt2jQefPBBfv75Z1q0aMGYMWPyPW7ZsmWEhIQQHR3NH3/8UeDzCSFEuaMIIYQos7p27ap4e3sriYmJufbFxsbmeF6pUiWlW7duObYBSo8ePRS1Wq2sW7cux74dO3YogNK7d28FUOLi4go1ltq1aytarVYZOXJkjn1//vmnAigrV660bVu0aJECKHv37s1R9o033lAAZcWKFbeMTVEUZfjw4UpR/NmLiYlRVCqVMmzYsFz7LBZLjmu/fm0NGjRQAGX//v05yg8YMEBxcnLKse2BBx5QateunWNbZmamUqlSJcXFxUUxmUy3jTGve68oipKRkaH06NFDAZT58+fn2p+WlqY4OTkpn376qdKwYUNl4MCBtz2XEEKUV9LiJIQQZdj58+epXbs27u7uufb5+PgUqI7AwEDatWvHd999l2P7smXLqFu3LnXq1CmSWEJCQnjuuefuuNXpZm3btrWd+1YGDhzIvHnzAHJ0e7suPT2dV199leDgYOzs7KhevTofffQRiqLcNoaLFy+iKAqtW7fOtU+lUuV57SNHjsTDw+Ouuw7a29vTtGlTUlNTuXr16l3VAeDg4MCSJUvw9PTkvffey3W9a9euJTMzkyeeeIKnnnqKNWvWFKh1TwghyiNJnIQQogyrVKkS+/fv59ixY/dUT//+/fn5559JS0sDwGQysXLlSvr371+ksUyYMAGTycQHH3xwxzEDXLp0CQAPD49blnv++efp2LEjgK3L25IlSwBQFIXHH3+cTz75hM6dOzNz5kyqV6/O2LFjb9mF7bpKlSoBsHLlSjIyMgoUt6urK6+88go///wzBw4cKNAx/3Xp0iVUKlWeieqdcHZ2pmfPnkRGRnLixIkc+5YtW8aDDz6In58fTz31FKmpqfz888/3dD4hhCirJHESQogy7LXXXiMjI4MGDRrQqlUr3njjDX777TeMRuMd1dOnTx/MZjPr1q0D4LfffiM+Pp5+/foVaSyVK1fm2Wef5auvviI6Ovq250hOTiY+Pp4rV66wevVqpk6dip2dXa5JL/6rZcuWVKtWDYBnnnnG9gPw008/8ccff/DOO+/w1VdfMXz4cH766Sf69OnD7Nmzb9ua5e/vz3PPPcf69esJCgqiV69efPzxx5w6deqWx40aNQoPD48Cjbsym83Ex8cTHx/P6dOnef3119m3bx9du3bFwcHhtsffzvVWxZuv9erVq/z+++889dRTAFSsWJGWLVuybNmyez6fEEKURZI4CSFEGdaxY0d27tzJ448/zuHDh5kxYwadOnUiMDCQn376qcD1eHh40LlzZ5YvXw7Ad999R6tWrWytKUUZy8SJEwvc6tShQwe8vb0JDg6mT58+ODk58dNPPxEUFFTgOP9rw4YNaDQaRo0alWP7q6++iqIobNy48bZ1LFq0iLlz5xIaGsratWt57bXXqFmzJg8//HC+s/65ubnx8ssv89NPP3Hw4MFb1n/q1Cm8vb3x9vamRo0afPjhhzz++OMsXry4wNd5K9dn8ktNTbVt+/7771Gr1fTu3du2rV+/fmzcuJHExMRCOa8QQpQlkjgJIUQZ17RpU9asWUNiYiJ79uxh/PjxpKam0qdPn1xdr26lf//+bN68mcuXL7Nu3bo76qZ3L7Fcb3X68ssvb9vqNG/ePDZv3syqVavo2rUr8fHx2NnZ2fYbDAZiYmJy/JjN5lvW+c8//xAQEICLi0uO7TVr1rTtB0hLS8tRb1xcnK2sWq1m+PDh7N+/n/j4eH788Ue6dOnCH3/8YWuxycvo0aNxd3e/7VinkJAQNm/ezK+//sr8+fMJDAwkLi6uwDMe3s71Lpo3vwZLly6lWbNmXLt2jXPnznHu3DkaNmyIwWBg5cqVhXJeIYQoSyRxEkKIckKv19O0aVPef/99PvvsM4xG4x19wH388cexs7NjwIABZGdn8+STTxZbLNfHOk2fPv2W9TZr1owOHTrQu3dvfvrpJ+rUqUP//v1tH/zDw8Px9/fP8RMREXHX13Gzjz76KEe9TZs2zbOcl5cXjz/+OBs2bOCBBx5g+/bttuTrvwra6uTk5ESHDh145JFHePHFF9mwYQN79uzhzTffLJRruz4urUqVKgCcPXuWvXv3sn37dqpWrWr7adOmDYB01xNC3Je0JR2AEEKIwtekSROAAo0bus7BwYEePXqwdOlSunTpQoUKFYotlrCwMJ555hm++OILmjdvXqB6NRqNbT2iuXPnMm7cOOrXr8/mzZtzlPPz8wPIMYvezSpVqsTvv/9OampqjhaX62OUrndXfO6552yJA1CgsUVNmjThr7/+Ijo6Ot9ujy+//DKzZs1i6tSpBZ7ooV69erbX67XXXqNixYoFOi4vaWlprF27luDgYFsr27Jly9DpdCxZsgSNRpOj/Pbt2/n000+5fPnyPZ1XCCHKGmlxEkKIMuzPP//Mc8rsDRs2AFC9evU7qu+1115j8uTJd7SwamHFMnHiRIxGIzNmzCjwOdu3b0+zZs2YNWsWWVlZeHh40KFDhxw/17uzOTk5AZCUlJSjjq5du2I2m5k7d26O7Z988gkqlYouXboA1i6FN9d7ffrxmJiYPLshGgwGtmzZglqttrXk5OV6q9OPP/7IoUOHCnztr7/+OkajkZkzZxb4mP/KzMzk2WefJSEhgQkTJtiSy2XLltG2bVv69u1Lnz59cvyMHTsWwDYeTggh7hfS4iSEEGXYyJEjycjIoGfPntSoUQODwUB4eDgrVqwgJCSEQYMG3VF99evXp379+iUSy/VWp2+++eaOzjt27FieeOIJFi9ezAsvvJBvucaNGwPW2ew6deqERqPhqaee4rHHHuPBBx9kwoQJXLp0ifr16/Pbb7/x448/8vLLLxMWFnbL81+5coVmzZrx0EMP8fDDD+Pn58fVq1dZvnw5hw8f5uWXX75t693o0aP55JNPOHz4sC3Bu51atWrRtWtXvv76ayZNmoSXl9cty0dGRrJ06VLA2sp04sQJVq5cSUxMDK+++irPP/88ALt37+bcuXOMGDEiz3oCAwNp1KgRy5Yt44033ihQrEIIUS6U4OK7Qggh7tHGjRuVwYMHKzVq1FCcnZ0VvV6vVKlSRRk5cqQSGxubo2ylSpWUbt265dgGKMOHD7/lOSZPnqwASlxcXJHGoiiKcvbsWUWj0SiAsnLlStv2RYsWKYCyd+/eXMeYzWYlLCxMCQsLU0wmU77xmUwmZeTIkYq3t7eiUqmUm/8EpqamKq+88ooSEBCg6HQ6pWrVqsqHH36oWCyWW16zoihKSkqKMnv2bKVTp05KUFCQotPpFBcXF6Vly5bKV199laOOP//8M9e1XXf9dXZycsqx/YEHHlBq166d57m3bt2qAMrkyZNvGWOlSpUUQAEUlUqluLq6KrVr11aGDRum7N69O0fZkSNHKoBy/vz5fOubMmWKAiiHDx++5XmFEKI8USlKAZZFF0IIIYQQQoj7mIxxEkIIIYQQQojbkMRJCCGEEEIIIW5DEichhBBCCCGEuA1JnIQQQgghhBDiNiRxEkIIIYQQQojbkMRJCCGEEEIIIW7jvlsA12KxEBUVhYuLi22FdCGEEEIIIcT9R1EUUlNTCQgIQK2+dZvSfZc4RUVFERwcXNJhCCGEEEIIIUqJiIgIgoKCblnmvkucXFxcAOuL4+rqWsLRgNFo5LfffuORRx5Bp9OVdDiiEMg9LX/knpZPcl/LH7mn5ZPc1/KnNN3TlJQUgoODbTnCrdx3idP17nmurq6lJnFydHTE1dW1xH9xROGQe1r+yD0tn+S+lj9yT8snua/lT2m8pwUZwiOTQwghhBBCCCHEbUjiJIQQQgghhBC3IYmTEEIIIYQQQtzGfTfGqSAURcFkMmE2m4v8XEajEa1WS1ZWVrGcr7zQaDRotVqZUl4IIYQQQhQLSZz+w2AwEB0dTUZGRrGcT1EU/Pz8iIiIkCTgDjk6OuLv749ery/pUIQQQgghRDknidNNLBYLFy9eRKPREBAQgF6vL/JkxmKxkJaWhrOz820X3RJWiqJgMBiIi4vj4sWLVK1aVV47IYQQQghRpCRxuonBYMBisRAcHIyjo2OxnNNisWAwGLC3t5cP/3fAwcEBnU7HP//8Y3v9hBBCCCGEKCryST0PksCUDXKfhBBCCCFEcZFPnkIIIYQQQghxG5I4CSGEEEIIIcRtyBinImK2KOy5mMDV1Cx8XOxpFuqJRi2z5gkhhBBCCFEWSeJUBDYdi2bqzyeITs6ybfN3s2fyY7XoXMe/SM45cOBAvvnmG9tzT09PmjZtyowZM6hXr16RnFMIIYQQQoj7hXTVK2SbjkXz4tIDOZImgJjkLF5ceoBNx6KL7NydO3cmOjqa6OhotmzZglar5dFHHy2y8wkhhBBCCHGnEj7/gqpvjCPh8y9KOpQ7IonTbSiKQobBVKCf1Cwjk386jpJXPf/+O+WnE6RmGXMcl2kw51mfouRVU/7s7Ozw8/PDz8+PBg0aMG7cOCIiIoiLiwPgjTfeoFq1ajg6OlK5cmUmTZqE0Wi0HX/48GEefPBBXFxccHV1pXHjxuzbt8+2f/v27bRt2xYHBweCg4MZNWoU6enpd/qSCiGEEEKI+1Tc/PkkzJuHCkiYN4+4+fNLOqQCK9Guen///Tcffvgh+/fvJzo6mrVr19KjR49bHrN161bGjBnD8ePHCQ4OZuLEiQwcOLDIYsw0mqn11q+FUpcCxKRkUXfKbwUqf+LtTjjq7+4WpaWlsXTpUqpUqYKXlxcALi4uLF68mICAAI4ePcqwYcNwcXHh9ddfB+Dpp5+mYcOGfPbZZ2g0Gg4dOoROpwPg/PnzdO7cmXfffZeFCxcSFxfHiBEjGDFiBIsWLbqrGIUQQgghxP0jbv584j+dk2Pb9efeL71UEiHdkRJNnNLT06lfvz6DBw+mV69ety1/8eJFunXrxgsvvMCyZcvYsmULQ4cOxd/fn06dOhVDxKXbL7/8grOzM2B9bf39/fnll19s6x1NnDjRVjYkJITXXnuN77//3pY4Xb58mbFjx1KjRg0Aqlatais/bdo0nn76aV5++WXbvk8//ZQHHniAzz77TBagFUIIIYQQ+corabqurCRPJZo4denShS5duhS4/Oeff05oaCgff/wxADVr1mT79u188sknRZY4Oeg0nHi7YHXvuZjAwEV7b1tu8aCmNAv1BMBisZCakoqLq0uuBV0ddJo7ivXBBx/ks88+AyAxMZH58+fTpUsX9uzZQ6VKlVixYgWffvop58+fJy0tDZPJhKurq+34MWPGMHToUJYsWUKHDh144oknCAsLA6zd+I4cOcKyZcts5RVFwWKxcPHiRWrWrHlHsQohhBBCiPvDrZKm68pC8lSmZtXbuXMnHTp0yLGtU6dOtlaQvGRnZ5OdnW17npKSAoDRaMwxvuf6tuvJgMVisW231xZsKFjrMC/8XO2JTcnKc5yTCvBzs6d1mJdtanJFUWHSa3DQaVCpck5XrihKgcc5KYpiG7t03ZdffomHhwdffvklXbt25emnn2bKlCk88sgjuLm5sWLFCmbOnGm71rfeeounnnqKDRs2sHHjRiZPnsx3331Hz549SUtL43//+x8jR47Mde6KFSvmeL2Ki8ViQVEUjEYjGs2dJZlF6frv1X9/v0TZJfe0fJL7Wv7IPS2f5L6WbQmff0HCvHkFKhv/6RwsZgueLzxfxFHdcCe/V2UqcYqJicHX1zfHNl9fX1JSUsjMzMTBwSHXMdOmTWPq1Km5tv/22284Ojrm2KbVavHz8yMtLQ2DwXBXMY59OITX1p5CBTmSp+sp0WsPhZCelprruNTU3NvuhNFoxGQy2RJDsCYWarWa5ORk/vzzT4KDgxkxYoRt/7lz51AUJccxfn5+DB48mMGDBzNkyBC+/vprHn74YerUqcPRo0fx8fHJde6srCyysrJybS9qBoOBzMxM/v77b0wmU7Gf/3Y2b95c0iGIQib3tHyS+1r+yD0tn+S+Fi9FUTBhQkFBr9IDYFJMXDJdwoiRbCUbg2LAqBjJxvrYX+NPfX19ALKVbL5J+4YP513gTlYyvTZvHrsqBhfBFeUtIyOjwGXLVOJ0N8aPH8+YMWNsz1NSUggODuaRRx7J0U0NrAlAREQEzs7Odz1mp2dTVxwcHHj7l5PEpNxIJvzc7JnUrSad6/jlKK8oCqmpqbi4uORqcboTOp0Os9lsu/mJiYnMmzePtLQ0evXqRUpKCleuXGHDhg00bdqUDRs2sH79elQqFa6urmRmZvL666/Tu3dvQkNDuXLlCocPH6ZXr164uroyYcIEWrVqxYQJExgyZAhOTk6cOHGC33//nTlzbt30WlSysrJwcHCgXbt2pWqMldFoZPPmzXTs2NE2uYYo2+Selk9yX8sfuaflk9zXW1MUBYPFgKIo2Gutn4cyTZkcjT9KpimTDFMGGaYMMk2ZZBqtz+t716d9UHsArmVeY+z2sdb9/5bPNGaSac7Eolh4ouoTjG86HoCk7CSmrJ6SbyxdQrrQtVVXAIxmI++seIcf2qrpu63gPZO8hg+nateud/di3IWbGxBup0wlTn5+fsTGxubYFhsbi6ura56tTWCdotvOzi7Xdp1Ol+vNZzabUalUqNXqXOON7kTXegF0quPPnosJXE3NwsfFnmahnrbueTe73sXt+nnvlkql4tdffyUwMBCwzqBXo0YNVq5cyUMPPQTAK6+8wqhRo8jOzqZbt25MmjSJKVOmoFar0el0JCQkMHDgQGJjY6lQoQK9evXi7bffRq1W06BBA/766y8mTJjAAw88gKIohIWF0bdv33uK+16o1WpUKlWe97I0KK1xibsn97R8kvta/sg9LZ/K+n1VFAWjxdotTK+xtuCkG9M5Hn/cmtwYM24kOv8+bubXjFaBrQCISIlg4o6JNxIc442EyKyYGVJnCC83fhmAmMwYXvjjhXxjecryFB1DOwKgM+k4FHco37KZ5kzb6+6mdqOKexUcdY44ah1x0DrkeFzbq7atrE6n45P2n+DYwRHjdxvRLVh129eowqiRxT7G6U5+p8pU4tSyZUs2bNiQY9vmzZtp2bJlCUWUP41aRcswr2I73+LFi1m8ePEty8yYMYMZM2bk2HZ9fJher2f58uW3PL5p06b89lvBplIXQgghhCirjGZrgqPTWD9UpxhSOJ1wOs/kJsOUQZvANjT2bQzAmcQzvL/7/VzJTaYxE5Ni4uVGLzOk7hAA/kn5hyG/Dck3DrVKbUucLFg4cPVAvmUzTDe6nDnpnawJzr8JjYPOIUei08inka2sq50rM9vPtO7TOuKoc8zx2F5zo1ePXqNnbfe1BX4dO1Syzk1gGlKD08t/QZ2R/9COjIHdS/XEEFDCiVNaWhrnzp2zPb948SKHDh3C09OTihUrMn78eCIjI/n2228BeOGFF5g7dy6vv/46gwcP5o8//uCHH35g/fr1JXUJQgghhBCihFxvwdGprQlOYlYi55LO5Uparj/uULEDtSvUBuBw3GFm7Z+VKwm6nuBMajGJJ6s/CcDJaycZ+tvQfONw0bvYEieD2cD+2P35lr05wXHRuxDmFpar5eZ68tLAp4GtrLeDNx898FGeyc31467ztPcscIKjU+voWKljgcreLY2HB5+8VpnWK07S4nTuic9WtFVzqs4llivKPQ1dKWolmjjt27ePBx980Pb8+likAQMGsHjxYqKjo7l8+bJtf2hoKOvXr+eVV15h9uzZBAUF8fXXX8saTkIIIYQQpZjJYkKFCo3aOgvutcxrXEy+eGP8zb9jb64nOV1Du1LFowoAu2N2syBtAd9t+o4sc1aORMdoMTKj3Qy6hFqXt9kfu59Xtr6SbxwBzgG2xCnDmMG+2H35ls0w3khw3O3cqexWOc+kxUHrQC2vWraywS7BtgTn5iTIUetoa/m5uey6HusK9Bo66hzpFFK2PvMqZjMqjQajxcg5zTV299LQe7slx5inFW3VrG6jxis9BqPFaOvGWBqVaOLUvn37W063nVfXs/bt23Pw4MEijEoIIYQQ4v5ktvw73ltlHb8clxHH5dTL+XZR61GlB8Eu1hnQtkZsZcmJJXm29hgsBuY/PJ+2QW0B+PvK37wV/la+cVTzqGZLnFINqVw0XYSEvMv+N8EJdQvNN8Gp4l4lxzk+fODDPMfqXH98XXXP6vzY48cCvYZudm5lLsEpCua0NC4/NwDPQYNwe+xRvq82iISNr0IoKKmOcMgRGmTwZGgGT0aCZ/fJpTppgjI2xkkIIYQQQoBFsX5jfz3BiU2PJTItMs/kJsOYQd/qffF29AZg48WNrD67OlcrT4Ypg2xzNku6LLF1Edt4cSMf7vsw3zga+TSyJU4JWQnsidmTb9mbu6h5OXgR4hqSZ3LjqHW01QlQx6sOfR370rJpS1ztXXO15DjpnGxlm/g14acePxXoNfRy8KJzSOcClRV3RjGbiXr1NbJOnODqhx/i8mB7/H6bjJ/h3zWTaiRbfwAMAGrYMRdq9QbpqieEEEIIUbR2x+xmdspsvGK8aBPcpqTDAawJjgqVbdxGdFo0MRkxeY6/yTBm8GytZ3GzcwNg7dm1bLi4IUe56/9mmbNY130dYe5hAKw+u5rPDn+Wbxxtg9raEqerGVfZHb0737J5JTgOWodc3c4cdY74ON5Y37Gpb1Omt52e74xrznpnW9l2Qe1oF9SuQK+hn5MfdfV1aRfYrkzPqnc/ifvkE9L++guVnR1B8+aivrARMhNvcYQFUiLBbABt7tmwSwtJnIQQQghR5imKwpxDc4izxDHn0BxaB7W+o0Hm/01wItMiicuIy9Vycz2JGVxnsG3NnO9OfsefEX/mau25/vN7n9/xdfIF4NsT37L05NJ84+hauastcYpMi2RX9K58y2aaMm2PKzhUoJJrpVyzqF1/7mHnYSvbJrANFRwq5Bhzc/NjF72LrWy3yt3oVrlbgV7DYNdggl2Lb+FSUTol//gj175eAID/u+/iEOID8x+37mz4DDQdhtFkYseOHbRu3Rqd9t90xMm7VCdNIImTEEIIIcqB8KhwTiScAOBEwgmm7pyKr6MvL9R/wZYMLTq2iF3Ru3K08NhmUjNlsrv/bhx11nEtnx36jB/P5z+m5YlqT9gSp8upl+8owanoUjHPLmrXn1/XoVIHQt1C8x1/c3OC82T1J20zwN1OmHuYraVKiMKUefgw0ZOsY9e8/vc/3B7tBsv7QWYC+NaFbp+AVg9GI8mOkeBfH8pQK6IkTkIIIYQo0xRFYdaBWTm2rT67GoBBdQbZEpxzSecIjwrPt55MU6YtcfJx9CHYJTjftW2uT38N0C20G7W9aufbRc3dzt1WdkjdIbY1fG6nhmcNanjWKFBZIUqa6do1IkaMQDEYcH7oIbxfHg0Hl8KZjaDRQ68vrElTGSaJkxBCCCHKtPCocE4lnMq1vW1gW9skCgA9q/SkhX+LfKeJ9rC/0Z1tVKNRjGo0qkDnr+tdl7rede/9QoQowzQeHrj37EXan38SMGMGKlMmbJ5k3fngBPC1TgNvtijsvpjA/ngVXhcTaFnFB4269E4IcTNJnIQQQghRZimKwju73sm1Xa1Sk5CVkGNR0CZ+TYozNCHuKyq1Gp8xr1DhxRdQO/z7vnt2Hez9GlqNBGDTsWim/nyC6OQsQMO3Z/fh72bP5Mdq0bmOf4nFXlDqkg6gXDv/J8xtZv23iA0cOBCVyjqoVafT4evrS8eOHVm4cCEWiyVX+U6dOqHRaNi7d2++dX3wwQc5tq9bty7HQNutW7eiUqmoXbs2ZrM5R1l3d/c81+ESQgghClN4VDiRaZG5tlsUC8evHb9l1zwhxL1L/eNPLNnZtue2pAkgoAF0nwtqDZuORfPi0gP/Jk03xCRn8eLSA2w6Fl1MEd89SZyKiqLAlqkQf9r67y0W+i0snTt3Jjo6mkuXLrFx40YefPBBRo8ezaOPPorJZLKVu3z5MuHh4YwYMYKFCxfmWZe9vT3Tp08nMfFWU0daXbhwgW+//bbQrkMIIYQoCEVRmHNwDiry7uajQsWcg3NQiuFvsBD3o9QtW7jy0kv889xzWDL/nQQl7gzEHM1RzmxRmPrzCfJ6J17fNvXnE5gtpfu9KolTQRnS8/8xZuUue3oDRB20Po86aH1uSAdjZu6yxozcdd4FOzs7/Pz8CAwMpFGjRrz55pv8+OOPbNy4MUfrz6JFi3j00Ud58cUXWb58OZmZmbnq6tChA35+fkybNu225x05ciSTJ08m+6ZvG4QQQoiiZjAbiEmPQcnz4xgoKMSkx2C0GIs5MiHKv6zTZ4gc+zoADnXqWluazEZYMxS+fBCOr7WV3XMxgbTEbHxMqjx/vE0qUhOz2HMxoaQup0BkjFNBvR+Q/76qj8DTK288nxEGpv8kI9/3t/5bqQ0MWm/brPq0Pu4Z13LXOSX5HoK94aGHHqJ+/fqsWbOGoUOHoigKixYtYt68edSoUYMqVaqwatUqnn322RzHaTQa3n//ffr378+oUaMICgrK9xwvv/wyS5cuZc6cObz22muFErcQQghxK0fjjjJj7ww+euAj20x4JpOJHdt30LpNa7T/rg3jae+JXlO2Z/ISorQxJSRw5aWXUDIycGzRAt9xb1h3/P0hRB8GBw+o2NJWPjYpg2dT7XBS8p8EIk2lEJuUAXgVcfR3T1qcioKSe0xRSapRowaXLl0C4PfffycjI4NOnToB8Mwzz7BgwYI8j+vZsycNGjRg8uTJt6zf0dGRyZMnM23aNJKTCyfhE0IIIfKTYcxg3LZxHIo7xNpza6nlVYtaXrWo6VmTAG0ANT1r2rb5OfmVdLhClCuKwUDkqNEYIyPRVaxI0KxPUOl0ELkf/v7IWqjbTHC58d7zcXMgRa1gyad12IJCqlrBx80hz/2lhbQ4FdSbUfnvU2luPFYU8KkBMcdAMecs41cnZ8sUoIw6THJqKq4uLqjVRZPHKopim9Rh4cKF9O3b1/ZNXL9+/Rg7diznz58nLCz3YnjTp0/noYceum1L0pAhQ/j444+ZPn0677//fuFfhBBCCPGv6Xunczn1Mn5Ofrze9PWSDkeI+4aiKMS8+x4Z+/ahdnIieP48NO7u1qEoa563fvat0wfq9MpxXLbJwg57I33S7fKsV42KE14wtXLpbW0CaXEqOL1T/j86+xvlzm+xNlEqOWeZQzFbt18Oz12vzjF3nYXo5MmThIaGkpCQwNq1a5k/fz5arRatVktgYCAmkynfSSLatWtHp06dGD9+/C3PodVqee+995g9ezZRUbdIMoUQQoh78Ps/v7Pm7BpUqHi/zfu42bmVdEhC3DdMV6+S+uuvoFIR8PFH2FWpYt3x+1S4dhac/aDrhzmO+WFfBEO/3cdFrYVYtSVXq5MFhWiNhWFP1Cz16zlJ4lSYFAX+eJf8X1a1dX8xzu7zxx9/cPToUXr37s2yZcsICgri8OHDHDp0yPbz8ccfs3jx4lxTil/3wQcf8PPPP7Nz585bnuuJJ56gdu3aTJ06tSguRQghxH0uNj2WKTunADC4zmCa+jUt2YCEuM/ofH0JWfkD/u++i0v79taNV/bD7s+sj7vPA0dPwNo6NeuXk7z9wxHMFoUB9qcI8DyG+j+zYKpRERp0GrdTRb98z72SrnqFyWyA5EggvzFOFkiJtJbT5t1UeS+ys7OJiYnBbDYTGxvLpk2bmDZtGo8++ijPPfccjRs3pk+fPtSpUyfHccHBwYwfP55NmzbRrVu3XPXWrVuXp59+mk8//fS2MXzwwQe28VNCCCFEYbEoFibsmEBydjK1vGoxvMHwkg5JiPvGzcM+9BUroq9Y8cbOgIbwyHuQfIVM/3ZEHbjK5VMJHNofiz7dTHO9jhrdKtImNYXwlcvQO2Wi1je3HqsCvf4ASYe2oqr2dAlc2Z2RxKkwae3gf39Cenz+ZZy8iyRpAti0aRP+/v5otVo8PDyoX78+n376KQMGDODgwYMcPnyYr776Ktdxbm5uPPzwwyxYsCDPxAng7bffZsWKFbeN4aGHHuKhhx7it99+u+frEUIIIa5LyU4h3ZCOg9aBD9p+gE6jK+mQhLgvmFNTiRj2PyoMfwnntm1z77dAeMQjRJ5J5NqP223br89l2TzAjf91qgHUABWE/7AMjdmCzqElpoxdZCWE0+rJp2nZu1/xXNA9kMSpsLkFWX+K2eLFi3Os1fRfjRs3vuUCgBs2bMhR13+FhITkWqepffv2edb566+/3j5gIYQQ4g6427vzbddvOXXtFKFuoSUdjhD3BcVsJvK118g8dIjoyZMJWvMzMZczyUw1UKtyPHhVQaN35OLheFITrOuaptrBWcVEjL3C6Kfq8EijQFt9LXv3I+6fi5zdHY45aw9gptUTZSNpAkmchBBCCFGK3dxFSKfWUde7bglHJMT9I/KjWfxz9BpJ1fqQ0aAjm8fvRlHAzkFNDb+hqB3d4JnVNH88lLgMA2/tPM+F1EwqOGqY+VAF7CN2svrXozR4pCthja3d8xp1eZyzu8MBM2qNlpZ9ykbSBJI4CSGEEKIUm753Og5aB16q/5J0zxOiGP323ibOXa6HUq+hdcNVEwBuPg4Eag5gTE/HzsUHXPxJqpDC5K9+wyP5Mn3NsfhHRbP3+I2eSu6+frbE6fKxw9aNajUWs4mdq5dLi5MQQgghxL34+8rfLDu5DIDWAa1p4tekhCMSonwxGszEnE8m8kwiUWeS6PpSPeyddGQeOoRpxx8oFTvjpMumYrMQAqt5EFjNA+d/VmNeO4EsHLDr9QXrTyQwadnfPH3pxlqlZsDeyZmgWnUIrlWXSvUaAbBz9XJ2rlpOi979iLdzpkJ2GuE/WN/jZSF5ksRJCCGEEKXOtcxrTNoxCYBnaj4jSZMQhcBkNBNzIYXIM4lEnk4k9lIKFtON8epRZ5MIDlCIGDmSgGQDYaEK1WZPx2IxE3P+HMc3/UzElqVEpbUkuJIv18468e76A6ByJsvNn1pVK1GpTj2Ca9WlQnAlVOobS/TsXL2c8B+W0erJp2nyeB82bNhAs55Potaoy0zyJImTEEIIIUoVRVF4K/wtErISqOJehZcbv1zSIQlRJplNFixmBZ2dBoAzu2P5c+mpHGWc3O0IrO5OYDUPfENd0TqocGn/INrDh7n2UGNWT5tM5OkTmGyThLkAcC7ewrxfToBKxcBWIYx79MtbLmCrWCy22fOMRqNt+/VkSbHkt5xP6SGJkxBCCCFKlR9O/8DfV/5Gr9Yzvd107DRFs4yHEOWN2Wzh6qVUW4tSzPlkWvQMo/5DwQAEVnfH0VVPYHUPAqu5E1jdAzdvB8wmEzHnz3BuzyHqd+yK39tTsaSns/ejd7ly8hgADg56grRRBLlk8GPIGL6JqgAqFRO61mRo21DbJC75afVE/us0lfaWpuskcRJCCCFEqXEh6QIf7vsQgJcbv0w1j2olHJEQpVt2ponjf0cSeTqRqPPJmLLNOfbH/ZNqe+xawYGB01tbE6Vzpzm1bRsRJ44SdeYUJkM2qFRUa9kWB2cXNM7ONOjUjWot2xBcqy5eLhpM60aw6Fotvon2Rq9V8/GT9XmsfkBxX3KJkcRJCCGEEKXGpZRLqFVqWgW04uma+X9DLcT9yGJRiI9IxZhlJrC6BwBqtYrdP13AYraOVbJ30hFQzdr1LrC6O57+TrbjVSoVu9f+wK7V32MyGnLUrTea8XFwIjspCQdna3e86i1vLHh7+VoGA+NGc+FaOq72Wr56rgnNK3sV9SWXKpI4CSGEEKLUeKjiQ6x6bBUOWgfUKvXtDxCiHFMsCvGRaUSeTiTyTBJRZ5MwZJrwrujCk282BUBnp6FBh4q2LnheAU6YTUaiz53m9I6/uXLyGA8O/B/eFUMAcHR3x2Q04OjmTlCtuvh7+aDM/Ryn5BQ8n34a96DgnEEkXOBIhieDF+8lPs1AoLsjiwc1paqvSzG/GiVPEichhBBClLibF7qt6FqxhKMRouRtXXaKcweukp1uyrFdb6/B2cMOi0VB/e9kDE27BRN99hRndm7jyomjRJ09hfmmCRgijh+xJU5VmrYkoFpNPAOCMCcmcqnPExiTU3Bs2QLf8eNyBhF5AMvXHTljbkuSYRC1/D1ZNKgpvq72RXrtpZV8lVOEdkbtpPu67uyM2lks54uLi+PFF1+kYsWK2NnZ4efnR6dOndixYwdgbZ5dt25dscQihBBCFFRydjLPbHyGPdF7SjoUIYqVoigkRKVzdOsVtnxzAkW5MTW4IdNEdroJnZ2GirW9aNkrjCfGN2HIx23pOKQ62RlptrKRp07ww9tvsnPVd0ScOIrZaMTRzZ3qLdvSYehLVGnSwlbWwdkFr8BgMBq5MmoUxqgodJUqEvTJJ6i0N7WpGDNJ/m4IasWEvZJBq6p+/PBCy/s2aQJpcSoyiqIw+8BsLiRfYPaB2bTwb3Hb2UbuVe/evTEYDHzzzTdUrlyZ2NhYtmzZwrVr1wr1PAaDAb1eX6h1CiGEuD8pisLUnVM5EneEd3e/y5rH16BVy8cTUT4pikJSbAaRZ5KsM9+dSSIz5cZYowYdK+IV4AxAw06VqPdQMN6VXLCYjUSfOcXZ3dv4c9Exos+eokHnx2j/7BAAAqrVwKWCNwHVahJcqy7Btevi4R+Y72dPRVGIeeddMvftR+3sTPD8+Wjc3XPs37/wFZqkX+Cq4s7eWhNY8GRTdJr7u81F/mcqoAxjRr77NGpNjqlSM4wZ7IraxfFrxwE4fu04f17+kxYBLVCr1Nhr7XOUzTRlojVqUd+0SJijzvGO4ktKSmLbtm1s3bqVBx54AIBKlSrRrFkzAEJCQgDo2bOnbd+lS5c4f/48Y8aMYdeuXaSnp1OzZk2mTZtGhw4dbHWHhIQwZMgQzp49y7p16+jVqxeLFy++o/iEEEKIvKw7t47N/2xGq9Iyrc00SZpEuXK9Bel6ArNvwyX2/HwxRxmNTo1/mBuB1dyxc9DZtnv42bN77Qq2fnOUmHOnMZtydtm7FvGP7bHO3p7/zVtU4LiMEREk//ILqFQEfvwRdmFhtn0Gk4Wvvv2G4dHLAQivPZkpT7Qr8gaAskD+dyqg5t81z3df28C2zO8w3/b8gRUPkGXOylFm9NbRADTxbcKizjd+sbuu7UpidmKuOo8OOHpH8Tk7O+Ps7My6deto0aIFdnY517zYu3cvPj4+LFq0iM6dO6PRWBdCS0tLo2vXrrz33nvY2dnx7bff8thjj3H69GkqVrzRx/yjjz7irbfeYvLkyXcUlxBCCJGfyymX+WDPBwAMbzic2hVql3BEQty7lPjMf9dRsrYqtX+mBpVqW2ef8wlxRa1V4RfqRmB1D4Kqu+Mb4obFYiDqzCmunLhMjdbWL8A1Wi1H//iN9MQEAJw9PAn6tzUpuFZd3P3ufhpwfcWKhHy3jMwjR3H+9wt3gJQsI2O+/Zspke+CCs4H96HHk4Pv4dUoXyRxKgIWpfhXPtZqtSxevJhhw4bx+eef06hRIx544AGeeuop6tWrh7e3NwDu7u74+fnZjqtfvz7169e3PX/nnXdYu3YtP/30EyNGjLBtf+ihh3j11VeL74KEEEKUa0aLkfHbxpNhyqCJbxMG1R5U0iEJcVeyM4xcPBJv7X53OpHUazm/PI88nWhLnIJqeDBsZjsUxUjU6VNcPLCdv5ceJfrsGSxmEw6ublRvZW3dUalUNO/5JBqtjuDadXH39S/UVh/7mjWxr1nT9jwmOYuBi/YwKP5TgrTxZDoFEfbMrEI7X3kgiVMB7e6/O999GrXG9lhRFMLcwzideDpHAqVWqanuUZ35D8/PceyGnhtITU3FxcUlR1e9u9G7d2+6devGtm3b2LVrFxs3bmTGjBl8/fXXDBw4MM9j0tLSmDJlCuvXryc6OhqTyURmZiaXL1/OUa5Jkyb3FJsQQghxsy+PfMmR+CO46Fx4v837Of6WClGapSdnYzKYcfO2DqtIS8pmy+KTtv1qtQqfEBfrOkrVPPALc7Pt02jU/L5oHkf/2IzFnLPrnbNXBYJr1cWYlYnewVp3w06PFlrc5tRUrowYiffLo3Fs2DDHvtMxqQxctIfo5Cz2ODWnl/44Dk9+BXb335TjtyKJUwEVdMxReFQ4JxNO5tpuUSycTDjJgasHaB3YOke9Jq0JR53jPSdOAPb29nTs2JGOHTsyadIkhg4dyuTJk/NNnF577TU2b97MRx99RJUqVXBwcKBPnz4YDDkXRXNycsrzeCGEEOJOKYrCP8nW8RlvtXwLf2f/Eo5IiPxlpBhsEzlEnk4kKTaDqk18eGRoHQA8/Z2oWMsTryBnAqt74B/mBhiJOn2SS4e3s/374/QaNwU7R+tnSb2jExazCRcvb4Jr1SGodl2Ca9XDzce3yMYRKWYzka++Ssbu3US9MY6w9b+g0lnHU4Wfj+f5JftJzTJRxceZlwe+jM7lFdA5FEksZZkkToVIURTmHJyDChUKSq79KlTMOTiHVgGtim2AXa1atWxTkOt0Osxmc479O3bsYODAgbZJI9LS0rh06VKxxCaEEOL+pFKpmN5uOk9Wf5ImftKjQZQ+iqKw/YezRJxKJDE6PedOFWRn3vg8pVKp6PS/6kSdOsHlIzvY8f1RYs+fxXLTZ66o0ycIbWj9XW/Y6VHqd+iMq3fRJUr/dfXjmaT/vQ2VvT2BM2fakqYfD0Xy2srDGM0WHqyo55OBLXF3lJmT8yOJUyEyWozEpMfkmTQBKCjEpMdgtBjRawr3l/LatWs88cQTDB48mHr16uHi4sK+ffuYMWMG3bt3B6yz423ZsoXWrVtjZ2eHh4cHVatWZc2aNTz22GOoVComTZqExVL8Y7SEEELcH64vdKtSqSRpEqVCVrqRqDNJpFzLpEEH68RYKpWK6PPJtqTJK8iZoGoeBFRzJ6CqO2q1EUNmhq1L3Ym//2TLgpzDMVy9fa0tSrXq4lu5im27i1eFYroyq6S160hYuBCAgPffw6FObRRF4bO/zjNj02kApoQcZ0DyZ6gufgK1exZrfGWJJE6FSK/R8/2j35OQlZBvGU97z0JPmsA6q17z5s355JNPOH/+PEajkeDgYIYNG8abb74JwMcff8yYMWP46quvCAwM5NKlS8ycOZPBgwfTqlUrKlSowBtvvEFKSkqhxyeEEEJsvLiRzf9s5q0Wb+Fu717S4Yj7VHamiaiz1m53kWcSib+SBop1bFKtNgHo7a0fj5t0DUFRFAKreqDWGIk8dYKI49vZtfIYMRfO8vDgF6jfsSsAwbXq/psoWWe9C6pZBzcf35K8TAAyDh4k5q23APB68QVcu3bFZLYw5efjLN1lHc8+ppkjA07PRZWdAvHnSjLcUk8Sp0Lm5+SHn5Pf7QsWMjs7O6ZNm8a0adPyLfPYY4/x2GOP5dgWEhLCH3/8kWPb8OHDczyXrntCCCHuVXRaNO/sfIdUYyq1vWozpO6Qkg5J3IfCV5/j0O+XUf7TOcjDz5HAah6YDBb0/y636VdZz76f17Br1VFiL5xD+U+PnLjLN9ZR8gwMYtjcBUUd/h0xxsRwZeQoFKMRl44d8B45kgyDiVHLD/L7yauoVPBWtxoMujAGslMgsDG0eaWkwy7VJHESQgghRJEyW8y8uf1NUo2p1KtQjwG1B5R0SKIcM2abiT6XZJ3M4UwiHQbVwt3H2qXO2dMORQE3HwfrOkr/dr/T6kxEnjpB7PkjtrFIGq2OfT+vRfl3lmQ3H1/rOkr//rh6+9jOWRoXh9W4uODYsCGGy5cJ+OADrmUYGfLNPg5HJGGnVTP7qQZ0zvgFLmwFrQP0/AI0khrcirw6QgghhChSi44vYl/sPhy1jnzQ9gO0avn4IQqPyWAm+nyybdHZq5dSsFhuNClFnk60JU7VmvlRuYEPOjsTV04eJ+LEDnavOcbVi+dRFAv+1WrYEic7R0da9H4Kd18/gmrVwbWCT57nL63UTk4Ezp6FOTmZfzJh4KJw/rmWgbujjgUDmtDYKQE+n2Qt3PFtqFC1ZAMuA+R/LiGEEEIUmePxx5l3cB4A45qNI9g1uIQjEmWdyWjGbLRg52idGe7yiQQ2fn40RxkXT3sCq7sTWM2D4Fqetu32TjpWz36Lf44csrUkXefu649PpVDbBCYArZ7oX8RXU/gyDhzAoWFD6yQsajVHUmHoN+EkpBsI9nRg8aBmhHnaw8InwJQJoQ9A06ElHXaZIImTEEIIIYpEhjGDcdvGYVJMdKzUkR5VepR0SKIMMpssZCdo2L/xH2LOpxBzPoWGj1Sk+eOVAQio6o6Lpz0BVd0JqOZOUHUP9PZmrpw6zpUTOzi+9Qq9xk2xJUM6e3sUxYK7n7+t211QrbrFPttdUUjZvJnIkaNw7dqVgA9nsPlUHCOXHyTbZKFekBsLBjTF28UOjFkQ0ACunYUe86EQ1hK9H0jiJIQQQogiEZ0eTZY5Cx9HHya3nFwqx4GI0slkMHNoSwSRpxOJPp+M2ehIHJdt++OvpNke2zvpePLNelw5eYyIEzvY/9Mxrv5zgZtngEiKjcbDLwCANk8N4MGB/8PFs+wnSjfLOn2aqDfGAaDx8mLJnggm/3QcRYGHavgwt39DHPX/fvTX2UO3j+GBceDsXYJRly2SOAkhhBCiSIS5h7HqsVVEp0fjZudW0uGIUspitnD1cipZaUZC6lqTGY1WzaHNl8nOMAGg1lsIqe1DcE0vAqu5Y+dkxmwyotFau+vtWrOc/et/zFGvR0AQwTXrEFS7Lo6u7rbtngGBxXNhxch07RpXXnwJJSMDx5YtWVz/cT7/8TgA/ZpV5J3utdFq1GA2gkpzo4VJkqY7IomTEEIIIQrVzWNE3OzcJGkSOVgsCvERqUSets56F3UuCWOWGWdPOyrV8fp3bI6KRp0qodVr8A1zZvvOXwmtaE/06V0c+OUocZcv8cTEd6lYpz4AQbXqcfHgfoJq1bF1vXP28LxNJOWDYjBwZfRojFFRaCtWZE7bQazabp0qfWyn6rzUPuxGa++WtyHqoLV7nnvFEoy6bJLESQghhBCFxqJYeHXrq7QNakvPKj2le57IYfsPZzm5MxpDpinHdjtHLd7BLpiMFnR6DQCh9XUc3PQzBzccJf7yJS79p664fy7aEqewxs2o0qR5MVxB6aIoCjHvvEPmvv2onJ2Z9eDzrD+TglatYkafevRqFHSj8D/hED4HUCDmmCROd0ESpyIUN38+8XPmUmHkCLxfeqmkwxFCCCGK3LKTy/j98u9si9xGS/+W+Dv7l3RIopgpFoWE6HSunE4k+lwSHQbVQquzJkMWRcGQaUJvryGgqjuB1T0IrOaBo6uZqNMniLt0loBqNQAwZmVxcOPPtno9A4OtkznUrktQzTo4uXvY9t2vCbrh3DmS1v0IajXz2wxkfbIdLnZaPn+2Ma2r3DSGKzsV1r4AKNDgGajRtcRiLsskcSoicfPnE//pHADbv5I8CSGEKM/OJJ5h1v5ZALzW5DVJmu4TiqKQGJ3x7zpKiUSeTSIrzWjbX7d9CoHVrElO3QcCqd7cDyc3M1FnThBxfCeHNx0lPsLataxWu4dsiZN3xRAade2Ob1hVTl6J5vHefdDpdMV/gaWYXdWqWD6cw5Lv/uAn+xD8XO1ZNKgpNf1dcxb8dQIk/QNuFaHztJIJthyQuQeLwM1J03Xxn84hbv78IjvnwIEDUalUvPDCC7n2DR8+HJVKxcCBA23bYmJiGDlyJJUrV8bOzo7g4GAee+wxtmzZYisTEhJi7WesUuHg4EBISAhPPvkkf/zxR5FdhxBCiLIp25zNG3+/gcFioF1QO/pW71vSIYkioihKjgVmj/x5heVv7+bv789w/mAcWWlGtHo1wbU8adGjMi5e9rayLl56Ns17ky9eeJafZ07j0K+/2JKmCsGVcPe9kWyr1GoeHDCMqs1bo3VwLL4LLEP+PhPHk+GZfB/QjOq+Lqwd3ip30nTmNzjwjfVxj/lg75q7IlEg0uJUyPJKmq4r6pan4OBgvv/+ez755BMcHBwAyMrK4rvvvqNixRv9WC9dukTr1q1xd3fnww8/pG7duhiNRn799VeGDx/OqVOnbGXffvtthg0bhsFg4NKlSyxdupQOHTrwzjvvMGHChCK5DiGEEGXPrP2zOJd0Dk97T95u9fZ923WqPFIUhZT4TCJPJ3HldCJRZxJp2TOM6i2sSY5/mBsanRr/MDcCq3kQWM0dZw+LtUXpxE52XjLQ6YXRAGhvajGqUDGEoJp1bF3vHF1lEpGCMKemEvXaWA52eZoxe1IxWRRaVvbi82cb4+bwnxa5jAT4aYT1cYuXILRt8QdcjkjiVECWjIz8d2o0qO3sbpk0Xfff5MmSkYElMxOLVptj8TG1451/s9KoUSPOnz/PmjVrePrppwFYs2YNFStWJDQ01FbupZdeQqVSsWfPHpycnGzba9euzeDBg3PU6eLigp+fHwAVK1akXbt2+Pv789Zbb9GnTx+qV69+x3EKIYQoX3ZE7mDpyaUAvNP6HbwcvEo4InGvDJkmzh+8SuSZJCJPJ5KWmJ1jf+SZJFvi5B3sQv/JdYk+a+16d2zLMa5dubHmkkan4+HBL6LV6wHoMnwMzp5ekijdBcVsJnLMGNK3bUd/4BTmh16lR8MgZvSpj16bR0eyjARw8AR7N3j4reIPuJyRxKmATjdqnO8+pwfa4VC//m2TputuTp4udHwEc2Iisf8pU/PUybuKc/DgwSxatMiWOC1cuJBBgwaxdetWABISEti0aRPvvfdejqTpOnd399ueY/To0bzzzjv8+OOPvP7663cVpxBCiPLjXNI5VKjoW70v7YLalXQ44i6kJWaRnWHCK9AZAJPRwh/f3uiBotao8A11tbYoVffA1dNi26dSq9iyYC4XDuzNUad3xRCCatcluGZduKkF0iekchFfTfkV8+FHpG/bTpZGx4eN+/Hig1V57ZHqqNX5tPBWqAL/2wppMaBzKNZYyyNJnApJ/Jy5d1y+KLrsPfPMM4wfP55//rH2F96xYwfff/+9LXE6d+4ciqJQo0aNuz6Hp6cnPj4+XLp0qRAiFkIIUdYNqD2A+t71qeF5939bRPFKT862TuRwOpHIM0kkx2USVMOD7i83BMDRVU+1Zr64eNoTWM0DFy8zMeesXe+O/3mMxKgrDJu3ENcKPgAE165H6rX4f9dQqkNQzTo4uMhYmsIUu3I1SYsXA/BJo74MHNiZZ1tUyruwotxIVnX24BFSLDGWd5I4FVD1A/vz36nRcG3BggK3OAFUGGntb1p582+kpKbi6uKCWn3vc3V4e3vTrVs3Fi9ejKIodOvWjQoVbkxHqSjKLY4uuJsXNxRCCCEa+DQo6RBEAexYfY5LR+JJis05BEGlAotZyfH3ve4Deo5u2cjmv6yJ0n8PuHrxgi1xatytB00e7Vks13A/ity+i4TJU9ACP9TsyLPjh9Kxlm/ehRUFlveD4KbQajRo5ON+YZFXsoBuN+boeutRQZKnCqNG2sqrHR1Rm0zWfwshcQJrd70RI6yJ2bx583Lsq1q1KiqVKscEEHfq2rVrxMXF5Rg3JYQQ4v5yMfkiU3dOZXLLyYS6yd+D0iYzzUDk6SSuRaXR/LEbXeMSotKsSZPKOjYpsJq7tUWpgoWrF0+SGB2JZ4B10dSUuFiObNlkPVClwqdSZYJr1yGoZl0Ca9bGwdnFVq98mVp0zh49x7WRo3CzmNgbXI/us6bQoJJn/gfsWwhnNsKFP6FWD/AKK7ZYyztJnApRQZKnm5OmotK5c2cMBgMqlYpOnTrl2Ofp6UmnTp2YN28eo0aNyjXOKSkp6bbjnGbPno1araZHjx6FHLkQQoiywGg2Mm7bOE5cO8HMfTOZ83DBe1yIopGVbiTqbNK/Xe8SuRaZbttXp20gTu52ADTsWJHabQNxrWAm7tIpIk7s4o+FR0mMjgKg9ZPP0KL3UwAE1apL427dCapVj6AatbF3di7+C7vP7b2UwMvLjzLCyYcKDq60WziXkOBbJE3XzsNvE62PO0yRpKmQSeJUyG6VPBVH0gSg0Wg4efKk7fF/zZs3j9atW9OsWTPefvtt6tWrh8lkYvPmzXz22We2YwFSU1OJiYnBaDRy8eJFli5dytdff820adOoUqVKkV+LEEKI0mfeoXmcuHYCNzs3JraYWNLh3Pf2rr/Inl8uwn9643sGOBFY3SPHmkuuXiZWvTfRlihdp1Kp8QmtjKObu22bo6sb7Z8bVpShi1tYfySaV344hMGiZ1XfsXz+eFW8g73zP8BihnUvgjEDQtpCs+eLL9j7hCRORSCv5Km4kqbrXF3zH5BZuXJlDhw4wHvvvcerr75KdHQ03t7eNG7cmM8++yxH2bfeeou33noLvV6Pn58fLVq0YMuWLTz44INFfQlCCCFKob0xe1l4bCEAk1tOxtcpn3EWolAZskzWFqV/pwdv168afqHW6bzdfRxBAQ8/xxuz3nmZiI84TcTx3Rz53YvWT1pn23X29CItMfHfRCmMoFp1CK5Vl8AatbB3khal0uLrbRdY/t3vGNwCeaSWL7OfaoiDPveX4TmEfwoRu0HvYl3otpCGgIgbJHEqIrbkac5cKowcUeRJ0+J/Z1nJz7p163I89/f3Z+7cucydm/9sgDJrnhBCiJulGFJ4c/ubKCj0rNKTjpU6lnRI5ZbRYCb6bBKRZxK5cjqJuMupKDe1HEWeTrQlTpXqetHnjepcizhNxIndbP3mKMmxMbayHv4BtsRJrdHwxMR38QwMws4x97IkomRZLArvrj/J6R9+ZO7ebznTqjPd3v0IrfY2SVPMMfjjPevjLtPBvWLRB3sfksSpCHm/9FKxtjIJIYQQRUVRFN7d+S4x6TFUdKnIuGbjSjqkcsVoMGPKNuPgYl0kNv5yKj/POZyjjGsFewKrexBYzQOvwButCXp7Les/fSfHzHcqtRrfylVs04PfPFuef1VZvL40yjKaeWXFIU5v38/HB5YD0DzM+/ZJE8DVE6BSQ/Vu0KB/EUd6/yrxxGnevHl8+OGHxMTEUL9+febMmUOzZs3yLGs0Gpk2bRrffPMNkZGRVK9enenTp9O5c+dijloIIYS4v2SYMohMi0Sj0jCt7TQcdbeebVbcmsloJvZCClfOWNdSir2UQs1WAbTvb01qfEJc8fB3wjfE5d+ud0YSo84ScXwPfy85SmZKMsMXfo9Ga/0oV6lufewdnf5dcLYOAdVrYXebGYFF6ZGYbmDYt/s4e+Yyn+5ZjL3ZiFOrVvi+8XrBKqj3JPjVBUevHIsNi8JVoonTihUrGDNmDJ9//jnNmzdn1qxZdOrUidOnT+Pj45Or/MSJE1m6dClfffUVNWrU4Ndff6Vnz56Eh4fTsGHDErgCIYQQ4v7gpHNicZfFHLp6iHre9Uo6nDLJYlHYv/ESkacTibmQgtlkybE/KebGTHgarZpWPfWc2v4r25YeJSXuao6yKrWahMgIvCtZp4J/aNALMiV4GRWRkMGARXuIiElmxt5v8clIRF+pEoGfzESlvYOP6j41iy5IAZRw4jRz5kyGDRvGoEGDAPj8889Zv349CxcuZNy43F0AlixZwoQJE+jatSsAL774Ir///jsff/wxS5cuLdbYhRBCiPuNTq2jqV/Tkg6jTDCbLVy9lEpaQhZVm1on0FCrVZzaFUNKXCYAjq56Aqt7EFDVDbcKJpJiz5KRkoyjq3XsUuz5sxz/awtgTZT8wqr+2/WuLoHVa6J3uNGiJElT2XTkShKDF+8jPjWLN0/+SI34i6hdXAj6bD4aN7dbH5ydBisHwgNvWBe7FUWuxBIng8HA/v37GT9+vG2bWq2mQ4cO7Ny5M89jsrOzsbe3z7HNwcGB7du353ue7OxssrOzbc9TUlIAa7c/o9GYo6zRaERRFCwWCxZLzm+BioqiKLZ/i+uc5YXFYkFRFIxGY57TrpeU679X//39EmWX3NPySe5rwXx97GvSjGkMrzccnUZX0uHcUkneU4tZIT4ilagzyUSdSybmfDImgwWdvYaKdT1Qa6yJTf2HA7FYFFy9DKRcPUfkqV3s+P4YadfiAeg84lWqtWgDQMV6DWmSnk5gzdr4V6uB3t4hxznvl9/d8vpe3XomjlHfHybTaKGjJoG253eBWo3vjOmog4Nve73qXyeiObcZJf4Mphd2QSl/f96sNN3TO4lBpVz/5F7MoqKiCAwMJDw8nJYtW9q2v/766/z111/s3r071zH9+/fn8OHDrFu3jrCwMLZs2UL37t0xm805kqObTZkyhalTp+ba/t133+H4n76/Wq0WPz8/goOD0ev193iFoqgZDAYiIiKIiYnBZDKVdDhCCFHuRJgi+CrtKyxYeNrpaWrqpCtQXpLP6Em7pEcx52z1UesU7DxNuNfJRqO3ftzKjIsldscWTBnpOStRqbD38sG9Vj2cg0KKKXJRUsJjVay8oMaCihpuFgZVt+B1+gTapGSSW7W87fHeKUdodf4jAHZUGUe8S62iDrncysjIoH///iQnJ99yOR8oBZND3InZs2czbNgwatSogUqlIiwsjEGDBrFw4cJ8jxk/fjxjxoyxPU9JSSE4OJhHHnkk14uTlZVFREQEzs7OuVq2ioqiKKSmpuLi4iLN7HcoKysLBwcH2rVrV2z3qyCMRiObN2+mY8eO6HRl59sfkT+5p+WT3NdbSzem029jPyxY6FKpC6+2frWkQ7qtorynikXhWlS6tUXpbBIP9K9qmwHvgPoy+87/g95Bi38VN/yruuLqmU1awgUiTx0n0K02dR60Tt2eei2eRZt/Qq3R4htWhaCadQisURv/qjXQlaK/ZaVJeXqvKorCrC3nWXHhAgC9Ggbwbvda6DRqeKxrwSrJTET7pXXSCHPT/9HskdeKKtwiU5ru6fXeaAVRYolThQoV0Gg0xMbG5tgeGxuLn59fnsd4e3uzbt06srKyuHbtGgEBAYwbN47KlSvnex47Ozvs7OxybdfpdLlulNlsRqVSoVarURfTomHXu+ddP68oOLVajUqlyvNelgalNS5x9+Selk9yX/P28Z6PuZJ2BX8nfya2mlimXqPCuKeKRSEhOp0rp62z3kWdTSI740bvhqsX06nS2LoOUs1WAVQItJCWcJ7Ik3vYt/YoaQnXbGUNGek0fMT6odjTz5++kz/At3IVSZTuUFl/rxpMFsatPcKaA5EAvNrSn55/LEbX5nV0gYEFr+jH8ZAWA15V0XSciqYMvyal4Z7eyflLLHHS6/U0btyYLVu20KNHD8CaRGzZsoURI0bc8lh7e3sCAwMxGo2sXr2aJ598shgiFkIIIe4Pv176lXXn1qFWqZnWdhqu+lt3XykPFEXBYlbQaK1fYp7ZG8vvi07kKKOz0+BfxZ2Aam64eN7Y7uiq5ZeZb2AyGmzbNFotflWqE1y7LpXqNshRT1CtOkV2HaJ0Ss0y8uLSA2w/F49GreL97jVp+fU0Urdtw3glkpBVKwvW8+jYGji2ClQa6PkF6GXK+eJUol31xowZw4ABA2jSpAnNmjVj1qxZpKen22bZe+655wgMDGTatGkA7N69m8jISBo0aEBkZCRTpkzBYrHw+usFnOO+GKQmZJGVlv8gMwcXHc4e8g2TEEKI0ikmPYapO61jg4fUGUJj38YlHNGt3fx312QyYUhWEx+RhvbfaZzz+7urKApJsRlEnkki8kwikWeSaPRIRRp0qAiAfxU3tHo1/mFuBFRzx9Uzm4zkC0Se2sPetUc55uzCgI/mAaDV6QiuXRdjdjZBteoSXKsu/tWqo9Pn7vEi7j8xyVkMXLSHUzGpOOo1zHu6EbXWLiJh2zZUDg74v/N2wYdrnN5g/bfdaxBUut+b5VGJJk59+/YlLi6Ot956i5iYGBo0aMCmTZvw9bVO23n58uUc3deysrKYOHEiFy5cwNnZma5du7JkyRLc3d1L6ApyMhstrJy2l8zU/BMnR1c9z73XCo2ucLvlDRw4kG+++QawTnLh6elJvXr16NevHwMHDszxOh48eJD333+fv//+m+TkZIKDg2nfvj1jx46lWrVqtnLffPMNc+fO5fjx42g0Gho1asTYsWN59NFHbWW2bt3Kgw8+aHteoUIFmjZtyvTp06lbt26uODt16sTvv//Orl27aNo059SZAwcOJCkpiXXr1hXWyyKEEOIOnU86j9liprZXbV5s8GJJh3NLef/ddWJN+EHbs5v/7hoNZs7uieXK6USiziSSnmzIUV/0uWQadLA+dvVyoN2Tas7v38y+dUdJT0rMUTY7LY3sjAzbIrM9x02RscoilzOxqQxcuIeo5CwqONuxaGBTgvf8QfTixQAETJuGfa07mNih11dQrTPU6l40AYtbKvHJIUaMGJFv17ytW7fmeP7AAw9w4sSJPMuWBmqtChdPezLTjJDXXIUqcPawQ60tmv9YO3fuzKJFizCbzcTGxrJp0yZGjx7NqlWr+Omnn9Bqtfzyyy/07t2bTp06sWzZMsLCwrh69SorV65k0qRJrFixAoDXXnuNuXPn8u6779KjRw+MRiNLly6le/fuzJ49O9c9O336NK6urkRFRTF27Fi6devGuXPncsxOePnyZcLDwxkxYgQLFy7MlTgJIYQoea0DW7PqsVWAdd2m0qwgf3ftHLW2v7sq4K/vT2MxKbbj/UKtLUpuXplkpvyDxVILtdq6xMU/Rw9wOvxvADQ6HQFVa1hblGrXxb9KdbQ3/Y2TpEn8187z1/jfkn2kZpmo7O3EN4Oa4XXpNJcnTwagwvDhuHbudGeVqlRQt08RRCsKosQTp7LCmG3Od59KDVqdBpVKRfPHK/PznMN5F1SgSdeQHP+5GrPNmAxmjNlm1Oob/+vr7O58XSI7OzvbxBqBgYE0atSIFi1a8PDDD7N48WL69+/PoEGD6Nq1K2vXrrUdFxoaSvPmzUlKSgJg165dfPzxx3z66aeMHDnSVu69994jKyuLMWPG0L17d4KDg237fHx8cHd3x8/Pj5dffpnHH3+cU6dOUa/ejdXlFy1axKOPPsqLL75IixYtmDlzJg4OOdekEEIIUfKCXYNvX6gUUKlUOLseJirjKlqHFrn2mzJ2kRyrRqWy7tPqNdRpG4jOXoOrVyZZKZeIPLOHAz8fJSM5CYCAatXwrVwFgBqt2uHm7UtQrTq5EiUhbuXHQ5GMXXkEg9lC0xAPvnquCU5J8VwcORLFaMTlkUeoMPylglWWcBHCP4UOU8G+/I83LM0kcSqgL0f/le++SnW8eHREfQCCa3lav9LKZ3Wsg5svE1rf2/Z86aRdeY6JGv75Q/cU73UPPfQQ9evXZ82aNXh5eREfH5/vmLDrXR6XL1+Os7Mzzz//fK4yr776KjNnzmT16tW8/PLLufYnJyfz/fffA+RobVIUhUWLFjFv3jxq1KhBlSpVWLVqFc8+++y9X6QQQoh7kmnKZNQfoxhSdwgt/HMnIKWZu68TpqxwUIHW/kbspsxdmLLCcfJ4ELPJYpv0wadiLH8s+sKWKF2n1enxr1YD803rAobUb0RI/UbFch2ifFAUhc//usD0TacA6FbXn4+frI+9ToMxRY3O1xettzcBH0xDVZDZlC1mWPciXN4J2anQ++sivgJxK5I4FTKVSoVao7J1A8hrf3GrUaMGR44c4ezZs7bnt3LmzBnCwsLyXAQ4ICAAV1dXzpw5k2N7UFAQAOnp1gX9Hn/88Rzn+f3338nIyKBTJ2uT9DPPPMOCBQskcRJCiFLg430fsyt6FxeSL7Ch1wbsNGVnUoOWffoRfT6Ziwd+AQW0Di0wZu7EnLUT37B6uFUwcPnYQUIbWAfSO7q6kZGchFanJ6B6DdtkDn5VqqMtw9M6i5JntihM+ek4S3b9A8DQNqG82bUmarX1s5/Oz49Ky5ZiTklF7VjA2fB2zrUmTXpneGhSUYUuCkgSpwL63+wH8t2n+s8XBkM+asu6jw8SfyUVRbF2R60Q5EKPVxva3jzXPfNOC1JTU3BxcS2ydZwURUGlUqEo+TSD5XPMndi2bRuOjo7s2rWL999/n88//zzH/oULF9K3b1/bLEf9+vVj7NixnD9/nrCwsDs6lxBCiMLzV8RfrDhtHd/6Tut3ylTSdF2L3k8RcSIBU1Y4pqydXO/2EXv+CLHnwcnT05Y4+VetQd8pH0iiJApVpsHMqO8PsvlELCoVTOpWi8FtQgHIvnABu3/XHFU7OKAu6DCF2OPwx7vWx52ngUeloghd3AFZcbWAdHaafH+0upzjkfT2Wlr0qMz13ENRoEWPyujttWj1mlz1avW56yxMJ0+eJDQ01DZj3qlTp25Zvlq1aly4cAGDwZBrX1RUFCkpKTlm3wPrOKnq1aszYMAAhg4dSt++fW37EhISWLt2LfPnz0er1aLVagkMDMRkMrFw4cJCuEIhhBB3Iz4znrfC3wLg2VrP0iqgVQlHdHcCqrgT1vj6uCzrH1+NVk/Fug1o3fdZaj/QwVZWq9cTVLOOJE2i0FxLy6bfV7vYfCIWvVbN/P6NbElTyq+/caHbo8TNnXdnX0qbDLDmeTAbrLPoNZQeOqWBJE5FJLiWJz6VXADwqeRiHftUAv744w+OHj1K7969eeSRR6hQoQIzZszIs+z1ySGeeuop0tLS+OKLL3KV+eijj9DpdPTu3Tvfcw4fPpxjx47ZJqBYtmwZQUFBHD58mEOHDtl+Pv74YxYvXozZnP/EG0IIIYqGoihM2jGJhKwEqnlUY3Sj0SUdUoFZLArhq88RF5EKgDE7i38O/7u+DdaeHU279+KJie/SoldffEOlZ4MoGpfi0+n9WTiHIpJwd9Tx3dDmdKnrD0DWyZNEjRsHioIlNeXOhmv8NR1ij4KDJzz2qbX7kihx0lWviKhUKlr0CGPbijO06BFWLGObsrOziYmJyTEd+bRp03j00Ud57rnn0Gg0fP311zzxxBM8/vjjjBo1iipVqhAfH88PP/zA5cuX+f7772nZsiWjR49m7NixGAyGHNORz549m1mzZuWYUe+/HB0dGTZsGJMnT6ZHjx4sWLCAPn36UKdOzpXSg4ODGT9+PJs2baJbt26AdXKJQ4cO5Sjn5eV1y/MJIYS4c8tPLWd75HbsNHZMbzu9zHTRMxnMbF54gguH4jizJ4an327Jvl/WkpWWik/l2qRnP0zFKpfZtfp71BoNLXv3K+mQRTl18HIiQ77ZR0K6gSAPB74Z3Iwwb2cATPHxRLw0HCUzE6c2bfAZO7bgFWenwcGl1sePzQIX38IPXtwVSZyKUHBNT/pPKb7ZiTZt2oS/vz9arRYPDw/q16/Pp59+yoABA2zjp7p37054eDjTpk2jf//+pKSkEBwczEMPPcS7775rq2vWrFnUq1eP+fPnM3HiRNsCuOvWreOxxx67bSwjRoxg5syZzJgxg8OHD/PVV1/lKuPm5sbDDz/MggULbInT1q1badiwYY5yQ4YM4euvZRYZIYQoTCcTTgLwSuNXqOJRpYSjKZjMNAMb5h8h5kIKKnU2YQ2N7PvlB8J/WEarJ5+myeN92LBhAx27DsDD14nwH5YBSPIkCt1vx2MY9f1BsowW6ga6sWBgE3xc7AGwGAxcGTUaU3Q0+pAQAmd+jEp7Bx+57Zzhhe1wfI0sdFvKSOJUTixevJjF/65CfTtNmjRh9erVty03ePBgBg8efMsy7du3z7PPbnBwMEajdZr1N954I9/jN2zYYHt8J9cghBDi3rzd6m26hHahpX/Lkg6lQJLjMvl5ziGSr2ai02eg5hf2rouiWss2tHryaVr27mf7uwM3kiXFYimpkEU5tWTnJSb/dByLAg9W92Zu/0Y42Vk/UiuKQszUqWQeOIDaxYWg+fPRuN7F2kvO3tA897IwomRJ4iSEEELch1QqVZmZDCL2Ugrr5x0mM9WIg1MK2alrSE1OwMnDk2bd++ATUjnP46SlSRQmi0Vhxq+n+fyv8wD0axbMO93roNXcmDIgc98+klevAbWawJkzsascWvATXNgKGdegTv7jyEXJksRJCCGEuE/sjNrJD6d/4K2Wb+Fh71HS4RTYgV//ITPViLP7VVKi12DIysArqCK9xk/BtYJPSYcn7gPZJjNjVx7hp8NRALz2SDWGP1gl1xh2x6ZN8X/vPSwZGTi3bVPwE2QmwtoXITUKjJnQ8JnCDF8UEkmchBBCiPtAUlYSE7dP5GrmVYJdgxnTeExJh1RgDw+oSVbqUS4d+AGL2URQzTp0f20i9s7OJR2auA8kZxp5fsk+dl1IQKtWMb13PXo3Dsq3vHvvXnd+ko1vWJMmzzCo3fMeohVFSaYjF0IIIco5RVGYsnMKVzOvEuoWyov1XyzpkG5JURQuHom3jaGNv3yWC3u/w2I2Ub1lW3pPeEeSJlEsIpMyeeLzcHZdSMDZTsuiQU1zJU3m5GSixo3HdO3a3Z3k+Do4sgJUauj5Beid7j1wUSSkxUkIIYQo59aeW8uWy1vQqrV80PYDHLQOJR1SvswmC38uPcXpXTG07BlGo06V8K9ag3odOqN3cKRd/4Go1PK9ryh6J6JSGLR4D7Ep2fi62rFoYDNqBeSc6EExmYgc8yrpO3ZguBJBpSVL7mwJmtRY+OUV6+M2r0Bw00K8AlHYJHESQgghyrF/Uv7hgz0fADCy4UhqedUq4YjyZ8g0senLo0ScTASVCa3OukC6SqWiw5CXJGESxWbb2TheXHqAtGwT1XydWTyoGQHuub9wuPrhh6Tv2IHKwQG/N9+8s6RJUeDnUZCZAL514YFxhXgFoijI/0BCCCFEOWW0GBm/bTyZpkya+TVjYO2BJR1SvtKTslnz8QEiTiai0WXj6LCes7u/wWwyAUjSJIrN6v1XGLRoL2nZJlpU9mTlC63yTJqSVq8m4ZtvAQiYNg37Wnf4pcSVvXBmE2j00OsL0OoLI3xRhKTFSQghhCinYtJjuJZ5DRe9C++1eQ+1qnQmH9ei0vhlzmHSErPRO6SjZK8j4WosGckuJMVG4xUYXNIhivuAoijM/eMcH28+A0D3BgHM6FMPO60mV9mM/fuJnjIVgArDh+PaudOdnzC4GTyzBpIug2/te4pdFA9JnIQQQohyKtglmFWPr+J80nn8nPxKOpw8ZWeaWPfxQbLSjTi6JpJxbRVZaam4evvS+82peAbkP3uZEIXFZLYw6cdjLN8TAcCL7cMY+0h11OrcXe+MkZFcGTkKjEZcHnmECsNfuvsTV3n47o8Vxa50fvUkhBBCiELhonehgU+Dkg4jX3YOWpp3r4yrZzQp0cvISkvFt3IV+r/7kSRNolikZ5sY9u0+lu+JQK2Cd7rX5o3ONfJMmgAsBgMaFxfsatYk4INpd96N9MSP1lYmUeZI4lTIwlcuY+fq5Xnu27l6OeErlxXJeQcOHEiPHj1ybJs2bRoajYYPP/wwV/nFixejUqno3Llzju1JSUmoVCq2bt1q26ZSqbC3t+eff/7JUbZHjx4MHDiwsC5BCCFEIZm6cyo/nP7BNp13aaMoCtkZxhsbLCeJu7ACk8FAaMMmPDl5Gk7uZWeBXlF2XU3N4qkvd/Hn6TjsdWq+eLYJz7YMueUxdqGhhPywguDP5qN2dLzDE56C1cNgfiuIP3f3gYsSIYlTIVOp1YT/kDt52rl6OeE/LCvWwa0LFy7k9ddfZ+HChXnu12q1/P777/z555+3rUulUvHWW28VdohCCCEK2foL61l1ZhXv736fC8kXSjqcXCwWhW0/nGXV9P1kpVmTJ+9KoWjt7Kj7cCd6jJ2E3r70Tpcuyo/zcWn0mh/O0chkPJ30LB/Wgo61fPMtb4yMtD3WuLmh87vD7q9mI6z9H5izoWIL8Aq729BFCZHEqYCMWVn5/pgMBlu5lr370aJXX8J/WMaOFUswZmWxY8USwn9YRotefWnyWK/c9WbnrvNe/fXXX2RmZvL222+TkpJCeHh4rjJOTk4MHjyYceNuP/3liBEjWLp0KceOHbvn2IQQQhSNyLRI3t31LgDP13ueMPfS9cHMZDDz65fHOPrnFZJiM4g4mQCAb2gYz06fTcdhI1Brcg/EF6Kw7buUQO/PwrmSmEmIlyNrXmxFw4r5t3KmbNrEuU6dSVh2Dz2H/v4Qog+Dgwd0nwt3MnW5KBVkcogC+nRAn3z3hTZsQq9xU2zP961fB8CuNSvYtWaFbfuuNSu4cuo4fSd/YNu2YNRQMlNTctX56opf7ineBQsW0K9fP3Q6Hf369WPBggW0atUqV7kpU6ZQpUoVVq1aRZ8++V9j69atOXPmDOPGjeOXX+4tNiGEEIXPbDHz5rY3STOmUd+7PsPqDSvpkHLITDOwYf4RYi6koFIbcHX7Gyc3f8D6Db+HX0DJBijuGxuPRjN6xSEMJgsNgt1ZMKAJXs52+ZbPOnGCqHHjwWTCGHHl7k56ZT/8/ZH1cbePwaV0TtYibk1anMqhlJQUVq1axTPPPAPAM888ww8//EBaWlqusgEBAYwePZoJEyZg+netjPxMmzaNTZs2sW3btiKJWwghxN1bcGwBB64ewFHryLS209CqS893o8lxmaz58AAxF1LQ6TPQa9cRe+EIG+Z8ZFunSYjisGD7RV767gAGk4WOtXxZPqzFLZMmU3w8EcNHoGRl4dSmDT6vvXrnJzVmwtrnQTFDnd7WH1EmlZ7/VUu5Ud+synfff8ctvfTlMvb8uJJda1ag1mqxmEy06NWXZt2fgP/M0DLk069JSU3B1cUVdSGNf1q+fDlhYWHUr18fgAYNGlCpUiVWrFjBkCFDcpV/4403+OKLL1i4cCFPPvlkvvXWqlWL5557jnHjxrFjx45CiVUIIcS9Oxp3lM8OfQbAm83fJNil9Kx7FH8llZ9mHyIz1Yi9UzKG1LWkJifg5OHJ469OQKOVjyKi6FksCu9tOMmC7RcBeLZFJaY8XhtNPjPngXX2vCsjRmKKjkYfGkrgzI9R3c3v6855cO0sOPtB14/u9hJEKSD/WxWQzt6+wGX3rV/LrjUraPXk07Ts3c82MYRaq6Vl73656tUZDOjs7QstcVqwYAHHjx9He9Ob22KxsHDhwjwTJ3d3d8aPH8/UqVN59NFHb1n31KlTqVatGuvWrSuUWIUQQty7Y9eOYcFCp5BOPB72eEmHk4Ojqx06Ow0aTSQp0WswZGXgGRhM7/FTcfX2KenwxH0gy2jm1R8Os/5oNADjutTg+XaVUd1ijJGiKMRMnkLmoUOoXV0Jmj8Pjavr3QXQcgRkJUNoO3D0vLs6RKkgiVMhu54kXU+aANu/4T8sy/G8KBw9epR9+/axdetWPD1vvDkTEhJo3749p06dokaNGrmOGzlyJJ9++imzZ8++Zf3BwcGMGDGCN998k7Cw0jXoWAgh7lf9avSjpmdNQt1Cb/lhsCQ4uuqp1TKFv5d9j8VsIqhmHbq/NhF7Z+eSDk3cB5IyDAz7dh97LyWi06j46In6dG8QeNvj0rdtI3ntWlCrCZw5E7vQ0LsPQmcPj7xz98eLUkMSp0KmWCw5kqbrrj9XLJYiPf+CBQto1qwZ7dq1y7WvadOmLFiwIM91nezt7Zk6dSrDhw+/7TnGjx/PV199xcWLF+nbt2+hxC2EEOLelJZFbhVFYc8vF3H1cqBmK38URSHy9AEsZhPVW7al8/AxaHW6kg5T3AciEjIYuGgP5+PScbHX8uWzTWgZ5lWgY53atsXn9ddRabU4t2l9dwGc3ghVOoJGPm6XFzI5RCFr9UTupOm6lr370eqJp4vkvBaLBbVazdKlS+ndO+9Bh7179+bbb7/FaDTmuX/AgAFUrlz5tufy9PTkjTfeIKsQpk0XQghxd65mXGXYb8O4kFR61moymy388e1J9q2/xNalp0iOy0ClUtFlxKs8NOh5uo0aK0mTKBZHryTTc3445+PSCXCzZ/WLrQqcNIF1/UqvwYPwfO7Zuwvg5C+w/ClY3M26fpMoFyRxKieuXr1KQEAA8fHxjB07Ns8yr7/+OrGxseh0OgYOHEhSUlKO/RqNhuPHj6MoCu3bt7dtVxSFHj165Cg7fvx4FEVh8eLFhXshQgghbsuiWJiwfQK7oncxdedUFEUp6ZAwZJpYP+8Ip3bGgMpEpVpXcK1gXchWp7ejYefHinUReHH/+vP0Vfp+uZP4tGxq+ruydnhrqvm63PY4c3IyMe+8izkt/d4CSIuDn0dbH1dsARr5sqC8kLbDMi4xMZEdO3awdetWXnjhhZIORwghRDFYemIpu6J3Ya+xZ3KrySU+rik9KZtf5h0mPiINtTYTO92vnNx2ATcfNa2fLJqeFkLk5fs9l5mw7hhmi0LbqhWY/3QjXOxvn7goJhORr4whPTwcw5UIKn7xxd0FoCjWpCkjHnxqw4Nv3l09olSSxKmMGzx4MHv37uXVV1+le/fuJR2OEEKIInY64TSzDswCYGzTsVR2u30X66KUEJXOz3MPkZaQjd4+DcXwI4lRsdg7uxBSr2GJxibuH4qi8MnvZ/l0y1kAejcK4oPeddFpCtbKefXDD0kPD0fl4IDPyy/ffSCHl8Pp9aDWQa8vQJv/GlGi7JHEqYxbu3ZtSYcghBCimGSZshi3bRxGi5H2we15otoTJR0SF4/EkZaQjaNrIhnXVpGVloqrty+935yKZ0BQSYcn7gNGs4Xxa46yav8VAEY9VIVXOlYrcEts0urVJHzzLQAB0z/AvmbNuwsk6TJsfMP6+ME3wa/u3dUjSi1JnIQQQogy4pP9n3Au6Rxe9l5MbTW1xLvoATTqVIlrEUc5+fcyTAYDvpWr0PONyTi5e5R0aOI+kJpl5KVlB9h2Nh6NWsV7PerwVLOKBT4+Y/9+oqdMBaDCyBG4PvLI3Qez/lXIToHg5tB69N3XI0otSZzyYCniKcNF4ZD7JIS4nxjMBk5cOwHAu23exdO+ZBbSVBSFM7tjqNzIB51eQ3pSIif/WoTJaCC0QWMefWUcenuHEolN3F9iU7IYuGgvJ6NTcNRrmNe/EQ/WKPiiysbISK6MHAVGIy6dO1PhpZfuLaCO74AhHR6fA2rNvdUlSiVJnG6i1+tRq9VERUXh7e2NXq8v8m/zLBYLBoOBrKws1DLbUIEoioLBYCAuLg61Wo1ery/pkIQQosjpNXoWdV7EruhdtAlsUyIxWCwKO1ae5cifVwg5EEfXF+ri7OFJx+dHcuXkMToMeQm1Rj4wiqJ3JjaVgQv3EJWcRQVnOxYNbErdILc7qsOckoJKq8WuVk0Cpr1/75/5fGrAoA33Voco1SRxuolarSY0NJTo6GiioqKK5ZyKopCZmYmDg0Op6HJRljg6OlKxYkVJOIUQ9w2tWltiSZPJYGbzohNcOBiHopjx9DfDv3+2arV9kFptHyyRuMT9Z+f5a/xvyT5Ss0xU9nbim0HNCPZ0vON67GvWJGTlSlAsqB3uspXUbIKrx8G//t0dL8oUSZz+Q6/XU7FiRUwmE2azucjPZzQa+fvvv2nXrh06WRSwwDQaDVqtVpJNIUS5t/LMSi4lX2J0o9HoNSXTwp6VZmT9/CPEXEhGpc7G2WkLx7YkUO/Bj2UskyhWPx2O4rUfDmMwW2hSyYOvnmuCh9OdvS9McXFovb0B0PkWvGtfnrbPhK3T4OG3oM0r91aXKPUkccqDSqVCp9MVSyKj0WgwmUzY29tL4iSEECKHC8kXmLFnBlnmLKq4V6Fn1Z7FHkNKfCY/zzlMUmwGOn0GauUX4i9fQWfvQGJ0pCROolgoCny1/SIzfrVON96ljh+f9G2Ave7OuoambNxI1Pg3CXj/PVy7dr23oKIOwl/TQbGAW/C91SXKBEmchBBCiFLIaDYy7u9xZJmzaOnfku5Vin+tPsWisOGzoyTFZmDvmIwhbS2pyQk4eXjSa9wUfEJKdg0pcX8wWxRWX1SzLdaaNA1pE8qErjVRq++s10nm8eNEjX8TJSuLzCNH7y1xMmbCmufBYoLaPaFO77uvS5QZMjhECCGEKIXmHJrDyYSTuNu5826bd1Griv9Ptkqt4sFna+BW4RoZ15aTkZyAZ2Aw/d/5SJImUSwyDWZGLD/Etlg1KhVM7FaTSY/WuuOkyRQXx5XhI1CysnBq2xafsa/dW2B/vAvxp8HZF7rNBBk6cF+QFichhBCilNkTvYfFxxYDMKXVFHwc73Ecxh1KS8zG2cMOgIyks8RdWIbFbCKoZh26vzYRe2fnYo1H3J+upWUz9Nt9HLychFalMPPJ+jze8M67xFkMBq6MHIUpJgZ9aCiBH3+E6l5mf7y4DXbOsz5+fA44lszSAKL4SeIkhBBClCLJ2cmM3z4eBYXeVXvzcMWHi+3ciqKw55eLHPo9gp5jGuJTyRW/KtVw9/XDu1IonYePQSvjcUUx+OdaOgMW7uHStQzcHLQMqJxFlzp+d1yPoijETJ5C5qFDqF1dCf5sPhpX17sPLDsN1r0EKNBoAFTrdPd1iTJHEichhBCiFDmTeIYMYwaVXCvxetPXi+28ZrOFrUtPcWpnDIqicOV0Ij6VXHFwdqHv1Ok4OLugkuUfRDE4FJHEkMV7uZZuIMjDga+fbcTpvX/dVV2pmzeTvHYtqNUEfjITfUjIvQWnd4K2r8Cer6HTe/dWlyhzJHESQgghSpGmfk1Z9fgq0gxpOOrufG2au2HIMrHpy2NEnEgAlQlXt62olEygEgCOrne2sKgQd2vziVhGLj9AltFCnUBXFg5sioe9htN3WZ/Lww/jNWwoWm9vnFu3vvcAVSpoMtja2qSWxZ7vN5I4CSGEEKVMoHNgsZ0rPSmbX+YdJj4iDbU2Ezvdr8RdusC2ZWeo3qINjm7uxRaLuL8t2fUPk388hkWB9tW9mde/EU52WoxG413XqdJo8Hn11XsPLiMB1Fqw/7ebnyRN9yVpcxdCCCFKmMli4uU/XyY8MrxYz5uakMWqGfuIj0hDb5+KxrKGxKgL2Du70HvCO5I0iWJhsShM33SKSeusSdNTTYP5+rkmONnd3ff75uRkrs78BIvBUDgBKgr8PAo+aw0RewqnTlEmSYuTEEIIUcK+PPIlWy5vYU/MHn7t/SsuepdiOa+Tmx6vQGcspmgyrq0mKy0VNx9feo2fimdAULHEIO5v2SYzr686wo+HogB4tWM1RjxUBdVdTu+tmExEvvIK6eE7MV6JIHDmzHsP8sgPcPJna4uT1u7e6xNlliROQgghRAk6dPUQXxz5AoCJzScWS9KkKAoqlQq1Rk3VRumc3/UdJoMB38pV6PnGZJzcPYo8BiGSM428sGQ/Oy9cQ6tW8UHvevRpfG8Je+z0GaSH70Tl6IjX//5XCEFegQ1jrY/bjwP/+vdepyizJHESQgghSkiaIY1x28ZhUSw8WvlRulbuWuTnPPT7ZRJjM2jfvzoqlYqU+BhMBgOhDRrz6Cvj0Ns7FHkMQkQlZTJo0V5Ox6bibKfls2ca0baq9z3VmbhyJYlLlgAQMP0D7GvUuLcgLRbr1OPZyRDYBFq/cm/1iTJPEichhBCihEzbM43ItEgCnAJ4s/mbRXoui0Vhx6qzHPnjCgCV63tTqY4XTR7tiYtXBao1b436XhYFFaKATkanMGjRXmJSsvBxsWPRoKbUDri3mRsz9u0j5u13AKgwaiSuHTvee6B7v4aLf4HWAXp+ARr52Hy/k98AIYQQogRsuriJn87/hFqlZlrbaUXaRc9kMPP7ohOcPxiHopjxDzmDb2hzAFQqFTVatSuycwtxsx3n4nl+yX7Ssk1U9XFm8eBmBLrfWyun4UokV0aOAqMRly6dqfDii/ceaPw52PyW9XHHt6FClXuvU5R5MqueEEIIUQL2x+4HYGjdoTTybVRk58lKM/LT7EOcPxiHSp2Ni8uvXDq0kQ2fzkBRlCI7rxD/tebAFQYs3ENatonmoZ6seqHVPSdNAKbYGBSLBftatQh4//27nlgiBwd3qPIwVG4PTYfee32iXJAWJyGEEKIETGgxgTaBbWgV2KrIzpESn8nPcw6TFJuBVp+BRvmF+Igr6OwdaNS1e+F8wBTiNhRFYf7W83z4q3UZ28fqB/DRE/Ww0xZO11DHxo0J/WEFKr0etUMhjdFzqgB9l4IxA9TSziCsJHESQgghSsgDwQ8Uaf1JsRkkx2Vi75iMIW0tackJOHl40mvcFHxCKhfpuYUAMJktTPrxOMv3XAbg+Qcq80anGqjV9560m5OT0bhZx0bpK1W65/oAyEyytjYBqFSgdyqcekW5ICm0EEIIUUyOXzvOmK1jSMhKKJbzVaztRaMOKjKuLScjOQHPwGD6v/ORJE2iWKRnm/jfkv0s33MZtQre7l6b8V1qFkrSlLJxI+c6PkLa9h2FEOm/jFmwsDOsHAQZxfMeFWWLtDgJIYQQxSDDmMG4v8dxKeUSrnpXprSaUiTnObUzGr8wN9x9HDGbjBz7cwmGrAyCatah+2sTsXd2LpLzCnGzuNRshnyzlyNXkrHXqfn0qYY8UtuvUOrOPH6cqPFvomRlkR4ejnOb1oVSL3++B3EnISMeFEvh1CnKFUmchBBCiGLw0b6PuJRyCR8HH15u9HKh168oCnt+uci+9Zdw9XbgyfFNsHPU0f21iRzY+DMPD3kRrU5X6OcV4r/Ox6UxcNEeIhIy8XTS8/WAJjSqWDiLKpvi4rgyfARKVhZO7dri8+qYQqmXf8IhfI718WOfWsc4CfEfkjgJIYQQRezPy3+y8sxKAN5r+x7u9u6FWr/ZbGHr0lOc2hmDolgIqJyN3sH6J967UiidXhhVqOcTIj/7/0lgyDf7SMowUsnLkcWDmhFaoXDGCVmys4keMRJTTAz6ypUJ/PhjVIWx9lh2Kqx9AVCgwTNQo+gXohZlkyROQgghRBGKz4xncvhkAAbWHkgL/xaFWr8hy8SvXx7j8okEwIir+18c3XKS6i29CapRu1DPJcStbDoWzejvD5FtslA/2J0FA5pQwdmucCpXFOLefpvMw4dRu7kRPH8eGpdCWvvs1wmQ9A+4VYTO0wqnTlEuSeIkhBBCFBGLYmHi9okkZidSw7MGIxuOLNT605Oz+WXuYeIj0lBrM7HT/UrcpQtodXoyU1MK9VxC3MqiHRd5+5cTKAp0qOnDnH6NcNAXznTjAC6HDpH608+g0RD0yUz0ISGFU/GZ3+DAN9bHPeaDvWvh1CvKJUmchBBCiCISlxHHpZRL2Gns+KDtB+g1+kKtf/sPZ4mPSENvn4bF8COJUbHYO7vQY+wkAmvUKtRzCZEXi0Xh/Q0n+Xr7RQCeaVGRqY/XQVMIM+fdLK1uXVyyDTjWqoVTq0Jc+0xrBy4BULsnhLYtvHpFuVTi05HPmzePkJAQ7O3tad68OXv27Lll+VmzZlG9enUcHBwIDg7mlVdeISsrq5iiFUIIIQrO18mXVY+tYs5DcwhzDyv0+tv1q4ZPpXQMqctJuxaLq7cv/d75UJImUSyyjGZGfn/QljS90bkG73Qv/KQJQNFq8Zk6BY9nni7ciis/AC+Fw8OTCrdeUS6VaOK0YsUKxowZw+TJkzlw4AD169enU6dOXL16Nc/y3333HePGjWPy5MmcPHmSBQsWsGLFCt58881ijlwIIYQoGGe9My0DWhZafQlR6bbHaQlRRJ1YTFZaKr6Vq9D/3Y/wDAgqtHMJkZ+kDAPPLdjD+iPR6DQqZj/VgBfbh6FSFV7SZE5KIv7Lr1DMZgBUKlXh1W823njs4AE6h8KpV5RrJdpVb+bMmQwbNoxBgwYB8Pnnn7N+/XoWLlzIuHHjcpUPDw+ndevW9O/fH4CQkBD69evH7t278z1HdnY22dnZtucpKdY+30ajEaPRmN9hxeZ6DKUhFlE45J6WP3JPy6eivK9zD8/Fx8GHJ6o+UagfJI/8EcmudRdo91RVarTyw83Xn9CGTTBmZdF55Kvo7R3u699Tea8WjyuJmQz59gAX4tNxsdcyv18DWlT2LNTXXTGZiHr5FTJ37SLz0iVo3qzw6k+JRvttV8xtx6LU6weF+B4VBVOa3qt3EoNKURSlCGPJl8FgwNHRkVWrVtGjRw/b9gEDBpCUlMSPP/6Y65jvvvuOl156id9++41mzZpx4cIFunXrxrPPPptvq9OUKVOYOnVqnnU5OjoW2vUIIYQQAGeNZ/km3TrY/Hnn5wnWBt9znYoCyafsSLukR1EUnCtl4VHbZN1nNoNKhUpd4r3vxX0gIg2+PKUhxajCXa/wfE0zAUXwccr7x5/wCA/Hotdz+aUXMfj7F07FikKL8x/hm3qURMdQtlV7C0VVeJNYiLInIyOD/v37k5ycjKvrrScHKbEWp/j4eMxmM76+vjm2+/r6curUqTyP6d+/P/Hx8bRp0wZFUTCZTLzwwgu37Ko3fvx4xoy5sThaSkoKwcHBPPLII7d9cYqD0Whk8+bNdOzYEZ0sTFguyD0tf+Selk9FcV8TsxKZtWEWAE9UfYLnmz5/z3WajBb+/PY0aZfiURQTbp7huDnY07nzK5Is/Ye8V4vW32fjmf/9YTKMZmr4OvPVc43wc7Uv9PMkr1pFXHg4AAEzphParl2h3Vf1/kVoDh1F0drj/OwyulSoVhghiztUmt6r13ujFUSZmlVv69atvP/++8yfP5/mzZtz7tw5Ro8ezTvvvMOkSXkP6rOzs8POLvcaAjqdrsRv1M1KWzzi3sk9LX/knpZPhXVfFUXh3b3vEp8VT2W3yoxtNhad9t7qzUozsvGzY0SfT0alysbJeQtXL5wi/h8NCRG98KsiH/ryIu/Vwrdi72XeXHsMs0WhTZUKfPZMI1zsC/81zti7l7j33gfAe/QoPDp3tnWluuf7eu08bLGuqabqMAWdv6xzVtJKw3v1Ts5fYolThQoV0Gg0xMbG5tgeGxuLn59fnsdMmjSJZ599lqFDhwJQt25d0tPT+d///seECRNQyzdvQgghSsjqs6v5M+JPtGot09tNx0F7b4PNjQYzaz7aT2JMBlp9BhrlF65FXEFn78DjY8ZL0iSKhaIozPr9LLO3nAWgV6NAPuhVD7228D9zGa5EcmXUaDCZcO3aBa8XXii8yi1mWPsCGDMgpC00u/fWYHH/KbFMQ6/X07hxY7Zs2WLbZrFY2LJlCy1b5j37UEZGRq7kSKOx9kstoaFaQgghBJeSLzFj7wwARjccTQ3PGvdcp06voVozX+ydkrFk/UDy1Ss4eXjy1NTphNRvdM/1C3E7RrOF11cdsSVNIx+qwsdP1C+SpAnAcOE8lsxM7GvXxv+99wp1YhV2zIYre0DvYl3oVr5sF3ehRLvqjRkzhgEDBtCkSROaNWvGrFmzSE9Pt82y99xzzxEYGMi0adMAeOyxx5g5cyYNGza0ddWbNGkSjz32mC2BEkIIIYrbwasHMZgNNPdrznO1n7unuiwWBfW/6+B4ByeTcW05hswMPAOD6T1+Kq7ePoURshC3lJZt4qVlB/j7TBwatYp3utehf/OKRXpO53btqLRsKVovL9QOhTw9uCEdUEGX6eBetNchyq8STZz69u1LXFwcb731FjExMTRo0IBNmzbZJoy4fPlyjhamiRMnolKpmDhxIpGRkXh7e/PYY4/x3nvvldQlCCGEEPSs2pNQt1D8nPxQq+7+m+wTO6I4sT2Kx0c3QG+vRa1WYzYaCKpZh+6vTcTe2bkQoxYib1dTshi4aC8nolNw0GmY93RDHqrhe/sD75IlPR21kxMADrWLaNzRw5OgTi/wkcWhxd0r8ckhRowYwYgRI/Lct3Xr1hzPtVotkydPZvLkycUQmRBCCFFwDXwa3PWxiqKw95eL7F1/CYCTO6Kp/3AwwbXr0Wfiu/iFVUOr1xdOoELcwtnYVAYu2ktkUiYVnPUsHNiUekHuRXa+5PXrufrBdILmzsGhfv3CP4Gi3FinyVcmgxD3Rjp4CiGEEHchxZDCiC0jOJd47p7qMZst/LHkFHvXX0JRLHj6HiSgqtm2P6hmHUmaRLHYdeEavT8LJzIpk8oVnFjzYusiTZoyjx4j+s0JmOLiSP19y+0PuFOXd8HCThB/b+9RIa6TxEkIIYS4Q4qi8O7Od/nryl+M/XssFsVyV/UYskxsmHeEU+HRgBFX9z+IOvUn62a8jclgKNyghbiFnw9H8dyCPaRkmWhcyYPVL7aiolcRrGz7L+PVq1wZMQIlOxvnBx7A++XRhXuC7DTrLHoRuyF8duHWLe5bJd5VTwghhChrfrnwCxsvbUSj0jC11dS7GteUnpzNL3MPEx+RhlqbiZ3uV+IuXUCr09P+2aHSyiSKhaIofLXtAu9vOAVAlzp+fNK3Afa6opt0y5KdzZURIzHFxqIPCyPg449QFfYkX5snQeJFcA2Cju8Ubt3iviWJkxBCCHEHrqRe4b3d1kmJXqj/AvW8691VPWajhYxkA3r7NCyGH0mMisXe2YUeYycRWEMGsIuiZ7YovPPLCRaHXwJgUOsQJnarhUZdiNOA/4eiKERPmkTWkSOo3dwInj8PTWFPenLud9i30Pq4xzxwcC/c+sV9SxInIYQQooBMFhNvbn+TdGM6DX0aMrTu0Luuy7WCA80fd+LPRfPISkvF1duX3m9OxTMgqBAjFiJvWUYzo78/yK/HYwGY2K0mQ9tWLvLzJq1aRcpPP4NGQ9CsT9BXqlS4J8hIgB//nXSs+QtQuX3h1i/ua5I4CSGEEAX09dGvOXj1IM46Z6a1nYZWfWd/Rs/tv4pGqyK0vjcAp3f8RFZaKr6Vq9Dzjck4uXsURdhC5JCQbmDoN3s5cDkJvVbNJ082oFs9/2I5t1vXrqT99RdOLVri1LJl4Z9gw1hIjQavqvCwzMIsCpckTkIIIUQBmC1m9sTsAeDN5m8S6Bx4R8cf+v0yO1afQ6tV88T4pngGONFlxKuEr1xGm6eeQ29fyAt+CpGHf66lM3DRXi7Gp+PmoOOr55rQLNSz2M6vdnIi6NNPb0wRXpiyU+HaOVBpoOcXoC+6yS3E/UkSJyGEEKIANGoNX3X8ij8j/uThig8X+DjForBj1TkO/xGBoij4VU7E3c/6gc7eyZmHBj5fVCELkcOhiCSGLN7LtXQDge4OfDO4KVV8XIr8vOakJJI3bMCjXz9UKhUqdRFN6mznAkN/h8s7Iahx0ZxD3NckcRJCCCEKSKPW0KFShwKXNxnN/L7oBOcPxKEoJtw9wzm3ax8Hq6lo3K1H0QUqxH/8fiKWEcsPkGW0UCfQlYUDmuLjal/k51WMxv+zd9fxTd37H8df0abu3iKlpVDcfQwbMqRsd36nd8ywyf1tY253zMcEtt3ZnY8pjCEbMtxhaKG0FKi7W/T8/kgJMGBYm1Q+z8ejD5JzTs750JCQd75G5v0PUL15M5bsbEL+/e+GvaBGB20va9hriBZL1nESQggh/sbyY8t5eevLGK3GC3pcbZWZX97axeGdBahURry8fycvbTtqjQY3z3qeRUyIv/Hl5mPc9cV2as02hrYPZv5dA5wSmgDyXnqZ6s2bUXt44DNhYsNcZOfnsPI5sMjaZ6JhSYuTEEIIcRa5Vbk8s/EZyk3lhHuGc0unW877sfvWZJGTWoZWX41G+ZWijEx0BncmPjiLNt16NmDVQtgpisKrvyUzb/VhAK7rHc0Lkzuj0zjne/OS+d9R8tVXoFIR8eorGOLb1/9Fio/A0kfBXAUBMdDjn/V/DSHqSHASQgghzsCm2HhiwxOUm8rpFNiJGzrccEGP7zmmNQXpRzm68zsqy4rx9A/gqkefIaRNw0/5LITJYuPhH3azYFc2AA+MbM+MEbGoGmJShjOo2rqV3OftC88Gz5yJ94jzHxd43mxWWHCvPTS1Hgzdbqz/awhxEglOQgghxBl8kfQFW3K24K5156UhL6HT6M75mNy0MoJbe6PRqDFWV3J463sYq6oIiIzm6lnP4hMc4oTKRUtXXmvmni92sPFwEVq1itlXdeGa3tFOu74pM5OsGTPBYsFn3DgC776rYS60aa59Igi9FyTOg4aadEKIOhKchBBCiL84WHyQOTvnAPBwn4dp49vmnI9J2pDN6q+Sie8byvBbO+Lu5c3Af9xIytZNTPr3Exi8ZFyTaHg5ZTXc/uk2DuZW4KnX8N4/e3FZ+2Cn1lC7dy/WykoMnToR/p8XGqaVKy8JVtlbtBjzEvjX80K6QpyBBCchhBDiJDWWGh5Z+wgWm4Vh0cO4Ou7qvz1eURS2LT7Ktl+PoCgKZlM1NpuCRqOi57hJdB89HrVG46TqRUt2MLec2z7ZRm55LSHebnx6ex86Rfg6vQ6fsWPRBAaib9UKtXsDrE9mMcHPd4HVBO3HyLgm4TQSnIQQQoiTpJakUlBdQJB7EM8OfPZvvy23Wm2s+SqZAxtzUBQb/kHbyE3OwFzbAU3dzHkSmoQzbEgt5J4vdlBhtBAb4sX/bu9DlL9zF4C1mUyo9XoAPPv2bbgL5e6FojRwD4AJDbSYrhBnIMFJCCGEOEmX4C78MPEH8qvz8Tf4n/U4U62F3z7cR/r+YsCMj98aclP3gEpF+r7dtO83yHlFixbtp52ZPPLjHsxWhb5tA/jw5t74epx7TF59Klu8mIK33yZ63jzc2rVr2ItF9YJ710NpBniHNuy1hDiJjKITQggh/iLCK4LuId3Pul9RFJbM20P6/mLUmhrc3RdTcHQPWp2eCQ88KqFJOIWiKMz9I5UHv9uN2aowvms4X/yrr9NDU83efeQ89jjmY+mU/fqrcy4aEAMxQ51zLSHqSHASQgjR4imKwlMbnmJ91vrzOl6lUtFrTBsMHlWolZ8oyU7D4OXNP554QUKTcAqL1cbjC/bx6m/JANx9WQxvX98DN61zu4aa8/PJnDYNxWjEa+hQgqdNa7Brqde9BkfWNtj5hTgXCU5CCCFavPnJ8/k59Wfu/+N+CmsKz3qcxWx13NYbijGWf0NlUR4+waHc8PyrRHZIcEa5ooWrNlm4+4sdfL0lHZUKnp3YiVnjOqJWO3esj81oJHPadCx5eehj2xHx+muoGmhMX3D5XjRrX4LPJ0HR4Qa5hhDnIsFJCCFEi5ZWlsZr218D4IFeDxDkHnTG4w7vzOfLJzdTklsFgKefP1q9G6Exsdz4wmsEREQ5rWbRchVUGLnhv5tZeTAfN62a927qxa0D2zi9DkVRyHnySWr37EHj60v0vHloGmrK/ZpSeqR/ZL/d+18Q2MBjqIQ4C5kcQgghRItlUSw8tuExjFYjgyIGcWOHG8943O6VGaz/IQUU2PtHJpfdEI9XQCDXPPUfPP380RsaYMplIf4iraCS2z7dRnpxNf4eOj66tQ+9Wp99ApOGVPLNN5T/sgg0GiLfmoO+VasGu5bm90fRmUtQAmJQjXq2wa4jxLlIcBJCCNFirahdwSHjIfzd/Hl+0POnTT2u2BQ2/JjK7pUZKIqCf/BugqLUQDwA/mERLqhatEQ7jhVz52fbKak20yrAg//d3oeYYNctquwzdiwVS5fhPWY0nv37N9yF9i9Ave8HFFRYJ85Dq/dsuGsJcQ4SnIQQQrRIW3K3sN5onwzi2YHPEuwRfMp+i9nKik8PcHhnPopixS9wI7kp2/j9yFqiOibgExTiirJFC7RsXy4zv/0To8VGtyhfPr6tD0Febi6tSevvT6tPP4GGXKesIg9+fQCAlNDxtI3s3XDXEuI8yBgnIYQQLdKmnE0AXB17NcNaDTtln7HazC9v7eLwznxUKiNe3r+Rd3gbao2GkVOmSWgSTvO/DUe496sdGC02RnYM4Zu7+rssNFlKSihbdGK6cZVW+7cLRF+y3d9ATTFKSGcOhk1uuOsIcZ6kxUkIIUSLdH+P+7GmW7mv532n7dPo7N8ranVVaFhMUUYmOoM7Ex+cRZtuPZ1dqmiBbDaF2UsP8OG6IwDc1K8Vz07shFbjmu+8FbOZrPsfoHrLFsxZWQTdc3fDX3TQTPAOxxLUAWX70Ya/nhDnIC1OQgghWqyOuo64a0+f2EGr09B3vA824/eU5Wfi6R/A9c++LKFJOEWt2cr0b/90hKaHx8TzQmJnl4UmgLzZs6nesgW1hwdew4ed+wH1QaWCbtdBiEzzLxoHCU5CCCFajPTydB5a/dAZ12pK31/EtsVHHPeP7dlKdVkxAZHR3Pj8a4S0iXFmqaKFKq02ccsnW1m8JwedRsWc67pz3+WxDdsl7hxKvv2Wkq+/AZWKiNdexdC+fcNdzGaDNa9AdXHDXUOIiyRd9YQQQrQIZpuZWetmsadwDwoKLw962bHvwMZs/vgyGcWmEBjhRUyPYAb+4wa0ej3dRo7F0FDr0whxksySam77dBup+ZV4u2n54OZeDIw987pizlK1ZSu5L/wHgOD778d7+PCGveDmefDHf2D3tzB1K2jko6poPORfoxBCiBbhg90fsKdwD956bx7u8zAAigI7lh5jx5J0FEUhJCqLyPiBAKjUavolXuPKkkULsi+rjNv/t42CCiPhvgY+vb0PHcJ8XFqTKSODrBkzwGLB58orCbxrSsNeMP8grHzOfnvgdAlNotGRrnpCCCGavZ15O/lw74cAPNX/KcI8w7BZbZTsc6sLTTb8graSse87fn//TRRFcXHFoiVZc6iA6z7YREGFkQ5h3vx030CXhyaAqs2bsZaVYejcmfD/vNCw3QWtZvj5LrAaIXYU9Lqt4a4lxEWSKC+EEKJZqzBV8Nj6x7ApNia2m8iYtmMw1Vr47b9JVGfqATM+fmvIS90DKhWRHTu5dDyJaFm+257BrJ/2YrUpDIoN5L1/9sLHoHN1WQD4X3MN2sBADJ06oTYYGvZia1+FnN3g7g+T3rVPDCFEIyPBSQghRLP24pYXyarMItIrkll9ZwGQfaiUjKQSoAqD4XcKjh5Bq9Mzbvq/ies30LUFixZBURTeWpnCnBUpAEzuEcnLV3dFr3V9ZyDFYkGltX9EbPAxTQCZO2Dta/bbV74B3mENf00hLoLrX51CCCFEAykzlrErfxdqlZqXhryEl94+yUObrkF0H+WNxfgNpTlHMHh5848nXpDQJJzCbLXxyI97HKFp6rB2vHFtt0YRmsoW/cqRa6/FnJ3tvIuueg4UK3S+Gjpf5bzrCnGBLrjFyWazsWbNGtatW8exY8eorq4mODiYHj16MHLkSKKjoxuiTiGEEOKC+br58sPEH9iSs4XQirZU6Y14+rlhs1lJXvcRlupyfIJDuPqx5wiIiHJ1uaIFqDRauO+rnaw9VIBaBc8nduamfq1dXRYANXv3kvPEEyhGI6U//0zw1KnOufA1n8EfL8LljzrnekJcpPP+aqOmpoYXXniB6Ohoxo0bx9KlSyktLUWj0ZCamsrTTz9N27ZtGTduHJs3b27ImoUQQojz5qnzpHVhZxa++SeL3t2NqcaCWq1hxJ33YQgO5ZqnX5LQJJwiv7yW6z7YxNpDBbjrNHx4S+9GE5rMeflk3jcVxWjE6/LLCbrnHudd3N0Pxr0CHgHOu6YQF+G8W5zat2/PgAED+PDDDxk1ahQ63ekDF48dO8bXX3/N9ddfz+OPP86UKQ08baUQQghxBp/t/wytWsuNHW5kz6pM1v+QAgoYPI2oNPZB5xHxCUSOnICnn7+LqxUtQWp+Bbd+so2s0hqCvPR8fGsfukX7ubosAGy1tWROm4aloAC3uFgiXnsVlUbTsBetLYPkZdD1WpkIQjQZ5x2cfv/9dzp27Pi3x7Ru3ZpZs2bx73//m/T09EsuTgghhLhQ+wr3MWfHHCw2K7otURRstqIoCv7Bu8nYvYmS7EhC2sQAyOx5wim2Hinmzs+2UV5roW2QJ5/d3pdWgR6uLguwT1KR88ST1O7di8bXl6i5c9E4Y8HnpY/C7q8hazuMe7XhrydEPTjv4HSu0HQynU5Hu3btLqogIYQQ4mJVm6t5dN2jKFb4Z/bDFGRYURQLfgEbyU3ZDkBm0l5HcBKiof26J5sH5+/GZLXRs5UfH93ahwBPvavLcij5/HPKf/0VNBoi35qDvlWrhr/ogV/toQmVfUIIIZqIS5qO3GKx8MEHH7B69WqsViuDBg1i6tSpGBp6rn8hhBDiDF7Z9grHyo8xJuM2vLIjUKmMeHqtJC/tIGqNhlF3Tafz5SNdXaZoARRF4eP1R3hh8QEARncK5a3re2DQNXAXuAvkPWoUpQsW4nfNP/Ds37/hL1hZAItm2m8PmgmtnHBNIerJJQWnGTNmcOjQIa666irMZjOff/4527dv55tvvqmv+oQQQojzsvLYSn5M+REVKiZeO4ijX5RhLF9IUUYmOoM7Ex+cRZtuPV1dpmgBrDaF539N4n8bjwJw28A2PDk+AY268XUN1UVE0Gb+t6j1TmgFUxR7aKouhJBOMOyxhr+mEPXogoLTzz//zOTJkx33f//9d5KTk9HUDSAcPXo0/Z3xbYUQQghxkvzqfF5c+zIAt3W+ja4BbUgqfoTK4iI8/QO46tFnpHuecIpas5X7v93Fsv25ADxxZUf+NbhtoxpPZykpoWb3brwvvxzAOaEJYPc3kLwY1Dq46gPQujnnukLUkwtaae2TTz4hMTGR7LpF0Xr27Mk999zDsmXLWLRoEQ8//DB9+vRpkEKFEEKIs1m/+U+u3DyTIcZxTO8+Ha+AQAKjWhEQGc2Nz78moUk4RXGViZs+2sKy/bnoNWreuaEHdw6JaVShSTGbyZp5P5n33EuJM3sIGStgWd06TcNmQVgX511biHpyQS1OixYtYv78+Vx++eVMnz6d//73vzz//PM8/vjjjjFOzzzzTAOVKoQQQpzuwMYc8n7Q42bTMbRyElq1FpVKxYQHZmGzWXH38nZ1iaIFSC+q5tZPt3KksAofg5YPb+lNv5hAV5d1mrzZs6neuhW1hwcevXs778Ju3nDDfNj+MQyc6bzrClGPLniM03XXXcfo0aN5+OGHGT16NO+//z6vv/56Q9QmhBBCnJWiKGxfcpSti47UTTd+gEAfDTAAADePxjHds2j+dmeU8q/PtlFYaSLSz53/3d6HuNDGF9hLvv2Wkq+/AZWKiNdewy0uzrkFtB5g/xGiibqoySH8/Pz473//y9q1a7nlllsYM2YMzz//vMymJ4QQwilsVhsrv0ri0MZ8FMWGX9A2clM2kJsC7fsPolXnrq4uUbQQKw/kMe3rP6kxW+kU4cOnt/UhxKfxfR6q2rKV3Bf+A0DwAw/gPXyYcy5cdNg+KURQrHOuJ0QDuqAxTunp6Vx77bV06dKFm266ibi4OHbs2IGHhwfdunVj6dKlDVWnEEIIAYDVamPJe3s5tDEfq2KkSPUxeakbQKVi2G13SWgSTvPVlmNM+Xw7NWYrl7UPZv7dAxplaDJlZJA1YwZYLPiMH0/glDudc2GrGX78F7w/GA4ucc41hWhAFxScbrnlFtRqNa+++iohISHcfffd6PV6nn32WRYsWMDs2bO59tprG6pWIYQQAo1GTZVnCSZKKTK/h1dJBVqdnokPzKLn2ImuLk+0AIqi8OpvB3n8533YFLi2dxQf39obL7dLWuWlwVQsX4G1rAxDly6Ev/C88yarWPcGZP8JWj2Ed3PONYVoQBf0Ct++fTu7d++mXbt2jB49mrZt2zr2dezYkbVr1/Lf//633osUQgghjiuuLeYz67P0q3XHp0aNwcubxP97ksgOCa4uTbQAJouNR3/cw09/ZgFw/8g4Zo6Ia1Qz5/1V4B23ow0MwKN/f9TOGlaR/SesfcV+e9zr4BvpnOsK0YAuKDj16tWLp556iltvvZUVK1bQpcvpU0nedddd9VacEEIIAZCTWsruVRmMvC2Bpzc+jVJchWetJ97BIfzjsecIiIhydYmiBSivNXPvlzvYkFqERq1i9uQuXNsn2tVlnZVis6FS2zsX+U6a5LwLm2vgp7vBZoGEROjyD+ddW4gGdEFd9T7//HOMRiMPPPAAWVlZfPDBBw1VlxBCCAHA4Z35LJyzi8M7C/jimyWszlhNXriFXv+6mZteeF1Ck3CKnLIarn1/ExtSi/DUa/jktj6NOjSV/fIL6bfehqWkxPkXX/UCFCaDVyhc+QY04tY4IS7EBbU4tW7dmh9++KGhahFCCCFOsXtVBuu/TwEF/IKPsMVjK1TDzJ4zGdbpOleXJ1qIg7nl3P7pNnLKagn2duPT2/rQOdLX1WWdVc3u3eQ88SSKyUTpDz8QNGWK8y6esQ02zbXfnvgOeDa+tayEuFjnHZyqqqrw9PQ87xNf6PFCCCHEcYpNYeNPqexakVG3RtNucg+tontlFCPvepbxHRJdXaJoITamFnL3FzuoMFqIDfHif7f3Icq/8a4RZs7LI3PadBSTCa9hwwj817+cW0BEd7j8UajMh/ajnXttIRrYeXfVi42N5aWXXiInJ+esxyiKwvLlyxk7dixvv/12vRQohBCiZbGabfz+yf660GTFN2A9uSmrAOg4aCgTOkxGrbqgnuZCXJQFf2Zx66dbqTBa6Ns2gB/vGdioQ5OttpbMqdOwFBTgFhdLxKuvOsY4OY1GZw9OV77u3OsK4QTn3eK0evVqHnvsMZ555hm6detG7969iYiIwGAwUFJSQlJSEps2bUKr1TJr1izuvvvuhqxbCCFEM1VRXEv6/mJUKhOeXivITzuIolYx4s776DFirKvLEy2AoijMW32YV39LBuDKruG8fk03DDqNiys7O0VRyHn8CWr37UPj50fUvHlovJzY8yf/AAS0s089DjKuSTRL5x2c4uPj+fHHH0lPT+f7779n3bp1bNy4kZqaGoKCgujRowcffvghY8eORaNpvG8sQgghGje/UA+G3hDB2i9eoygjE4tWYVWPPAwBh+nh6uJEs2ex2nj6l/18tSUdgLsui+HRMR1Qqxt3ECj+5FPKFy8GrZbIt95CH+3EiSuqCuGzCeAVBjd+C74yYYtoni54pbZWrVrx0EMP8dBDDzVEPUIIIVqggowKzLVWIuL8AEha8w1l+ZlYPbQs6ZGOV1Q4d3S+w7VFimav2mRhxjd/suJAPioVPD0+gdsGtT33AxsBr8uHUjJ/PoF33I5nv77Ou7CiwK8PQFUBeATZf4RophrnEtdCCCFajPSkIpZ9sA+1RsVV/9eLgHBPRk2ZRk5FNp9FbMLooWLuZS/joWu8Y0tE01dYaeRfn21nd0Ypblo1b13fgzGdw1xd1nlza9eOmAU/o/Zw8utk7/dw4BdQa+GqD0DnpAV2hXABCU5CCCFc5uCmHP744iA2m0JwtAlPX/v4iGJNJf+L30m1xcqM7jPoHNTZxZWK5uxIYRW3frKV9OJq/D10fHRrb3q1DnB1WedkKSnBdPgwHr17Azg/NJVlweJ/229f/iiEd3Pu9YVwMpmWSAghhNMpisK2xUdY+dkBrFYbvoFJZOydy+Ht67DYLMxaN4tqSzU9Q3pKFz3RoHaml3DVvA2kF1cTHeDOj/cObBKhSTGbyZoxk2O33U7ZokXOL8Bmg4X3gbEMInvDoAecX4MQTiYtTkIIIZzKZrWx5utkkjbkoCg2/IK2kZe6AYDCzHS0ZWmklaXhpfNi9pDZaNQy4ZBoGL/tz2XGN39itNjoGuXLx7f2IdjbzdVlnZfc//yH6m3bUHt6YujQwfkFbP8Y0laD1h0mfwAa+Ugpmr9G0eI0d+5c2rRpg8FgoF+/fmzduvWsx15++eWoVKrTfq688konViyEEOJi7V6VSdKGHMCMj+8qe2hSqRh2211cduNttPdvz08Tf+L1y18nwivC1eWKZuqzjUe558sdGC02RnQI4du7+jeZ0FT89deUfjsfVCoiXnsVt7g45xcROwJaDYArnoegWOdfXwgXuKivBz799FO8vLy45pprTtn+/fffU11dza233nre55o/fz4PPvgg77//Pv369WPOnDmMHj2a5ORkQkJCTjv+p59+wmQyOe4XFRXRrVu302oRQgjROHW9PIpjezMoTl9AwbE0tDo946b/m7h+Ax3HhHmGEebZdAbmi6bDZlN4edlBPlibBsCN/Vrx3MROaDWN4rvkc6ravJm8/7wIQPCDD+A9bJhrCgmIgdsWA417mnYh6tNFvUvMnj2boKDTp5sMCQnhxRdfvKBzvfHGG0yZMoXbb7+dhIQE3n//fTw8PPjkk0/OeHxAQABhYWGOn+XLl+Ph4SHBSQghGrGK4loUmwKA1VJL4dH/UZKThsHLm3888QJx/Qby2rbXWJu51sWViubMaLEyc/4uR2j6v9Hx/Cexc5MJTab0dLJm3g9WKz4TJxB4553OL6Is68RttQbUTeN3J0R9uKgWp/T0dNq2PX1dg9atW5Oenn7e5zGZTOzYsYNZs2Y5tqnVakaOHMmmTZvO6xwff/wx119/PZ6eZ14d22g0YjQaHffLy8sBMJvNmM3m8661oRyvoTHUIuqHPKfNjzynlyY3rZzf/ruf+H6h9J8cg0qro33/wSRvXMuk/3sS/4hIlhxewmdJn/HFgS/4efzPRHs3/OKd8rw2P3/3nJbVmLnv611sPVqCTqNidmInJnWPwGKxOLvMi1b8w49Yy8pw69KZoKeecn7tuXvQ/m8Mtl63YxvxrH0KcieQ12rz05ie0wupQaUoinKhF2jVqhXvvvsuEydOPGX7woULmTp1KpmZmed1nuzsbCIjI9m4cSMDBgxwbH/44YdZs2YNW7Zs+dvHb926lX79+rFlyxb69j3zYm/PPPMMzz777Gnbv/76azycPW2nEEK0MDW5Wop2G8CmQutjJrR/LSqNfVY9m9mERu9Gqa2UdyvepVapZajbUEa5j3J12aKZKTbCBwc05NaoMGgU7oi3Ee97wR9/XE9R8Nu4kYouXbD6+Dj10mqbiaHJT+NTm0W2b2+2tZ0OKummJ5q+6upqbrzxRsrKyvA5x+vqor4quOGGG5gxYwbe3t5cdtllAKxZs4aZM2dy/fXXX8wpL8rHH39Mly5dzhqaAGbNmsWDDz7ouF9eXk50dDRXXHHFOX85zmA2m1m+fDmjRo1Cp9O5uhxRD+Q5bX7kOb04+9Zks3HXYVDAJyAFtSqFK654Bp3biQH4VpuVe1fdS215LZ0COvHKFa+gUzvndyzPa/Nzpuc0Kaec/3zxJ/k1RkJ93Pjo5p50CPN2caUXRlEUVMdDiosmw1KvfBpNbRaKZzDBt3/JOM/Th2w0FHmtNj+N6Tk93hvtfFxUcHr++ec5evQoI0aMQKu1n8Jms3HLLbdc0BinoKAgNBoNeXl5p2zPy8sjLOzvBwVXVVXx7bff8txzz/3tcW5ubri5nT5Ljk6nc/kTdbLGVo+4dPKcNj/ynJ4fxaaw8efD7FqejqIo+AftJjd1FQApm9bRbdRYx7Ff7PuC7fnbcde68/LQl/Fwc35PAHlemwerTWHnkWJ2FKoIzKxgQGwIG1ILuffLHVSZrMSHevO/O/oQ7uvu6lIvSNkvv1D+++9Evvwy6rMMS2hwxzbC5nkAqCa8jc4v3CVlyGu1+WkMz+mFXP+igpNer2f+/Pm88MIL7Nq1C3d3d7p06ULr1q0v+Dy9evVi5cqVJCYmAvYAtnLlSqZNm/a3j/3+++8xGo3885//vJi/ghBCiAay6osDHNyUi6JY8Q3YSG7qNgAGXftPuo4c4zguqSiJd/58B4BH+z5Ka58L+z9EiOOW7cvh2UVJ5JTVAho+T9mOr7uOilozNgUGxATywS298DE0rQ/dNbt3k/PEkygmE6U//kjALbc4vwhjBfx8D6BAj39Ch3HOr0GIRuKSRvXFxcURd4lrBzz44IPceuut9O7dm759+zJnzhyqqqq4/fbbAbjllluIjIxk9uzZpzzu448/JjExkcDAwEu6vhBCiPrVunMQyVvS8fRYQX7aQdQaDaPumk7ny0eectyajDVYbBZGthrJ5NjJLqpWNHXL9uVw75c7+euIpbIa+4Dvvm38+eyOvui1TWv2N3NeHpnTpqOYTHiNGIG/q74o/u1xKD0Gvq1g9OxzHy9EM3ZRwenqq6+mb9++PPLII6dsf+WVV9i2bRvff//9eZ/ruuuuo6CggKeeeorc3Fy6d+/OsmXLCA0NBewz+Kn/MtVlcnIy69ev5/fff7+Y8oUQQtSzk8dghLZR4274haKMdHQGdyY+8Chtuvc67TH3dr+X9v7t6RXa68T4DSEugNWm8OyipNNC08kySmrQqJvWvy9bbS2ZU6dhKSjALS6OiJdfRuWqab9bD4KkhZA4DwyuHxsuhCtdVHBau3YtzzzzzGnbx44dy+uvv37B55s2bdpZu+atXr36tG3x8fFcxGSAQgghGkBBRgWrv0pm7N2d8fI3YDGbqCkvxdPPn8mPPkNo23ZnfeyI1iOcWKlobrYeKa7rnnd2OWW1bD1SzIB2TaOHiqIo5Dz2OLX79qHx8yPqvXlovFw0tgmg23UQP1ZCkxBc5AK4lZWV6PX607brdLoLmplCCCFE05aRVMzPr+8k/2g5G35MBcA/LIKrZj3LjS+8flpoKqwp5JG1j1BYU+iKckUzk1/+96HJcVzF+R3XGBR9+BHlS5aAVkvk22+hj4pyfhGKYh/bdJyEJiGAiwxOXbp0Yf78+adt//bbb0lISLjkooQQQjR+Bzfn8Ou7uzHXWvEJOEbbLlWOfWHt4vAJDjnleEVReGLDEyw5soRZ62b99XRCnDdFUfgjOZ+3V6Wc1/Eh3oYGrqj+ePbrizY4mLAnnsDzb5ZbaVD7foR3esEhGRIhxMkuqqvek08+yVVXXcXhw4cZPnw4ACtXruSbb765oPFNQgghmh5FUdix9BhbfklDURT8gg6Ql7qMZXMXE9zqLfzDI8/4uK8Pfs2GrA24adx4pM8jZzxGiL+jKAqrkwuYszKF3Rml5zxeBYT5GujbNqDBa6sv7t26EbP4VzSuWmuyPBsWPwi1ZZC9E9pf4Zo6hGiELio4TZgwgQULFvDiiy/yww8/4O7uTteuXVmxYgVDhw6t7xqFEEI0EjarjTXfHiJpXTaKYsMvaCt5qRsB6DpqLH6hZ17fJaUkhTe2vwHAg70eJNY/1mk1i6bveAvTnBUp7MksA8CgU3Nz/9bEhnjz6I977Med9Jjj00E8PSGh0U8OYSkuxpKbi6Gu147LQpOiwMKp9tAU0QOGPOSaOoRopC56OvIrr7ySK8+wevW+ffvo3LnzJRUlhBCicbKYbeSllaNgxsd3DXmpe0ClYtitU+g5duIZH2Oymnh03aOYbCYGRw7mhg43OLlq0VQpisLKA/YueccDk7tOw80DWjNlSAzB3vYF7n3dtSet42QX5mvg6QkJjOnsmsVaz5diMpE1YyY1+/YR+eYbeA8b5rpitn8Mh1eB1gCTPwBN01r3SoiGdknrOB1XUVHBN998w0cffcSOHTuwWq31cVohhBCNjN6gZcQtbVg050sKjqWh1ekZN/3fxPUbeNbHvLXzLQ6VHCLAEMDzg56XqcfFOSmKwooD+by18hD7suyTTrnrNNwyoDVTLoshyMvtlOPHdA5nVEIYm1Lz+X3dFq4Y0o8BsSGNvqVJURRyX/gP1du3o/b0RB8d7bpiig7D70/ab498BoLjXVeLEI3UJQWntWvX8tFHH/HTTz8RERHBVVddxdy5c+urNiGEEI1AaV412SmlJAyOACB502+UZKdh8PIm8f+eJLLD2ScFqrHUsDZzLQDPDXyOIPcgp9QsmiZFUVielMdbK1PYn20PTB56DbcMaMOUIW0J/EtgOplGraJf2wCKDij0axvQ6EMTQMnXX1P63XegUhHx+mu4xbqoC6vNCj/fA+ZqaDME+t7tmjqEaOQuODjl5ubyv//9j48//pjy8nKuvfZajEYjCxYskBn1hBCimclNK2Px3D3UVpkxeOqI6RHMgH/cQE15Gb3GJxIQ8fdTJbtr3Zk/fj6rMlYxNFrGwIozUxSF35PyeGtFCkk59sDkqddwy8A2TBkSQ4Dn6UugNHVVmzaR9+JsAEIeehDvyy93XTGWWgiIgfwD9oVuXbXYrhCN3AUFpwkTJrB27VquvPJK5syZw5gxY9BoNLz//vsNVZ8QQggXSdtVwO8f78dqtuEbWEJIGy8ANFoto+4686LlZ+Kh82B8zPiGKlM0YTZbXWBamcKBkwLTrQPbcGczDUwApmPHyLz/AbBa8Z00kYB//cu1Bek94aoPoCwTfF2wbpQQTcQFBaelS5cyY8YM7r33XuLi4hqqJiGEEC62549M1n13CBTwDUwl//CvbPo+h5FTpp7XGKUFqQuoNFVyY8cbUavk22txKptN4bf9uby1MoWDufaFVr3ctNw6sDV3Do7Bv5kGpuOKv/gSW1kZhm5dCXvuOdeN+7NaQK2B49eX0CTE37qg4LR+/Xo+/vhjevXqRceOHbn55pu5/vrrG6o2IYQQTqbYFDb9fJg/l6fXrdG0m7zUVfZ9KCiKDZVK87fnOFp2lBe3vEiNpYZA90DGth3rjNJFE2CzKSzbn8vbfwlMtw9qw78Gt8XPo3kHpuNCZz2KJsAfv3/8A7Xb2cdtNbg/XrB3z5vwNniHuq4OIZqICwpO/fv3p3///syZM4f58+fzySef8OCDD2Kz2Vi+fDnR0dF4e3s3VK1CCCEaWPqB4rrQZMU3YCN5qdsAGHTtP+l31XXn/GbcbDPz6LpHqbHU0C+sH6PbjHZG2aKRs9kUlu6zB6bkPHtg8q4LTHe0oMB0nEqjIfi++1xbRPpm2PAWKDbI3AYdpTutEOdyUf0nPD09ueOOO1i/fj179+7loYce4qWXXiIkJISJE8+8jocQQojGr3WnQLoOD8HL+zfy07ah1mgYfe/99L/6+vPqTvTervfYX7QfH70PLwx+QbrptXA2m8Kve7IZ89Zapn69k+S8CrzdtMwYEcf6R4bz4BXxLSY0lS1cSPbjj2MzmVxdChgr7bPoKTbodoOEJiHO0yWv4xQfH88rr7zC7NmzWbRoEZ988kl91CWEEMJJKopr0blpMHjqUBSFjD2fUpSRjM7gzsQHZ9GmW8/zOs/23O18tPcjAJ4a8BRhnmENWbZoxKw2hcV7c3hnZQop+ZUAeBu03DGoLXcMaouvR8taWLVm1y5ynngSxWzGvUtX/K+/zrUFLX8SSo6ATxSMecm1tQjRhNTLArgAGo2GxMREEhMT6+uUQgghGlhhZgW/vrMbn2B3Js7sjlanof/V17Piw3kkPvwkIW1izus85aZyHlv/GAoKibGJ0kWvhbLWtTC9syqV1LrA5GPQcsfgttw+qC2+7i0rMAGYc3PJmDYdxWzGa+QI/K69xrUFpa6A7XVfcifOBXc/l5YjRFNSb8FJCCFE05JxoJilH+zFXGtF7w7GagtaXw0xPfpwx5wP0OrPvwvVzrydFFQXEO0dzaN9H23AqkVjdDwwvb0yhcMFVYA9MN05JIbbBrXBx9DyAhOAraaGzPumYi0sxK19eyJffhmVK9dIqi6GhXVLCfS7B2Iud10tQjRBEpyEEKIFSt6cw6rPD2KzKfgEHKMidzXmmnbgGw5wQaEJ4PLoy/ly3JcAeOo8671e0ThZbQqLdmfz9qoU0uoCk6+7jjsHt+XWFhyYwL6ob87jj1OblITG35+oeXNRe7r4tVGRC1o3CIyFEU+7thYhmiAJTkII0YIoisKOZcfYsjCtbrrxA+SlLgNg94qlDP3nHRd97k5BneqrTNHIWaw2Fu3J5p2VqaQV2gOTn0ddYBrYBu8WHJiOK/rgA8qXLAWtlsi35qCPagRrJIUmwD0b7AFK7+HqaoRociQ4CSFEC7L11yNsX3wURbHhF7iNvNQNAPQaP5nLbrztgs5ltVl5bvNz3NjhRuID4hugWtHYWKw2Fu7K5t0/UjlyUmCaMiSGWwa0lsB0EkPnLqh9fAh56CE8+/Z1bTGKcmKRWzcvcIt1bT1CNFESnIQQogWJ6x3K3j+OoNOuIu/wHlCpGHbLnfQcN+mCz/Xxvo/5KeUn/kj/g9/+8RvuWvcGqFg0BharjQW7snl3VQpHi6oB8PfQMeWyGG4Z0AYvN/k48VdegwfRbukStIGBri1EUeC7W6DNYOgzBVw5xkqIJk7e6YQQopmzWW2oNfYPSwZPCwa3X8lLS0Gr0zN2+kO07zfogs+5t2Av83bNA+D/+vyfhKZmymK18fOfWbz7RyrH6gJTgKeeKUNiuHlAawlMf2EpLsZWXo6+TRsA14cmgJ2fwYFf4NBvEDcKAs5vpkwhxOnkHU8IIZqx0rxqlry3h8uub09UhwB0ejdUahUGL28S/+9JIjskXPA5q83VPLruUayKlTFtxjA+RhbPbG7MxwPTqlTSi08Eprsui+Hm/q3xlMB0GsVkInPGDIwpqUS987bru+cBFB+BZY/Zb494SkKTEJdI3vmEEKKZyk0rY/HcPdRWmdnwYyrXzuqDzmBg8sNPUVtVSUDExQ1Wf3nby6RXpBPmGcYT/Z9AdXzshGjyzFYbP+/M4p0/UsgorgEgsC4w/VMC01kpikLu8y9Qs30Hak9PtAEBri4JbFZYcC+Yq6D1YOh/n6srEqLJk3dAIYRohtJ2FfD7x/uxmm14+WUT2U6DSm3/BtzD1w8PX7+LOu/yY8v5KeUnVKh4cfCL+Lr51mPVwlXMVhs/7shk7upUR2AK8joRmDz08nHh75R89TWl338PKhWRb7yOW2wjmHxh07uQvgn0XpA4T8Y2CVEP5J1QCCGamb2rM1k3/xCKAj6BqRQc/pXCIzYi4+No26P3JZ176ZGlANzR+Q76hPWpj3KFC5ksNn7cmcncP1LJLDkRmO4Z2o6b+rXGXa9xcYWNX9WmTeTNng1AyL8fwmvoUBdXBOQlwaoX7LfHvAT+rV1bjxDNhAQnIYRoJhSbwuaFh9n5W3rdGk27yUtdBUCX4VfQumuPS77Gq5e9ysLDC5kQM+GSzyVcx2Sx8cMOe2DKKj0emNy4Z2iMBKYLYDp6lMz7HwCrFd9JEwm44+LXQatXWdvtXfXaj4Ee/3R1NUI0GxKchBCiuVBBVakJRbHiG7CRvNRtAAy89ib6X3V9vYxF0qg1XBV31SWfR7iGyWLj+x0ZzPvjsCMwBXu7cc/QdtzYt5UEpgtU+P4H2MrKcO/WjbDnnms84/163gKhncEn8sT6TUKISybBSQghmgmVSsWAq6LIPvgp+WkHUGs0jLprOp0vH3lJ500uTubHlB95oNcDMu14E2W0WPlueybv/ZFKdlktACHHA1O/Vhh0EpguRthzz6Lx9yfg9ttQu7m5upxTRfZ0dQVCNDsSnIQQogmrKK5l7x+Z9J/cDrVaRdbBvRQcPYDO4M7EB2fRptulfXiqtdTyyNpHOFx2GBUqZvWbVU+VC2cwWqx8ty2DeasPk3NSYLrv8nZc31cC06VS6/WEPvKwq8uwM1XDz3fD0EcgrLOrqxGiWZLgJIQQTVRhZiW/vrOLqjITGp2afhNjaN9vEMNunUJUQhdC2lz6mi1v7HiDw2WHCXIP4u5ud9dD1cIZas1Wvttu75KXW24PTGE+Bu69vB3X9YmWwHQJyhYuxJiSQvADD6DSNKLf44qn7Qvd5u6BaTtAIx/xhKhv8qoSQogmKONgMUvf34u51oqHTyFtu3Zy7Os5blK9XGNt5lq+OfgNAM8Pep4AQyNYm0b8rVqzlfnbMpi3OpW8ciNgD0z3DWvHtb0lMF2qml27yHniSRSzGbe4OHwn1c9r7ZIdXgVb/2u/Pf5NCU1CNBB5ZQkhRBOTvDmHVZ8fxGZT8PY/RtGxhSz/7yaufWo2OoOhXq5RVFPEkxueBOCfHf/J4MjB9XJe0TBqzVa+3ZrOe2sOOwJTuK+B+y5vx7V9onHTSmC6VObcXDKmTUcxm/EaOQKfCY1kZsmaElgw1X67zxRoN9y19QjRjElwEkKIJkJRFHYsO8aWhWl1040fIC91GQA+IWGo6mmBS0VReGrjUxTXFhPrF8v9ve6vl/OK+ldrtvL1lnTeX3OY/Ap7YIrwNXDvsFiu7R0lgame2GpqyLxvKtbCQtzatyfy5Zfr7fV2yZY+AhXZENAORj3r6mqEaNYkOAkhRBNRXljD9iVHURQbfoHbyEvdAECv8ZMZetPt9fZBLrMyk135u9Cr9bx82cu4aRrZbGGCWrOVr+oCU8FJgWnq8Fj+0UsCU31SFIWcxx+nNikJjb8/UfPmofb0dHVZdkkLYc98UKlh8gegbyR1CdFMSXASQogmwjfYg+E3x7L5x3nkHd4DKhXDbrmz3sY0HRftHc1PE39iX+E+2vu3r9dzi0tTY7Ly1ZZjfLA2zRGYIv3cmTrMHpj02kbSCtKMFL3/PuVLloJWS9Tbb6GPinR1SSfs+c7+5+AHIbqPa2sRogWQ4CSEEI1YdbmJmgoTgZFeAKRu+ZaCo3vQ6HSMm/5v2vcb1CDXDfUMJdQztEHOLS7c8cD0/po0CitPBKZpw2O5uqcEpoakb90alZsboY8/hkefRhZOrv0cdn0FXa93dSVCtAgSnIQQopEqzatm0bu7sRitXP1IL3wC3Rl47T/JO3KYK+6aTmSHhHq93od7PiTWL5ZhrYbV63nFxas2Wfhy8zH+uzaNwkoTAFH+7kwbFstVEpicwmfcONx79EAXHu7qUk6n1kDPW1xdhRAthgQnIYRohHLTylg8bw+1lWa8/G3YLAoAfqFh3Prau6jV9TuGZUvOFt758x0UFH6Y8APxAfH1en5xYapNFr7YZA9MRVX2wBQd4M70YXFM7hmJTiOBqSFZiotRzBZ0oSEAjSs0lRyDbR/BsMdA5+7qaoRoUSQ4CSFEI5O2q4DlH+/HYrbh5ZdNWdYCijJ98AvtC1DvoanMWMZj6x9DQeEf7f8hocmFqowWvth8jA9PCkytAjyYNjyWyT0kMDmDYjKROWMG5mPpRL33Hu6dO537Qc5is8GCe+HYBqgqhMnvuboiIVoUCU5CCNGI7F2dybr5h1AU8AlMpeDwryiKjaQ1K2nXq2+9X09RFJ7d9Cz51fm08WnD//X+v3q/hji3KqOFzzcd48N1aRTXBabWgR5MGxZLogQmp1EUhdznn6dm+w7UXl6oPRpZi87mefbQpPOEoQ+7uhohWhwJTkII0Ugc2JjD2m8P1a3RtJu81FUAdBl+BSPvnNog11x4eCHLjy1Hq9Ly0mUv4aHzaJDriDOrNFr4fNNRPlybRkm1GYA2gR5MGx5HYvcItBKYnKrkiy8p/f4HUKuJfON13GJiXF3SCfkHYOVz9ttjXoSAtq6tR4gWSIKTEEI0Eu16BrNn1TFMVb+Rl7oNgIHX3kT/q65HpVLV+/UyyjOYvWU2AFN7TKVTYCPqktTMVdSaHS1MpXWBqW2QJ9OGxTJJApNLVG7YQN5LLwEQ8u9/43XZZS6u6CRWM/x8N1iNEDsKet7q6oqEaJEkOAkhhAuZTVa0OjUqlQq12obK+gv5aXtQazSMums6nS8f2WDXXpm+kmpLNb1Ce3F7p9sb7DrihIpaM59tPMpH6484AlNMkCfTR8QyoasEJlcxHT1K1gMPgs2G7+TJBNx+m6tLOtXaVyFnN7j7w6R3oQG+SBFCnJsEJyGEcJGK4lp+fXc37fuG0mtMGzQ6HQGRUeSmpTDxgUdp071Xg17/ts63Ee0dTUJgApp6nnBCnKq81sxnG+yBqaymLjAFezJjeBwTukWgUcsHYVfKf+NNbOXluHfvTtizzzRIC+9Fqy23z6IHcOUb4B3m2nqEaMEkOAkhhAsUZlby67u7qSo1svePTDoPjcLNXcvw2++m57hJBEREOqWOEa1HOOU6LVV5rZn/bTjKR+vSKK+1ANAu2JMZI+IY31UCU2MR/uKLaPz8CJ4+DbVe7+pyTmXwgbvXwb4foPNVrq5GiBZNgpMQQjhZxsFilr2/F1OtFQ+ffHwD09Dq+gOg1mgaNDRVmip5edvLTO8xnRCPkAa7TktXVmMPTB+vPxGYYkO8mD48VgJTI6Tx8iT8uWddXcbZ+UbCoJmurkKIFk+CkxBCOFHyllxWfX4Am1XB2/8YRccWUnzMws4lC+kz8eoGv/6LW15kUdoiUktS+frKrxtXl6RmoKzGzCfrj/DJhiNU1AWmuBAvZoyIY1yXcAlMjUjpzwuwFhcRcMcdjfN1cHQ9mKqh/RWurkQIUUeCkxBCOMnO346x6efDddONHyAvdRkA7QcMoceYCQ1+/aVHlrIobRFqlZqH+z7cOD8sNlFl1WY+3nCET08KTO1D6wJT53DUEpgaleqdf5L71FMoZjO6qGh8RjeycFJbBj/dDeWZcNVH0PUaV1ckhECCkxBCOI3eXYui2PAL3EZe6gYAeo2fzNCbbkelbtjZ1HIqc3h+0/MA3NX1LnqE9GjQ67UUpdUmPll/hE83HKXCaA9M8aHezBgRx9jOYRKYGiFzTg6ZM2agmM14jxqJ96iGm7nyoi191B6a/NtA/FhXVyOEqCPBSQghnCS+byD7Vq0l68BOUKkYdsud9Bw3qcGva7VZmbV+FhXmCroGdeXurnc3+DWbu9JqEx/XBabKusDUIcwemMZ0ksDUWNmqq8mYOhVrYSFu8fFEvPRSg39pccEOLILdXwMqmPwBuHm5uiIhRB0JTkII0UCqy02s/z6FIdfF4e6lpyw/l/wjB9Dq9Iyd/hDt+w1ySh2f7v+UHXk78NB68NKQl9Cq5a3/YpVUmfhofRqfbTx2SmCaOSKO0RKYGjVFUch+7HGMSQfQBAQQPW8uak9PV5d1qsoCWHS//fagmdCqv0vLEUKcSv73FEKIBlCaV82id3dTXlCDudbClVO7EdSqDRMfegyd3o3IDglOqcNsM7M4bTEAj/Z9lGifaKdct7kprjLx0bo0Ptt4lCqTFYCO4T7MHBHHFQmhEpiagMJ586hYtgx0OqLefgtdpHOm/D9vigKLZkJ1IYR0gmGPuboiIcRfSHASQoh6lptWxuJ5e6itNOPuVURsL3/HvjZdnTu2SKfW8dW4r1h8ZDGJsYlOvXZzUFxl4sN1aXx+UmBKCPdh5sg4RnWUwNSUaPz9QaMh7Kkn8ejd29XlnO7YRkheDGodXPUBaN1cXZEQ4i8kOAkhRD06sruA3z/aj8Vsw8s3m9Lsn1jxkYHQNq/jFxbukpo8dB5c015m5boQRZVGPlx3hM83HaW6LjB1irC3MI1KCJUZCZuggBtvxGvgQPRt2ri6lDNrMwiu+xIqciGsi6urEUKcgQQnIYSoJ/vWZLL220MoCvgEpFKQ9iuKYiM6oQsefn5OreWP9D9Ir0jn5oSbUasa2eD3Rqyo0sh/16XxxaZjjsDUOdKHmSPaM7JjiASmJsZSXIxKo0Hj6wvQeEPTcR0bflkCIcTFk+AkhBD1wGy08ufydGw2Bb+g3eSlrgKgy/ArGHnnVNQajdNqKagu4KmNT1FqLMVd68618dc67dpNVWGlkf+utQemGrM9MHWJ9OX+kXEM7yCBqakofv8D4ubOpTg9g5C77yJz+gyshYVEvfcebjFtXV3emSUvg4ge4B3q6kqEEOcgwUkIIeqBzk3DuHs7s3Tum+SlbgNg4LU30f+q6536odum2HhiwxOUGkvpENBBxjWdQ7kJXlqWzNdbMx2BqWuUPTANi5fA1JQUzJtH8dy5qIDiuXOp+uMPjElJqL29AcXV5Z1ZwSH4/lbQecCUlRAQ4+qKhBB/Q4KTEEJcJGONhdzDZbTuHAhAyuYl5B3ehlqjYdRd0+l8ufMX1vz6wNdszN6Im8aNl4e8jF6jd3oNTUF+RS3v/5HKF39qMNuOAdAt2o/7R8RxeXywBKYmpmDePArffueUbcakJFCpiHzjddxiGmEgsZrh57vAUgutB4F/I20RE0I4SHASQoiLUFlSy6/v7qYkp5orp3alVadAek2YTFZyEr3GTaJN915Or+lQySHe3PEmAP/u/W9i/Brhh0UXy6+o5YM1aXy5+RhGiw1Q0S3Kl/tHtefy9hKYmqIzhSYHRaFm7168hgxxblHnY90bkP0nGHxh0rsg//aEaPQkOAkhxBlUFNdSW2kGwGKxYCpTU5hRiVarpTS/mnXfpVBTbsLNowaDlw4And6Nq2Y965IP30arkUfWPoLJZuKyqMu4Lv46p9fQmOWX1/L+mjS+2nI8MEH3aF/6exbz4I190eulZa4p+tvQVOf4/uD77nNGSecnayesfcV+e9zr4BPh2nqEEOdFgpMQQvyF1Wzj+9nbqKkwn7TVk582/nnKce5eeVQW/MShTcWEtL4ZwGUtFrvyd3G0/CgBhgCeG/ictJzUySuv5b3Vh/lma7ojMPVs5cf9I9vTv40vS5culd9VE3U+oem4RhWezDXw8z1gs0BCInT5h6srEkKcJ5fPUTt37lzatGmDwWCgX79+bN269W+PLy0tZerUqYSHh+Pm5kb79u1ZsmSJk6oVQrQEaq0K7wAD/N3naVsyZdnzMdfWkHlgH1aL+W8Obnj9wvvx1bivePWyVwl0D3RpLY1Bblktz/yynyGv/MH/Nh7FaLHRq7U/X/yrLz/eO5DLpFtek6YoynmHpuMK33m3gaq5QBvfgcJk8AqFK9+QLnpCNCEubXGaP38+Dz74IO+//z79+vVjzpw5jB49muTkZEJCQk473mQyMWrUKEJCQvjhhx+IjIzk2LFj+Dl5fRQhRPOmUqnoNzGGRe/sPm2foihYjduw1KwHoP2AIYy97wE0Wp2zyzxNQmCCq0twudyyWt5bnco32zIw1bUw9W7tz/0j2zMoNlDCUhNnzsuj7JdfKFu48IIfGzR9WgNUdBH63wcVOdB+DHjKlxxCNCUuDU5vvPEGU6ZM4fbbbwfg/fffZ/HixXzyySc8+uijpx3/ySefUFxczMaNG9Hp7B9S2jT2xeyEEE1Sxv5l6N1yMJt6otTNZKwoNiw1f2A12gNVr/GTGXrT7ajUrmm8VxSFV7e/yoSYCXQM7OiSGhqLnLIa3lt9mG+3ZmCy2gNT3zYBzBwZx8B2EpiaMkVRKP91MWULFlC1aRPY7M+vys0NfatWGFNSznmOoBnTG0c3PQA3Lxj/pqurEEJcBJcFJ5PJxI4dO5g1a5Zjm1qtZuTIkWzatOmMj/nll18YMGAAU6dOZeHChQQHB3PjjTfyyCOPoDnL4pJGoxGj0ei4X15eDoDZbMZsdm3XmuN1nPynaPrkOW36ygpqyDxYQnnuarQGE1r3/iiKgrlqMTaz/UNa6649GXT9LVisVrBaXVLnj6k/8kXSF/x46EeWJC7BR+/jkjpcKaeslvfXpvH9jizMVnvC7dPGnxnD2tGvrT8qlQqLxXLGx8prteko+uRjjAcOAmDo2QPviRPxuuIKNN7eFL//AcVz5571sQFTp+I3ZYrLn2fVkTUobYaAyuWjJJocea02P43pOb2QGlwWnAoLC7FarYSGnrpSdmhoKAcPHjzjY9LS0li1ahU33XQTS5YsITU1lfvuuw+z2czTTz99xsfMnj2bZ5999rTtv//+Ox4eHpf+F6kny5cvd3UJop7Jc9q0KDaoydNSlaHDWKQFEtAayrHUbgQUtO4DOL6IpmerGHSde7l0fGWBtYB5FfMAGKodyvoV611WiysUG2FFlprN+Sqsir01KdZHYUyUjTjfAooPFrD0zP+VnEZeq42HrqgIn5078dq3j4x778VmMADg3bUr+ohIynv1xBxY171t3Tr7n62iCRg1iqAzPI+Fo0ZxqFU0uHgsdGjZn/RPe5N8785sbvcQiurMX/aKvyev1eanMTyn1dXV531sk5pVz2azERISwn//+180Gg29evUiKyuLV1999azBadasWTz44IOO++Xl5URHR3PFFVfg4+P6b2fNZjPLly9n1KhRju6HommT57RpMdVY2PlbBoe25FFbaUZRLCjWTLwDitH4V1BbEU15wSYstVsBK7H9xzBu2t0urdlsNXPb8tswY6ZfWD+eH/Y86hbyLXZWaQ3vrz3Cj7tPtDD1a+vP9GHt6Nc24ILOJa/VxsFaUUHl779T8csianfudGwfqFLhM26c/c7xP89m3LjTWp4Cpk4l9h7XvlYBqCpE++FDAAR2GsrYERNcXFDTI6/V5qcxPafHe6OdD5cFp6CgIDQaDXl5eadsz8vLIyws7IyPCQ8PR6fTndItr2PHjuTm5mIymc64Doebmxtubm6nbdfpdC5/ok7W2OoRl06e08ZLURTHmBe1SkPSmo3UlB9FRQ5Wcy6KzUpRhf1Yg5c3qDSgWEGlYeL9U10+XmbunrkcKD6Ar5sv/xn8H9z0p7/HNTeZJdXM/eMwP+zIcASmge0CmTkijn4xlzbAXl6rrmFKT6fgrbepWLEC5XiXepUKz4ED8U1MxHvkCNQX8LyE1k3+UDR3LoFTpzruu5SiwG//B1UFENwRzYin0Mi/tYsmr9XmpzE8pxdyfZcFJ71eT69evVi5ciWJiYmAvUVp5cqVTJt25je7QYMG8fXXX2Oz2VDXDcY+dOgQ4eHhsnihEOKcinMq+fO3PWQeSOWW2TejUqvQaNXo9XuoNB5yHOfp509kfAKRHRIoyspkz4qlgD08bf7pWwZcfYPL/g7bcrfxyb5PAHhmwDOEeoae4xFNW0ZxNfNWp/L99kwsNntgGhQbyMwR7el7gS1MwvVsNTWo3d0BUOn1lC9ZAoqCvl07fBMn4TtxIrrQi/83HXDP3WxuFU3cuVqonGXPd3BgEai1cNUHoDO4uiIhxCVwaVe9Bx98kFtvvZXevXvTt29f5syZQ1VVlWOWvVtuuYXIyEhmz54NwL333su7777LzJkzmT59OikpKbz44ovMmDHDlX8NIUQjZbVYyElNZe8fWzm2ex9VpUdBsfdlTvtzGO16RQPQ/YphFKS3rQtLnfANCUWlUrHpx2/Ys2Ip/a++gUI3L4KMlWz87isAl4WnBakLUFCYHDuZka1HuqQGZ8gormbuH6n8sONEYBocG8TMkXH0aSOBqSmxlJQ4ZsXT+PvT6qMPAdCFhRH62GO4d++OoXMnl7fk1ruyLFjyf/bblz8K4d1cW48Q4pK5NDhdd911FBQU8NRTT5Gbm0v37t1ZtmyZY8KI9PR0R8sSQHR0NL/99hsPPPAAXbt2JTIykpkzZ/LII4+46q8ghGik1n7zDTsWfY/Najplu0qlISAyBt8QxbGtx5jTxxxs+vEbNn73FQOvvYneE//BkiVL6Dv5WtQatUvD0/ODnqdbcDfGx4x3+rWdIb2omnf/SOGnnVmOwDQkLoiZI+LoLYGpyVBMJirWrKFswUIq16yBupkNVXo91vJyNHVjjANu/qcry2xYi2aCsQwie8OgB1xdjRCiHrh8cohp06adtWve6tWrT9s2YMAANm/e3MBVCSGagoqiQrKSk8g6mERWchJj7r2fkDYxAKhU7vbQpHJD5x5NVIcEugzvQ9tundCeR9dexWZj4LU3MeDqG06ZqvR4WFLq1pJxNrVKzbXx17rk2g3pWFEV765K5ac/s7CeFJjuHxlHr9YSmJqS4i++pHDuXKylpY5thk6d8J00CZ/xVzpCU7M37DGoLoTJH4DG5R+3hBD1QF7JQogmo7K4iNTtW8hOtgel8oL8U/av/24NVz1sD069xg2jujyA9v070LpzMGr1hXUDGnjNTWfd5+yWpsyKTL488CUzeszAQ9d4llGoD0cLq3j3j1R+PikwDW0fzIwRcfRq7e/i6sT5MOflofbwQOPtDYDKTY+1tBRtcDA+EyfgO2kShvbtXVylC0T2hCl/QHPrgihECybBSQjRKJlNRvJSU3D39SUw0j4WqSgzg5UfzztxkEqF3j0cqzUUtTaS/IxQzEYrOjcNHj6+jL7rMhdVX38sNguz1s1iV8EuKkwV/Gfwf1xdUr04UmhvYVqw60Rgujw+mJkj4ujRSgJTY2erqaFixQrKFiykatMmQmfNcnS78xk7Fl14OJ4DBqDStrCPGVYLFB+G4Hj7fQlNQjQrLewdTQjRWFWXl5GdfKCu691+8tIOY7Na6DV+Mpff/C8AwtvHE9G+C6jDKSvww2INQaXSo1GraNs1iITBEWh0zWs9ow/3fsiugl146by4r/t9ri7nkqUVVDoCU11eYlh8MDNHtqd7tJ9LaxN/T7HZqNmxg9IFC6hY9hu2qirHPmNqquO2xtsbryFDXFGi6214E1a/DKP/A/0awRpSQoh6JcFJCOFSNRXlfPPUw5RkZ562z9M/AN1J45H0Bnfa9r6THUuPAeATZCBhcAQdB4bj6dv81jLaXbCbD3Z/AMDj/R8n0ivSxRVdvMN1gWnhSYFpeIcQZo6Io5sEpkZPMZtJmzgJ05Ejjm26qCh8J03Cd9JE9K1aubC6RiJnN6x+CWwWMPi5uhohRAOQ4CSEaHBWi4X8o4fJOphEdvIB3L19GHWXfVIYg5c3pmr7N9eBUa2I7JDgWEPJZPTkwIYcMpKKiU6wTxCQMCiCktxqOg2JILpDAKoLHLvUVFSZq3h07aNYFStj245tsrPopeZX8u6qFH7Zne0ITCM7hjBjRBxdo/xcWps4O2tFBdVbt+I9YgQAKp0OfbsYLPn5eI8dg19iIu49e6JSN68W3otmroWf77GHpo4ToGvzm8BFCCHBSQjRQI7t3UVm0l6yDiaRk3oIi8no2Ofh68fIKVNRqVSoVComP/I0PiGhuHt5YzZaSdmex/JPM8k7Ug5ARXGtIzj5BLkz9u4uLvk7OdNLW18iszKTcM9wnuj/hKvLuWCp+ZW8UxeYFEdgCmXmiDi6RPm6tjhxRorVStXGjZQtWEjFihUoRiPtfv/N0ZoU9sQTaHx9HQvYipP88R/ITwLPYBg/R8Y2CdFMSXASQlyy8sICCjOOEtOjj2Pbhu++JOfQQcd9g5c3EfEd7a1J8QmgKI4PF6ExsRRkVLD1l2QObc3FVGsFQK1W0bZ7EJ2GNN0uahejsKaQ1RmrUaHixcEv4qNvOtM3p+ZX8PbKVBbtORGYRiXYA1PnSAlMjZExJYXSBQso/2URloICx3Z9u3ZYCgocwUkXFuaqEhu3Yxth4zv22xPeBs8g19YjhGgwEpyEEBfEZrNSlJHuWDspKzmJisICVCo10z79Fr27fbrs9n0H4h8WUdf1rhMBEZF/261n1ecHKMyoBMAn2J1OgyPoMCAcD59zr7nU3AS5B/HjxB/ZnLOZ3mG9XV3OeTmUV8HbK1NYvDfHEZiuSAhlhgSmRq1i9Woy77nXcV/j54fPlVfim5iIoXMnVNJy8veMlfYueijQ45/QYZyrKxJCNCAJTkKI87Z14Q9sXfA9xuqqU7ar1GpC2rSjsqSEgLrg1HvCVWc9T0F6BQc2ZNN/cjv0BvvbUJehUWQcKCZhSARR7f2b7dil8xXiEcLEdhNdXcY5JedW8PaqFJacFJhGd7IHpk4REpgaE8VkomLNGgB8Ro0CwLNfPzR+frj37oXvpEl4Dx2K6jwWiBZ1dO7Q507Y+RmMnu3qaoQQDUyCkxDiFNXlZXVTgieRnZzE6HtmEhhl76qjd/fAWF2FzuBORPsORMYnEBHfkfC4ePSGvx/3YKq1kLItj/3rsilIrwAgMMrL0Q0vYXAECYMjGvYv18jNPzifIPcgRrQe4epSzik590QL03FjO4cxfXgcCRFNp2thc6coCrX79lH28wLKFy/GWlaGPiYG75EjUalUqN3diV39B2qDwdWlNk1qDQyaAf3uAa0ETiGaOwlOQrRwVaUlHPlzuyMsleRknbI/88B+R3Bq328g4XHxBLdqg1qjOa/z5x8rZ//6bFK25mE21o1d0qho1yOYoCjv+v3LNGEHig7w0raXsNgsfDr600bbRe9gbjlvr0xhyd5cx7ZxXeyBqWO4BKbGwpyXR9kvv1C2YCGmw4cd27UhIXgPH4ZiNjtaliQ0XYSaUtDoQW9vYZfQJETLIMFJiBbEajGTfyQNd28f/MLCAcg/msZv7791ynFB0a3tEzl06ESrzt0c2z18/fDw9Tvv61WVGfnhpe2OLlx+oR4kDI6gQ/8w3L3lg8ZxNZYaHln3CBabheHRw+kV2svVJZ3mQI49MC3dZw9MKhWM6xzO9BGxdAiTwNTY5L/6GuW//gqAys0N75Ej8U1MxHPgAFTn+aWHOAtFgUUzIW8/XP0RRHR3dUVCCCeR4CREM2asriL70EFHt7vj04L3mfQPLrvxNgB7l7sOnRzrJ4W374C714W3BCmKQkF6BTmpZXQbEQ2Ap68bMd2DUWvVdBoSQUScnww2P4PXt7/OkbIjBLsH88zAZxrV72h/dhlvr0zht/15QF1g6hLOjOFxxIdJi6GrKTYbNTt2ULpgAQG33oqhfXsAfCcnYs7JwTdxEj5jxqDxlueq3uz7EZIWgFoLKK6uRgjhRBKchGiGqsvL+OH5xynIOIajuaeOwdvnlA/mbh6eXP/syxd9LVONhUPb8ti/Lss+K54K2nYPwifQPuZp9F2dG1UQaGzWZKxhfvJ8AF4Y/AL+Bn8XV2S3L8semH5POhGYxneNYPrwWNqHyodwVzOlp1O2YCFlv/yCOTMTAI2PL4ZHHgbAa9AgvAYNcmWJzVN5Nix+0H77sochoodr6xFCOJUEJyGaKJvNSmH6MbKTD5CVnIS7jw/Db7sbAHdvHypKikFR8AsLr5vEIYHIDgkERERdcpBRFIX8oxXsX59FyrY8LCYbABqtmna9glFsJ8KahKazK6wp5KmNTwFwc8LNDIwY6OKK7IHprZUpLD8pME2oC0xxEphcymYyUbZgAWULf6Fmxw7HdrWnJ95jx+Az+goXVtcCKAosnAa1ZfbANORBV1ckhHAyCU5CNCGZSfvIPLCPrEMHyE4+gKmm2rHPyz+AYbfehUqlQqVSkfh/T+IXGoanX/23YKTuyOf3j/Y77vuHedBpSCTx/cMweOrq/XrN1cpjKymuLaa9f3tm9pzp0lr2ZtoD04oDJwLTxG72wBQbIoGpMVABBW/OwVpSAmo1ngMH2qcQHzkCtfvfz2op6sH2T+DwStAaYPIHoJH3OiFaGglOQjRS1WWlFGYcO2VyhtVffExeWorjvt7dnfC4Do7xSSeLjO9YL3UoikLekXIsJitRHQIAaN05EHdvHa06BdJpcARh7XylZekiXNfhOoI9gon2jsZN4+aSGvZklvLWihRWHswHQF0XmKYNjyM2xMslNQkwpqRQumABNTt20vrrr1Cp1aj0egKnTAGbFZ8JE9CFhrq6zJaj6DD8/oT99shnIDjepeUIIVxDgpMQjYCiKJTkZJF1MImsZPtEDiU52ag1GqZ9Oh+dm3264Ng+/fELDbMHpQ6dCGrVGrW6YWbIMlabSd6SR9L6LIqyqgiM9OK6J/qgUqnQG7Tc+uIgNDp1g1y7JRnearhLrrs7o5S3Vqaw6qTANKl7JNOGx9IuWAKTK1iKiyn/dTFlCxdSu/9Ei271tu149usLQOAdt7uqvJZN5w7R/cBmgb53u7oaIYSLSHASwsW2LfqJbQt/oKai/NQdKhUBEVFUFhfhH25fJLb/Vdc1aC2KopCbVk7SuixSd+RjMdeNXdKpCY72wmK2odNrHNvEhTNZTbyy7RWmdJlCqKfzWwx2ZZTy1opD/JFcANgDU2KPSKYNiyVGApNL1OzfT+G896hcswYsFvtGrRavy4fiO2kSHj26u7Q+AfhEwM0/g7Ec1PLeJ0RLJcFJCCeoraok59BB+yKzyUlccfcM/MMiANBoNNRUlKPV6QmLbU9khwQi4jsSEdcRg5dzP8iu+y6FvX9kOu4HRHjSaUgk7fuGytilevL2zreZnzyfLTlbWDBpAZoGajH8qz/TS3hrZQqr6wKTRq0isa6FqW2Qp1NqEHaKoqAYjY6FZxWjkcqVKwEwdOqEb2IiPuOvROvfOGZYbNFMVaCve32oVGDwdW09QgiXkuAkRAOw1taQvHEtuSnJZCUnUfiXacGzDux3BKf2/QcTFtuekLaxaHXOCyeKopBzuAyfQANe/vYPcK0SAjiwPpvYPqF0GhxBaFsfGbtUjzZlb+KzpM8AeKj3Q04JTTvTS3hrRQprDp0ITJPrWpjaSGByKnNuLmW/LKJs4UI8+vQm/JlnAHDv0YPg++/He8Rw3OLiXFukOMFihI+vgMieMPpFcJNJUoRo6SQ4CXGJjk8LbvD0wic4BIDawnx+++nLU46zTwtuX2i2ddcTa394BQTiFRDotHprq8wkb85l/7osSnKr6TmmNQMS2wHQqlMgt708CDcPaV2qb6W1pTyx3j64/Nr213J59OUNer0dx0qYs+IQ61IKAXtguqqHvYWpdaAEJmexVVdTsWIFZQsWULVps+MLFFtFBcqTT6LSaFCpVATdI+NmGp0/XoS8fVCRC8OfkuAkhJDgJMSFMtfWkpN6iOy6bnfZhw5iqqmm/1XXMei6mwEwBIcS2i6OqA4JRMZ3IiK+Y4NMC36+FEUhJ7WU/euyObyzAKvFPnZJq1efsuaSWq2S0NQAFEXhmU3PkF+TTxufNvy7z78b7Frbjxbz1soUR2DSqlVc3TOKqcNiaRXo0WDXFafLf3MOJV98ga36xLIB7r174ZeYiPeYMag0zummKS5C+mbY+Lb99oQ54BXs0nKEEI2DBCchzlN1WSk/v/Ic+UcOY7NaT9mnd3fHenxQN6BxM3Dds6+gc2LXu7NRFIUfX9lB3pETk08ERXvRaXAEcX3DcHOXt4GG9nPqz6xMX4lWreXly17GXVv/a+5sO1rMWytSWJ96IjD9o5c9MEUHSGByBlN6OrrwcFTHX/eKgq26Gl1UFL6TJuGbOAl9dLRrixTnZqyEn+8BxQbdboSOE1xdkRCikZBPTEKcRFEUirMzyU4+QNbBJDz8/LjsxtsAcPf2oSQnC5vVildgEJHxCY71kxpyWvALdXxmvLAYH8diuCGtvCnKrqJ9n1A6DYkguJW3jF1yEpti4/vk7wGY3mM6CYEJ53jEhdl6pJi3Vh5iQ2oRYA9M1/SO4r7LJTA5g7WigvKlSylb+As1O3YQ9d48vIcNA8D/+uvwGjIY9169UMlMbE3H8ieh5Aj4RMHYl1xdjRCiEZHgJFq8nJRkMg/uJ+ugff2kk6cF9wkOdQQnlVrNxAcfxy8sDJ+gEBdVe3Y1FSYObsolaUM2pXnVXPXvnoTH+gHQ+8q29E9sh15al5xOrVLzyZhP+OHQD9zY4cZ6O++WtCLmrEhhU5o9MOk0Kv7RK5r7Lm8ngamBKRYLVZs2UfbzAipWrkQxGu071GqMycmO4KSLiEAXEeHCSsUFS1kB2z+x306cK7PoCSFOIZ+iRItSW1VJUWYGkfEdHduWf/guBceOOO5rdXrC4trbW5TiE1AUxdE606pzV6fX/HcUm0LWoRL2r88m7c8CbFb7eCWdm4bS/BpHcPLw0buwSuGudefmhJvr5Vyb04qYs+IQm9OKAXtguqa3PTBF+UtgamiWoiKOJE7GUlDg2KaPbYdfYiI+EyagC3X+2lyiHik28AiELtdAzOWurkYI0chIcBLNlqIoVBQWkHVwf936SQcozDiGRqNh2qffodXbw0S7Xn3xDQklMj6BiPgEQmPaodG6fmzSuVQU17LwzT8pK6hxbAtp7U3C4Aji+oSiN8jL25V25O1gT8Eebu10K2rVpXfT2nTYHpi2HDkRmK7rE829l8cS6Vf/Y6aEnaW4mNr9SXgNGQyANjAQTUAAitmMz/jx+CYmYuiUIF1fm4v2V8B9m0Evi0ELIU4nn6xEs7T915/ZsXgBlcVFp+3zCQ6horjQsY7S8ZnwGjvFplBeVINvsL1VwcvPDUVR0Bk0tO8bRqfB9rFLwvUqTBU8tu4xsquyUVC4o/MdF3UeRVHYVNclb2tdYNJr1HWBqR0REpgahM1konL1asoWLKRy7VpUOh3t169D7Wmfxj3q3XfQhYai0ktLbrNhs8Lxcapeja8rthCicZDgJJos+7Tg9gVmsw4mMWrKNHxD7N1kFEWhsrgItUZDaNtYIuI7EtkhgYj2rp0W/GJUl5s4uCmH/euzMdVYuG32IDQ6NSq1irH3dMEnyF1alxqZ/2z5D9lV2UR5RXFt+2sv+PGKorDxcBFvrUhh69ETgen6vvbAFO4rgam+KYpC7d69lC1YSPnixVjLyhz73OLjMefl4RYTAyAz4zU35Tnw2XgY/iR0SnR1NUKIRkw+bYkmo6aygoz9exyTOOQdOYxiszn2ZyUnOYJT/IDBhMXEEhbbHp2bwVUlXzTFppB5sIT967M4sqsQW91aS3qDhqLsSkJa+wAQFCUtTI3N4rTFLE5bjEalYfaQ2XhdQJcfRVHYkFrEWysPse1oCQB6rZob+kRzjwSmBlXy9dfkPf+C4742JATfiRPwnTQJt7g4F1YmGpSiwC/ToSgV1r8BHcaDRj4aCSHOTN4dRKOkKArFWZnoPdzxDggCIGP/Hha9MfuU47wDgx1Tgkd16OTY7hMU0ihnvjsfGQeKWf3VQcoLax3bQtv60GlIBLG9QtG5NY5pz8XpsiqzeGGz/cP33V3vpntI9/N6nKIorE8tZM6KFHYcOxGYbuzbinuGtiPMt+mF/8bMVl1NxcqVqAKDHNu8hw0j/7XX8R4+HN/ERDwHDpAFaluCnZ9B6nLQuMHkDyQ0CSH+lrxDiEbBYjaTl5bqmMgh+9BBaivKGXjtTQy4+gYAIuMTCG7VhogOnerCUscmG45OptgUjDUWDJ72CSk8fd0oL6xF764lvl8YCYMjCIqSgcqNndVm5bF1j1FprqRbcDemdJ1yzscoisK6lELmrDjEzvRSANy0am7sZw9MoT4SmOqLYrNRvX07ZQsWUrFsGbbqajyGDIHxVwL2qcPbb9yA2l1a9VqM4iOw7DH77RFPQUjHvz9eCNHiSXASLlVVWsKiN18i9/AhrGbzKfu0ejdMNSdmjPP08+eWV991dokNpqrMyIENOSRtyCYsxpcr/mVvMQuI8GTcvV2I6hiATi/feDcV+4v2s7dwL546T2YPmY1Wffa3V0VRWFsXmP48KTDd1K819wyNIUQCU70xHTtG2cKFlC38BXNWlmO7LjoaQ/du9q5adSQ0tSA2Kyy4F8xV0HoQ9L/P1RUJIZoACU6iwSmKQnlBHlkHk8hKTsLTz5+B19wEgLu3D/lH07Cazbj7+NrXTqrrehfSNqZJTAt+IWw2hYwDxSSty+bInkKUurFLFrMNq9mGRmeftrptt2BXlikuQtfgrnxz5TdkVWYR7X3myQMURWH1oQLeWpHCroxSAAw6e2C6e2gMId4SmOpb9qOzqPnzTwDUXl74jB2Db2Ii7j17YrFYYMkSF1coXGLTu5C+yT7teOI8UF/6kgFCiOZPgpNoEHlHDjuCUvbB/VSWFDv2+YdHOIKTWqNhwgOP4hsShn94RLNeC2X/uix2LD1GRfGJsUvhsb50GhxBu54hjtAkmq74gHjiA+JP264oCquTC5izMoXdJwWmf/ZrzV0SmOqFYrFQtXEjZb8sIvTxx9D622fP9L1qMmpPT3wTE/EeOQK1QX7XAvtMegBjZoN/G5eWIoRoOiQ4iUtmqq2hJDuL0JhYx7al775OUWa6475jWvAO9hYlRVEcIalt915Or9kZbDYFm9WGWmMPRMYaCxXFtbh5aOnQP5yEwREERHi6uEpxqd7b/R6XRV1Gp8BOp+1TFIU/kvN5a0UKuzPt01sbdGpu7t+auy5rR7C3m7PLbXZqDx2ibMFCyhb9grWgEAD3Ht0JuMn+5Yz/Ndfgf801rixRNEZjX4Iu10BkT1dXIoRoQiQ4iQtWWVJMdt3aSVnJSeQfTUOj1THt028dXeva9uiNT1AwEXVd78LaxTXJacEvRmWJkfIUPd9s3saAxHbE9wsDoOOAcDx93WjXIxitjF1qFlYeW8m8XfP4aM9HLLlqCaGeJ9YRW3Uwn7dWprCnLjC56zTcMqA1Uy6LIchLAtOlsFZWUvbTT5QtWEhtUpJju8bPD5/x4/Ho08eF1YkmI6p5fmknhGg4EpzEeduxeCF//raIsrzc0/a5+/hQUViIX1g4AEP/eYezy3Mpm9VG+v5i9q/P5tjeQhTFDTCSsj3PEZzcvfWO26Lpy6/O5+lNTwPwz4R/EuoZiqIorDiQz9srU9ibZQ9MHnoNNw9ozZQhEpjqi2I0kvfKq2CxgE6H9+VD8Z00Ca/LLkOl17u6PNFYZWyDP/4DE98BP1nEWAhx4SQ4iVNYzGbyDqeQlZxE1sH9jLxzKt51a51YTEZ7aFKpCG7VxjGJQ0R8Aj5BLXMyA8WmsG3xEQ5szKGyxOjYrg+wMHhCJ+J6S1BqjmyKjcfXP06ZsYyOAR2Z2m0qv+/P5a2VKezPLgfsgemWAW2YMqQtgRKYLoqiKNTu3UvZggVYCgqIeucdALSBgQTedivasHB8rhznGM8kxFmZquDnu6H4MKx9xR6ehBDiAklwauGM1VVkHqhbOyk5idzDKadMC56VnESHgZcBED/wMkLbtiO8fQfcPFru2JyTx2ep1CqyDpVSWWLE4Kmjw4Aw2vcLYePO1cT2DkGrky55TZ3VprDlSDE7ClUEHilmQGwIXx34ks05mzFoDEwI/zeT521xBCZPvYZbBrZhypAYAjyl9eNimHNzKVv4C2ULF2JKS3NsN2Vmoo+KAiDk3/92VXmiKVr+tD00eUfAqOdcXY0QoomS4NSCKIpCWX4eOjc3PP3s39Ae+XM7i99+9ZTjPHz96lqSOhLWrr1ju19oGH6hLbcFpbyohgMbckjekss/HumNh4/9Q3HvK9tQW2EmpnswGp0a81/WoxJN17J9OTy7KImcslpAw+cp2wkJLMIUOgcAQ8VknvyhALAHptsGteHOwTH4S2C6KFWbNlH04YdUbdrsWF9JZTDgPXIkvomJ6MLDXVyhaJIOr4JtH9pvJ84Fd2mhFEJcHAlOzZjNaqXg2BGyDu4nK/kAWclJVJUUM/iGW+mXaJ9lKrJDJ/wjok5aP6kjfmHNe1rwC2G12ji2t4j967JJTyqCurUyk7fk0mNUKwCiOwS4sELRUJbty+HeL3ei/GV7mXY9esWMuaIjGZld8XLTctvANvxrcFsJTBdIsdlQLBbUdeOSLAUFVG3cBIBH7974Tk7Ee/RoNF5erixTNGU1JbBgqv12nynQbrhr6xFCNGkSnJqhypJils59g5xDBzEba0/Zp9Zoqakod9z3Dgzijjffd3aJjV5tpZldK9M5sDGH6jKTY3tkvD+dhkQQIwvUNmtWm8Kzi5IcoUnjkYJb2CKMuRMw5k3AZgrBUt6FqcNimTIkBj8PCUwXwnTsGGULF1K28Bf8b7iewDvvBMB75EiCZkzHd8IE9NEyeF/Ug6WPQEU2BLSDUc+6uhohRBMnwakJqywusk/ikJyEp1+AoxXJ3dub7OQDWExG3Dw8iYjvaG9Rik8gNDYOnV4Gqp+PXcszsFpsuHvr6DgwnI6DIvAL8XB1WaIB1JqtZJXWkFlSQ2ZJNVvSiuu65wEouIX8hsYtH7eQ36g+OhVzyQAABscGS2g6T9bycsqXLqNswQJq/vzTsb1i+QpHcFJ7eBB8332uKlE0N7VlkL0LVGqY/AHoW+7YXCFE/ZDg5AIbv/8KlVrNgKtvOG3fph+/QbHZGHjNTaftK8w45lg7KTs5ibL8PMe+wKhWjuCk0eoYN+Pf+IWGExTVCpVa3XB/mWagvLCGpPXZlORWM/aeLgAYvHT0ndgWn0B32nYLQqOV32FTZrRYySo5Hozs4ej4nxklNRRUGM/6WI3nITTumfbb7ploPFOwVtnH/uVX1J71ccJOURRyHnuc8sWLUUx1rbdqNZ6DBuE7aRLeI0e4tkDRfBl84e41cHQ9RMvaXkKISyfByQVUajUbv/sKgN4T/+HYvunHb9j43VcMvPYmLCYTpbnZBLVq49j/y+v/oSQn+8R5VGqCWrchMj6BqI6dTrlGXJ8BDfuXaOKsFhtHdheStD6LjAMlju1FWZUERtrHU/S8orWryhMXyGixklNaS2ZJDRkl1ScFI3s4yis/ezA6zlOvISpAj59/HrWqLA4WpaB2y0HjccxxjKKAW/DvVFfFASpCvFvGos4XynjkCG5t2wKgUqlQjEYUkwm3uFh8ExPxGT8BXWiIi6sUzZHqyBqGJT2KqqMntB8JOneIG+XqsoQQzYQEJxc43tK08buvsFlt4ObFhvlfsGPRT0TGJ3B01062/DQftVbHtE++Ra2xT2ndumtPvAKCHOsnhcd1wM1Duo5diPLCGvavy+LAxhxqKk7MfhedEECnIRH4hcnvszEyWWzklJ3aWpRRfCIc5VXUHp+E7aw89Bqi/N2J9vcg3E+Dh2cxii6HVn7BjI8bjp+HjqLaIoZ9NwwU0J9hzg+Vyt7qpPVMIVjblb5tZWKQ4yxFRZT/+iulCxZiPHCAmEW/4BYXB0DQvfcQcMcdGDolyMQzouEoCuo/XsDHmI2yaBrcvw808jFHCFF/5B3FRU4OTyfLSk5y3Hbz9KKiqADfEPsU4CPuuMd5BTZT+ccq2PlbOgAePno6DgwnYXAEPkHuLq6sZTNbbXUtRtV/6Upnb0HKLT93MHLX2YOR/ceD6AD7n+G+Oo7WbCOnJo3U0lRSSlLYXp6OrcwGwGXWy7i5+xgAAg2BxPvHE+QRhMYcxurMVah0pahUJy6uKCr0wb/z1KDr0KhbdgiwmUxU/rGasgULqFy3DiwW+w6djpp9+x3B6fifQjSowytR59jHz6kqcmDFMzD6BdfWJIRoViQ4udCAq29g0w/2MU0AARFRRHZIIKJuanC/0HD5dvYSlOZVk7QhGy9/A12H2RfNbNstiNheIcT1DqV110A0Ghm75AwWq42cspO70p0IR1klNeSU1WA7RzAy6NRE+XucEo6OtyBF+hkwq0pILU0ltXQ/bho3bux4I2AfY3PvN89SZa465Xw+eh9i/WLpFHiim6tKpeKHiT8AsCFrA2vzfzytDpVKQeOeiU/AESDi0n4xTVhtcjLpt9yKtazMsc3QubO9K96V49D6y1o5wknMNXB4NSycigKosK8coTq23t6/Vv4fFULUEwlOLnR8IghUKlAUOgweesYJI8T5s5ptpO0uYP+6bLKS7WOXvAMNdBkaiUqtQqNVM3pKZxdX2fxYrDZyy2tPGVeUUXwiHOWW12I9RzLSa9WOIHRyMDp+O8hLf8oXCT8c+oGdRUl8l5VKakkqFeYKx742Pm0cwUmlUnFF6yuwKTbi/OOI84sj1j+WYPfgs34xoSgK7/z5DipUKKet5AQqVLzz5zsMjBjYYr7cMOfmYs7MxKN3bwD7GCaVCm1ICL6TJuI7aRJusbEurlK0OL89Dts/hbovRo6/GlUA2X/C4ZUQO9JV1QkhmhkJTi5yfCKI/sMH0KX4S/YG/NPRbU/C04Urzatm//psDm7KobaybuySClp3CiRhcITjW0hxcaw2hbzy2lPGFTnGGpVUk1N2nsHIz53Iv3SlOx6OgjzdUJ/U9a3aXM3h0sOklqby+4FD1FpreXrA04793yV/x4HiA477GpWGNj5tiPOPIz4g/pRrPzfouQv6+5ptZnKrcs8YmgAUFHKrcjHbzOg1zXc6clt1NRXLl1O2cCFVmzaji4ig3fLfUanVqPR6Wn/9NfrWrVDVjcMUokGVZ0PyEuhxM2jrltVQa+yhSa0DmwVOfs2qNLDqBWg3QlqdhBD1QoKTCzhmz7vmJvoVfYDamM0AlqO+5m4JTxdpx9KjHNycC4Cnr56OgyLoOCgcn0AZu3Q+rDaF/Irav0y6cCIkZZfWYDlXMNKo60KR+19ajDyI9ncnyOvUYHQmXx34is05m0ktSSWzMvOUfTq1jsf7PY5WbX/bGh8znkGRg4j1iyXOP442Pm3qLcToNXq+Hf8txbXFAFgsFjas38CgwYPQau3XDzAENMvQpNhsVG/bTtmCBVT89hu26mrHPl14ONbSUrQB9kkx3GLauqpM0VIUJMPBX+HAr5C9077Nr/WJmfJ63wE+kbD04dMfq1il1UkIUa8kOLmAYrMx8NqbGNAtGL60D2RV5/zJgBHBoLrJMeZJnFlJbhX712fToX84QVH2qcM7XRZJTZWZToMjaN05ELWMXTqFzaaQX2E8df2i4hoyS6sdwchs/ftgpNOoiPA7uSvdqeEoxPvvg5FNsZFRkUlqSSoppSmklqSSUZHBV1d+hVplf7525O1gdcZqx2MCDYHE+cc5wpFVsaKte9u6pdMtl/x7+TthnmGEedonZjGbzRzRHqFjQEd0Ol2DXtfV8l97neJPPnHc17Vq5eiKp4+KcmFlosUozYBtH8HBxVCUctIOFUT3tS9oe5xfa9j9DaAGzvR/p1panYQQ9UaCkwsMvOYm+4DVD4ehqNSoFBsKKlQrnmXA3Wvlzf0MLGYraX/axy5lp5QCYDXZGHqjvUtWWIwv46d2c2GFrmWzKRRWGk+aeOHUabuzS2sxWf8+kGvV9mB0ppnpovzdCfE2nNcsckrd9HfHx/58deArFqctJrU0lRpLzWnH51TlEOkVCcCkdpPoHdqbOP842vm1I8Ag0303JGt5OeXLluHerTuGePuivt7Dh1H63Xf4jB2Lb+Ik3Hv2bDHjuISLWExQWwZewfb7pkrYMMd+W62DmKHQYTzEjwPv0FMfazVBWRZnDk3Yt5dn2Y873r1PCCEukgQnVzm8ErL/PGkgqwK5e2Buf+h2LcSNhtBOLT5EFedUkbQ+m4ObczBW2ac6VqmgdZcg2nYPcnF1zqMoCgWVxtPWL8osqSarpIbM0hpMlr8PRhq1igg/A1F+p0++EB3gQajP+QWjk5WbyjlcepiUkhRSSlLqZrVL5edJPxPkbn9+CqoL2Fu4FwC9Wk+MX4yjBSnWLxZ/txOzrw2NHnqBvxlxoRSLhaqNG+1d8VasRDGZ8L/xBsKeegoA9169iFu/DrVBFvcVDchYCanL7a1Kh36H2BFwzaf2fcEdoN899tal2FFg8Dn7ebRucNcfUFUIgNliYcOGDQwaNAhdXbdaPIMlNAkh6oUEJ1dQFHvXAZXG3gf7ZIUHYeVz9h+fSLhnPXi0zG/dbTaFhXP+pLrMBIBXgBsJgyLoODAcL//m9aFOURQKK02nrV90cjgyniMYqVUQ7uv+l0kXToSjMB8D2ovswlhrqUWr1jrGF32X/B3/3fNf8qrzznh8ammqIziNbTuWhMAE4vzjiPaOdpxDOFdt8iHKFiyg7NdFWAsKHdvd4mJxa9/ecV+lUqGS0CQaQmUBHFpqH6+UthqsxhP7cnafmDpcpYKxL5//eX2j7D8AZjNlHlkQ3g2aebdaIYTzyScYV6hrbTqryF6QlwRaw6mhac2r4OYN7a+AgJiGr9PJirIrSdmWR98JMajVKtRqFQmDIyjKrKTTkEiiEwLOOblAY6UoCkVVpr9MunBqy1Gt+fyCUeTJXelOCkdhvgZ0lzi2y2KzkF6RTmpJqmOx2NTSVNIr0vnfmP/RI6QHAFq11hGaQj1CT5nmO9bP/nNcfED8abPcCedSbDYy7roLS579OdP4++Mzfjy+kyZh6JQgXfGEc3x9zan/9wXE2LvgdRgPUX1afA8LIUTjJ8HJ2Y63Nv3dQFbFBg+nQdlJs4pZjPY+36ZKWPYIBMbau/O1vwJaDQRt05zdy2Kykrozn6R12eQcti+kGRbjS5su9taKvuPbNokPdYqiUFJtJrOkmqMFFazMUrF10QGyy06sbVRjtv7tOVQqCPcxnHmR1wCPeglGJ9ebW5WLp94TH729G8yStCU8ueFJTDbTGR+TVprmCE6XRV3G52M/p51fO8fjReNgM5mo/GM1FStXEPHii6i0WlRqNX5XX4UxJQXfxES8hgxBpW+a7xmikVPqup0fXAyHfoNbF53oahd/pX1/x7qwFNxBwpIQokmR4ORs5zuQVa2B4PYnPc4Mlz9q/48ofRMUpdp/Ns8FvRf0vxeGP+GMv0G9KMqqZP/6bA5tycVYXTd2Sa2ibdcg3L1PfKBrLKFJURRKq82nrV908v1q08nBSAPpGaecQ6WCUG+DIwj9NRyF+7qj19b/bIAltSX2MUildWOQ6lqTKs2VPD/oeRJjEwEI9gjGZDPhrnV3tBodH4sU5x9HoCHQcc4g9yBHVzzheoqiULtnD2ULF1K2eAm2MvuXEL7jx+N12WUABM+Y4coSRXNmtUDGZnsXvIOLoSz9xL7UFdD5KvvtIQ/B0P9zTY1CCFEPJDg528UOZHXzgoHT7T+1ZXD4D0hZDim/Q1W+vVvfcdXFsHmevUUqsqc9hDUiRVmVfPv8Vsd970ADCYPtY5c8fV0zgFdRFMpqzGfsQnf8dqXRcs7zhPq4Eennjqq6mL4J7Wgd5HUiGPkZcNM23HNRZa7icOlh/A3+RHtHA7ApexN3Lb/rjMdrVVrHOkUAXYK6sOSqJUR6RTqmBxeNm6WoiNIffqRs4UJMaWmO7drQUHwnTkQf0/y69IpG5vAq+OFfUHPivQStu32yhw7jIebyE9vV8r4ihGjaJDi5wqUOZDX4QqdE+4/NBjm7wDvsxP7Dq2Dtq/Yfj0D7wn9xV9j/I3P3P8tJG05hZgXFOVW072OvMSDCk5DW3ngFGOg0OILojgGonDB2yR6M6tYvKjl9ZrqK8whGId5up61fdLwFKdzXgEGnwWw2s2TJEsaNimuQNX8sNgtpZWknZrGrWxcpqzILgLu63sX0HtMBiPGNQYWKKO+oU1uQ/OJo7dManeZEfQatwRG4RNNgzs2l4M03AVAZDHiPGoVv4iQ8+/dHpWlcX5iIZqCmxD4DnnfoiUAUGGsPTe7+0H6svRtezDDQe7i0VCGEaAiNIjjNnTuXV199ldzcXLp168Y777xD3759z3js//73P26//fZTtrm5uVFbW+uMUhsftdreqnQynwjoNBlSV0F1EeyZb/9RqSG6H4x7FcK6NGhZZqOVlO15JK3PJu9IOVo3DW26BKE3aFGpVFz9cK96X6S2vNZM5l9C0cnd6Spqzx2MgrzcztqVLtLPHYPOeR9GrTYrWZVZpJSm4O/mT89Q+/OcU5nD1b9cfcbHBLsHo1GdqDHEI4TNN27GQycfYpoyxWajett2yhYsQO3hQdiT9m65hoQEfK+6Co9ePfEePRqNl5eLKxXNTlkWJC+BA4vg2AawWexfxB0PTn6t4M6VEN4dNI3iI4UQQjQYl7/LzZ8/nwcffJD333+ffv36MWfOHEaPHk1ycjIhISFnfIyPjw/JycmO+41lHEyj0Xqg/cdqhowt9nFRKcuh4IB9fJT7STP1HVkLpipoexnoPS/50gUZFSStyyZ5ay7mWvuYH7VaRetOARirLegN9n9yFxOaKmrNZ+hCd2KR1/LzCkZ6Ik9ev+iklqNIP3fc9a75lt5sM7M1Z+spM9kdLj1MrdX+hcDYNmMdwSnSO5IQ9xAivSNPm8nO33Bqi6JKpZLQ1ISZjh6ldOFCyhf+gjk7GwC1hwchDz2I2sMDlUpFxIv/cXGVollaPweSFkL2zlO3h3Syr690sqjeTitLCCFcyeXB6Y033mDKlCmOVqT333+fxYsX88knn/Doo4+e8TEqlYqwsLAz7hMn0eigzWD7zxXPQ8kxyNgKvpEnjlk/xz49usbNflz70fZvEwPaXvDl9q3JZM03hxz3fYLd6TQ4gg4DwvHwOfcMXpVGi30x15LqUxd5LbXfLq02n/McgZ7609YvivL3IDrAnQg/dzz0rv0nX2Ysc3Sv89B5MKHdBPsOBaatmobFdmr406v1tPNrR5R3lGObWqVmxTUr5AuDZqx82W8Uf/YZNX+emLpZ7eWFz9ix+E5OROXu7sLqRLNjs0HBQQhNOLEtdUVdaFLZeyp0HA/x4yCwncvKFEIIV3Ppp0iTycSOHTuYNWuWY5tarWbkyJFs2rTprI+rrKykdevW2Gw2evbsyYsvvkinTp3OeKzRaMRoPLHIXnl5OQBmsxmz+dwfxBva8RqcUotXBHRMhJOupQ6KR12Ygqos3R6gDq+EpQ+jBMZhi78S27Czz9RXmFEJQFC0vXtQZAdfNDo1rbsE0HFgOBFxvo6xS2azmSqjhezSWjJLa8gqrXEEo6zSWrJKayg5j2Dk76Ej0s+dSD+DIxhF+rkT5edOhJ8BT7e/+yetOOX3fPJz+uuRX+0tSGX2FqT8mnzHcQkBCYxpNcZxf1D4IHRqHe382hHra29BivKKQlM3uUdj+PfaUjX061Sx1M0sWTdBTE1Kij00qdV4DBiA96SJeA4bhrpuYVqL5dytq+LcnPr+29hYTaiOrkeVvBj1oaVQXYhl5n775ESAqs9dkDAZJW40eIWeeFwj/1216Oe0GZPntflpTM/phdSgUhRFacBa/lZ2djaRkZFs3LiRAQMGOLY//PDDrFmzhi1btpz2mE2bNpGSkkLXrl0pKyvjtddeY+3atezfv5+oqKjTjn/mmWd49tlnT9v+9ddf4+EhXZgAUBS8a7MJLd9FaPluAioPocZGvncnNsU+4jgsvHQ7hW5xlBQGUZWhw1ymwRBsIah3DQAmKxRVQbFVRbERio11f9aqKDJCleXcLSQeWoUANwhws/8Z6KYQYDhx39DIxrtbFStFtiLyrfnkWnNRUBjlPsqx/43yNyi2FZ/yGF+VL2GaMCK1kQw3DHd2yaIR0efk4rNjBz67dpE/cSKVXe1jD7UlpXjv2UN5j+5YfWSdLFE/tNYaQsr3EF66g9Dy3ehsNY59ZrWBrTH3U+id8DdnEEKI5qe6upobb7yRsrIyfM7xf26TC05/ZTab6dixIzfccAPPP//8afvP1OIUHR1NYWHhOX85zmA2m1m+fDmjRo1qkBnYLkptGaojq8HNByVmGAAFB4+S/MlHpNRehlmp6yakUigN0rElBLJKjRRVnXnh1JP5umvrWoyOtxYZiKq7HeHnjrfB5b1Hz+mHlB/4s+BPUktTOVp+FLPtxDcV3jpvlk9azooVKxg1ahQfHfiIClOFoxUpxjcGb723C6sXF6M+X6eWoiIqlyyh/JdFmA4edGz3GjuGsFdeudRSxQVolO+/9U1RHIvMqnZ/jfbXE+t5KZ4h2NqPRYkfh9J68OnLYDRBLeI5bYHkeW1+GtNzWl5eTlBQ0HkFJ5d+Sg0KCkKj0ZCXl3fK9ry8vPMew6TT6ejRowepqaln3O/m5oab2+n/Geh0Opc/USdrDPXUmq11XegUMmv6kJldQ8aOvfjsLSeyxAaMBsBPk0knj+XEu/9BlUpFh/xufGUZQRHt8XbTElU3I130X8YZRfq74+veeH7nZ1NUU3TKJA351fnMGznPsX9N1ho2ZG9w3PfQejim+Y71i0WltX9I0el0zOgli442J5fyOrWZTGTNvJ/KdevgeFc7nQ7vyy/HN3ESXkOGoGpE70ktSWN4/61XxWn2hWgP/GpftqL/vfbtHcfbF02PHwMdxqOK7I2mma6t1OyeUwHI89ocNYbn9EKu79LgpNfr6dWrFytXriQxMREAm83GypUrmTZt2nmdw2q1snfvXsaNG9eAlTYMq01hy5FidhSqCDxSzIDYEDQNuJ6R0WKtm3zh1Jnpjk/ZXVBhBAXCrCqKNQqmulK6GjWEouOQzkqeVyUDfFOJV5eiqrURZKvias06+o38B959r7AHo4o8+7oewR0c33Q2dt8c/IaVx1aSUppyyqKwx5XUljhmrJvQbgK9w3o7ZrQL9ww/ZcHYxtBfV7ieoiiY09PRt24NgFqvx1JcBBYLhi5d8E2chM+4cWj9nb+2mmhmFAVydtvD0sFfIT/pxD6V6kRw8gyEaVvPfA4hhBDn5PJ+UQ8++CC33norvXv3pm/fvsyZM4eqqirHLHu33HILkZGRzJ49G4DnnnuO/v37ExsbS2lpKa+++irHjh3jzjvvdOVf44It25fDs4uSyCmrBTR8nrKdcF8DT09IYEzn8Is6p9FiJae09qQwdOoir3nlxrM+Vq9Ad5OG7mYtwRY1GW3dMHT0s3eh83Ij3MvAbZE++LhrUamusj/Iaob0zZDyG1G9J8Dx1qTdX8OKZ8C3FbS/wj5LX5shLlsQ0WQ1caTsCCmlKadM9f3jxB/x1NmnYD9SdoQtufauoSpURHtH26f49o8lzi8ON82JVssrY650yd9DuE7x+x8QN3cuxekZhE7/+y91zDk5lC38hbKFCzFnZhK3bi0aPz8AwmbNQu3tjVs7mZlM1BObFeb2g6KUE9tUGvssqR3GQ4em96WiEEI0Vi4PTtdddx0FBQU89dRT5Obm0r17d5YtW0ZoqH0Wn/T0dNQndSUoKSlhypQp5Obm4u/vT69evdi4cSMJCU1nQOuyfTnc++VO/jq4LLeslnu/3Ml7/+x5xvBkstjIKas5bf2i4+Eor6KWc41Y89BrTkzR7WcgwqbFPaOaqtQKbGYbABqtmqsSwuk7IebvT6bRQdsh9p+T1ZTYpzcvS4dtH9l/tAb7WlFxV0D3G+tlzai/stqsqFQqR+vPd8nf8eWBL0kvT8eqWE87/nDpYboGdwVgXNtxJAQmEOcXR4xfDO5ame5Z2BXMm0fx3LmogOK5c1Fr1ATfd98px9iqq6lYvpzSBQuo3ryF4y9ElcFAbVISngMHAuDevbuTqxfNiqka0v6ArJ0w4kn7NrUGAmOhPAtiR9jDUtwV4BHw9+cSQghxwVwenACmTZt21q55q1evPuX+m2++yZtvvumEqhqG1abw7KKk00IT4Nj2+M/7KK02k103ZffxrnS55ecORu46zSnjiqL83YkOOLHIq7+HDpVKhdVq48eXd1CQXkFF3WP9wzzoNCSS+P5hGDwvob/pqOdg6CNwZB2k/AaHfofyTEj5HY6u7CuDhwAALe9JREFUhx43nzi25Cj4RNpD2HlSFIW86jzHekjHW5LSytL4atxXxAfEA2C0GjlSdgQAb703cX5xjnFIx3+O6x7Sne4h3S/+7yyapYJ58yh8+51Tth2/fzw8Va5bR+bM+1Gqqx3HePTpg29iIt6jR6Pxqv8vCUQLUl1sX8T84K+QuhIsdTPh9bwZ/NvYb1/5GngEgk6+8BFCiIbUKIJTS7L1SHFd97yzK6oy8ehPe8+4z6BTn7a4q6MFyd+dAE/9GRdGVRSF4pwqVJ72hWg1GjVe/m4U51QR2yuEToMjCGvnW3+Lquo97QOQ48fYv33PP2APUcZK0BlOHPfVNfYxUe2G2RffjR0JXiGO3WXGMvQavaMFaNHhRczeOpsKU8VfrwhASmmKIzgNbzWcGN8YYv1iCfEIkQVjxf+3d+dxVVV7/8A/+0wcZmRGGUQBEcwpxUBNMZUSLXt8sulRG65eTbuZWVldM2+ZeV/erJs2vBq03+0pG560Mk3RHMghFSHQHBDnAZlkns6wf38sOIcDh0mFA4fP+/Xar2DttfdZh8W282Wt9V2tYi1oqlU3eNJGRkKuqoI6OBju990L9/vug8bK1ghErXJ6B/DbKuD8PqDuiLl7MBCZKKbjmcr4+0ZE1B4YOLWznJKmg6Zakf6uuD2kW4MgydvFemDUmMoyHU4dzMax5CsouFKG/3k9Fu4+IggZ8UA4NI6qmxtdaglJEjvS+9WbTlleAJTnA1VFKD/+A86e3oxTGjVOd+uB0y7dcFquRE7VdawavQpjQ8YCANw0biipLoFSUiLELcQ0glQ7mtTDpYfp9j1celh8T9RSTQVNteoGT702boAmLIzBOd0YWQZyTwBaD8CtZpp2VQlwLll87ddPBEuRiYB//06TdIeIyN4wcGpnvq7a5isBWDIpGrG9vW7oNWRZRvaZYvyZfBmZKTkw1K5dUiuQe6HEFDi5ebf/tA6dUYfzRefh5uAGXydfYGEm9qSvw7w/3qkzfbESqLxq+u5y6WXAaASqS3C73+34btJ3CHUPhUapaff2k30wVlVBn5sHfW4O9Lm50OfmQq6sgteTT7QoaKpVf9oeUYsZjcClQ2IK3olNIoX46JeB0TWbjoeNBca/IYIlz2bWmxIRUbtg4NTOYkI9EeCuRXZRpdV1ThIAf3ctYkJvbGFvwdUybP34KAqulJnKPLs7I3pkD0TE+LX96FINo2zEldIrpv2QMgtFNruzRWehN+rx7O3P4ol+TwAKJQJD4yH/8Q48tZ4Idw1BuFGBsKJrCIu8H2H9HxWZ7y6lAJ+Nh0twLPqEjxfT+rwj+JdXsmAoLYM+xxwMGcvL0G3qVNP5y88tROlvv8FYVNTgWkmjgecTjyPvvdWtes2891YzcKKWMeiBM7uAEz8BJzYDZTnmc0oHoLLO76WDCxD3dLs3kYiIGsfAqZ0pFRKWTIrCnC+OQAIsgqfaEGDJpKgW7+ckyzIqSnRwchOjL65eWpRer4JKrUDYUD9Ej+gOv1C3NptCJMsy8ivzkXk9Ex4OHujr1RcAcLLgJKZummr1GieVEyr15imLIa4h2DV1F7wcmxhhu3gAMOrF1JVzyUDSYsAjGAhPEEFUzxFcGG2nZFmGobAQ+pxc6PNyYSwphdvdCabzV5cuRdm+fdDn5lkkaABEMOTxwAOm339jVaUpaJLUaqh8fMThK/4LnQ7eT89r8YgTAHg3k56cujijQWS+AwDZCHz3BFBVEyA5uIstGyITxQiTg6vt2klERM1i4GQDd/cLwAf/M7jOPk6Cfyv2caos1eHk79k4lnwZkCQ8/GoMJEmCWqNE4lO3wSvQFQ6Ot7Z7dUYdjuUdw6nrp0RGu5qsdterrgMApoRPwWtxrwGAKaV3sGuwaS+k2vVIAc4BFoGcUqFsOmgCgNi5QJ97RIa+zG0ieCq8ABz6WByPbwFCRMpnGPSAkr/aHZ2s10OfXyBGhoqLTCm7ASDnX2+j7PffxchRXh5QZ1NhSa2Ga8J40++QIS8PuvMXTOcVTk5Q+fqagiK5uhqSg9iHy3fBc/CdPx8qHx8o3Kz/QaF29KglwZP3357maBM1VHINOLlZTMErvAjM/V2Mjqs0wKD/AfSVIljqOVKUERFRp8BPlzZyd78AjIvyx/7TOdiW/DvGjxyG2DDfJkeaZFnG1dOFOJZ8BVlHcmHQi7VLKo0CxXkVcPcRG8x2D+92U22rTeOdeT0TLmoXxAfHi3J9FaZtmdagvgQJwW7B6KY1v66D0gH7H94PpULZoP4N8+wF3DFbHNVlwJndIoi6dAgIjDHX2/KCyEQVMV6MSAUNYyDVjmrXDxkKC+HYL9pUnvfxxyg/dKhmbVEuDAUFYp0HAKjViEz/wxTIVJ87h8r0dIv7Kj08oPLxFsFQZSUkRzHC6DV7NjynTzcFSgrnxtN/O/QKbdF7aEnwxKCJLORn1axX+hm4eBAW8wlyTwK+keLru9+0SfOIiOjm8dOkDSkVEoaFeiL/uIxhoZ5NBk3n0vOw9/9Oo/CaeSqSd5CLWLs01A+aGxxdkmUZ2y9st9gP6ULJBRhl8YE2xj/GFDi5aFwwwGcAXDQuCPeoyWbXLRyh7qFWN4y9pUFTfRpnIHKCOOrL2iH2h8o9Dux9V0yHCRsjgqjwcYCzd9u1y44ZSstgKMiHJjjYVHb9q69QnppqWlOkz80zrx+qFwxVZhxF2Z5ky5sqFFB5eYlgqLwcUk3Q4zljOtzvu9cUDCm9vaHQWP/LvGN0tNXym9VU8MSgiSzsXA7sfsuyrMftNZnwJgE+EbZpFxER3VIMnGygpKASlaVi6pFer0d1kQJ5F0uhUonucHRVw9nDAQadESqNCD6UKgUKr5VD5aBExFA/RI/sDp9g1xatXardMDbzukjQoJAUmBE9AwAgSRKWHViG/Mp8i2vcNG4I7xaOAT4DLMq/mPDFTb//NjdzJ5D1qxiNykwCKgqAYxvE4dkL+FuqrVvYYciyDGNxMZTu7qayop82oSIjvU4wlGteP6RSiWBIoQAAlO0/gJJt2xrct3b9kLG0FEpXsW7D44H/hsudI83rinx8oPT0hKRsGGA7DRnSRu+4dawFTwyaujCDToxmn9gEDHhIBEcAEDQUUKjEWsvIiUCfCYA7t0IgIrI3DJzamUFnxLfLD6GiRFen1Bnf7zN/mFc7KOHkrkHoAB8MnxIGAAiM7Ia7ZvRFr0E+0Gib77YNmRuQkZdhWodUojNvGOvv7G8KnABgXMg4VOgrEN4t3LQWydvRu/PuSePkCdz23+IwGoDLKcCprSKQCjGvo4FBB7wfK6byRYwHesUDWjfbtfsWkg0Gi4CkZOdOVP75pykIMgVEeXmALFsEQyU7dqDkl1+s3leh0YhAy8MDAOA2aSIcB/SHytvbIiBSuDfcTNll5Mi2ebNtzOepp2A0GJG/Zg285s5l0NTVVJeLUewTPwMntwCVhaJcqTEHTqGjgOdPA443N02aiIg6NgZO7UyhkuDqqUVFqQ5W85ED0FUZUJRTgTOpOYj7r96QJAmSQkJkrDlpRLmuHFmFWThdeBqnrp9CcXUxlo1YZjr/feb3SMtNM32vklTo6d7TNL3OKBuhkMQH5VfueKVN3muHoFACQTHiuGuxCKRqXTgA5GeKI+0LQKEGQmLNmfq8wjp0uvPyQ4dQeepUg5EhfW4ujEVF6JOWagqein78ESVbrAdDAGAoLITKU6TAdx0TD01gD4tAqLH1Q27jxrXdG+xAPGf/FQeCgxA+wcrUULJP5QXAD/PE6LW+wlzu5CUS1USYMztCqWbQRETUBTBwameSJGHYvb3w03t/NFrH3dcRg8eHIGyIr8Vf7b85+Q2SLyfj9PXTuFR6yfK+kLD4jsXQqsQGuxN7TcRQ/6GmQKmnW0+ole2zh1OHVnfdVVAMMG1jzZS+bUD+aeDsHnFsewVIeFNk82tHlcePo+rMmXrBUG5NMoXrCE/eYwqGCr78sulg6Pp1qLzFei7n2FgoXVysBkMqb29IddYPud97b9u+SaKOqPCi2IS21yjxvdYDuHxYBE0ewWIKXuREJpshIurC+K+/DQRFecI3xBU5F0rqjTrJ0HhLKJuSgbWF3+Nc0jl8mfgl1AoR8PyR+wd2Xdxlqu3t6I0wjzBTcCTXudmDkQ+2y3vp1FQOQO94cdy9XGTFytwmpvWd32s5re/EZuDI/6vJ1DcecA9s8ctUX7yI6gsXrAdDefno9fMmUzCU//HHKN68pdF7GQoKxH5DAJwGDQL0euvBUM36oVrdpk4FplrfV4uoS5JlIOc4IrI3QvXpSiA7HXD2AZ47Kf7AolAAk94F3HoA/rd16NFnIiJqHwycbECSJMTcG4pN76XXP4PvfT7ApbQTppILxRfQ26M3ACCxVyKivaIR3i0cvT16w1PrCbqFvHoDXnOAO+YAVaWA2sl87vhPwKkt4gCgd42CrttQ6J37Qi93gz4/H/o8MU1Ol5MLPPyQ6dLcVe+gePPmRl9Wn58Pta8vAMChb1845eY12JjVFAzVrC0CAM/p0+E5ffqt/RkQ2bsrqUDGd8CJn6G+fhZ9a8slBeAdAZTnAy7ieUSfe2zVSiIi6oAYONnIRfcTyHE+D++yQCighBEG5DlfAnqUYpLfJIR1EyNJ/s7+pmviuschrntcE3elm2GsroY+Jxf63JyGo0OXixE042VIZ3YAlw7h2tYrKL6QBCDJ6r2Ukyaavtb07AmH8LDGR4bqZLTznjkT3jNntvVbJeo69FWApDRPrzv6PbB/NQBAVjog2zkKPiMfgypqIrcqICKiJjFwsgFZlrE6bTVKgo2YcHw2AEABJQ4Hb4G71h3LRizrvBntOiBDaZn1YCg3F92XLYOkFlMhr778Coo3bWr0PvoVq6COfxEoy4e67EWorh+ESlMFlasGqqGTzQkULm0DirYD2VFA4GD4/O1p+Pzt6fZ6u0RUWSS2IjixSfz3gc+B8LHiXPRkoCQb6DsR+pA7cXD7HkwYOAFQcw0oERE1jYGTDey7sg/H8o8B7kCO83n4loUgx/k8LrgfB/LF+eE9htu6mR2aLMswFBZaDYb8nn/eFAxdefkVFH3/faP38V2wAGp/Maqn8vEx7T9kbZqcwrlm6p6zF3z/+Ql8AcCgB0quAh5B4pyuAvKKZ+FeVgF8uhFwDRCb7oaPB3qNBhxc2+xnQtSllWQDJzcDxzeJBC/GOls+nNlpDpx63A5M+Vh8rdM1vA8REVEjGDi1M1mW8V7qe5AgQZZk/B68CcPPTcHvwZsASWTHey/1PcR1j+uSo06yXg99fkHNHkPmYMj7L38xZX7LfvNNFH61HnIjH3q8nnjCFAzVrglSODlB5etrOU3O1wcKrdZ0ne+z8+H7wvOt+7krVeagCQBkGYZxbyB33xfwLz8JqeSqSCpx5P+JdOexTwHj/tG6HwoRNa3gLPDvgZZl3hFAZCIQOQnoPsgmzSIiIvvCwKmd6Yw6ZJdlmzLgXfY4hW8GLjedlyEjuywbOqMOGqWmsdt0OtbWD3V74AFTMJTzzjso/O7/YCgoAIzGBtd73H8/1AFiHytJrTYFTUp39wYjQ1KdKTfec2bDZ+5TDfYfsqZuSu4bpnGCPHgGDmb7YML4MVBfPmjO1Hf9rMjaVas0F0heKUajeo4QWf6IqHGyLJI7nNgEyEZg7GuivFtPse+a1qMmWJoI+ETYsKFERGSPGDi1M41Sg/UT16OgsgAAoNfrsfe3vRg+YjhUKtEdnlrPThM01V8/5DZunCkAyfv4YxT98AP0uXkwFhU1uNY1Ph7q7t0BALJOB0NenjihUEDl5WUxMgSFwnSd12OPwfORR6D08YGimWBH6WrDqXEqLRB2lzjufkukO9eaE0HgdBLw+4fiUDuLqXy10/rce9is2UQdikEntgc4vklMxSu+LMo1rsDol8QfHCQJ+GsyoHFq+l5EREQ3gYGTDfg7+5uy5V17bzVGrVkDr7kG+D09z8YtE+qvH3KOiTGN4hR8+SWKN2+uOZcHubzc4lqnHduh7iE+9BuLilB9Ost0rv76Idlo3neq28OPwD0x0bT/UO2+RtbU7mPUqUgS4B1mWeYdAQyaJhavl2YDJ38WBwD43QZMXgMEDGj/thJ1FLtWAAfeByoLzWVqZ7FeKXKSZV0GTURE1MYYONlQ7vvvo2DNGkgACtasgUKpgM9TT7XZ68kGg9hvKDcX2ogIUzBU9MMPKE5KMm/KmptnsX6o9/bt0ASKYEh/9SoqDqdY3Ffh7GwKiGS93lTufv/9cI6LMydXcHdvdP2QuH8XG2UJHCIOWRabb57aBmRuBS4dBq4dBVy7m+ue2iY+PIaNBZy4fxfZobJ8sU9a33sBrZsokxTi997JW+yp1HcSEDoKUGubvBUREVFbYOBkI7nvv4+8f79nUVb7fWuDp9r1Q2p/P0g10/2Kk5JQumuXaWRIn5cLQ755/VDv7UnQBAYCAKpOZ6F0+44G961dPyRXVpjK3O65B9q+fc2JFry9G10/5NC7Nxx6927Ve+mSJEmMLAUMAEY9D5TlieDJpc7I2r5/A+eSxQfJwKFiOl9EAuDXT1xP1BldP2/OhHdhn1i3pHYC+v2XOD/wYaDncCBoGKBofBSaiIioPTBwsgFrQVOtusGTobQMCidHSDXre0r37kXZvn310m+b1w/1TtoGTZDI8FaZcRRF/2clDXfN+iFjaampyPWuMVAF+EPl4wN17aasjawf0kZFQRsVdVPvn5rh7A30uduyLDgWKC8Aco4BF38Xx6+vi1GpqPuAe96yTVuJWqs0Bzi8ViR4yE63POd/G6Co878l90BxEBERdQAMnNpZU0FTrbx/v4e8Dz4EdDqLYKj84CEUfPqZ1WsktRqGwkKgpq7ziOFQOGot0283sn7IceBAOA4ceNPvjdrQmFfEUXhRJJU4tQ04uxsouQIUnLGsm/YVEDwM8Oxlm7YS1WU0ABXXxR8EAEBXAex6U3wtKYDguJpMeIlAtxDbtZOIiKgZDJzaUUuCJpOaNUb63DxT4OQ0dCjkykqRftvb23Jz1nrrh5xjYuAcE3PL3wPZmEcQMOQJcegqgfO/ialNtQovAhtni6+9woDwBCBivPhwquocmRrJDugqRWB//Cfg5Bax6eyj34hz3UKAO54C/KKBiHsAZy/btpWIiKiFGDi1o7z3VrfuAkmC02Dzxo0uI4bDZcTwW9wq6rTUWpEsoq7KQqDnSODCfiD/tDgOrAE0LiLd+R1PiTUjRLdaZZEYCT2xCTi9Hag2TwfG1TSRVlxZs8fa3cut3oKIiKgjY+DUjryfntfyEaea+kSt4n8b8Ngm8SE2a6dIdZ65DSjLER9o+00x1y26BBRfBXoM5sJ7unlfTxOjTLXcepin4IUMNwdNREREnRQDp3ZUmy2vJcGT99+ebtPU5GTntO5A9GRxGI3iL/6Z24DeY8x10r4Edi4DnLzEyFX4eLFZr2M3GzWaOoW8zJopeJuBh74yZ3/scw9Qeq0mWJoIdB/EjI9ERGRXGDi1s5YETwya6JZSKMSoUo/BluVGPeDgDpTnA+lfi0NSiNTP4eOBmFmAg4tt2kwdh9EIXEkVI5YnNgF5p8znTm0BBk8XX8fMAu6YY5s2EhERtQMGTjbQVPDEoInaTfzLwJ3Pi9Tmp7aKaX25x8X6qGvHgLinzXVzjgMewYDG+p5dZKcuHgS+mSGyN9ZSqIHQO8XIUkSdtPmc7klERHaOgZONWAueGDRRu1OqgZ4jxDH+dbEhaeY2oKrEvCZFloGvHhLroXqOEBvvho8HPENt23a6tarLRFIHtTMQXpN0xLMXUJotkouEjQX6TgLCx4mpoERERF0MAycb8nnqKRgNRuSvWQOvuXMZNJHtdQsBYmZalpUXALIRMFQBWTvEseUFwDtCBFBR9wFBTH3fKZXliXThJ34GzuwE9JUidX1t4OTsDTz+CxAwQGRxJCIi6sIYONmY5+y/4kBwEMInTLB1U4isc/YCnkkHck8CmVtFyukL+8Val7xTYkPT2sDJoBdrplz9bNtmatqhT4Cj34t+lI3m8m49gaChYpSxNrFD8DCbNJGIiKijYeBERM2TJMA3UhzDnwEqCsUIxaltYsSp1qVDwNq7gYCBYjQqIgHoPlgkqCDbkGWRCc8nwlyWmQSc3yu+DhggsuBFJgK+UcyER0RE1AgGTkTUeo4eQPT94qjr6h81/00Tx55/Ak7eYl1M+DgRTDm4tnNjuyCjAbhwQEzBO7EJKDwPPPOHGFECgKEzxYbIkYki6QcRERE1i4ETEd06d8wG+v1Xzca7W8UmvOV5wB9fieMvO4DAIaJudTmgduQIx62iqwDO7BKB0sktYspkLZUWyD5qDpzCx5rXMREREVGLMHAiolvLxRcY9Kg4DDox8pG5Fbh0WEzbq7XlBfFBP3y8OELvBDRONmt2p3dqK/DtDPP3Wg+RLrzvRLHxMVPJExER3RQGTkTUdpRqIHSkOOqSZeBcMlB0ETj8qThUWqDnyJp05+PMoyNkqfhKzRS8n8XPdeRzojxsrEgfHjZWrFkKiTOnlCciIqKbxsCJiNqfJAFz9ovg6dRWsXdU0UXgdJI4vCOAeYfM9Y3Grp1gIvcUcOInESxdTjGXl+eZAycHF+BvqbZpHxERURfAwImIbEPjJEaXIhLECFTuCXMQFTjUXE9fBbw7UKQ8j0gQIyouvjZrdruSZeCTscDlw3UKJfHz6TsR6JNos6YRERF1NQyciMj2JAnw7SuOEfNFwFDr/F6g5Arw50ZxAGKtVO2UvoBB9jEapa8WI3Dn9wJjFoufiSQBnqEiW2GvUWIKXp97AFd/W7eWiIioy2HgREQdT91Me6GjRDa+U1tFkomrfwBXjohj13Jg/DIgbp7t2nozqkqA09vFFLxT24CqIlHebwrgFy2+HvsakPg2oHWzWTOJiIiIgRMRdXQKpUhhHjgEGPMKUHxVrIPK3CbSnfeON9c9+j1w+DNzpj6fPh0z3fmFA0Dy2yKroKHKXO7sC0ROAJQO5jL3wHZvHhERETXEwImIOhe3AGDwdHHoqy0zx534WUx3O5cMJC0Wm7uG16yj6jlC7BtlCwVnRTtrg6DqUjF6BgDdQsV6pchJIjhUKG3TRiIiImoSAyci6rxUGsvvx/xdJE7I3CaCp8ILwKGPxaF2AhYcBxw92r5dsgxkZ4jNaE/8DFw7CsTOAxKWifM97wTuehXoMwHwieyYo2JERERkgYETEdkPz1DgjtniqC4DzuwWIzuZSYCzt2XQtOlZwMFVjEgFDQOUN/nPodEIXNhXs8fSJhG01ZKUQMV18/cqjTmNOBEREXUKDJyIyD5pnMV6ocgJYgSovMB8rqoUOPIfwKgD9r4LOLgDYWNEEBU+TgRZdUhndyP+z0WQ+joDEWPNJ+rvL/XdE0DpNfG1yhEIu0tkwotIAJw82/DNEhERUVtj4ERE9k+SAGcv8/cKJXD/h2JKX2YSUFEAHNsgDkjAsNnAPW+JurIMxc434FZ1BcadbwCBg8V1JzaJ6XhPHxH3UyiAgY8AJdfEmqVe8WKvKiIiIrILDJyIqOtROwK3/bc4jAbgcop5893sdDHlr1bGt1BcTQUA8d9/9gZgNJ+/nCI25wVE6nAiIiKySwyciKhrUyhF4BMUA9y1WKQ7V2vFOVkGdvyj3gVGwCeqJhNeIhAwoN2bTERERO2PgRMRUV1uAeavs3YARRcb1kl4HQgb27CciIiI7Jai+SpERF2QLAO/viEy4tUlKUW5LNumXURERGQTDJyIiKzJ2gFcSQVkg2W5bBDlWTts0y4iIiKyCQZORET11Y42NfpPpIKjTkRERF0MAyciovoM1UDRZVhkz7NgBIovi3pERETUJTA5BBFRfSoHYNZOoCwPAKDT67F3714MHz4calXNP5vOPqIeERERdQkMnIiIrHEPFAcA6HQocrosUo+r1bZtFxEREdkEp+oRERERERE1g4ETERERERFRMxg4ERERERERNaNDBE5r1qxBz549odVqMWzYMBw8eLBF161fvx6SJGHy5Mlt20AiIiIiIurSbB44ff3111iwYAGWLFmCI0eOYMCAAUhISEBOTk6T1507dw4LFy7EyJEj26mlRERERETUVdk8cHr77bcxc+ZMPP7444iKisKHH34IJycnfPbZZ41eYzAY8Oijj2Lp0qXo1atXO7aWiIiIiIi6IpumI6+urkZKSgpeeuklU5lCocDYsWOxf//+Rq/7xz/+AV9fXzz55JNITk5u8jWqqqpQVVVl+r64uBgAoNPpoNPpbvId3LzaNnSEttCtwT61P+xT+8R+tT/sU/vEfrU/HalPW9MGmwZOeXl5MBgM8PPzsyj38/PDiRMnrF7z22+/4dNPP0VaWlqLXmP58uVYunRpg/Jt27bBycmp1W1uK0lJSbZuAt1i7FP7wz61T+xX+8M+tU/sV/vTEfq0vLy8xXU71Qa4JSUlmDZtGj7++GN4e3u36JqXXnoJCxYsMH1fXFyMoKAgjB8/Hm5ubm3V1BbT6XRISkrCuHHjoObGmnaBfWp/2Kf2if1qf9in9on9an86Up/WzkZrCZsGTt7e3lAqlbh27ZpF+bVr1+Dv79+gflZWFs6dO4dJkyaZyoxGIwBApVLh5MmT6N27t8U1Dg4OcHBwaHAvtVpt846qq6O1h24e+9T+sE/tE/vV/rBP7RP71f50hD5tzevbNDmERqPB7bffjh07dpjKjEYjduzYgdjY2Ab1IyMjkZGRgbS0NNNx7733Ij4+HmlpaQgKCmrP5hMRERERURdh86l6CxYswIwZMzBkyBDExMTgnXfeQVlZGR5//HEAwPTp09GjRw8sX74cWq0W/fr1s7jew8MDABqUExERERER3So2D5wefPBB5Obm4tVXX0V2djYGDhyIX375xZQw4sKFC1AobJ41nYiIiIiIujCbB04AMG/ePMybN8/quV27djV57bp161r1WrIsA2jdQrC2pNPpUF5ejuLiYpvP8aRbg31qf9in9on9an/Yp/aJ/Wp/OlKf1sYEtTFCUzpE4NSeSkpKAIDroYiIiIiICICIEdzd3ZusI8ktCa/siNFoxJUrV+Dq6gpJkmzdHFN69IsXL3aI9Oh089in9od9ap/Yr/aHfWqf2K/2pyP1qSzLKCkpQffu3ZtdHtTlRpwUCgUCAwNt3YwG3NzcbP6LQ7cW+9T+sE/tE/vV/rBP7RP71f50lD5tbqSpFrMuEBERERERNYOBExERERERUTMYONmYg4MDlixZAgcHB1s3hW4R9qn9YZ/aJ/ar/WGf2if2q/3prH3a5ZJDEBERERERtRZHnIiIiIiIiJrBwImIiIiIiKgZDJyIiIiIiIiawcCJiIiIiIioGQyc2tCePXswadIkdO/eHZIkYePGjc1es2vXLgwePBgODg4ICwvDunXr2ryd1HKt7dNdu3ZBkqQGR3Z2dvs0mJq1fPlyDB06FK6urvD19cXkyZNx8uTJZq/79ttvERkZCa1Wi9tuuw2bN29uh9ZSS91Iv65bt67Bs6rVatupxdScDz74AP379zdtmBkbG4stW7Y0eQ2f046vtf3K57TzeeuttyBJEubPn99kvc7wvDJwakNlZWUYMGAA1qxZ06L6Z8+eRWJiIuLj45GWlob58+fjL3/5C7Zu3drGLaWWam2f1jp58iSuXr1qOnx9fduohdRau3fvxty5c3HgwAEkJSVBp9Nh/PjxKCsra/Saffv24eGHH8aTTz6J1NRUTJ48GZMnT8bRo0fbseXUlBvpV0DsYl/3WT1//nw7tZiaExgYiLfeegspKSk4fPgwxowZg/vuuw/Hjh2zWp/PaefQ2n4F+Jx2JocOHcJHH32E/v37N1mv0zyvMrULAPKGDRuarPPCCy/I0dHRFmUPPvignJCQ0IYtoxvVkj7duXOnDEC+fv16u7SJbl5OTo4MQN69e3ejdaZOnSonJiZalA0bNkz+61//2tbNoxvUkn5du3at7O7u3n6NopvWrVs3+ZNPPrF6js9p59VUv/I57TxKSkrk8PBwOSkpSR41apT8zDPPNFq3szyvHHHqQPbv34+xY8dalCUkJGD//v02ahHdKgMHDkRAQADGjRuHvXv32ro51ISioiIAgKenZ6N1+Kx2Pi3pVwAoLS1FSEgIgoKCmv2rN9mOwWDA+vXrUVZWhtjYWKt1+Jx2Pi3pV4DPaWcxd+5cJCYmNngOreksz6vK1g0gs+zsbPj5+VmU+fn5obi4GBUVFXB0dLRRy+hGBQQE4MMPP8SQIUNQVVWFTz75BKNHj8bvv/+OwYMH27p5VI/RaMT8+fMxfPhw9OvXr9F6jT2rXLvWMbW0X/v06YPPPvsM/fv3R1FREVauXIm4uDgcO3YMgYGB7dhiakxGRgZiY2NRWVkJFxcXbNiwAVFRUVbr8jntPFrTr3xOO4f169fjyJEjOHToUIvqd5bnlYETURvq06cP+vTpY/o+Li4OWVlZWLVqFf7zn//YsGVkzdy5c3H06FH89ttvtm4K3UIt7dfY2FiLv3LHxcWhb9+++Oijj/D666+3dTOpBfr06YO0tDQUFRXhu+++w4wZM7B79+5GP2RT59CafuVz2vFdvHgRzzzzDJKSkuwucQcDpw7E398f165dsyi7du0a3NzcONpkR2JiYvjBvAOaN28eNm3ahD179jT7V8vGnlV/f/+2bCLdgNb0a31qtRqDBg3C6dOn26h11FoajQZhYWEAgNtvvx2HDh3Cu+++i48++qhBXT6nnUdr+rU+PqcdT0pKCnJycixm1hgMBuzZswerV69GVVUVlEqlxTWd5XnlGqcOJDY2Fjt27LAoS0pKanKeL3U+aWlpCAgIsHUzqIYsy5g3bx42bNiAX3/9FaGhoc1ew2e147uRfq3PYDAgIyODz2sHZjQaUVVVZfUcn9POq6l+rY/Pacdz1113ISMjA2lpaaZjyJAhePTRR5GWltYgaAI60fNq6+wU9qykpEROTU2VU1NTZQDy22+/Laempsrnz5+XZVmWFy1aJE+bNs1U/8yZM7KTk5P8/PPPy8ePH5fXrFkjK5VK+ZdffrHVW6B6Wtunq1atkjdu3ChnZmbKGRkZ8jPPPCMrFAp5+/bttnoLVM+cOXNkd3d3edeuXfLVq1dNR3l5uanOtGnT5EWLFpm+37t3r6xSqeSVK1fKx48fl5csWSKr1Wo5IyPDFm+BrLiRfl26dKm8detWOSsrS05JSZEfeughWavVyseOHbPFW6B6Fi1aJO/evVs+e/asnJ6eLi9atEiWJEnetm2bLMt8Tjur1vYrn9POqX5Wvc76vDJwakO1qajrHzNmzJBlWZZnzJghjxo1qsE1AwcOlDUajdyrVy957dq17d5ualxr+3TFihVy7969Za1WK3t6esqjR4+Wf/31V9s0nqyy1p8ALJ69UaNGmfq41jfffCNHRETIGo1Gjo6Oln/++ef2bTg16Ub6df78+XJwcLCs0WhkPz8/ecKECfKRI0fav/Fk1RNPPCGHhITIGo1G9vHxke+66y7Th2tZ5nPaWbW2X/mcdk71A6fO+rxKsizL7Te+RURERERE1PlwjRMREREREVEzGDgRERERERE1g4ETERERERFRMxg4ERERERERNYOBExERERERUTMYOBERERERETWDgRMREREREVEzGDgRERERERE1g4ETERFRjerqaoSFhWHfvn2N1jl37hwkSUJaWlqr7r1o0SI8/fTTN9lCIiKyFQZORERkc7m5uZgzZw6Cg4Ph4OAAf39/JCQkYO/evaY6PXv2hCRJOHDggMW18+fPx+jRo03fv/baa5AkCZIkQalUIigoCLNmzUJBQUGz7fjwww8RGhqKuLi4Fre9NpCqPTQaDcLCwvDGG29AlmVTvYULF+Lzzz/HmTNnWnxvIiLqOBg4ERGRzU2ZMgWpqan4/PPPcerUKfz4448YPXo08vPzLepptVq8+OKLzd4vOjoaV69exYULF7B27Vr88ssvmDNnTpPXyLKM1atX48knn7yh97B9+3ZcvXoVmZmZWLp0KZYtW4bPPvvMdN7b2xsJCQn44IMPbuj+RERkWwyciIjIpgoLC5GcnIwVK1YgPj4eISEhiImJwUsvvYR7773Xou6sWbNw4MABbN68ucl7qlQq+Pv7o0ePHhg7diweeOABJCUlNXlNSkoKsrKykJiYaFF+8OBBDBo0CFqtFkOGDEFqaqrV6728vODv74+QkBA8+uijGD58OI4cOWJRZ9KkSVi/fn2T7SAioo6JgRMREdmUi4sLXFxcsHHjRlRVVTVZNzQ0FLNnz8ZLL70Eo9HYovufO3cOW7duhUajabJecnIyIiIi4OrqaiorLS3FxIkTERUVhZSUFLz22mtYuHBhs695+PBhpKSkYNiwYRblMTExuHTpEs6dO9eithMRUcfBwImIiGxKpVJh3bp1+Pzzz+Hh4YHhw4fj5ZdfRnp6utX6f//733H27Fn87//+b6P3zMjIgIuLCxwdHREaGopjx441O8Xv/Pnz6N69u0XZl19+CaPRiE8//RTR0dGYOHEinn/+eavXx8XFwcXFBRqNBkOHDsXUqVMxffp0izq19z9//nyTbSEioo6HgRMREdnclClTcOXKFfz444+4++67sWvXLgwePBjr1q1rUNfHxwcLFy7Eq6++iurqaqv369OnD9LS0nDo0CG8+OKLSEhIaDajXUVFBbRarUXZ8ePH0b9/f4vy2NhYq9d//fXXSEtLwx9//IFvvvkGP/zwAxYtWmRRx9HREQBQXl7eZFuIiKjjYeBEREQdglarxbhx47B48WLs27cPjz32GJYsWWK17oIFC1BRUYH333/f6vnazHb9+vXDW2+9BaVSiaVLlzb5+t7e3rh+/foNtz8oKAhhYWHo27cvHnjgAcyfPx//+te/UFlZaapTm9nPx8fnhl+HiIhsg4ETERF1SFFRUSgrK7N6zsXFBYsXL8ayZctQUlLS7L3+/ve/Y+XKlbhy5UqjdQYNGoQTJ05YpBDv27cv0tPTLYKf+unQG6NUKqHX6y1GxY4ePQq1Wo3o6OgW3YOIiDoOBk5ERGRT+fn5GDNmDL744gukp6fj7Nmz+Pbbb/HPf/4T9913X6PXzZo1C+7u7vjyyy+bfY3Y2Fj0798fb775ZqN14uPjUVpaimPHjpnKHnnkEUiShJkzZ+LPP//E5s2bsXLlykbfR3Z2Ni5duoQtW7bg3XffRXx8PNzc3Ex1kpOTMXLkSNOUPSIi6jwYOBERkU25uLhg2LBhWLVqFe68807069cPixcvxsyZM7F69epGr1Or1Xj99dctRoOa8uyzz+KTTz7BxYsXrZ738vLC/fffb5F0wsXFBT/99BMyMjIwaNAgvPLKK1ixYoXV68eOHYuAgAD07NkTs2bNwoQJE/D1119b1Fm/fj1mzpzZovYSEVHHIsl15yQQERF1Yenp6Rg3bhyysrLg4uJyS++9ZcsWPPfcc0hPT4dKpbql9yYiorbHESciIqIa/fv3x4oVK3D27Nlbfu+ysjKsXbuWQRMRUSfFESciIiIiIqJmcMSJiIiIiIioGQyciIiIiIiImsHAiYiIiIiIqBkMnIiIiIiIiJrBwImIiIiIiKgZDJyIiIiIiIiawcCJiIiIiIioGQyciIiIiIiImsHAiYiIiIiIqBn/H0tAmmLrs1y9AAAAAElFTkSuQmCC"/>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
