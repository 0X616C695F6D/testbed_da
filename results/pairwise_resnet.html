<!DOCTYPE html>

<html lang="en">
<head><meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>pairwise_resnet</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<style type="text/css">
    pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: var(--jp-cell-editor-active-background) }
.highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) }
.highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */
.highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */
.highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */
.highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */
.highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */
.highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */
.highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */
.highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */
.highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */
.highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */
.highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */
.highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */
.highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */
.highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */
.highlight .pm { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation.Marker */
.highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */
.highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */
.highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */
.highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */
.highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */
.highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */
.highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */
.highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */
.highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */
.highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */
.highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */
.highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */
.highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */
.highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */
.highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */
.highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */
.highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */
.highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */
.highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */
.highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */
  </style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
 * Mozilla scrollbar styling
 */

/* use standard opaque scrollbars for most nodes */
[data-jp-theme-scrollbars='true'] {
  scrollbar-color: rgb(var(--jp-scrollbar-thumb-color))
    var(--jp-scrollbar-background-color);
}

/* for code nodes, use a transparent style of scrollbar. These selectors
 * will match lower in the tree, and so will override the above */
[data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar,
[data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
}

/* tiny scrollbar */

.jp-scrollbar-tiny {
  scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent;
  scrollbar-width: thin;
}

/* tiny scrollbar */

.jp-scrollbar-tiny::-webkit-scrollbar,
.jp-scrollbar-tiny::-webkit-scrollbar-corner {
  background-color: transparent;
  height: 4px;
  width: 4px;
}

.jp-scrollbar-tiny::-webkit-scrollbar-thumb {
  background: rgba(var(--jp-scrollbar-thumb-color), 0.5);
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:horizontal {
  border-left: 0 solid transparent;
  border-right: 0 solid transparent;
}

.jp-scrollbar-tiny::-webkit-scrollbar-track:vertical {
  border-top: 0 solid transparent;
  border-bottom: 0 solid transparent;
}

/*
 * Lumino
 */

.lm-ScrollBar[data-orientation='horizontal'] {
  min-height: 16px;
  max-height: 16px;
  min-width: 45px;
  border-top: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] {
  min-width: 16px;
  max-width: 16px;
  min-height: 45px;
  border-left: 1px solid #a0a0a0;
}

.lm-ScrollBar-button {
  background-color: #f0f0f0;
  background-position: center center;
  min-height: 15px;
  max-height: 15px;
  min-width: 15px;
  max-width: 15px;
}

.lm-ScrollBar-button:hover {
  background-color: #dadada;
}

.lm-ScrollBar-button.lm-mod-active {
  background-color: #cdcdcd;
}

.lm-ScrollBar-track {
  background: #f0f0f0;
}

.lm-ScrollBar-thumb {
  background: #cdcdcd;
}

.lm-ScrollBar-thumb:hover {
  background: #bababa;
}

.lm-ScrollBar-thumb.lm-mod-active {
  background: #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb {
  height: 100%;
  min-width: 15px;
  border-left: 1px solid #a0a0a0;
  border-right: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb {
  width: 100%;
  min-height: 15px;
  border-top: 1px solid #a0a0a0;
  border-bottom: 1px solid #a0a0a0;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-left);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='horizontal']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-right);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='decrement'] {
  background-image: var(--jp-icon-caret-up);
  background-size: 17px;
}

.lm-ScrollBar[data-orientation='vertical']
  .lm-ScrollBar-button[data-action='increment'] {
  background-image: var(--jp-icon-caret-down);
  background-size: 17px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Widget {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
}

.lm-Widget.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.lm-AccordionPanel[data-orientation='horizontal'] > .lm-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  display: flex;
  flex-direction: column;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-CommandPalette-search {
  flex: 0 0 auto;
}

.lm-CommandPalette-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  min-height: 0;
  overflow: auto;
  list-style-type: none;
}

.lm-CommandPalette-header {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-CommandPalette-item {
  display: flex;
  flex-direction: row;
}

.lm-CommandPalette-itemIcon {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemContent {
  flex: 1 1 auto;
  overflow: hidden;
}

.lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemLabel {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.lm-close-icon {
  border: 1px solid transparent;
  background-color: transparent;
  position: absolute;
  z-index: 1;
  right: 3%;
  top: 0;
  bottom: 0;
  margin: auto;
  padding: 7px 0;
  display: none;
  vertical-align: middle;
  outline: 0;
  cursor: pointer;
}
.lm-close-icon:after {
  content: 'X';
  display: block;
  width: 15px;
  height: 15px;
  text-align: center;
  color: #000;
  font-weight: normal;
  font-size: 12px;
  cursor: pointer;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-DockPanel {
  z-index: 0;
}

.lm-DockPanel-widget {
  z-index: 0;
}

.lm-DockPanel-tabBar {
  z-index: 1;
}

.lm-DockPanel-handle {
  z-index: 2;
}

.lm-DockPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-DockPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-DockPanel-handle[data-orientation='horizontal'] {
  cursor: ew-resize;
}

.lm-DockPanel-handle[data-orientation='vertical'] {
  cursor: ns-resize;
}

.lm-DockPanel-handle[data-orientation='horizontal']:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-DockPanel-handle[data-orientation='vertical']:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

.lm-DockPanel-overlay {
  z-index: 3;
  box-sizing: border-box;
  pointer-events: none;
}

.lm-DockPanel-overlay.lm-mod-hidden {
  display: none !important;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-Menu {
  z-index: 10000;
  position: absolute;
  white-space: nowrap;
  overflow-x: hidden;
  overflow-y: auto;
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-Menu-content {
  margin: 0;
  padding: 0;
  display: table;
  list-style-type: none;
}

.lm-Menu-item {
  display: table-row;
}

.lm-Menu-item.lm-mod-hidden,
.lm-Menu-item.lm-mod-collapsed {
  display: none !important;
}

.lm-Menu-itemIcon,
.lm-Menu-itemSubmenuIcon {
  display: table-cell;
  text-align: center;
}

.lm-Menu-itemLabel {
  display: table-cell;
  text-align: left;
}

.lm-Menu-itemShortcut {
  display: table-cell;
  text-align: right;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-MenuBar {
  outline: none;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-MenuBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex-direction: row;
  list-style-type: none;
}

.lm-MenuBar-item {
  box-sizing: border-box;
}

.lm-MenuBar-itemIcon,
.lm-MenuBar-itemLabel {
  display: inline-block;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-ScrollBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-ScrollBar[data-orientation='horizontal'] {
  flex-direction: row;
}

.lm-ScrollBar[data-orientation='vertical'] {
  flex-direction: column;
}

.lm-ScrollBar-button {
  box-sizing: border-box;
  flex: 0 0 auto;
}

.lm-ScrollBar-track {
  box-sizing: border-box;
  position: relative;
  overflow: hidden;
  flex: 1 1 auto;
}

.lm-ScrollBar-thumb {
  box-sizing: border-box;
  position: absolute;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-SplitPanel-child {
  z-index: 0;
}

.lm-SplitPanel-handle {
  z-index: 1;
}

.lm-SplitPanel-handle.lm-mod-hidden {
  display: none !important;
}

.lm-SplitPanel-handle:after {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  content: '';
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle {
  cursor: ew-resize;
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle {
  cursor: ns-resize;
}

.lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after {
  left: 50%;
  min-width: 8px;
  transform: translateX(-50%);
}

.lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after {
  top: 50%;
  min-height: 8px;
  transform: translateY(-50%);
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabBar {
  display: flex;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.lm-TabBar[data-orientation='horizontal'] {
  flex-direction: row;
  align-items: flex-end;
}

.lm-TabBar[data-orientation='vertical'] {
  flex-direction: column;
  align-items: flex-end;
}

.lm-TabBar-content {
  margin: 0;
  padding: 0;
  display: flex;
  flex: 1 1 auto;
  list-style-type: none;
}

.lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content {
  flex-direction: row;
}

.lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content {
  flex-direction: column;
}

.lm-TabBar-tab {
  display: flex;
  flex-direction: row;
  box-sizing: border-box;
  overflow: hidden;
  touch-action: none; /* Disable native Drag/Drop */
}

.lm-TabBar-tabIcon,
.lm-TabBar-tabCloseIcon {
  flex: 0 0 auto;
}

.lm-TabBar-tabLabel {
  flex: 1 1 auto;
  overflow: hidden;
  white-space: nowrap;
}

.lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
}

.lm-TabBar-tab.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar-addButton.lm-mod-hidden {
  display: none !important;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab {
  position: relative;
}

.lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab {
  left: 0;
  transition: left 150ms ease;
}

.lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab {
  top: 0;
  transition: top 150ms ease;
}

.lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging {
  transition: none;
}

.lm-TabBar-tabLabel .lm-TabBar-tabInput {
  user-select: all;
  width: 100%;
  box-sizing: border-box;
  background: inherit;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-TabPanel-tabBar {
  z-index: 1;
}

.lm-TabPanel-stackedPanel {
  z-index: 0;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapse {
  display: flex;
  flex-direction: column;
  align-items: stretch;
}

.jp-Collapse-header {
  padding: 1px 12px;
  background-color: var(--jp-layout-color1);
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  align-items: center;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  text-transform: uppercase;
  user-select: none;
}

.jp-Collapser-icon {
  height: 16px;
}

.jp-Collapse-header-collapsed .jp-Collapser-icon {
  transform: rotate(-90deg);
  margin: auto 0;
}

.jp-Collapser-title {
  line-height: 25px;
}

.jp-Collapse-contents {
  padding: 0 12px;
  background-color: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

/* Icons urls */

:root {
  --jp-icon-add-above: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5MikiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik00Ljc1IDQuOTMwNjZINi42MjVWNi44MDU2NkM2LjYyNSA3LjAxMTkxIDYuNzkzNzUgNy4xODA2NiA3IDcuMTgwNjZDNy4yMDYyNSA3LjE4MDY2IDcuMzc1IDcuMDExOTEgNy4zNzUgNi44MDU2NlY0LjkzMDY2SDkuMjVDOS40NTYyNSA0LjkzMDY2IDkuNjI1IDQuNzYxOTEgOS42MjUgNC41NTU2NkM5LjYyNSA0LjM0OTQxIDkuNDU2MjUgNC4xODA2NiA5LjI1IDQuMTgwNjZINy4zNzVWMi4zMDU2NkM3LjM3NSAyLjA5OTQxIDcuMjA2MjUgMS45MzA2NiA3IDEuOTMwNjZDNi43OTM3NSAxLjkzMDY2IDYuNjI1IDIuMDk5NDEgNi42MjUgMi4zMDU2NlY0LjE4MDY2SDQuNzVDNC41NDM3NSA0LjE4MDY2IDQuMzc1IDQuMzQ5NDEgNC4zNzUgNC41NTU2NkM0LjM3NSA0Ljc2MTkxIDQuNTQzNzUgNC45MzA2NiA0Ljc1IDQuOTMwNjZaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC43Ii8+CjwvZz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTExLjUgOS41VjExLjVMMi41IDExLjVWOS41TDExLjUgOS41Wk0xMiA4QzEyLjU1MjMgOCAxMyA4LjQ0NzcyIDEzIDlWMTJDMTMgMTIuNTUyMyAxMi41NTIzIDEzIDEyIDEzTDIgMTNDMS40NDc3MiAxMyAxIDEyLjU1MjMgMSAxMlY5QzEgOC40NDc3MiAxLjQ0NzcxIDggMiA4TDEyIDhaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5MiI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KC0xIDAgMCAxIDEwIDEuNTU1NjYpIi8+CjwvY2xpcFBhdGg+CjwvZGVmcz4KPC9zdmc+Cg==);
  --jp-icon-add-below: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgY2xpcC1wYXRoPSJ1cmwoI2NsaXAwXzEzN18xOTQ5OCkiPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGQ9Ik05LjI1IDEwLjA2OTNMNy4zNzUgMTAuMDY5M0w3LjM3NSA4LjE5NDM0QzcuMzc1IDcuOTg4MDkgNy4yMDYyNSA3LjgxOTM0IDcgNy44MTkzNEM2Ljc5Mzc1IDcuODE5MzQgNi42MjUgNy45ODgwOSA2LjYyNSA4LjE5NDM0TDYuNjI1IDEwLjA2OTNMNC43NSAxMC4wNjkzQzQuNTQzNzUgMTAuMDY5MyA0LjM3NSAxMC4yMzgxIDQuMzc1IDEwLjQ0NDNDNC4zNzUgMTAuNjUwNiA0LjU0Mzc1IDEwLjgxOTMgNC43NSAxMC44MTkzTDYuNjI1IDEwLjgxOTNMNi42MjUgMTIuNjk0M0M2LjYyNSAxMi45MDA2IDYuNzkzNzUgMTMuMDY5MyA3IDEzLjA2OTNDNy4yMDYyNSAxMy4wNjkzIDcuMzc1IDEyLjkwMDYgNy4zNzUgMTIuNjk0M0w3LjM3NSAxMC44MTkzTDkuMjUgMTAuODE5M0M5LjQ1NjI1IDEwLjgxOTMgOS42MjUgMTAuNjUwNiA5LjYyNSAxMC40NDQzQzkuNjI1IDEwLjIzODEgOS40NTYyNSAxMC4wNjkzIDkuMjUgMTAuMDY5M1oiIGZpbGw9IiM2MTYxNjEiIHN0cm9rZT0iIzYxNjE2MSIgc3Ryb2tlLXdpZHRoPSIwLjciLz4KPC9nPgo8cGF0aCBjbGFzcz0ianAtaWNvbjMiIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMi41IDUuNUwyLjUgMy41TDExLjUgMy41TDExLjUgNS41TDIuNSA1LjVaTTIgN0MxLjQ0NzcyIDcgMSA2LjU1MjI4IDEgNkwxIDNDMSAyLjQ0NzcyIDEuNDQ3NzIgMiAyIDJMMTIgMkMxMi41NTIzIDIgMTMgMi40NDc3MiAxMyAzTDEzIDZDMTMgNi41NTIyOSAxMi41NTIzIDcgMTIgN0wyIDdaIiBmaWxsPSIjNjE2MTYxIi8+CjxkZWZzPgo8Y2xpcFBhdGggaWQ9ImNsaXAwXzEzN18xOTQ5OCI+CjxyZWN0IGNsYXNzPSJqcC1pY29uMyIgd2lkdGg9IjYiIGhlaWdodD0iNiIgZmlsbD0id2hpdGUiIHRyYW5zZm9ybT0ibWF0cml4KDEgMS43NDg0NmUtMDcgMS43NDg0NmUtMDcgLTEgNCAxMy40NDQzKSIvPgo8L2NsaXBQYXRoPgo8L2RlZnM+Cjwvc3ZnPgo=);
  --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bell: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE2IDE2IiB2ZXJzaW9uPSIxLjEiPgogICA8cGF0aCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMzMzMzMzIgogICAgICBkPSJtOCAwLjI5Yy0xLjQgMC0yLjcgMC43My0zLjYgMS44LTEuMiAxLjUtMS40IDMuNC0xLjUgNS4yLTAuMTggMi4yLTAuNDQgNC0yLjMgNS4zbDAuMjggMS4zaDVjMC4wMjYgMC42NiAwLjMyIDEuMSAwLjcxIDEuNSAwLjg0IDAuNjEgMiAwLjYxIDIuOCAwIDAuNTItMC40IDAuNi0xIDAuNzEtMS41aDVsMC4yOC0xLjNjLTEuOS0wLjk3LTIuMi0zLjMtMi4zLTUuMy0wLjEzLTEuOC0wLjI2LTMuNy0xLjUtNS4yLTAuODUtMS0yLjItMS44LTMuNi0xLjh6bTAgMS40YzAuODggMCAxLjkgMC41NSAyLjUgMS4zIDAuODggMS4xIDEuMSAyLjcgMS4yIDQuNCAwLjEzIDEuNyAwLjIzIDMuNiAxLjMgNS4yaC0xMGMxLjEtMS42IDEuMi0zLjQgMS4zLTUuMiAwLjEzLTEuNyAwLjMtMy4zIDEuMi00LjQgMC41OS0wLjcyIDEuNi0xLjMgMi41LTEuM3ptLTAuNzQgMTJoMS41Yy0wLjAwMTUgMC4yOCAwLjAxNSAwLjc5LTAuNzQgMC43OS0wLjczIDAuMDAxNi0wLjcyLTAuNTMtMC43NC0wLjc5eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-bug-dot: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiPgogICAgICAgIDxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMTcuMTkgOEgyMFYxMEgxNy45MUMxNy45NiAxMC4zMyAxOCAxMC42NiAxOCAxMVYxMkgyMFYxNEgxOC41SDE4VjE0LjAyNzVDMTUuNzUgMTQuMjc2MiAxNCAxNi4xODM3IDE0IDE4LjVDMTQgMTkuMjA4IDE0LjE2MzUgMTkuODc3OSAxNC40NTQ5IDIwLjQ3MzlDMTMuNzA2MyAyMC44MTE3IDEyLjg3NTcgMjEgMTIgMjFDOS43OCAyMSA3Ljg1IDE5Ljc5IDYuODEgMThINFYxNkg2LjA5QzYuMDQgMTUuNjcgNiAxNS4zNCA2IDE1VjE0SDRWMTJINlYxMUM2IDEwLjY2IDYuMDQgMTAuMzMgNi4wOSAxMEg0VjhINi44MUM3LjI2IDcuMjIgNy44OCA2LjU1IDguNjIgNi4wNEw3IDQuNDFMOC40MSAzTDEwLjU5IDUuMTdDMTEuMDQgNS4wNiAxMS41MSA1IDEyIDVDMTIuNDkgNSAxMi45NiA1LjA2IDEzLjQyIDUuMTdMMTUuNTkgM0wxNyA0LjQxTDE1LjM3IDYuMDRDMTYuMTIgNi41NSAxNi43NCA3LjIyIDE3LjE5IDhaTTEwIDE2SDE0VjE0SDEwVjE2Wk0xMCAxMkgxNFYxMEgxMFYxMloiIGZpbGw9IiM2MTYxNjEiLz4KICAgICAgICA8cGF0aCBkPSJNMjIgMTguNUMyMiAyMC40MzMgMjAuNDMzIDIyIDE4LjUgMjJDMTYuNTY3IDIyIDE1IDIwLjQzMyAxNSAxOC41QzE1IDE2LjU2NyAxNi41NjcgMTUgMTguNSAxNUMyMC40MzMgMTUgMjIgMTYuNTY3IDIyIDE4LjVaIiBmaWxsPSIjNjE2MTYxIi8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yMCA4aC0yLjgxYy0uNDUtLjc4LTEuMDctMS40NS0xLjgyLTEuOTZMMTcgNC40MSAxNS41OSAzbC0yLjE3IDIuMTdDMTIuOTYgNS4wNiAxMi40OSA1IDEyIDVjLS40OSAwLS45Ni4wNi0xLjQxLjE3TDguNDEgMyA3IDQuNDFsMS42MiAxLjYzQzcuODggNi41NSA3LjI2IDcuMjIgNi44MSA4SDR2MmgyLjA5Yy0uMDUuMzMtLjA5LjY2LS4wOSAxdjFINHYyaDJ2MWMwIC4zNC4wNC42Ny4wOSAxSDR2MmgyLjgxYzEuMDQgMS43OSAyLjk3IDMgNS4xOSAzczQuMTUtMS4yMSA1LjE5LTNIMjB2LTJoLTIuMDljLjA1LS4zMy4wOS0uNjYuMDktMXYtMWgydi0yaC0ydi0xYzAtLjM0LS4wNC0uNjctLjA5LTFIMjBWOHptLTYgOGgtNHYtMmg0djJ6bTAtNGgtNHYtMmg0djJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-build: url(data:image/svg+xml;base64,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);
  --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);
  --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik05IDE2LjE3TDQuODMgMTJsLTEuNDIgMS40MUw5IDE5IDIxIDdsLTEuNDEtMS40MXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBzaGFwZS1yZW5kZXJpbmc9Imdlb21ldHJpY1ByZWNpc2lvbiI+CiAgICA8cGF0aCBkPSJNNi41OSwzLjQxTDIsOEw2LjU5LDEyLjZMOCwxMS4xOEw0LjgyLDhMOCw0LjgyTDYuNTksMy40MU0xMi40MSwzLjQxTDExLDQuODJMMTQuMTgsOEwxMSwxMS4xOEwxMi40MSwxMi42TDE3LDhMMTIuNDEsMy40MU0yMS41OSwxMS41OUwxMy41LDE5LjY4TDkuODMsMTZMOC40MiwxNy40MUwxMy41LDIyLjVMMjMsMTNMMjEuNTksMTEuNTlaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-code: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTExLjQgMTguNkw2LjggMTRMMTEuNCA5LjRMMTAgOEw0IDE0TDEwIDIwTDExLjQgMTguNlpNMTYuNiAxOC42TDIxLjIgMTRMMTYuNiA5LjRMMTggOEwyNCAxNEwxOCAyMEwxNi42IDE4LjZWMTguNloiLz4KCTwvZz4KPC9zdmc+Cg==);
  --jp-icon-collapse-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNNiAxM3YyaDh2LTJ6IiAvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1jb25zb2xlLWljb24tYmFja2dyb3VuZC1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtY29uc29sZS1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIj4KICAgIDxwYXRoIGQ9Ik0xMDUgMTI3LjNoNDB2MTIuOGgtNDB6TTUxLjEgNzdMNzQgOTkuOWwtMjMuMyAyMy4zIDEwLjUgMTAuNSAyMy4zLTIzLjNMOTUgOTkuOSA4NC41IDg5LjQgNjEuNiA2Ni41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-copyright: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGVuYWJsZS1iYWNrZ3JvdW5kPSJuZXcgMCAwIDI0IDI0IiBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCI+CiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0xMS44OCw5LjE0YzEuMjgsMC4wNiwxLjYxLDEuMTUsMS42MywxLjY2aDEuNzljLTAuMDgtMS45OC0xLjQ5LTMuMTktMy40NS0zLjE5QzkuNjQsNy42MSw4LDksOCwxMi4xNCBjMCwxLjk0LDAuOTMsNC4yNCwzLjg0LDQuMjRjMi4yMiwwLDMuNDEtMS42NSwzLjQ0LTIuOTVoLTEuNzljLTAuMDMsMC41OS0wLjQ1LDEuMzgtMS42MywxLjQ0QzEwLjU1LDE0LjgzLDEwLDEzLjgxLDEwLDEyLjE0IEMxMCw5LjI1LDExLjI4LDkuMTYsMTEuODgsOS4xNHogTTEyLDJDNi40OCwyLDIsNi40OCwyLDEyczQuNDgsMTAsMTAsMTBzMTAtNC40OCwxMC0xMFMxNy41MiwyLDEyLDJ6IE0xMiwyMGMtNC40MSwwLTgtMy41OS04LTggczMuNTktOCw4LThzOCwzLjU5LDgsOFMxNi40MSwyMCwxMiwyMHoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-delete: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2cHgiIGhlaWdodD0iMTZweCI+CiAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIiAvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjI2MjYyIiBkPSJNNiAxOWMwIDEuMS45IDIgMiAyaDhjMS4xIDAgMi0uOSAyLTJWN0g2djEyek0xOSA0aC0zLjVsLTEtMWgtNWwtMSAxSDV2MmgxNFY0eiIgLz4KPC9zdmc+Cg==);
  --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-duplicate: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTIuNzk5OTggMC44NzVIOC44OTU4MkM5LjIwMDYxIDAuODc1IDkuNDQ5OTggMS4xMzkxNCA5LjQ0OTk4IDEuNDYxOThDOS40NDk5OCAxLjc4NDgyIDkuMjAwNjEgMi4wNDg5NiA4Ljg5NTgyIDIuMDQ4OTZIMy4zNTQxNUMzLjA0OTM2IDIuMDQ4OTYgMi43OTk5OCAyLjMxMzEgMi43OTk5OCAyLjYzNTk0VjkuNjc5NjlDMi43OTk5OCAxMC4wMDI1IDIuNTUwNjEgMTAuMjY2NyAyLjI0NTgyIDEwLjI2NjdDMS45NDEwMyAxMC4yNjY3IDEuNjkxNjUgMTAuMDAyNSAxLjY5MTY1IDkuNjc5NjlWMi4wNDg5NkMxLjY5MTY1IDEuNDAzMjggMi4xOTA0IDAuODc1IDIuNzk5OTggMC44NzVaTTUuMzY2NjUgMTEuOVY0LjU1SDExLjA4MzNWMTEuOUg1LjM2NjY1Wk00LjE0MTY1IDQuMTQxNjdDNC4xNDE2NSAzLjY5MDYzIDQuNTA3MjggMy4zMjUgNC45NTgzMiAzLjMyNUgxMS40OTE3QzExLjk0MjcgMy4zMjUgMTIuMzA4MyAzLjY5MDYzIDEyLjMwODMgNC4xNDE2N1YxMi4zMDgzQzEyLjMwODMgMTIuNzU5NCAxMS45NDI3IDEzLjEyNSAxMS40OTE3IDEzLjEyNUg0Ljk1ODMyQzQuNTA3MjggMTMuMTI1IDQuMTQxNjUgMTIuNzU5NCA0LjE0MTY1IDEyLjMwODNWNC4xNDE2N1oiIGZpbGw9IiM2MTYxNjEiLz4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNOS40MzU3NCA4LjI2NTA3SDguMzY0MzFWOS4zMzY1QzguMzY0MzEgOS40NTQzNSA4LjI2Nzg4IDkuNTUwNzggOC4xNTAwMiA5LjU1MDc4QzguMDMyMTcgOS41NTA3OCA3LjkzNTc0IDkuNDU0MzUgNy45MzU3NCA5LjMzNjVWOC4yNjUwN0g2Ljg2NDMxQzYuNzQ2NDUgOC4yNjUwNyA2LjY1MDAyIDguMTY4NjQgNi42NTAwMiA4LjA1MDc4QzYuNjUwMDIgNy45MzI5MiA2Ljc0NjQ1IDcuODM2NSA2Ljg2NDMxIDcuODM2NUg3LjkzNTc0VjYuNzY1MDdDNy45MzU3NCA2LjY0NzIxIDguMDMyMTcgNi41NTA3OCA4LjE1MDAyIDYuNTUwNzhDOC4yNjc4OCA2LjU1MDc4IDguMzY0MzEgNi42NDcyMSA4LjM2NDMxIDYuNzY1MDdWNy44MzY1SDkuNDM1NzRDOS41NTM2IDcuODM2NSA5LjY1MDAyIDcuOTMyOTIgOS42NTAwMiA4LjA1MDc4QzkuNjUwMDIgOC4xNjg2NCA5LjU1MzYgOC4yNjUwNyA5LjQzNTc0IDguMjY1MDdaIiBmaWxsPSIjNjE2MTYxIiBzdHJva2U9IiM2MTYxNjEiIHN0cm9rZS13aWR0aD0iMC41Ii8+Cjwvc3ZnPgo=);
  --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-error: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjE5IiByPSIyIi8+PHBhdGggZD0iTTEwIDNoNHYxMmgtNHoiLz48L2c+CjxwYXRoIGZpbGw9Im5vbmUiIGQ9Ik0wIDBoMjR2MjRIMHoiLz4KPC9zdmc+Cg==);
  --jp-icon-expand-all: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTggMmMxIDAgMTEgMCAxMiAwczIgMSAyIDJjMCAxIDAgMTEgMCAxMnMwIDItMiAyQzIwIDE0IDIwIDQgMjAgNFMxMCA0IDYgNGMwLTIgMS0yIDItMnoiIC8+CiAgICAgICAgPHBhdGgKICAgICAgICAgICAgZD0iTTE4IDhjMC0xLTEtMi0yLTJTNSA2IDQgNnMtMiAxLTIgMmMwIDEgMCAxMSAwIDEyczEgMiAyIDJjMSAwIDExIDAgMTIgMHMyLTEgMi0yYzAtMSAwLTExIDAtMTJ6bS0yIDB2MTJINFY4eiIgLz4KICAgICAgICA8cGF0aCBkPSJNMTEgMTBIOXYzSDZ2MmgzdjNoMnYtM2gzdi0yaC0zeiIgLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-filter-dot: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWRvdCIgZmlsbD0iI0ZGRiI+CiAgICA8Y2lyY2xlIGN4PSIxOCIgY3k9IjE3IiByPSIzIj48L2NpcmNsZT4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-filter: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTE0LDEyVjE5Ljg4QzE0LjA0LDIwLjE4IDEzLjk0LDIwLjUgMTMuNzEsMjAuNzFDMTMuMzIsMjEuMSAxMi42OSwyMS4xIDEyLjMsMjAuNzFMMTAuMjksMTguN0MxMC4wNiwxOC40NyA5Ljk2LDE4LjE2IDEwLDE3Ljg3VjEySDkuOTdMNC4yMSw0LjYyQzMuODcsNC4xOSAzLjk1LDMuNTYgNC4zOCwzLjIyQzQuNTcsMy4wOCA0Ljc4LDMgNSwzVjNIMTlWM0MxOS4yMiwzIDE5LjQzLDMuMDggMTkuNjIsMy4yMkMyMC4wNSwzLjU2IDIwLjEzLDQuMTkgMTkuNzksNC42MkwxNC4wMywxMkgxNFoiIC8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-folder-favorite: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgwVjB6IiBmaWxsPSJub25lIi8+PHBhdGggY2xhc3M9ImpwLWljb24zIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzYxNjE2MSIgZD0iTTIwIDZoLThsLTItMkg0Yy0xLjEgMC0yIC45LTIgMnYxMmMwIDEuMS45IDIgMiAyaDE2YzEuMSAwIDItLjkgMi0yVjhjMC0xLjEtLjktMi0yLTJ6bS0yLjA2IDExTDE1IDE1LjI4IDEyLjA2IDE3bC43OC0zLjMzLTIuNTktMi4yNCAzLjQxLS4yOUwxNSA4bDEuMzQgMy4xNCAzLjQxLjI5LTIuNTkgMi4yNC43OCAzLjMzeiIvPgo8L3N2Zz4K);
  --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-home: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjRweCIgdmlld0JveD0iMCAwIDI0IDI0IiB3aWR0aD0iMjRweCIgZmlsbD0iIzAwMDAwMCI+CiAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPjxwYXRoIGNsYXNzPSJqcC1pY29uMyBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xMCAyMHYtNmg0djZoNXYtOGgzTDEyIDMgMiAxMmgzdjh6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);
  --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-info: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUwLjk3OCA1MC45NzgiPgoJPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KCQk8cGF0aCBkPSJNNDMuNTIsNy40NThDMzguNzExLDIuNjQ4LDMyLjMwNywwLDI1LjQ4OSwwQzE4LjY3LDAsMTIuMjY2LDIuNjQ4LDcuNDU4LDcuNDU4CgkJCWMtOS45NDMsOS45NDEtOS45NDMsMjYuMTE5LDAsMzYuMDYyYzQuODA5LDQuODA5LDExLjIxMiw3LjQ1NiwxOC4wMzEsNy40NThjMCwwLDAuMDAxLDAsMC4wMDIsMAoJCQljNi44MTYsMCwxMy4yMjEtMi42NDgsMTguMDI5LTcuNDU4YzQuODA5LTQuODA5LDcuNDU3LTExLjIxMiw3LjQ1Ny0xOC4wM0M1MC45NzcsMTguNjcsNDguMzI4LDEyLjI2Niw0My41Miw3LjQ1OHoKCQkJIE00Mi4xMDYsNDIuMTA1Yy00LjQzMiw0LjQzMS0xMC4zMzIsNi44NzItMTYuNjE1LDYuODcyaC0wLjAwMmMtNi4yODUtMC4wMDEtMTIuMTg3LTIuNDQxLTE2LjYxNy02Ljg3MgoJCQljLTkuMTYyLTkuMTYzLTkuMTYyLTI0LjA3MSwwLTMzLjIzM0MxMy4zMDMsNC40NCwxOS4yMDQsMiwyNS40ODksMmM2LjI4NCwwLDEyLjE4NiwyLjQ0LDE2LjYxNyw2Ljg3MgoJCQljNC40MzEsNC40MzEsNi44NzEsMTAuMzMyLDYuODcxLDE2LjYxN0M0OC45NzcsMzEuNzcyLDQ2LjUzNiwzNy42NzUsNDIuMTA2LDQyLjEwNXoiLz4KCQk8cGF0aCBkPSJNMjMuNTc4LDMyLjIxOGMtMC4wMjMtMS43MzQsMC4xNDMtMy4wNTksMC40OTYtMy45NzJjMC4zNTMtMC45MTMsMS4xMS0xLjk5NywyLjI3Mi0zLjI1MwoJCQljMC40NjgtMC41MzYsMC45MjMtMS4wNjIsMS4zNjctMS41NzVjMC42MjYtMC43NTMsMS4xMDQtMS40NzgsMS40MzYtMi4xNzVjMC4zMzEtMC43MDcsMC40OTUtMS41NDEsMC40OTUtMi41CgkJCWMwLTEuMDk2LTAuMjYtMi4wODgtMC43NzktMi45NzljLTAuNTY1LTAuODc5LTEuNTAxLTEuMzM2LTIuODA2LTEuMzY5Yy0xLjgwMiwwLjA1Ny0yLjk4NSwwLjY2Ny0zLjU1LDEuODMyCgkJCWMtMC4zMDEsMC41MzUtMC41MDMsMS4xNDEtMC42MDcsMS44MTRjLTAuMTM5LDAuNzA3LTAuMjA3LDEuNDMyLTAuMjA3LDIuMTc0aC0yLjkzN2MtMC4wOTEtMi4yMDgsMC40MDctNC4xMTQsMS40OTMtNS43MTkKCQkJYzEuMDYyLTEuNjQsMi44NTUtMi40ODEsNS4zNzgtMi41MjdjMi4xNiwwLjAyMywzLjg3NCwwLjYwOCw1LjE0MSwxLjc1OGMxLjI3OCwxLjE2LDEuOTI5LDIuNzY0LDEuOTUsNC44MTEKCQkJYzAsMS4xNDItMC4xMzcsMi4xMTEtMC40MSwyLjkxMWMtMC4zMDksMC44NDUtMC43MzEsMS41OTMtMS4yNjgsMi4yNDNjLTAuNDkyLDAuNjUtMS4wNjgsMS4zMTgtMS43MywyLjAwMgoJCQljLTAuNjUsMC42OTctMS4zMTMsMS40NzktMS45ODcsMi4zNDZjLTAuMjM5LDAuMzc3LTAuNDI5LDAuNzc3LTAuNTY1LDEuMTk5Yy0wLjE2LDAuOTU5LTAuMjE3LDEuOTUxLTAuMTcxLDIuOTc5CgkJCUMyNi41ODksMzIuMjE4LDIzLjU3OCwzMi4yMTgsMjMuNTc4LDMyLjIxOHogTTIzLjU3OCwzOC4yMnYtMy40ODRoMy4wNzZ2My40ODRIMjMuNTc4eiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaW5zcGVjdG9yLWljb24tY29sb3IganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);
  --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtanNvbi1pY29uLWNvbG9yIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0Y5QTgyNSI+CiAgICA8cGF0aCBkPSJNMjAuMiAxMS44Yy0xLjYgMC0xLjcuNS0xLjcgMSAwIC40LjEuOS4xIDEuMy4xLjUuMS45LjEgMS4zIDAgMS43LTEuNCAyLjMtMy41IDIuM2gtLjl2LTEuOWguNWMxLjEgMCAxLjQgMCAxLjQtLjggMC0uMyAwLS42LS4xLTEgMC0uNC0uMS0uOC0uMS0xLjIgMC0xLjMgMC0xLjggMS4zLTItMS4zLS4yLTEuMy0uNy0xLjMtMiAwLS40LjEtLjguMS0xLjIuMS0uNC4xLS43LjEtMSAwLS44LS40LS43LTEuNC0uOGgtLjVWNC4xaC45YzIuMiAwIDMuNS43IDMuNSAyLjMgMCAuNC0uMS45LS4xIDEuMy0uMS41LS4xLjktLjEgMS4zIDAgLjUuMiAxIDEuNyAxdjEuOHpNMS44IDEwLjFjMS42IDAgMS43LS41IDEuNy0xIDAtLjQtLjEtLjktLjEtMS4zLS4xLS41LS4xLS45LS4xLTEuMyAwLTEuNiAxLjQtMi4zIDMuNS0yLjNoLjl2MS45aC0uNWMtMSAwLTEuNCAwLTEuNC44IDAgLjMgMCAuNi4xIDEgMCAuMi4xLjYuMSAxIDAgMS4zIDAgMS44LTEuMyAyQzYgMTEuMiA2IDExLjcgNiAxM2MwIC40LS4xLjgtLjEgMS4yLS4xLjMtLjEuNy0uMSAxIDAgLjguMy44IDEuNC44aC41djEuOWgtLjljLTIuMSAwLTMuNS0uNi0zLjUtMi4zIDAtLjQuMS0uOS4xLTEuMy4xLS41LjEtLjkuMS0xLjMgMC0uNS0uMi0xLTEuNy0xdi0xLjl6Ii8+CiAgICA8Y2lyY2xlIGN4PSIxMSIgY3k9IjEzLjgiIHI9IjIuMSIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSI4LjIiIHI9IjIuMSIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-julia: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDMyNSAzMDAiPgogIDxnIGNsYXNzPSJqcC1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjY2IzYzMzIj4KICAgIDxwYXRoIGQ9Ik0gMTUwLjg5ODQzOCAyMjUgQyAxNTAuODk4NDM4IDI2Ni40MjE4NzUgMTE3LjMyMDMxMiAzMDAgNzUuODk4NDM4IDMwMCBDIDM0LjQ3NjU2MiAzMDAgMC44OTg0MzggMjY2LjQyMTg3NSAwLjg5ODQzOCAyMjUgQyAwLjg5ODQzOCAxODMuNTc4MTI1IDM0LjQ3NjU2MiAxNTAgNzUuODk4NDM4IDE1MCBDIDExNy4zMjAzMTIgMTUwIDE1MC44OTg0MzggMTgzLjU3ODEyNSAxNTAuODk4NDM4IDIyNSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzM4OTgyNiI+CiAgICA8cGF0aCBkPSJNIDIzNy41IDc1IEMgMjM3LjUgMTE2LjQyMTg3NSAyMDMuOTIxODc1IDE1MCAxNjIuNSAxNTAgQyAxMjEuMDc4MTI1IDE1MCA4Ny41IDExNi40MjE4NzUgODcuNSA3NSBDIDg3LjUgMzMuNTc4MTI1IDEyMS4wNzgxMjUgMCAxNjIuNSAwIEMgMjAzLjkyMTg3NSAwIDIzNy41IDMzLjU3ODEyNSAyMzcuNSA3NSIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzk1NThiMiI+CiAgICA8cGF0aCBkPSJNIDMyNC4xMDE1NjIgMjI1IEMgMzI0LjEwMTU2MiAyNjYuNDIxODc1IDI5MC41MjM0MzggMzAwIDI0OS4xMDE1NjIgMzAwIEMgMjA3LjY3OTY4OCAzMDAgMTc0LjEwMTU2MiAyNjYuNDIxODc1IDE3NC4xMDE1NjIgMjI1IEMgMTc0LjEwMTU2MiAxODMuNTc4MTI1IDIwNy42Nzk2ODggMTUwIDI0OS4xMDE1NjIgMTUwIEMgMjkwLjUyMzQzOCAxNTAgMzI0LjEwMTU2MiAxODMuNTc4MTI1IDMyNC4xMDE1NjIgMjI1Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgPGcgY2xhc3M9ImpwLWp1cHl0ZXItaWNvbi1jb2xvciIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-jupyter: url(data:image/svg+xml;base64,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);
  --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);
  --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);
  --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);
  --jp-icon-launch: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMzIgMzIiIHdpZHRoPSIzMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik0yNiwyOEg2YTIuMDAyNywyLjAwMjcsMCwwLDEtMi0yVjZBMi4wMDI3LDIuMDAyNywwLDAsMSw2LDRIMTZWNkg2VjI2SDI2VjE2aDJWMjZBMi4wMDI3LDIuMDAyNywwLDAsMSwyNiwyOFoiLz4KICAgIDxwb2x5Z29uIHBvaW50cz0iMjAgMiAyMCA0IDI2LjU4NiA0IDE4IDEyLjU4NiAxOS40MTQgMTQgMjggNS40MTQgMjggMTIgMzAgMTIgMzAgMiAyMCAyIi8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);
  --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4K);
  --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);
  --jp-icon-move-down: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMTIuNDcxIDcuNTI4OTlDMTIuNzYzMiA3LjIzNjg0IDEyLjc2MzIgNi43NjMxNiAxMi40NzEgNi40NzEwMVY2LjQ3MTAxQzEyLjE3OSA2LjE3OTA1IDExLjcwNTcgNi4xNzg4NCAxMS40MTM1IDYuNDcwNTRMNy43NSAxMC4xMjc1VjEuNzVDNy43NSAxLjMzNTc5IDcuNDE0MjEgMSA3IDFWMUM2LjU4NTc5IDEgNi4yNSAxLjMzNTc5IDYuMjUgMS43NVYxMC4xMjc1TDIuNTk3MjYgNi40NjgyMkMyLjMwMzM4IDYuMTczODEgMS44MjY0MSA2LjE3MzU5IDEuNTMyMjYgNi40Njc3NFY2LjQ2Nzc0QzEuMjM4MyA2Ljc2MTcgMS4yMzgzIDcuMjM4MyAxLjUzMjI2IDcuNTMyMjZMNi4yOTI4OSAxMi4yOTI5QzYuNjgzNDIgMTIuNjgzNCA3LjMxNjU4IDEyLjY4MzQgNy43MDcxMSAxMi4yOTI5TDEyLjQ3MSA3LjUyODk5WiIgZmlsbD0iIzYxNjE2MSIvPgo8L3N2Zz4K);
  --jp-icon-move-up: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggY2xhc3M9ImpwLWljb24zIiBkPSJNMS41Mjg5OSA2LjQ3MTAxQzEuMjM2ODQgNi43NjMxNiAxLjIzNjg0IDcuMjM2ODQgMS41Mjg5OSA3LjUyODk5VjcuNTI4OTlDMS44MjA5NSA3LjgyMDk1IDIuMjk0MjYgNy44MjExNiAyLjU4NjQ5IDcuNTI5NDZMNi4yNSAzLjg3MjVWMTIuMjVDNi4yNSAxMi42NjQyIDYuNTg1NzkgMTMgNyAxM1YxM0M3LjQxNDIxIDEzIDcuNzUgMTIuNjY0MiA3Ljc1IDEyLjI1VjMuODcyNUwxMS40MDI3IDcuNTMxNzhDMTEuNjk2NiA3LjgyNjE5IDEyLjE3MzYgNy44MjY0MSAxMi40Njc3IDcuNTMyMjZWNy41MzIyNkMxMi43NjE3IDcuMjM4MyAxMi43NjE3IDYuNzYxNyAxMi40Njc3IDYuNDY3NzRMNy43MDcxMSAxLjcwNzExQzcuMzE2NTggMS4zMTY1OCA2LjY4MzQyIDEuMzE2NTggNi4yOTI4OSAxLjcwNzExTDEuNTI4OTkgNi40NzEwMVoiIGZpbGw9IiM2MTYxNjEiLz4KPC9zdmc+Cg==);
  --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);
  --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtbm90ZWJvb2staWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-numbering: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZpZXdCb3g9IjAgMCAyOCAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTQgMTlINlYxOS41SDVWMjAuNUg2VjIxSDRWMjJIN1YxOEg0VjE5Wk01IDEwSDZWNkg0VjdINVYxMFpNNCAxM0g1LjhMNCAxNS4xVjE2SDdWMTVINS4yTDcgMTIuOVYxMkg0VjEzWk05IDdWOUgyM1Y3SDlaTTkgMjFIMjNWMTlIOVYyMVpNOSAxNUgyM1YxM0g5VjE1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-offline-bolt: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDIuMDJjLTUuNTEgMC05Ljk4IDQuNDctOS45OCA5Ljk4czQuNDcgOS45OCA5Ljk4IDkuOTggOS45OC00LjQ3IDkuOTgtOS45OFMxNy41MSAyLjAyIDEyIDIuMDJ6TTExLjQ4IDIwdi02LjI2SDhMMTMgNHY2LjI2aDMuMzVMMTEuNDggMjB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-pdf: url(data:image/svg+xml;base64,PHN2ZwogICB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMiAyMiIgd2lkdGg9IjE2Ij4KICAgIDxwYXRoIHRyYW5zZm9ybT0icm90YXRlKDQ1KSIgY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI0ZGMkEyQSIKICAgICAgIGQ9Im0gMjIuMzQ0MzY5LC0zLjAxNjM2NDIgaCA1LjYzODYwNCB2IDEuNTc5MjQzMyBoIC0zLjU0OTIyNyB2IDEuNTA4NjkyOTkgaCAzLjMzNzU3NiBWIDEuNjUwODE1NCBoIC0zLjMzNzU3NiB2IDMuNDM1MjYxMyBoIC0yLjA4OTM3NyB6IG0gLTcuMTM2NDQ0LDEuNTc5MjQzMyB2IDQuOTQzOTU0MyBoIDAuNzQ4OTIgcSAxLjI4MDc2MSwwIDEuOTUzNzAzLC0wLjYzNDk1MzUgMC42NzgzNjksLTAuNjM0OTUzNSAwLjY3ODM2OSwtMS44NDUxNjQxIDAsLTEuMjA0NzgzNTUgLTAuNjcyOTQyLC0xLjgzNDMxMDExIC0wLjY3Mjk0MiwtMC42Mjk1MjY1OSAtMS45NTkxMywtMC42Mjk1MjY1OSB6IG0gLTIuMDg5Mzc3LC0xLjU3OTI0MzMgaCAyLjIwMzM0MyBxIDEuODQ1MTY0LDAgMi43NDYwMzksMC4yNjU5MjA3IDAuOTA2MzAxLDAuMjYwNDkzNyAxLjU1MjEwOCwwLjg5MDAyMDMgMC41Njk4MywwLjU0ODEyMjMgMC44NDY2MDUsMS4yNjQ0ODAwNiAwLjI3Njc3NCwwLjcxNjM1NzgxIDAuMjc2Nzc0LDEuNjIyNjU4OTQgMCwwLjkxNzE1NTEgLTAuMjc2Nzc0LDEuNjM4OTM5OSAtMC4yNzY3NzUsMC43MTYzNTc4IC0wLjg0NjYwNSwxLjI2NDQ4IC0wLjY1MTIzNCwwLjYyOTUyNjYgLTEuNTYyOTYyLDAuODk1NDQ3MyAtMC45MTE3MjgsMC4yNjA0OTM3IC0yLjczNTE4NSwwLjI2MDQ5MzcgaCAtMi4yMDMzNDMgeiBtIC04LjE0NTg1NjUsMCBoIDMuNDY3ODIzIHEgMS41NDY2ODE2LDAgMi4zNzE1Nzg1LDAuNjg5MjIzIDAuODMwMzI0LDAuNjgzNzk2MSAwLjgzMDMyNCwxLjk1MzcwMzE0IDAsMS4yNzUzMzM5NyAtMC44MzAzMjQsMS45NjQ1NTcwNiBRIDkuOTg3MTk2MSwyLjI3NDkxNSA4LjQ0MDUxNDUsMi4yNzQ5MTUgSCA3LjA2MjA2ODQgViA1LjA4NjA3NjcgSCA0Ljk3MjY5MTUgWiBtIDIuMDg5Mzc2OSwxLjUxNDExOTkgdiAyLjI2MzAzOTQzIGggMS4xNTU5NDEgcSAwLjYwNzgxODgsMCAwLjkzODg2MjksLTAuMjkzMDU1NDcgMC4zMzEwNDQxLC0wLjI5ODQ4MjQxIDAuMzMxMDQ0MSwtMC44NDExNzc3MiAwLC0wLjU0MjY5NTMxIC0wLjMzMTA0NDEsLTAuODM1NzUwNzQgLTAuMzMxMDQ0MSwtMC4yOTMwNTU1IC0wLjkzODg2MjksLTAuMjkzMDU1NSB6IgovPgo8L3N2Zz4K);
  --jp-icon-python: url(data:image/svg+xml;base64,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);
  --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);
  --jp-icon-react: url(data:image/svg+xml;base64,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);
  --jp-icon-redo: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjE2Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgICA8cGF0aCBkPSJNMCAwaDI0djI0SDB6IiBmaWxsPSJub25lIi8+PHBhdGggZD0iTTE4LjQgMTAuNkMxNi41NSA4Ljk5IDE0LjE1IDggMTEuNSA4Yy00LjY1IDAtOC41OCAzLjAzLTkuOTYgNy4yMkwzLjkgMTZjMS4wNS0zLjE5IDQuMDUtNS41IDcuNi01LjUgMS45NSAwIDMuNzMuNzIgNS4xMiAxLjg4TDEzIDE2aDlWN2wtMy42IDMuNnoiLz4KICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);
  --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);
  --jp-icon-share: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTSAxOCAyIEMgMTYuMzU0OTkgMiAxNSAzLjM1NDk5MDQgMTUgNSBDIDE1IDUuMTkwOTUyOSAxNS4wMjE3OTEgNS4zNzcxMjI0IDE1LjA1NjY0MSA1LjU1ODU5MzggTCA3LjkyMTg3NSA5LjcyMDcwMzEgQyA3LjM5ODUzOTkgOS4yNzc4NTM5IDYuNzMyMDc3MSA5IDYgOSBDIDQuMzU0OTkwNCA5IDMgMTAuMzU0OTkgMyAxMiBDIDMgMTMuNjQ1MDEgNC4zNTQ5OTA0IDE1IDYgMTUgQyA2LjczMjA3NzEgMTUgNy4zOTg1Mzk5IDE0LjcyMjE0NiA3LjkyMTg3NSAxNC4yNzkyOTcgTCAxNS4wNTY2NDEgMTguNDM5NDUzIEMgMTUuMDIxNTU1IDE4LjYyMTUxNCAxNSAxOC44MDgzODYgMTUgMTkgQyAxNSAyMC42NDUwMSAxNi4zNTQ5OSAyMiAxOCAyMiBDIDE5LjY0NTAxIDIyIDIxIDIwLjY0NTAxIDIxIDE5IEMgMjEgMTcuMzU0OTkgMTkuNjQ1MDEgMTYgMTggMTYgQyAxNy4yNjc0OCAxNiAxNi42MDE1OTMgMTYuMjc5MzI4IDE2LjA3ODEyNSAxNi43MjI2NTYgTCA4Ljk0MzM1OTQgMTIuNTU4NTk0IEMgOC45NzgyMDk1IDEyLjM3NzEyMiA5IDEyLjE5MDk1MyA5IDEyIEMgOSAxMS44MDkwNDcgOC45NzgyMDk1IDExLjYyMjg3OCA4Ljk0MzM1OTQgMTEuNDQxNDA2IEwgMTYuMDc4MTI1IDcuMjc5Mjk2OSBDIDE2LjYwMTQ2IDcuNzIyMTQ2MSAxNy4yNjc5MjMgOCAxOCA4IEMgMTkuNjQ1MDEgOCAyMSA2LjY0NTAwOTYgMjEgNSBDIDIxIDMuMzU0OTkwNCAxOS42NDUwMSAyIDE4IDIgeiBNIDE4IDQgQyAxOC41NjQxMjkgNCAxOSA0LjQzNTg3MDYgMTkgNSBDIDE5IDUuNTY0MTI5NCAxOC41NjQxMjkgNiAxOCA2IEMgMTcuNDM1ODcxIDYgMTcgNS41NjQxMjk0IDE3IDUgQyAxNyA0LjQzNTg3MDYgMTcuNDM1ODcxIDQgMTggNCB6IE0gNiAxMSBDIDYuNTY0MTI5NCAxMSA3IDExLjQzNTg3MSA3IDEyIEMgNyAxMi41NjQxMjkgNi41NjQxMjk0IDEzIDYgMTMgQyA1LjQzNTg3MDYgMTMgNSAxMi41NjQxMjkgNSAxMiBDIDUgMTEuNDM1ODcxIDUuNDM1ODcwNiAxMSA2IDExIHogTSAxOCAxOCBDIDE4LjU2NDEyOSAxOCAxOSAxOC40MzU4NzEgMTkgMTkgQyAxOSAxOS41NjQxMjkgMTguNTY0MTI5IDIwIDE4IDIwIEMgMTcuNDM1ODcxIDIwIDE3IDE5LjU2NDEyOSAxNyAxOSBDIDE3IDE4LjQzNTg3MSAxNy40MzU4NzEgMTggMTggMTggeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);
  --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);
  --jp-icon-table-rows: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMSw4SDNWNGgxOFY4eiBNMjEsMTBIM3Y0aDE4VjEweiBNMjEsMTZIM3Y0aDE4VjE2eiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-tag: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjgiIGhlaWdodD0iMjgiIHZpZXdCb3g9IjAgMCA0MyAyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CgkJPHBhdGggZD0iTTI4LjgzMzIgMTIuMzM0TDMyLjk5OTggMTYuNTAwN0wzNy4xNjY1IDEyLjMzNEgyOC44MzMyWiIvPgoJCTxwYXRoIGQ9Ik0xNi4yMDk1IDIxLjYxMDRDMTUuNjg3MyAyMi4xMjk5IDE0Ljg0NDMgMjIuMTI5OSAxNC4zMjQ4IDIxLjYxMDRMNi45ODI5IDE0LjcyNDVDNi41NzI0IDE0LjMzOTQgNi4wODMxMyAxMy42MDk4IDYuMDQ3ODYgMTMuMDQ4MkM1Ljk1MzQ3IDExLjUyODggNi4wMjAwMiA4LjYxOTQ0IDYuMDY2MjEgNy4wNzY5NUM2LjA4MjgxIDYuNTE0NzcgNi41NTU0OCA2LjA0MzQ3IDcuMTE4MDQgNi4wMzA1NUM5LjA4ODYzIDUuOTg0NzMgMTMuMjYzOCA1LjkzNTc5IDEzLjY1MTggNi4zMjQyNUwyMS43MzY5IDEzLjYzOUMyMi4yNTYgMTQuMTU4NSAyMS43ODUxIDE1LjQ3MjQgMjEuMjYyIDE1Ljk5NDZMMTYuMjA5NSAyMS42MTA0Wk05Ljc3NTg1IDguMjY1QzkuMzM1NTEgNy44MjU2NiA4LjYyMzUxIDcuODI1NjYgOC4xODI4IDguMjY1QzcuNzQzNDYgOC43MDU3MSA3Ljc0MzQ2IDkuNDE3MzMgOC4xODI4IDkuODU2NjdDOC42MjM4MiAxMC4yOTY0IDkuMzM1ODIgMTAuMjk2NCA5Ljc3NTg1IDkuODU2NjdDMTAuMjE1NiA5LjQxNzMzIDEwLjIxNTYgOC43MDUzMyA5Ljc3NTg1IDguMjY1WiIvPgoJPC9nPgo8L3N2Zz4K);
  --jp-icon-terminal: url(data:image/svg+xml;base64,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);
  --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtdGV4dC1lZGl0b3ItaWNvbi1jb2xvciBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xNSAxNUgzdjJoMTJ2LTJ6bTAtOEgzdjJoMTJWN3pNMyAxM2gxOHYtMkgzdjJ6bTAgOGgxOHYtMkgzdjJ6TTMgM3YyaDE4VjNIM3oiLz4KPC9zdmc+Cg==);
  --jp-icon-toc: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxwYXRoIGQ9Ik03LDVIMjFWN0g3VjVNNywxM1YxMUgyMVYxM0g3TTQsNC41QTEuNSwxLjUgMCAwLDEgNS41LDZBMS41LDEuNSAwIDAsMSA0LDcuNUExLjUsMS41IDAgMCwxIDIuNSw2QTEuNSwxLjUgMCAwLDEgNCw0LjVNNCwxMC41QTEuNSwxLjUgMCAwLDEgNS41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMy41QTEuNSwxLjUgMCAwLDEgMi41LDEyQTEuNSwxLjUgMCAwLDEgNCwxMC41TTcsMTlWMTdIMjFWMTlIN000LDE2LjVBMS41LDEuNSAwIDAsMSA1LjUsMThBMS41LDEuNSAwIDAsMSA0LDE5LjVBMS41LDEuNSAwIDAsMSAyLjUsMThBMS41LDEuNSAwIDAsMSA0LDE2LjVaIiAvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-tree-view: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik0yMiAxMVYzaC03djNIOVYzSDJ2OGg3VjhoMnYxMGg0djNoN3YtOGgtN3YzaC0yVjhoMnYzeiIvPgogICAgPC9nPgo8L3N2Zz4K);
  --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);
  --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-user: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIHZpZXdCb3g9IjAgMCAyNCAyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE2IDdhNCA0IDAgMTEtOCAwIDQgNCAwIDAxOCAwek0xMiAxNGE3IDcgMCAwMC03IDdoMTRhNyA3IDAgMDAtNy03eiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-users: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDM2IDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogPGcgY2xhc3M9ImpwLWljb24zIiB0cmFuc2Zvcm09Im1hdHJpeCgxLjczMjcgMCAwIDEuNzMyNyAtMy42MjgyIC4wOTk1NzcpIiBmaWxsPSIjNjE2MTYxIj4KICA8cGF0aCB0cmFuc2Zvcm09Im1hdHJpeCgxLjUsMCwwLDEuNSwwLC02KSIgZD0ibTEyLjE4NiA3LjUwOThjLTEuMDUzNSAwLTEuOTc1NyAwLjU2NjUtMi40Nzg1IDEuNDEwMiAwLjc1MDYxIDAuMzEyNzcgMS4zOTc0IDAuODI2NDggMS44NzMgMS40NzI3aDMuNDg2M2MwLTEuNTkyLTEuMjg4OS0yLjg4MjgtMi44ODA5LTIuODgyOHoiLz4KICA8cGF0aCBkPSJtMjAuNDY1IDIuMzg5NWEyLjE4ODUgMi4xODg1IDAgMCAxLTIuMTg4NCAyLjE4ODUgMi4xODg1IDIuMTg4NSAwIDAgMS0yLjE4ODUtMi4xODg1IDIuMTg4NSAyLjE4ODUgMCAwIDEgMi4xODg1LTIuMTg4NSAyLjE4ODUgMi4xODg1IDAgMCAxIDIuMTg4NCAyLjE4ODV6Ii8+CiAgPHBhdGggdHJhbnNmb3JtPSJtYXRyaXgoMS41LDAsMCwxLjUsMCwtNikiIGQ9Im0zLjU4OTggOC40MjE5Yy0xLjExMjYgMC0yLjAxMzcgMC45MDExMS0yLjAxMzcgMi4wMTM3aDIuODE0NWMwLjI2Nzk3LTAuMzczMDkgMC41OTA3LTAuNzA0MzUgMC45NTg5OC0wLjk3ODUyLTAuMzQ0MzMtMC42MTY4OC0xLjAwMzEtMS4wMzUyLTEuNzU5OC0xLjAzNTJ6Ii8+CiAgPHBhdGggZD0ibTYuOTE1NCA0LjYyM2ExLjUyOTQgMS41Mjk0IDAgMCAxLTEuNTI5NCAxLjUyOTQgMS41Mjk0IDEuNTI5NCAwIDAgMS0xLjUyOTQtMS41Mjk0IDEuNTI5NCAxLjUyOTQgMCAwIDEgMS41Mjk0LTEuNTI5NCAxLjUyOTQgMS41Mjk0IDAgMCAxIDEuNTI5NCAxLjUyOTR6Ii8+CiAgPHBhdGggZD0ibTYuMTM1IDEzLjUzNWMwLTMuMjM5MiAyLjYyNTktNS44NjUgNS44NjUtNS44NjUgMy4yMzkyIDAgNS44NjUgMi42MjU5IDUuODY1IDUuODY1eiIvPgogIDxjaXJjbGUgY3g9IjEyIiBjeT0iMy43Njg1IiByPSIyLjk2ODUiLz4KIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);
  --jp-icon-word: url(data:image/svg+xml;base64,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);
  --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K);
}

/* Icon CSS class declarations */

.jp-AddAboveIcon {
  background-image: var(--jp-icon-add-above);
}

.jp-AddBelowIcon {
  background-image: var(--jp-icon-add-below);
}

.jp-AddIcon {
  background-image: var(--jp-icon-add);
}

.jp-BellIcon {
  background-image: var(--jp-icon-bell);
}

.jp-BugDotIcon {
  background-image: var(--jp-icon-bug-dot);
}

.jp-BugIcon {
  background-image: var(--jp-icon-bug);
}

.jp-BuildIcon {
  background-image: var(--jp-icon-build);
}

.jp-CaretDownEmptyIcon {
  background-image: var(--jp-icon-caret-down-empty);
}

.jp-CaretDownEmptyThinIcon {
  background-image: var(--jp-icon-caret-down-empty-thin);
}

.jp-CaretDownIcon {
  background-image: var(--jp-icon-caret-down);
}

.jp-CaretLeftIcon {
  background-image: var(--jp-icon-caret-left);
}

.jp-CaretRightIcon {
  background-image: var(--jp-icon-caret-right);
}

.jp-CaretUpEmptyThinIcon {
  background-image: var(--jp-icon-caret-up-empty-thin);
}

.jp-CaretUpIcon {
  background-image: var(--jp-icon-caret-up);
}

.jp-CaseSensitiveIcon {
  background-image: var(--jp-icon-case-sensitive);
}

.jp-CheckIcon {
  background-image: var(--jp-icon-check);
}

.jp-CircleEmptyIcon {
  background-image: var(--jp-icon-circle-empty);
}

.jp-CircleIcon {
  background-image: var(--jp-icon-circle);
}

.jp-ClearIcon {
  background-image: var(--jp-icon-clear);
}

.jp-CloseIcon {
  background-image: var(--jp-icon-close);
}

.jp-CodeCheckIcon {
  background-image: var(--jp-icon-code-check);
}

.jp-CodeIcon {
  background-image: var(--jp-icon-code);
}

.jp-CollapseAllIcon {
  background-image: var(--jp-icon-collapse-all);
}

.jp-ConsoleIcon {
  background-image: var(--jp-icon-console);
}

.jp-CopyIcon {
  background-image: var(--jp-icon-copy);
}

.jp-CopyrightIcon {
  background-image: var(--jp-icon-copyright);
}

.jp-CutIcon {
  background-image: var(--jp-icon-cut);
}

.jp-DeleteIcon {
  background-image: var(--jp-icon-delete);
}

.jp-DownloadIcon {
  background-image: var(--jp-icon-download);
}

.jp-DuplicateIcon {
  background-image: var(--jp-icon-duplicate);
}

.jp-EditIcon {
  background-image: var(--jp-icon-edit);
}

.jp-EllipsesIcon {
  background-image: var(--jp-icon-ellipses);
}

.jp-ErrorIcon {
  background-image: var(--jp-icon-error);
}

.jp-ExpandAllIcon {
  background-image: var(--jp-icon-expand-all);
}

.jp-ExtensionIcon {
  background-image: var(--jp-icon-extension);
}

.jp-FastForwardIcon {
  background-image: var(--jp-icon-fast-forward);
}

.jp-FileIcon {
  background-image: var(--jp-icon-file);
}

.jp-FileUploadIcon {
  background-image: var(--jp-icon-file-upload);
}

.jp-FilterDotIcon {
  background-image: var(--jp-icon-filter-dot);
}

.jp-FilterIcon {
  background-image: var(--jp-icon-filter);
}

.jp-FilterListIcon {
  background-image: var(--jp-icon-filter-list);
}

.jp-FolderFavoriteIcon {
  background-image: var(--jp-icon-folder-favorite);
}

.jp-FolderIcon {
  background-image: var(--jp-icon-folder);
}

.jp-HomeIcon {
  background-image: var(--jp-icon-home);
}

.jp-Html5Icon {
  background-image: var(--jp-icon-html5);
}

.jp-ImageIcon {
  background-image: var(--jp-icon-image);
}

.jp-InfoIcon {
  background-image: var(--jp-icon-info);
}

.jp-InspectorIcon {
  background-image: var(--jp-icon-inspector);
}

.jp-JsonIcon {
  background-image: var(--jp-icon-json);
}

.jp-JuliaIcon {
  background-image: var(--jp-icon-julia);
}

.jp-JupyterFaviconIcon {
  background-image: var(--jp-icon-jupyter-favicon);
}

.jp-JupyterIcon {
  background-image: var(--jp-icon-jupyter);
}

.jp-JupyterlabWordmarkIcon {
  background-image: var(--jp-icon-jupyterlab-wordmark);
}

.jp-KernelIcon {
  background-image: var(--jp-icon-kernel);
}

.jp-KeyboardIcon {
  background-image: var(--jp-icon-keyboard);
}

.jp-LaunchIcon {
  background-image: var(--jp-icon-launch);
}

.jp-LauncherIcon {
  background-image: var(--jp-icon-launcher);
}

.jp-LineFormIcon {
  background-image: var(--jp-icon-line-form);
}

.jp-LinkIcon {
  background-image: var(--jp-icon-link);
}

.jp-ListIcon {
  background-image: var(--jp-icon-list);
}

.jp-MarkdownIcon {
  background-image: var(--jp-icon-markdown);
}

.jp-MoveDownIcon {
  background-image: var(--jp-icon-move-down);
}

.jp-MoveUpIcon {
  background-image: var(--jp-icon-move-up);
}

.jp-NewFolderIcon {
  background-image: var(--jp-icon-new-folder);
}

.jp-NotTrustedIcon {
  background-image: var(--jp-icon-not-trusted);
}

.jp-NotebookIcon {
  background-image: var(--jp-icon-notebook);
}

.jp-NumberingIcon {
  background-image: var(--jp-icon-numbering);
}

.jp-OfflineBoltIcon {
  background-image: var(--jp-icon-offline-bolt);
}

.jp-PaletteIcon {
  background-image: var(--jp-icon-palette);
}

.jp-PasteIcon {
  background-image: var(--jp-icon-paste);
}

.jp-PdfIcon {
  background-image: var(--jp-icon-pdf);
}

.jp-PythonIcon {
  background-image: var(--jp-icon-python);
}

.jp-RKernelIcon {
  background-image: var(--jp-icon-r-kernel);
}

.jp-ReactIcon {
  background-image: var(--jp-icon-react);
}

.jp-RedoIcon {
  background-image: var(--jp-icon-redo);
}

.jp-RefreshIcon {
  background-image: var(--jp-icon-refresh);
}

.jp-RegexIcon {
  background-image: var(--jp-icon-regex);
}

.jp-RunIcon {
  background-image: var(--jp-icon-run);
}

.jp-RunningIcon {
  background-image: var(--jp-icon-running);
}

.jp-SaveIcon {
  background-image: var(--jp-icon-save);
}

.jp-SearchIcon {
  background-image: var(--jp-icon-search);
}

.jp-SettingsIcon {
  background-image: var(--jp-icon-settings);
}

.jp-ShareIcon {
  background-image: var(--jp-icon-share);
}

.jp-SpreadsheetIcon {
  background-image: var(--jp-icon-spreadsheet);
}

.jp-StopIcon {
  background-image: var(--jp-icon-stop);
}

.jp-TabIcon {
  background-image: var(--jp-icon-tab);
}

.jp-TableRowsIcon {
  background-image: var(--jp-icon-table-rows);
}

.jp-TagIcon {
  background-image: var(--jp-icon-tag);
}

.jp-TerminalIcon {
  background-image: var(--jp-icon-terminal);
}

.jp-TextEditorIcon {
  background-image: var(--jp-icon-text-editor);
}

.jp-TocIcon {
  background-image: var(--jp-icon-toc);
}

.jp-TreeViewIcon {
  background-image: var(--jp-icon-tree-view);
}

.jp-TrustedIcon {
  background-image: var(--jp-icon-trusted);
}

.jp-UndoIcon {
  background-image: var(--jp-icon-undo);
}

.jp-UserIcon {
  background-image: var(--jp-icon-user);
}

.jp-UsersIcon {
  background-image: var(--jp-icon-users);
}

.jp-VegaIcon {
  background-image: var(--jp-icon-vega);
}

.jp-WordIcon {
  background-image: var(--jp-icon-word);
}

.jp-YamlIcon {
  background-image: var(--jp-icon-yaml);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * (DEPRECATED) Support for consuming icons as CSS background images
 */

.jp-Icon,
.jp-MaterialIcon {
  background-position: center;
  background-repeat: no-repeat;
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-cover {
  background-position: center;
  background-repeat: no-repeat;
  background-size: cover;
}

/**
 * (DEPRECATED) Support for specific CSS icon sizes
 */

.jp-Icon-16 {
  background-size: 16px;
  min-width: 16px;
  min-height: 16px;
}

.jp-Icon-18 {
  background-size: 18px;
  min-width: 18px;
  min-height: 18px;
}

.jp-Icon-20 {
  background-size: 20px;
  min-width: 20px;
  min-height: 20px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.lm-TabBar .lm-TabBar-addButton {
  align-items: center;
  display: flex;
  padding: 4px;
  padding-bottom: 5px;
  margin-right: 1px;
  background-color: var(--jp-layout-color2);
}

.lm-TabBar .lm-TabBar-addButton:hover {
  background-color: var(--jp-layout-color1);
}

.lm-DockPanel-tabBar .lm-TabBar-tab {
  width: var(--jp-private-horizontal-tab-width);
}

.lm-DockPanel-tabBar .lm-TabBar-content {
  flex: unset;
}

.lm-DockPanel-tabBar[data-orientation='horizontal'] {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for icons as inline SVG HTMLElements
 */

/* recolor the primary elements of an icon */
.jp-icon0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-accent0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-accent1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-accent2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-accent3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-accent4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-accent0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-accent1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-accent2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-accent3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-accent4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-none[fill] {
  fill: none;
}

.jp-icon-none[stroke] {
  stroke: none;
}

/* brand icon colors. Same for light and dark */
.jp-icon-brand0[fill] {
  fill: var(--jp-brand-color0);
}

.jp-icon-brand1[fill] {
  fill: var(--jp-brand-color1);
}

.jp-icon-brand2[fill] {
  fill: var(--jp-brand-color2);
}

.jp-icon-brand3[fill] {
  fill: var(--jp-brand-color3);
}

.jp-icon-brand4[fill] {
  fill: var(--jp-brand-color4);
}

.jp-icon-brand0[stroke] {
  stroke: var(--jp-brand-color0);
}

.jp-icon-brand1[stroke] {
  stroke: var(--jp-brand-color1);
}

.jp-icon-brand2[stroke] {
  stroke: var(--jp-brand-color2);
}

.jp-icon-brand3[stroke] {
  stroke: var(--jp-brand-color3);
}

.jp-icon-brand4[stroke] {
  stroke: var(--jp-brand-color4);
}

/* warn icon colors. Same for light and dark */
.jp-icon-warn0[fill] {
  fill: var(--jp-warn-color0);
}

.jp-icon-warn1[fill] {
  fill: var(--jp-warn-color1);
}

.jp-icon-warn2[fill] {
  fill: var(--jp-warn-color2);
}

.jp-icon-warn3[fill] {
  fill: var(--jp-warn-color3);
}

.jp-icon-warn0[stroke] {
  stroke: var(--jp-warn-color0);
}

.jp-icon-warn1[stroke] {
  stroke: var(--jp-warn-color1);
}

.jp-icon-warn2[stroke] {
  stroke: var(--jp-warn-color2);
}

.jp-icon-warn3[stroke] {
  stroke: var(--jp-warn-color3);
}

/* icon colors that contrast well with each other and most backgrounds */
.jp-icon-contrast0[fill] {
  fill: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[fill] {
  fill: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[fill] {
  fill: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[fill] {
  fill: var(--jp-icon-contrast-color3);
}

.jp-icon-contrast0[stroke] {
  stroke: var(--jp-icon-contrast-color0);
}

.jp-icon-contrast1[stroke] {
  stroke: var(--jp-icon-contrast-color1);
}

.jp-icon-contrast2[stroke] {
  stroke: var(--jp-icon-contrast-color2);
}

.jp-icon-contrast3[stroke] {
  stroke: var(--jp-icon-contrast-color3);
}

.jp-icon-dot[fill] {
  fill: var(--jp-warn-color0);
}

.jp-jupyter-icon-color[fill] {
  fill: var(--jp-jupyter-icon-color, var(--jp-warn-color0));
}

.jp-notebook-icon-color[fill] {
  fill: var(--jp-notebook-icon-color, var(--jp-warn-color0));
}

.jp-json-icon-color[fill] {
  fill: var(--jp-json-icon-color, var(--jp-warn-color1));
}

.jp-console-icon-color[fill] {
  fill: var(--jp-console-icon-color, white);
}

.jp-console-icon-background-color[fill] {
  fill: var(--jp-console-icon-background-color, var(--jp-brand-color1));
}

.jp-terminal-icon-color[fill] {
  fill: var(--jp-terminal-icon-color, var(--jp-layout-color2));
}

.jp-terminal-icon-background-color[fill] {
  fill: var(
    --jp-terminal-icon-background-color,
    var(--jp-inverse-layout-color2)
  );
}

.jp-text-editor-icon-color[fill] {
  fill: var(--jp-text-editor-icon-color, var(--jp-inverse-layout-color3));
}

.jp-inspector-icon-color[fill] {
  fill: var(--jp-inspector-icon-color, var(--jp-inverse-layout-color3));
}

/* CSS for icons in selected filebrowser listing items */
.jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

.jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* stylelint-disable selector-max-class, selector-max-compound-selectors */

/**
* TODO: come up with non css-hack solution for showing the busy icon on top
*  of the close icon
* CSS for complex behavior of close icon of tabs in the main area tabbar
*/
.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon3[fill] {
  fill: none;
}

.lm-DockPanel-tabBar
  .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty
  > .lm-TabBar-tabCloseIcon
  > :not(:hover)
  > .jp-icon-busy[fill] {
  fill: var(--jp-inverse-layout-color3);
}

/* stylelint-enable selector-max-class, selector-max-compound-selectors */

/* CSS for icons in status bar */
#jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] {
  fill: #fff;
}

#jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] {
  fill: var(--jp-brand-color1);
}

/* special handling for splash icon CSS. While the theme CSS reloads during
   splash, the splash icon can loose theming. To prevent that, we set a
   default for its color variable */
:root {
  --jp-warn-color0: var(--md-orange-700);
}

/* not sure what to do with this one, used in filebrowser listing */
.jp-DragIcon {
  margin-right: 4px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/**
 * Support for alt colors for icons as inline SVG HTMLElements
 */

/* alt recolor the primary elements of an icon */
.jp-icon-alt .jp-icon0[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-alt .jp-icon0[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-alt .jp-icon1[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-alt .jp-icon2[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-alt .jp-icon3[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-alt .jp-icon4[stroke] {
  stroke: var(--jp-layout-color4);
}

/* alt recolor the accent elements of an icon */
.jp-icon-alt .jp-icon-accent0[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-alt .jp-icon-accent0[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-alt .jp-icon-accent1[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-alt .jp-icon-accent2[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-alt .jp-icon-accent3[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-alt .jp-icon-accent4[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-icon-hoverShow:not(:hover) .jp-icon-hoverShow-content {
  display: none !important;
}

/**
 * Support for hover colors for icons as inline SVG HTMLElements
 */

/**
 * regular colors
 */

/* recolor the primary elements of an icon */
.jp-icon-hover :hover .jp-icon0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/* recolor the accent elements of an icon */
.jp-icon-hover :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* set the color of an icon to transparent */
.jp-icon-hover :hover .jp-icon-none-hover[fill] {
  fill: none;
}

.jp-icon-hover :hover .jp-icon-none-hover[stroke] {
  stroke: none;
}

/**
 * inverse colors
 */

/* inverse recolor the primary elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] {
  fill: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] {
  fill: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] {
  fill: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] {
  fill: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] {
  fill: var(--jp-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] {
  stroke: var(--jp-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] {
  stroke: var(--jp-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] {
  stroke: var(--jp-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] {
  stroke: var(--jp-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] {
  stroke: var(--jp-layout-color4);
}

/* inverse recolor the accent elements of an icon */
.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] {
  fill: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] {
  fill: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] {
  fill: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] {
  fill: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] {
  fill: var(--jp-inverse-layout-color4);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] {
  stroke: var(--jp-inverse-layout-color0);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] {
  stroke: var(--jp-inverse-layout-color1);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] {
  stroke: var(--jp-inverse-layout-color2);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] {
  stroke: var(--jp-inverse-layout-color3);
}

.jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] {
  stroke: var(--jp-inverse-layout-color4);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-IFrame {
  width: 100%;
  height: 100%;
}

.jp-IFrame > iframe {
  border: none;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-IFrame {
  position: relative;
}

body.lm-mod-override-cursor .jp-IFrame::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-HoverBox {
  position: fixed;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FormGroup-content fieldset {
  border: none;
  padding: 0;
  min-width: 0;
  width: 100%;
}

/* stylelint-disable selector-max-type */

.jp-FormGroup-content fieldset .jp-inputFieldWrapper input,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper select,
.jp-FormGroup-content fieldset .jp-inputFieldWrapper textarea {
  font-size: var(--jp-content-font-size2);
  border-color: var(--jp-input-border-color);
  border-style: solid;
  border-radius: var(--jp-border-radius);
  border-width: 1px;
  padding: 6px 8px;
  background: none;
  color: var(--jp-ui-font-color0);
  height: inherit;
}

.jp-FormGroup-content fieldset input[type='checkbox'] {
  position: relative;
  top: 2px;
  margin-left: 0;
}

.jp-FormGroup-content button.jp-mod-styled {
  cursor: pointer;
}

.jp-FormGroup-content .checkbox label {
  cursor: pointer;
  font-size: var(--jp-content-font-size1);
}

.jp-FormGroup-content .jp-root > fieldset > legend {
  display: none;
}

.jp-FormGroup-content .jp-root > fieldset > p {
  display: none;
}

/** copy of `input.jp-mod-styled:focus` style */
.jp-FormGroup-content fieldset input:focus,
.jp-FormGroup-content fieldset select:focus {
  -moz-outline-radius: unset;
  outline: var(--jp-border-width) solid var(--md-blue-500);
  outline-offset: -1px;
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-FormGroup-content fieldset input:hover:not(:focus),
.jp-FormGroup-content fieldset select:hover:not(:focus) {
  background-color: var(--jp-border-color2);
}

/* stylelint-enable selector-max-type */

.jp-FormGroup-content .checkbox .field-description {
  /* Disable default description field for checkbox:
   because other widgets do not have description fields,
   we add descriptions to each widget on the field level.
  */
  display: none;
}

.jp-FormGroup-content #root__description {
  display: none;
}

.jp-FormGroup-content .jp-modifiedIndicator {
  width: 5px;
  background-color: var(--jp-brand-color2);
  margin-top: 0;
  margin-left: calc(var(--jp-private-settingeditor-modifier-indent) * -1);
  flex-shrink: 0;
}

.jp-FormGroup-content .jp-modifiedIndicator.jp-errorIndicator {
  background-color: var(--jp-error-color0);
  margin-right: 0.5em;
}

/* RJSF ARRAY style */

.jp-arrayFieldWrapper legend {
  font-size: var(--jp-content-font-size2);
  color: var(--jp-ui-font-color0);
  flex-basis: 100%;
  padding: 4px 0;
  font-weight: var(--jp-content-heading-font-weight);
  border-bottom: 1px solid var(--jp-border-color2);
}

.jp-arrayFieldWrapper .field-description {
  padding: 4px 0;
  white-space: pre-wrap;
}

.jp-arrayFieldWrapper .array-item {
  width: 100%;
  border: 1px solid var(--jp-border-color2);
  border-radius: 4px;
  margin: 4px;
}

.jp-ArrayOperations {
  display: flex;
  margin-left: 8px;
}

.jp-ArrayOperationsButton {
  margin: 2px;
}

.jp-ArrayOperationsButton .jp-icon3[fill] {
  fill: var(--jp-ui-font-color0);
}

button.jp-ArrayOperationsButton.jp-mod-styled:disabled {
  cursor: not-allowed;
  opacity: 0.5;
}

/* RJSF form validation error */

.jp-FormGroup-content .validationErrors {
  color: var(--jp-error-color0);
}

/* Hide panel level error as duplicated the field level error */
.jp-FormGroup-content .panel.errors {
  display: none;
}

/* RJSF normal content (settings-editor) */

.jp-FormGroup-contentNormal {
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-FormGroup-contentItem {
  margin-left: 7px;
  color: var(--jp-ui-font-color0);
}

.jp-FormGroup-contentNormal .jp-FormGroup-description {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-default {
  flex-basis: 100%;
  padding: 4px 7px;
}

.jp-FormGroup-contentNormal .jp-FormGroup-fieldLabel {
  font-size: var(--jp-content-font-size1);
  font-weight: normal;
  min-width: 120px;
}

.jp-FormGroup-contentNormal fieldset:not(:first-child) {
  margin-left: 7px;
}

.jp-FormGroup-contentNormal .field-array-of-string .array-item {
  /* Display `jp-ArrayOperations` buttons side-by-side with content except
    for small screens where flex-wrap will place them one below the other.
  */
  display: flex;
  align-items: center;
  flex-wrap: wrap;
}

.jp-FormGroup-contentNormal .jp-objectFieldWrapper .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

/* RJSF compact content (metadata-form) */

.jp-FormGroup-content.jp-FormGroup-contentCompact {
  width: 100%;
}

.jp-FormGroup-contentCompact .form-group {
  display: flex;
  padding: 0.5em 0.2em 0.5em 0;
}

.jp-FormGroup-contentCompact
  .jp-FormGroup-compactTitle
  .jp-FormGroup-description {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color2);
}

.jp-FormGroup-contentCompact .jp-FormGroup-fieldLabel {
  padding-bottom: 0.3em;
}

.jp-FormGroup-contentCompact .jp-inputFieldWrapper .form-control {
  width: 100%;
  box-sizing: border-box;
}

.jp-FormGroup-contentCompact .jp-arrayFieldWrapper .jp-FormGroup-compactTitle {
  padding-bottom: 7px;
}

.jp-FormGroup-contentCompact
  .jp-objectFieldWrapper
  .jp-objectFieldWrapper
  .form-group {
  padding: 2px 8px 2px var(--jp-private-settingeditor-modifier-indent);
  margin-top: 2px;
}

.jp-FormGroup-contentCompact ul.error-detail {
  margin-block-start: 0.5em;
  margin-block-end: 0.5em;
  padding-inline-start: 1em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-SidePanel {
  display: flex;
  flex-direction: column;
  min-width: var(--jp-sidebar-min-width);
  overflow-y: auto;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);
  font-size: var(--jp-ui-font-size1);
}

.jp-SidePanel-header {
  flex: 0 0 auto;
  display: flex;
  border-bottom: var(--jp-border-width) solid var(--jp-border-color2);
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin: 0;
  padding: 2px;
  text-transform: uppercase;
}

.jp-SidePanel-toolbar {
  flex: 0 0 auto;
}

.jp-SidePanel-content {
  flex: 1 1 auto;
}

.jp-SidePanel-toolbar,
.jp-AccordionPanel-toolbar {
  height: var(--jp-private-toolbar-height);
}

.jp-SidePanel-toolbar.jp-Toolbar-micro {
  display: none;
}

.lm-AccordionPanel .jp-AccordionPanel-title {
  box-sizing: border-box;
  line-height: 25px;
  margin: 0;
  display: flex;
  align-items: center;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  font-size: var(--jp-ui-font-size0);
}

.jp-AccordionPanel-title {
  cursor: pointer;
  user-select: none;
  -moz-user-select: none;
  -webkit-user-select: none;
  text-transform: uppercase;
}

.lm-AccordionPanel[data-orientation='horizontal'] > .jp-AccordionPanel-title {
  /* Title is rotated for horizontal accordion panel using CSS */
  display: block;
  transform-origin: top left;
  transform: rotate(-90deg) translate(-100%);
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleLabel {
  user-select: none;
  text-overflow: ellipsis;
  white-space: nowrap;
  overflow: hidden;
}

.jp-AccordionPanel-title .lm-AccordionPanel-titleCollapser {
  transform: rotate(-90deg);
  margin: auto 0;
  height: 16px;
}

.jp-AccordionPanel-title.lm-mod-expanded .lm-AccordionPanel-titleCollapser {
  transform: rotate(0deg);
}

.lm-AccordionPanel .jp-AccordionPanel-toolbar {
  background: none;
  box-shadow: none;
  border: none;
  margin-left: auto;
}

.lm-AccordionPanel .lm-SplitPanel-handle:hover {
  background: var(--jp-layout-color3);
}

.jp-text-truncated {
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Spinner {
  position: absolute;
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 10;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-layout-color0);
  outline: none;
}

.jp-SpinnerContent {
  font-size: 10px;
  margin: 50px auto;
  text-indent: -9999em;
  width: 3em;
  height: 3em;
  border-radius: 50%;
  background: var(--jp-brand-color3);
  background: linear-gradient(
    to right,
    #f37626 10%,
    rgba(255, 255, 255, 0) 42%
  );
  position: relative;
  animation: load3 1s infinite linear, fadeIn 1s;
}

.jp-SpinnerContent::before {
  width: 50%;
  height: 50%;
  background: #f37626;
  border-radius: 100% 0 0;
  position: absolute;
  top: 0;
  left: 0;
  content: '';
}

.jp-SpinnerContent::after {
  background: var(--jp-layout-color0);
  width: 75%;
  height: 75%;
  border-radius: 50%;
  content: '';
  margin: auto;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }

  100% {
    opacity: 1;
  }
}

@keyframes load3 {
  0% {
    transform: rotate(0deg);
  }

  100% {
    transform: rotate(360deg);
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

button.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: none;
  box-sizing: border-box;
  text-align: center;
  line-height: 32px;
  height: 32px;
  padding: 0 12px;
  letter-spacing: 0.8px;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input.jp-mod-styled {
  background: var(--jp-input-background);
  height: 28px;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color1);
  padding-left: 7px;
  padding-right: 7px;
  font-size: var(--jp-ui-font-size2);
  color: var(--jp-ui-font-color0);
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

input[type='checkbox'].jp-mod-styled {
  appearance: checkbox;
  -webkit-appearance: checkbox;
  -moz-appearance: checkbox;
  height: auto;
}

input.jp-mod-styled:focus {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-select-wrapper {
  display: flex;
  position: relative;
  flex-direction: column;
  padding: 1px;
  background-color: var(--jp-layout-color1);
  box-sizing: border-box;
  margin-bottom: 12px;
}

.jp-select-wrapper:not(.multiple) {
  height: 28px;
}

.jp-select-wrapper.jp-mod-focused select.jp-mod-styled {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-input-active-background);
}

select.jp-mod-styled:hover {
  cursor: pointer;
  color: var(--jp-ui-font-color0);
  background-color: var(--jp-input-hover-background);
  box-shadow: inset 0 0 1px rgba(0, 0, 0, 0.5);
}

select.jp-mod-styled {
  flex: 1 1 auto;
  width: 100%;
  font-size: var(--jp-ui-font-size2);
  background: var(--jp-input-background);
  color: var(--jp-ui-font-color0);
  padding: 0 25px 0 8px;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
}

select.jp-mod-styled:not([multiple]) {
  height: 32px;
}

select.jp-mod-styled[multiple] {
  max-height: 200px;
  overflow-y: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-switch {
  display: flex;
  align-items: center;
  padding-left: 4px;
  padding-right: 4px;
  font-size: var(--jp-ui-font-size1);
  background-color: transparent;
  color: var(--jp-ui-font-color1);
  border: none;
  height: 20px;
}

.jp-switch:hover {
  background-color: var(--jp-layout-color2);
}

.jp-switch-label {
  margin-right: 5px;
  font-family: var(--jp-ui-font-family);
}

.jp-switch-track {
  cursor: pointer;
  background-color: var(--jp-switch-color, var(--jp-border-color1));
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 34px;
  height: 16px;
  width: 35px;
  position: relative;
}

.jp-switch-track::before {
  content: '';
  position: absolute;
  height: 10px;
  width: 10px;
  margin: 3px;
  left: 0;
  background-color: var(--jp-ui-inverse-font-color1);
  -webkit-transition: 0.4s;
  transition: 0.4s;
  border-radius: 50%;
}

.jp-switch[aria-checked='true'] .jp-switch-track {
  background-color: var(--jp-switch-true-position-color, var(--jp-warn-color0));
}

.jp-switch[aria-checked='true'] .jp-switch-track::before {
  /* track width (35) - margins (3 + 3) - thumb width (10) */
  left: 19px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toolbar-height: calc(
    28px + var(--jp-border-width)
  ); /* leave 28px for content */
}

.jp-Toolbar {
  color: var(--jp-ui-font-color1);
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: 2px;
  z-index: 8;
  overflow-x: hidden;
}

/* Toolbar items */

.jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer {
  flex-grow: 1;
  flex-shrink: 1;
}

.jp-Toolbar-item.jp-Toolbar-kernelStatus {
  display: inline-block;
  width: 32px;
  background-repeat: no-repeat;
  background-position: center;
  background-size: 16px;
}

.jp-Toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  display: flex;
  padding-left: 1px;
  padding-right: 1px;
  font-size: var(--jp-ui-font-size1);
  line-height: var(--jp-private-toolbar-height);
  height: 100%;
}

/* Toolbar buttons */

/* This is the div we use to wrap the react component into a Widget */
div.jp-ToolbarButton {
  color: transparent;
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0;
  margin: 0;
}

button.jp-ToolbarButtonComponent {
  background: var(--jp-layout-color1);
  border: none;
  box-sizing: border-box;
  outline: none;
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 6px;
  margin: 0;
  height: 24px;
  border-radius: var(--jp-border-radius);
  display: flex;
  align-items: center;
  text-align: center;
  font-size: 14px;
  min-width: unset;
  min-height: unset;
}

button.jp-ToolbarButtonComponent:disabled {
  opacity: 0.4;
}

button.jp-ToolbarButtonComponent > span {
  padding: 0;
  flex: 0 0 auto;
}

button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label {
  font-size: var(--jp-ui-font-size1);
  line-height: 100%;
  padding-left: 2px;
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar.jp-Toolbar-micro {
  padding: 0;
  min-height: 0;
}

#jp-main-dock-panel[data-mode='single-document']
  .jp-MainAreaWidget
  > .jp-Toolbar {
  border: none;
  box-shadow: none;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-WindowedPanel-outer {
  position: relative;
  overflow-y: auto;
}

.jp-WindowedPanel-inner {
  position: relative;
}

.jp-WindowedPanel-window {
  position: absolute;
  left: 0;
  right: 0;
  overflow: visible;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/* Sibling imports */

body {
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
}

/* Disable native link decoration styles everywhere outside of dialog boxes */
a {
  text-decoration: unset;
  color: unset;
}

a:hover {
  text-decoration: unset;
  color: unset;
}

/* Accessibility for links inside dialog box text */
.jp-Dialog-content a {
  text-decoration: revert;
  color: var(--jp-content-link-color);
}

.jp-Dialog-content a:hover {
  text-decoration: revert;
}

/* Styles for ui-components */
.jp-Button {
  color: var(--jp-ui-font-color2);
  border-radius: var(--jp-border-radius);
  padding: 0 12px;
  font-size: var(--jp-ui-font-size1);

  /* Copy from blueprint 3 */
  display: inline-flex;
  flex-direction: row;
  border: none;
  cursor: pointer;
  align-items: center;
  justify-content: center;
  text-align: left;
  vertical-align: middle;
  min-height: 30px;
  min-width: 30px;
}

.jp-Button:disabled {
  cursor: not-allowed;
}

.jp-Button:empty {
  padding: 0 !important;
}

.jp-Button.jp-mod-small {
  min-height: 24px;
  min-width: 24px;
  font-size: 12px;
  padding: 0 7px;
}

/* Use our own theme for hover styles */
.jp-Button.jp-mod-minimal:hover {
  background-color: var(--jp-layout-color2);
}

.jp-Button.jp-mod-minimal {
  background: none;
}

.jp-InputGroup {
  display: block;
  position: relative;
}

.jp-InputGroup input {
  box-sizing: border-box;
  border: none;
  border-radius: 0;
  background-color: transparent;
  color: var(--jp-ui-font-color0);
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
  padding-bottom: 0;
  padding-top: 0;
  padding-left: 10px;
  padding-right: 28px;
  position: relative;
  width: 100%;
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  font-size: 14px;
  font-weight: 400;
  height: 30px;
  line-height: 30px;
  outline: none;
  vertical-align: middle;
}

.jp-InputGroup input:focus {
  box-shadow: inset 0 0 0 var(--jp-border-width)
      var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-InputGroup input:disabled {
  cursor: not-allowed;
  resize: block;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input:disabled ~ span {
  cursor: not-allowed;
  color: var(--jp-ui-font-color2);
}

.jp-InputGroup input::placeholder,
input::placeholder {
  color: var(--jp-ui-font-color2);
}

.jp-InputGroupAction {
  position: absolute;
  bottom: 1px;
  right: 0;
  padding: 6px;
}

.jp-HTMLSelect.jp-DefaultStyle select {
  background-color: initial;
  border: none;
  border-radius: 0;
  box-shadow: none;
  color: var(--jp-ui-font-color0);
  display: block;
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  height: 24px;
  line-height: 14px;
  padding: 0 25px 0 10px;
  text-align: left;
  -moz-appearance: none;
  -webkit-appearance: none;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color2);
  cursor: not-allowed;
  resize: block;
}

.jp-HTMLSelect.jp-DefaultStyle select:disabled ~ span {
  cursor: not-allowed;
}

/* Use our own theme for hover and option styles */
/* stylelint-disable-next-line selector-max-type */
.jp-HTMLSelect.jp-DefaultStyle select:hover,
.jp-HTMLSelect.jp-DefaultStyle select > option {
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color0);
}

select {
  box-sizing: border-box;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-StatusBar-Widget {
  display: flex;
  align-items: center;
  background: var(--jp-layout-color2);
  min-height: var(--jp-statusbar-height);
  justify-content: space-between;
  padding: 0 10px;
}

.jp-StatusBar-Left {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-StatusBar-Middle {
  display: flex;
  align-items: center;
}

.jp-StatusBar-Right {
  display: flex;
  align-items: center;
  flex-direction: row-reverse;
}

.jp-StatusBar-Item {
  max-height: var(--jp-statusbar-height);
  margin: 0 2px;
  height: var(--jp-statusbar-height);
  white-space: nowrap;
  text-overflow: ellipsis;
  color: var(--jp-ui-font-color1);
  padding: 0 6px;
}

.jp-mod-highlighted:hover {
  background-color: var(--jp-layout-color3);
}

.jp-mod-clicked {
  background-color: var(--jp-brand-color1);
}

.jp-mod-clicked:hover {
  background-color: var(--jp-brand-color0);
}

.jp-mod-clicked .jp-StatusBar-TextItem {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-StatusBar-HoverItem {
  box-shadow: '0px 4px 4px rgba(0, 0, 0, 0.25)';
}

.jp-StatusBar-TextItem {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  line-height: 24px;
  color: var(--jp-ui-font-color1);
}

.jp-StatusBar-GroupItem {
  display: flex;
  align-items: center;
  flex-direction: row;
}

.jp-Statusbar-ProgressCircle svg {
  display: block;
  margin: 0 auto;
  width: 16px;
  height: 24px;
  align-self: normal;
}

.jp-Statusbar-ProgressCircle path {
  fill: var(--jp-inverse-layout-color3);
}

.jp-Statusbar-ProgressBar-progress-bar {
  height: 10px;
  width: 100px;
  border: solid 0.25px var(--jp-brand-color2);
  border-radius: 3px;
  overflow: hidden;
  align-self: center;
}

.jp-Statusbar-ProgressBar-progress-bar > div {
  background-color: var(--jp-brand-color2);
  background-image: linear-gradient(
    -45deg,
    rgba(255, 255, 255, 0.2) 25%,
    transparent 25%,
    transparent 50%,
    rgba(255, 255, 255, 0.2) 50%,
    rgba(255, 255, 255, 0.2) 75%,
    transparent 75%,
    transparent
  );
  background-size: 40px 40px;
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 14px;
  color: #fff;
  text-align: center;
  animation: jp-Statusbar-ExecutionTime-progress-bar 2s linear infinite;
}

.jp-Statusbar-ProgressBar-progress-bar p {
  color: var(--jp-ui-font-color1);
  font-family: var(--jp-ui-font-family);
  font-size: var(--jp-ui-font-size1);
  line-height: 10px;
  width: 100px;
}

@keyframes jp-Statusbar-ExecutionTime-progress-bar {
  0% {
    background-position: 0 0;
  }

  100% {
    background-position: 40px 40px;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-commandpalette-search-height: 28px;
}

/*-----------------------------------------------------------------------------
| Overall styles
|----------------------------------------------------------------------------*/

.lm-CommandPalette {
  padding-bottom: 0;
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Modal variant
|----------------------------------------------------------------------------*/

.jp-ModalCommandPalette {
  position: absolute;
  z-index: 10000;
  top: 38px;
  left: 30%;
  margin: 0;
  padding: 4px;
  width: 40%;
  box-shadow: var(--jp-elevation-z4);
  border-radius: 4px;
  background: var(--jp-layout-color0);
}

.jp-ModalCommandPalette .lm-CommandPalette {
  max-height: 40vh;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-close-icon::after {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-header {
  display: none;
}

.jp-ModalCommandPalette .lm-CommandPalette .lm-CommandPalette-item {
  margin-left: 4px;
  margin-right: 4px;
}

.jp-ModalCommandPalette
  .lm-CommandPalette
  .lm-CommandPalette-item.lm-mod-disabled {
  display: none;
}

/*-----------------------------------------------------------------------------
| Search
|----------------------------------------------------------------------------*/

.lm-CommandPalette-search {
  padding: 4px;
  background-color: var(--jp-layout-color1);
  z-index: 2;
}

.lm-CommandPalette-wrapper {
  overflow: overlay;
  padding: 0 9px;
  background-color: var(--jp-input-active-background);
  height: 30px;
  box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color);
}

.lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper {
  box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color),
    inset 0 0 0 3px var(--jp-input-active-box-shadow-color);
}

.jp-SearchIconGroup {
  color: white;
  background-color: var(--jp-brand-color1);
  position: absolute;
  top: 4px;
  right: 4px;
  padding: 5px 5px 1px;
}

.jp-SearchIconGroup svg {
  height: 20px;
  width: 20px;
}

.jp-SearchIconGroup .jp-icon3[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-input {
  background: transparent;
  width: calc(100% - 18px);
  float: left;
  border: none;
  outline: none;
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  line-height: var(--jp-private-commandpalette-search-height);
}

.lm-CommandPalette-input::-webkit-input-placeholder,
.lm-CommandPalette-input::-moz-placeholder,
.lm-CommandPalette-input:-ms-input-placeholder {
  color: var(--jp-ui-font-color2);
  font-size: var(--jp-ui-font-size1);
}

/*-----------------------------------------------------------------------------
| Results
|----------------------------------------------------------------------------*/

.lm-CommandPalette-header:first-child {
  margin-top: 0;
}

.lm-CommandPalette-header {
  border-bottom: solid var(--jp-border-width) var(--jp-border-color2);
  color: var(--jp-ui-font-color1);
  cursor: pointer;
  display: flex;
  font-size: var(--jp-ui-font-size0);
  font-weight: 600;
  letter-spacing: 1px;
  margin-top: 8px;
  padding: 8px 0 8px 12px;
  text-transform: uppercase;
}

.lm-CommandPalette-header.lm-mod-active {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-header > mark {
  background-color: transparent;
  font-weight: bold;
  color: var(--jp-ui-font-color1);
}

.lm-CommandPalette-item {
  padding: 4px 12px 4px 4px;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  font-weight: 400;
  display: flex;
}

.lm-CommandPalette-item.lm-mod-disabled {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item.lm-mod-active {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item.lm-mod-active .lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-inverse-font-color0);
}

.lm-CommandPalette-item.lm-mod-active .jp-icon-selectable[fill] {
  fill: var(--jp-layout-color0);
}

.lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) {
  background: var(--jp-layout-color2);
}

.lm-CommandPalette-itemContent {
  overflow: hidden;
}

.lm-CommandPalette-itemLabel > mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.lm-CommandPalette-item.lm-mod-disabled mark {
  color: var(--jp-ui-font-color2);
}

.lm-CommandPalette-item .lm-CommandPalette-itemIcon {
  margin: 0 4px 0 0;
  position: relative;
  width: 16px;
  top: 2px;
  flex: 0 0 auto;
}

.lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon {
  opacity: 0.6;
}

.lm-CommandPalette-item .lm-CommandPalette-itemShortcut {
  flex: 0 0 auto;
}

.lm-CommandPalette-itemCaption {
  display: none;
}

.lm-CommandPalette-content {
  background-color: var(--jp-layout-color1);
}

.lm-CommandPalette-content:empty::after {
  content: 'No results';
  margin: auto;
  margin-top: 20px;
  width: 100px;
  display: block;
  font-size: var(--jp-ui-font-size2);
  font-family: var(--jp-ui-font-family);
  font-weight: lighter;
}

.lm-CommandPalette-emptyMessage {
  text-align: center;
  margin-top: 24px;
  line-height: 1.32;
  padding: 0 8px;
  color: var(--jp-content-font-color3);
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Dialog {
  position: absolute;
  z-index: 10000;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  top: 0;
  left: 0;
  margin: 0;
  padding: 0;
  width: 100%;
  height: 100%;
  background: var(--jp-dialog-background);
}

.jp-Dialog-content {
  display: flex;
  flex-direction: column;
  margin-left: auto;
  margin-right: auto;
  background: var(--jp-layout-color1);
  padding: 24px 24px 12px;
  min-width: 300px;
  min-height: 150px;
  max-width: 1000px;
  max-height: 500px;
  box-sizing: border-box;
  box-shadow: var(--jp-elevation-z20);
  word-wrap: break-word;
  border-radius: var(--jp-border-radius);

  /* This is needed so that all font sizing of children done in ems is
   * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color1);
  resize: both;
}

.jp-Dialog-content.jp-Dialog-content-small {
  max-width: 500px;
}

.jp-Dialog-button {
  overflow: visible;
}

button.jp-Dialog-button:focus {
  outline: 1px solid var(--jp-brand-color1);
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button:focus::-moz-focus-inner {
  border: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus,
button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline-offset: 4px;
  -moz-outline-radius: 0;
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-accept:focus {
  outline: 1px solid var(--jp-accept-color-normal, var(--jp-brand-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-warn:focus {
  outline: 1px solid var(--jp-warn-color-normal, var(--jp-error-color1));
}

button.jp-Dialog-button.jp-mod-styled.jp-mod-reject:focus {
  outline: 1px solid var(--jp-reject-color-normal, var(--md-grey-600));
}

button.jp-Dialog-close-button {
  padding: 0;
  height: 100%;
  min-width: unset;
  min-height: unset;
}

.jp-Dialog-header {
  display: flex;
  justify-content: space-between;
  flex: 0 0 auto;
  padding-bottom: 12px;
  font-size: var(--jp-ui-font-size3);
  font-weight: 400;
  color: var(--jp-ui-font-color1);
}

.jp-Dialog-body {
  display: flex;
  flex-direction: column;
  flex: 1 1 auto;
  font-size: var(--jp-ui-font-size1);
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  overflow: auto;
}

.jp-Dialog-footer {
  display: flex;
  flex-direction: row;
  justify-content: flex-end;
  align-items: center;
  flex: 0 0 auto;
  margin-left: -12px;
  margin-right: -12px;
  padding: 12px;
}

.jp-Dialog-checkbox {
  padding-right: 5px;
}

.jp-Dialog-checkbox > input:focus-visible {
  outline: 1px solid var(--jp-input-active-border-color);
  outline-offset: 1px;
}

.jp-Dialog-spacer {
  flex: 1 1 auto;
}

.jp-Dialog-title {
  overflow: hidden;
  white-space: nowrap;
  text-overflow: ellipsis;
}

.jp-Dialog-body > .jp-select-wrapper {
  width: 100%;
}

.jp-Dialog-body > button {
  padding: 0 16px;
}

.jp-Dialog-body > label {
  line-height: 1.4;
  color: var(--jp-ui-font-color0);
}

.jp-Dialog-button.jp-mod-styled:not(:last-child) {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Input-Boolean-Dialog {
  flex-direction: row-reverse;
  align-items: end;
  width: 100%;
}

.jp-Input-Boolean-Dialog > label {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MainAreaWidget > :focus {
  outline: none;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error {
  padding: 6px;
}

.jp-MainAreaWidget .jp-MainAreaWidget-error > pre {
  width: auto;
  padding: 10px;
  background: var(--jp-error-color3);
  border: var(--jp-border-width) solid var(--jp-error-color1);
  border-radius: var(--jp-border-radius);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/**
 * google-material-color v1.2.6
 * https://github.com/danlevan/google-material-color
 */
:root {
  --md-red-50: #ffebee;
  --md-red-100: #ffcdd2;
  --md-red-200: #ef9a9a;
  --md-red-300: #e57373;
  --md-red-400: #ef5350;
  --md-red-500: #f44336;
  --md-red-600: #e53935;
  --md-red-700: #d32f2f;
  --md-red-800: #c62828;
  --md-red-900: #b71c1c;
  --md-red-A100: #ff8a80;
  --md-red-A200: #ff5252;
  --md-red-A400: #ff1744;
  --md-red-A700: #d50000;
  --md-pink-50: #fce4ec;
  --md-pink-100: #f8bbd0;
  --md-pink-200: #f48fb1;
  --md-pink-300: #f06292;
  --md-pink-400: #ec407a;
  --md-pink-500: #e91e63;
  --md-pink-600: #d81b60;
  --md-pink-700: #c2185b;
  --md-pink-800: #ad1457;
  --md-pink-900: #880e4f;
  --md-pink-A100: #ff80ab;
  --md-pink-A200: #ff4081;
  --md-pink-A400: #f50057;
  --md-pink-A700: #c51162;
  --md-purple-50: #f3e5f5;
  --md-purple-100: #e1bee7;
  --md-purple-200: #ce93d8;
  --md-purple-300: #ba68c8;
  --md-purple-400: #ab47bc;
  --md-purple-500: #9c27b0;
  --md-purple-600: #8e24aa;
  --md-purple-700: #7b1fa2;
  --md-purple-800: #6a1b9a;
  --md-purple-900: #4a148c;
  --md-purple-A100: #ea80fc;
  --md-purple-A200: #e040fb;
  --md-purple-A400: #d500f9;
  --md-purple-A700: #a0f;
  --md-deep-purple-50: #ede7f6;
  --md-deep-purple-100: #d1c4e9;
  --md-deep-purple-200: #b39ddb;
  --md-deep-purple-300: #9575cd;
  --md-deep-purple-400: #7e57c2;
  --md-deep-purple-500: #673ab7;
  --md-deep-purple-600: #5e35b1;
  --md-deep-purple-700: #512da8;
  --md-deep-purple-800: #4527a0;
  --md-deep-purple-900: #311b92;
  --md-deep-purple-A100: #b388ff;
  --md-deep-purple-A200: #7c4dff;
  --md-deep-purple-A400: #651fff;
  --md-deep-purple-A700: #6200ea;
  --md-indigo-50: #e8eaf6;
  --md-indigo-100: #c5cae9;
  --md-indigo-200: #9fa8da;
  --md-indigo-300: #7986cb;
  --md-indigo-400: #5c6bc0;
  --md-indigo-500: #3f51b5;
  --md-indigo-600: #3949ab;
  --md-indigo-700: #303f9f;
  --md-indigo-800: #283593;
  --md-indigo-900: #1a237e;
  --md-indigo-A100: #8c9eff;
  --md-indigo-A200: #536dfe;
  --md-indigo-A400: #3d5afe;
  --md-indigo-A700: #304ffe;
  --md-blue-50: #e3f2fd;
  --md-blue-100: #bbdefb;
  --md-blue-200: #90caf9;
  --md-blue-300: #64b5f6;
  --md-blue-400: #42a5f5;
  --md-blue-500: #2196f3;
  --md-blue-600: #1e88e5;
  --md-blue-700: #1976d2;
  --md-blue-800: #1565c0;
  --md-blue-900: #0d47a1;
  --md-blue-A100: #82b1ff;
  --md-blue-A200: #448aff;
  --md-blue-A400: #2979ff;
  --md-blue-A700: #2962ff;
  --md-light-blue-50: #e1f5fe;
  --md-light-blue-100: #b3e5fc;
  --md-light-blue-200: #81d4fa;
  --md-light-blue-300: #4fc3f7;
  --md-light-blue-400: #29b6f6;
  --md-light-blue-500: #03a9f4;
  --md-light-blue-600: #039be5;
  --md-light-blue-700: #0288d1;
  --md-light-blue-800: #0277bd;
  --md-light-blue-900: #01579b;
  --md-light-blue-A100: #80d8ff;
  --md-light-blue-A200: #40c4ff;
  --md-light-blue-A400: #00b0ff;
  --md-light-blue-A700: #0091ea;
  --md-cyan-50: #e0f7fa;
  --md-cyan-100: #b2ebf2;
  --md-cyan-200: #80deea;
  --md-cyan-300: #4dd0e1;
  --md-cyan-400: #26c6da;
  --md-cyan-500: #00bcd4;
  --md-cyan-600: #00acc1;
  --md-cyan-700: #0097a7;
  --md-cyan-800: #00838f;
  --md-cyan-900: #006064;
  --md-cyan-A100: #84ffff;
  --md-cyan-A200: #18ffff;
  --md-cyan-A400: #00e5ff;
  --md-cyan-A700: #00b8d4;
  --md-teal-50: #e0f2f1;
  --md-teal-100: #b2dfdb;
  --md-teal-200: #80cbc4;
  --md-teal-300: #4db6ac;
  --md-teal-400: #26a69a;
  --md-teal-500: #009688;
  --md-teal-600: #00897b;
  --md-teal-700: #00796b;
  --md-teal-800: #00695c;
  --md-teal-900: #004d40;
  --md-teal-A100: #a7ffeb;
  --md-teal-A200: #64ffda;
  --md-teal-A400: #1de9b6;
  --md-teal-A700: #00bfa5;
  --md-green-50: #e8f5e9;
  --md-green-100: #c8e6c9;
  --md-green-200: #a5d6a7;
  --md-green-300: #81c784;
  --md-green-400: #66bb6a;
  --md-green-500: #4caf50;
  --md-green-600: #43a047;
  --md-green-700: #388e3c;
  --md-green-800: #2e7d32;
  --md-green-900: #1b5e20;
  --md-green-A100: #b9f6ca;
  --md-green-A200: #69f0ae;
  --md-green-A400: #00e676;
  --md-green-A700: #00c853;
  --md-light-green-50: #f1f8e9;
  --md-light-green-100: #dcedc8;
  --md-light-green-200: #c5e1a5;
  --md-light-green-300: #aed581;
  --md-light-green-400: #9ccc65;
  --md-light-green-500: #8bc34a;
  --md-light-green-600: #7cb342;
  --md-light-green-700: #689f38;
  --md-light-green-800: #558b2f;
  --md-light-green-900: #33691e;
  --md-light-green-A100: #ccff90;
  --md-light-green-A200: #b2ff59;
  --md-light-green-A400: #76ff03;
  --md-light-green-A700: #64dd17;
  --md-lime-50: #f9fbe7;
  --md-lime-100: #f0f4c3;
  --md-lime-200: #e6ee9c;
  --md-lime-300: #dce775;
  --md-lime-400: #d4e157;
  --md-lime-500: #cddc39;
  --md-lime-600: #c0ca33;
  --md-lime-700: #afb42b;
  --md-lime-800: #9e9d24;
  --md-lime-900: #827717;
  --md-lime-A100: #f4ff81;
  --md-lime-A200: #eeff41;
  --md-lime-A400: #c6ff00;
  --md-lime-A700: #aeea00;
  --md-yellow-50: #fffde7;
  --md-yellow-100: #fff9c4;
  --md-yellow-200: #fff59d;
  --md-yellow-300: #fff176;
  --md-yellow-400: #ffee58;
  --md-yellow-500: #ffeb3b;
  --md-yellow-600: #fdd835;
  --md-yellow-700: #fbc02d;
  --md-yellow-800: #f9a825;
  --md-yellow-900: #f57f17;
  --md-yellow-A100: #ffff8d;
  --md-yellow-A200: #ff0;
  --md-yellow-A400: #ffea00;
  --md-yellow-A700: #ffd600;
  --md-amber-50: #fff8e1;
  --md-amber-100: #ffecb3;
  --md-amber-200: #ffe082;
  --md-amber-300: #ffd54f;
  --md-amber-400: #ffca28;
  --md-amber-500: #ffc107;
  --md-amber-600: #ffb300;
  --md-amber-700: #ffa000;
  --md-amber-800: #ff8f00;
  --md-amber-900: #ff6f00;
  --md-amber-A100: #ffe57f;
  --md-amber-A200: #ffd740;
  --md-amber-A400: #ffc400;
  --md-amber-A700: #ffab00;
  --md-orange-50: #fff3e0;
  --md-orange-100: #ffe0b2;
  --md-orange-200: #ffcc80;
  --md-orange-300: #ffb74d;
  --md-orange-400: #ffa726;
  --md-orange-500: #ff9800;
  --md-orange-600: #fb8c00;
  --md-orange-700: #f57c00;
  --md-orange-800: #ef6c00;
  --md-orange-900: #e65100;
  --md-orange-A100: #ffd180;
  --md-orange-A200: #ffab40;
  --md-orange-A400: #ff9100;
  --md-orange-A700: #ff6d00;
  --md-deep-orange-50: #fbe9e7;
  --md-deep-orange-100: #ffccbc;
  --md-deep-orange-200: #ffab91;
  --md-deep-orange-300: #ff8a65;
  --md-deep-orange-400: #ff7043;
  --md-deep-orange-500: #ff5722;
  --md-deep-orange-600: #f4511e;
  --md-deep-orange-700: #e64a19;
  --md-deep-orange-800: #d84315;
  --md-deep-orange-900: #bf360c;
  --md-deep-orange-A100: #ff9e80;
  --md-deep-orange-A200: #ff6e40;
  --md-deep-orange-A400: #ff3d00;
  --md-deep-orange-A700: #dd2c00;
  --md-brown-50: #efebe9;
  --md-brown-100: #d7ccc8;
  --md-brown-200: #bcaaa4;
  --md-brown-300: #a1887f;
  --md-brown-400: #8d6e63;
  --md-brown-500: #795548;
  --md-brown-600: #6d4c41;
  --md-brown-700: #5d4037;
  --md-brown-800: #4e342e;
  --md-brown-900: #3e2723;
  --md-grey-50: #fafafa;
  --md-grey-100: #f5f5f5;
  --md-grey-200: #eee;
  --md-grey-300: #e0e0e0;
  --md-grey-400: #bdbdbd;
  --md-grey-500: #9e9e9e;
  --md-grey-600: #757575;
  --md-grey-700: #616161;
  --md-grey-800: #424242;
  --md-grey-900: #212121;
  --md-blue-grey-50: #eceff1;
  --md-blue-grey-100: #cfd8dc;
  --md-blue-grey-200: #b0bec5;
  --md-blue-grey-300: #90a4ae;
  --md-blue-grey-400: #78909c;
  --md-blue-grey-500: #607d8b;
  --md-blue-grey-600: #546e7a;
  --md-blue-grey-700: #455a64;
  --md-blue-grey-800: #37474f;
  --md-blue-grey-900: #263238;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2017, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| RenderedText
|----------------------------------------------------------------------------*/

:root {
  /* This is the padding value to fill the gaps between lines containing spans with background color. */
  --jp-private-code-span-padding: calc(
    (var(--jp-code-line-height) - 1) * var(--jp-code-font-size) / 2
  );
}

.jp-RenderedText {
  text-align: left;
  padding-left: var(--jp-code-padding);
  line-height: var(--jp-code-line-height);
  font-family: var(--jp-code-font-family);
}

.jp-RenderedText pre,
.jp-RenderedJavaScript pre,
.jp-RenderedHTMLCommon pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
  border: none;
  margin: 0;
  padding: 0;
}

.jp-RenderedText pre a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedText pre a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* console foregrounds and backgrounds */
.jp-RenderedText pre .ansi-black-fg {
  color: #3e424d;
}

.jp-RenderedText pre .ansi-red-fg {
  color: #e75c58;
}

.jp-RenderedText pre .ansi-green-fg {
  color: #00a250;
}

.jp-RenderedText pre .ansi-yellow-fg {
  color: #ddb62b;
}

.jp-RenderedText pre .ansi-blue-fg {
  color: #208ffb;
}

.jp-RenderedText pre .ansi-magenta-fg {
  color: #d160c4;
}

.jp-RenderedText pre .ansi-cyan-fg {
  color: #60c6c8;
}

.jp-RenderedText pre .ansi-white-fg {
  color: #c5c1b4;
}

.jp-RenderedText pre .ansi-black-bg {
  background-color: #3e424d;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-bg {
  background-color: #e75c58;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-bg {
  background-color: #00a250;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-bg {
  background-color: #ddb62b;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-bg {
  background-color: #208ffb;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-bg {
  background-color: #d160c4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-bg {
  background-color: #60c6c8;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-bg {
  background-color: #c5c1b4;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-black-intense-fg {
  color: #282c36;
}

.jp-RenderedText pre .ansi-red-intense-fg {
  color: #b22b31;
}

.jp-RenderedText pre .ansi-green-intense-fg {
  color: #007427;
}

.jp-RenderedText pre .ansi-yellow-intense-fg {
  color: #b27d12;
}

.jp-RenderedText pre .ansi-blue-intense-fg {
  color: #0065ca;
}

.jp-RenderedText pre .ansi-magenta-intense-fg {
  color: #a03196;
}

.jp-RenderedText pre .ansi-cyan-intense-fg {
  color: #258f8f;
}

.jp-RenderedText pre .ansi-white-intense-fg {
  color: #a1a6b2;
}

.jp-RenderedText pre .ansi-black-intense-bg {
  background-color: #282c36;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-red-intense-bg {
  background-color: #b22b31;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-green-intense-bg {
  background-color: #007427;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-yellow-intense-bg {
  background-color: #b27d12;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-blue-intense-bg {
  background-color: #0065ca;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-magenta-intense-bg {
  background-color: #a03196;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-cyan-intense-bg {
  background-color: #258f8f;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-white-intense-bg {
  background-color: #a1a6b2;
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-default-inverse-fg {
  color: var(--jp-ui-inverse-font-color0);
}

.jp-RenderedText pre .ansi-default-inverse-bg {
  background-color: var(--jp-inverse-layout-color0);
  padding: var(--jp-private-code-span-padding) 0;
}

.jp-RenderedText pre .ansi-bold {
  font-weight: bold;
}

.jp-RenderedText pre .ansi-underline {
  text-decoration: underline;
}

.jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] {
  background: var(--jp-rendermime-error-background);
  padding-top: var(--jp-code-padding);
}

/*-----------------------------------------------------------------------------
| RenderedLatex
|----------------------------------------------------------------------------*/

.jp-RenderedLatex {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);
}

/* Left-justify outputs.*/
.jp-OutputArea-output.jp-RenderedLatex {
  padding: var(--jp-code-padding);
  text-align: left;
}

/*-----------------------------------------------------------------------------
| RenderedHTML
|----------------------------------------------------------------------------*/

.jp-RenderedHTMLCommon {
  color: var(--jp-content-font-color1);
  font-family: var(--jp-content-font-family);
  font-size: var(--jp-content-font-size1);
  line-height: var(--jp-content-line-height);

  /* Give a bit more R padding on Markdown text to keep line lengths reasonable */
  padding-right: 20px;
}

.jp-RenderedHTMLCommon em {
  font-style: italic;
}

.jp-RenderedHTMLCommon strong {
  font-weight: bold;
}

.jp-RenderedHTMLCommon u {
  text-decoration: underline;
}

.jp-RenderedHTMLCommon a:link {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:hover {
  text-decoration: underline;
  color: var(--jp-content-link-color);
}

.jp-RenderedHTMLCommon a:visited {
  text-decoration: none;
  color: var(--jp-content-link-color);
}

/* Headings */

.jp-RenderedHTMLCommon h1,
.jp-RenderedHTMLCommon h2,
.jp-RenderedHTMLCommon h3,
.jp-RenderedHTMLCommon h4,
.jp-RenderedHTMLCommon h5,
.jp-RenderedHTMLCommon h6 {
  line-height: var(--jp-content-heading-line-height);
  font-weight: var(--jp-content-heading-font-weight);
  font-style: normal;
  margin: var(--jp-content-heading-margin-top) 0
    var(--jp-content-heading-margin-bottom) 0;
}

.jp-RenderedHTMLCommon h1:first-child,
.jp-RenderedHTMLCommon h2:first-child,
.jp-RenderedHTMLCommon h3:first-child,
.jp-RenderedHTMLCommon h4:first-child,
.jp-RenderedHTMLCommon h5:first-child,
.jp-RenderedHTMLCommon h6:first-child {
  margin-top: calc(0.5 * var(--jp-content-heading-margin-top));
}

.jp-RenderedHTMLCommon h1:last-child,
.jp-RenderedHTMLCommon h2:last-child,
.jp-RenderedHTMLCommon h3:last-child,
.jp-RenderedHTMLCommon h4:last-child,
.jp-RenderedHTMLCommon h5:last-child,
.jp-RenderedHTMLCommon h6:last-child {
  margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom));
}

.jp-RenderedHTMLCommon h1 {
  font-size: var(--jp-content-font-size5);
}

.jp-RenderedHTMLCommon h2 {
  font-size: var(--jp-content-font-size4);
}

.jp-RenderedHTMLCommon h3 {
  font-size: var(--jp-content-font-size3);
}

.jp-RenderedHTMLCommon h4 {
  font-size: var(--jp-content-font-size2);
}

.jp-RenderedHTMLCommon h5 {
  font-size: var(--jp-content-font-size1);
}

.jp-RenderedHTMLCommon h6 {
  font-size: var(--jp-content-font-size0);
}

/* Lists */

/* stylelint-disable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon ul:not(.list-inline),
.jp-RenderedHTMLCommon ol:not(.list-inline) {
  padding-left: 2em;
}

.jp-RenderedHTMLCommon ul {
  list-style: disc;
}

.jp-RenderedHTMLCommon ul ul {
  list-style: square;
}

.jp-RenderedHTMLCommon ul ul ul {
  list-style: circle;
}

.jp-RenderedHTMLCommon ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol ol {
  list-style: upper-alpha;
}

.jp-RenderedHTMLCommon ol ol ol {
  list-style: lower-alpha;
}

.jp-RenderedHTMLCommon ol ol ol ol {
  list-style: lower-roman;
}

.jp-RenderedHTMLCommon ol ol ol ol ol {
  list-style: decimal;
}

.jp-RenderedHTMLCommon ol,
.jp-RenderedHTMLCommon ul {
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon ul ul,
.jp-RenderedHTMLCommon ul ol,
.jp-RenderedHTMLCommon ol ul,
.jp-RenderedHTMLCommon ol ol {
  margin-bottom: 0;
}

/* stylelint-enable selector-max-type, selector-max-compound-selectors */

.jp-RenderedHTMLCommon hr {
  color: var(--jp-border-color2);
  background-color: var(--jp-border-color1);
  margin-top: 1em;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon > pre {
  margin: 1.5em 2em;
}

.jp-RenderedHTMLCommon pre,
.jp-RenderedHTMLCommon code {
  border: 0;
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  line-height: var(--jp-code-line-height);
  padding: 0;
  white-space: pre-wrap;
}

.jp-RenderedHTMLCommon :not(pre) > code {
  background-color: var(--jp-layout-color2);
  padding: 1px 5px;
}

/* Tables */

.jp-RenderedHTMLCommon table {
  border-collapse: collapse;
  border-spacing: 0;
  border: none;
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  table-layout: fixed;
  margin-left: auto;
  margin-bottom: 1em;
  margin-right: auto;
}

.jp-RenderedHTMLCommon thead {
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  vertical-align: bottom;
}

.jp-RenderedHTMLCommon td,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon tr {
  vertical-align: middle;
  padding: 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}

.jp-RenderedMarkdown.jp-RenderedHTMLCommon td,
.jp-RenderedMarkdown.jp-RenderedHTMLCommon th {
  max-width: none;
}

:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th,
:not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr {
  text-align: right;
}

.jp-RenderedHTMLCommon th {
  font-weight: bold;
}

.jp-RenderedHTMLCommon tbody tr:nth-child(odd) {
  background: var(--jp-layout-color0);
}

.jp-RenderedHTMLCommon tbody tr:nth-child(even) {
  background: var(--jp-rendermime-table-row-background);
}

.jp-RenderedHTMLCommon tbody tr:hover {
  background: var(--jp-rendermime-table-row-hover-background);
}

.jp-RenderedHTMLCommon p {
  text-align: left;
  margin: 0;
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon img {
  -moz-force-broken-image-icon: 1;
}

/* Restrict to direct children as other images could be nested in other content. */
.jp-RenderedHTMLCommon > img {
  display: block;
  margin-left: 0;
  margin-right: 0;
  margin-bottom: 1em;
}

/* Change color behind transparent images if they need it... */
[data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background {
  background-color: var(--jp-inverse-layout-color1);
}

[data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background {
  background-color: var(--jp-inverse-layout-color1);
}

.jp-RenderedHTMLCommon img,
.jp-RenderedImage img,
.jp-RenderedHTMLCommon svg,
.jp-RenderedSVG svg {
  max-width: 100%;
  height: auto;
}

.jp-RenderedHTMLCommon img.jp-mod-unconfined,
.jp-RenderedImage img.jp-mod-unconfined,
.jp-RenderedHTMLCommon svg.jp-mod-unconfined,
.jp-RenderedSVG svg.jp-mod-unconfined {
  max-width: none;
}

.jp-RenderedHTMLCommon .alert {
  padding: var(--jp-notebook-padding);
  border: var(--jp-border-width) solid transparent;
  border-radius: var(--jp-border-radius);
  margin-bottom: 1em;
}

.jp-RenderedHTMLCommon .alert-info {
  color: var(--jp-info-color0);
  background-color: var(--jp-info-color3);
  border-color: var(--jp-info-color2);
}

.jp-RenderedHTMLCommon .alert-info hr {
  border-color: var(--jp-info-color3);
}

.jp-RenderedHTMLCommon .alert-info > p:last-child,
.jp-RenderedHTMLCommon .alert-info > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-warning {
  color: var(--jp-warn-color0);
  background-color: var(--jp-warn-color3);
  border-color: var(--jp-warn-color2);
}

.jp-RenderedHTMLCommon .alert-warning hr {
  border-color: var(--jp-warn-color3);
}

.jp-RenderedHTMLCommon .alert-warning > p:last-child,
.jp-RenderedHTMLCommon .alert-warning > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-success {
  color: var(--jp-success-color0);
  background-color: var(--jp-success-color3);
  border-color: var(--jp-success-color2);
}

.jp-RenderedHTMLCommon .alert-success hr {
  border-color: var(--jp-success-color3);
}

.jp-RenderedHTMLCommon .alert-success > p:last-child,
.jp-RenderedHTMLCommon .alert-success > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon .alert-danger {
  color: var(--jp-error-color0);
  background-color: var(--jp-error-color3);
  border-color: var(--jp-error-color2);
}

.jp-RenderedHTMLCommon .alert-danger hr {
  border-color: var(--jp-error-color3);
}

.jp-RenderedHTMLCommon .alert-danger > p:last-child,
.jp-RenderedHTMLCommon .alert-danger > ul:last-child {
  margin-bottom: 0;
}

.jp-RenderedHTMLCommon blockquote {
  margin: 1em 2em;
  padding: 0 1em;
  border-left: 5px solid var(--jp-border-color2);
}

a.jp-InternalAnchorLink {
  visibility: hidden;
  margin-left: 8px;
  color: var(--md-blue-800);
}

h1:hover .jp-InternalAnchorLink,
h2:hover .jp-InternalAnchorLink,
h3:hover .jp-InternalAnchorLink,
h4:hover .jp-InternalAnchorLink,
h5:hover .jp-InternalAnchorLink,
h6:hover .jp-InternalAnchorLink {
  visibility: visible;
}

.jp-RenderedHTMLCommon kbd {
  background-color: var(--jp-rendermime-table-row-background);
  border: 1px solid var(--jp-border-color0);
  border-bottom-color: var(--jp-border-color2);
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
  display: inline-block;
  font-size: var(--jp-ui-font-size0);
  line-height: 1em;
  padding: 0.2em 0.5em;
}

/* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0.
 * At the bottom of cells this is a bit too much as there is also spacing
 * between cells. Going all the way to 0 gets too tight between markdown and
 * code cells.
 */
.jp-RenderedHTMLCommon > *:last-child {
  margin-bottom: 0.5em;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Copyright (c) 2014-2017, PhosphorJS Contributors
|
| Distributed under the terms of the BSD 3-Clause License.
|
| The full license is in the file LICENSE, distributed with this software.
|----------------------------------------------------------------------------*/

.lm-cursor-backdrop {
  position: fixed;
  width: 200px;
  height: 200px;
  margin-top: -100px;
  margin-left: -100px;
  will-change: transform;
  z-index: 100;
}

.lm-mod-drag-image {
  will-change: transform;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-lineFormSearch {
  padding: 4px 12px;
  background-color: var(--jp-layout-color2);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
  font-size: var(--jp-ui-font-size1);
}

.jp-lineFormCaption {
  font-size: var(--jp-ui-font-size0);
  line-height: var(--jp-ui-font-size1);
  margin-top: 4px;
  color: var(--jp-ui-font-color0);
}

.jp-baseLineForm {
  border: none;
  border-radius: 0;
  position: absolute;
  background-size: 16px;
  background-repeat: no-repeat;
  background-position: center;
  outline: none;
}

.jp-lineFormButtonContainer {
  top: 4px;
  right: 8px;
  height: 24px;
  padding: 0 12px;
  width: 12px;
}

.jp-lineFormButtonIcon {
  top: 0;
  right: 0;
  background-color: var(--jp-brand-color1);
  height: 100%;
  width: 100%;
  box-sizing: border-box;
  padding: 4px 6px;
}

.jp-lineFormButton {
  top: 0;
  right: 0;
  background-color: transparent;
  height: 100%;
  width: 100%;
  box-sizing: border-box;
}

.jp-lineFormWrapper {
  overflow: hidden;
  padding: 0 8px;
  border: 1px solid var(--jp-border-color0);
  background-color: var(--jp-input-active-background);
  height: 22px;
}

.jp-lineFormWrapperFocusWithin {
  border: var(--jp-border-width) solid var(--md-blue-500);
  box-shadow: inset 0 0 4px var(--md-blue-300);
}

.jp-lineFormInput {
  background: transparent;
  width: 200px;
  height: 100%;
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  line-height: 28px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) 2014-2016, Jupyter Development Team.
|
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-JSONEditor {
  display: flex;
  flex-direction: column;
  width: 100%;
}

.jp-JSONEditor-host {
  flex: 1 1 auto;
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  border-radius: 0;
  background: var(--jp-layout-color0);
  min-height: 50px;
  padding: 1px;
}

.jp-JSONEditor.jp-mod-error .jp-JSONEditor-host {
  border-color: red;
  outline-color: red;
}

.jp-JSONEditor-header {
  display: flex;
  flex: 1 0 auto;
  padding: 0 0 0 12px;
}

.jp-JSONEditor-header label {
  flex: 0 0 auto;
}

.jp-JSONEditor-commitButton {
  height: 16px;
  width: 16px;
  background-size: 18px;
  background-repeat: no-repeat;
  background-position: center;
}

.jp-JSONEditor-host.jp-mod-focused {
  background-color: var(--jp-input-active-background);
  border: 1px solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

.jp-Editor.jp-mod-dropTarget {
  border: var(--jp-border-width) solid var(--jp-input-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/
.jp-DocumentSearch-input {
  border: none;
  outline: none;
  color: var(--jp-ui-font-color0);
  font-size: var(--jp-ui-font-size1);
  background-color: var(--jp-layout-color0);
  font-family: var(--jp-ui-font-family);
  padding: 2px 1px;
  resize: none;
}

.jp-DocumentSearch-overlay {
  position: absolute;
  background-color: var(--jp-toolbar-background);
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  border-left: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  top: 0;
  right: 0;
  z-index: 7;
  min-width: 405px;
  padding: 2px;
  font-size: var(--jp-ui-font-size1);

  --jp-private-document-search-button-height: 20px;
}

.jp-DocumentSearch-overlay button {
  background-color: var(--jp-toolbar-background);
  outline: 0;
}

.jp-DocumentSearch-overlay button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-overlay button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-overlay-row {
  display: flex;
  align-items: center;
  margin-bottom: 2px;
}

.jp-DocumentSearch-button-content {
  display: inline-block;
  cursor: pointer;
  box-sizing: border-box;
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-button-content svg {
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-input-wrapper {
  border: var(--jp-border-width) solid var(--jp-border-color0);
  display: flex;
  background-color: var(--jp-layout-color0);
  margin: 2px;
}

.jp-DocumentSearch-input-wrapper:focus-within {
  border-color: var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper {
  all: initial;
  overflow: hidden;
  display: inline-block;
  border: none;
  box-sizing: border-box;
}

.jp-DocumentSearch-toggle-wrapper {
  width: 14px;
  height: 14px;
}

.jp-DocumentSearch-button-wrapper {
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
}

.jp-DocumentSearch-toggle-wrapper:focus,
.jp-DocumentSearch-button-wrapper:focus {
  outline: var(--jp-border-width) solid
    var(--jp-cell-editor-active-border-color);
  outline-offset: -1px;
}

.jp-DocumentSearch-toggle-wrapper,
.jp-DocumentSearch-button-wrapper,
.jp-DocumentSearch-button-content:focus {
  outline: none;
}

.jp-DocumentSearch-toggle-placeholder {
  width: 5px;
}

.jp-DocumentSearch-input-button::before {
  display: block;
  padding-top: 100%;
}

.jp-DocumentSearch-input-button-off {
  opacity: var(--jp-search-toggle-off-opacity);
}

.jp-DocumentSearch-input-button-off:hover {
  opacity: var(--jp-search-toggle-hover-opacity);
}

.jp-DocumentSearch-input-button-on {
  opacity: var(--jp-search-toggle-on-opacity);
}

.jp-DocumentSearch-index-counter {
  padding-left: 10px;
  padding-right: 10px;
  user-select: none;
  min-width: 35px;
  display: inline-block;
}

.jp-DocumentSearch-up-down-wrapper {
  display: inline-block;
  padding-right: 2px;
  margin-left: auto;
  white-space: nowrap;
}

.jp-DocumentSearch-spacer {
  margin-left: auto;
}

.jp-DocumentSearch-up-down-wrapper button {
  outline: 0;
  border: none;
  width: var(--jp-private-document-search-button-height);
  height: var(--jp-private-document-search-button-height);
  vertical-align: middle;
  margin: 1px 5px 2px;
}

.jp-DocumentSearch-up-down-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-up-down-button:active {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-filter-button {
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-filter-button:hover {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled {
  background-color: var(--jp-layout-color2);
}

.jp-DocumentSearch-filter-button-enabled:hover {
  background-color: var(--jp-layout-color3);
}

.jp-DocumentSearch-search-options {
  padding: 0 8px;
  margin-left: 3px;
  width: 100%;
  display: grid;
  justify-content: start;
  grid-template-columns: 1fr 1fr;
  align-items: center;
  justify-items: stretch;
}

.jp-DocumentSearch-search-filter-disabled {
  color: var(--jp-ui-font-color2);
}

.jp-DocumentSearch-search-filter {
  display: flex;
  align-items: center;
  user-select: none;
}

.jp-DocumentSearch-regex-error {
  color: var(--jp-error-color0);
}

.jp-DocumentSearch-replace-button-wrapper {
  overflow: hidden;
  display: inline-block;
  box-sizing: border-box;
  border: var(--jp-border-width) solid var(--jp-border-color0);
  margin: auto 2px;
  padding: 1px 4px;
  height: calc(var(--jp-private-document-search-button-height) + 2px);
}

.jp-DocumentSearch-replace-button-wrapper:focus {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
}

.jp-DocumentSearch-replace-button {
  display: inline-block;
  text-align: center;
  cursor: pointer;
  box-sizing: border-box;
  color: var(--jp-ui-font-color1);

  /* height - 2 * (padding of wrapper) */
  line-height: calc(var(--jp-private-document-search-button-height) - 2px);
  width: 100%;
  height: 100%;
}

.jp-DocumentSearch-replace-button:focus {
  outline: none;
}

.jp-DocumentSearch-replace-wrapper-class {
  margin-left: 14px;
  display: flex;
}

.jp-DocumentSearch-replace-toggle {
  border: none;
  background-color: var(--jp-toolbar-background);
  border-radius: var(--jp-border-radius);
}

.jp-DocumentSearch-replace-toggle:hover {
  background-color: var(--jp-layout-color2);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.cm-editor {
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  border: 0;
  border-radius: 0;
  height: auto;

  /* Changed to auto to autogrow */
}

.cm-editor pre {
  padding: 0 var(--jp-code-padding);
}

.jp-CodeMirrorEditor[data-type='inline'] .cm-dialog {
  background-color: var(--jp-layout-color0);
  color: var(--jp-content-font-color1);
}

.jp-CodeMirrorEditor {
  cursor: text;
}

/* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width1) solid
      var(--jp-editor-cursor-color);
  }
}

/* When zoomed out less than 33% */
@media screen and (min-width: 4320px) {
  .jp-CodeMirrorEditor[data-type='inline'] .cm-cursor {
    border-left: var(--jp-code-cursor-width2) solid
      var(--jp-editor-cursor-color);
  }
}

.cm-editor.jp-mod-readOnly .cm-cursor {
  display: none;
}

.jp-CollaboratorCursor {
  border-left: 5px solid transparent;
  border-right: 5px solid transparent;
  border-top: none;
  border-bottom: 3px solid;
  background-clip: content-box;
  margin-left: -5px;
  margin-right: -5px;
}

.cm-searching,
.cm-searching span {
  /* `.cm-searching span`: we need to override syntax highlighting */
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.cm-searching::selection,
.cm-searching span::selection {
  background-color: var(--jp-search-unselected-match-background-color);
  color: var(--jp-search-unselected-match-color);
}

.jp-current-match > .cm-searching,
.jp-current-match > .cm-searching span,
.cm-searching > .jp-current-match,
.cm-searching > .jp-current-match span {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.jp-current-match > .cm-searching::selection,
.cm-searching > .jp-current-match::selection,
.jp-current-match > .cm-searching span::selection {
  background-color: var(--jp-search-selected-match-background-color);
  color: var(--jp-search-selected-match-color);
}

.cm-trailingspace {
  background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAYAAAB4ka1VAAAAsElEQVQIHQGlAFr/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7+r3zKmT0/+pk9P/7+r3zAAAAAAAAAAABAAAAAAAAAAA6OPzM+/q9wAAAAAA6OPzMwAAAAAAAAAAAgAAAAAAAAAAGR8NiRQaCgAZIA0AGR8NiQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQyoYJ/SY80UAAAAASUVORK5CYII=);
  background-position: center left;
  background-repeat: repeat-x;
}

.jp-CollaboratorCursor-hover {
  position: absolute;
  z-index: 1;
  transform: translateX(-50%);
  color: white;
  border-radius: 3px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 1px;
  padding-bottom: 1px;
  text-align: center;
  font-size: var(--jp-ui-font-size1);
  white-space: nowrap;
}

.jp-CodeMirror-ruler {
  border-left: 1px dashed var(--jp-border-color2);
}

/* Styles for shared cursors (remote cursor locations and selected ranges) */
.jp-CodeMirrorEditor .cm-ySelectionCaret {
  position: relative;
  border-left: 1px solid black;
  margin-left: -1px;
  margin-right: -1px;
  box-sizing: border-box;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret > .cm-ySelectionInfo {
  white-space: nowrap;
  position: absolute;
  top: -1.15em;
  padding-bottom: 0.05em;
  left: -1px;
  font-size: 0.95em;
  font-family: var(--jp-ui-font-family);
  font-weight: bold;
  line-height: normal;
  user-select: none;
  color: white;
  padding-left: 2px;
  padding-right: 2px;
  z-index: 101;
  transition: opacity 0.3s ease-in-out;
}

.jp-CodeMirrorEditor .cm-ySelectionInfo {
  transition-delay: 0.7s;
  opacity: 0;
}

.jp-CodeMirrorEditor .cm-ySelectionCaret:hover > .cm-ySelectionInfo {
  opacity: 1;
  transition-delay: 0s;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-MimeDocument {
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-filebrowser-button-height: 28px;
  --jp-private-filebrowser-button-width: 48px;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-FileBrowser .jp-SidePanel-content {
  display: flex;
  flex-direction: column;
}

.jp-FileBrowser-toolbar.jp-Toolbar {
  flex-wrap: wrap;
  row-gap: 12px;
  border-bottom: none;
  height: auto;
  margin: 8px 12px 0;
  box-shadow: none;
  padding: 0;
  justify-content: flex-start;
}

.jp-FileBrowser-Panel {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
}

.jp-BreadCrumbs {
  flex: 0 0 auto;
  margin: 8px 12px;
}

.jp-BreadCrumbs-item {
  margin: 0 2px;
  padding: 0 2px;
  border-radius: var(--jp-border-radius);
  cursor: pointer;
}

.jp-BreadCrumbs-item:hover {
  background-color: var(--jp-layout-color2);
}

.jp-BreadCrumbs-item:first-child {
  margin-left: 0;
}

.jp-BreadCrumbs-item.jp-mod-dropTarget {
  background-color: var(--jp-brand-color2);
  opacity: 0.7;
}

/*-----------------------------------------------------------------------------
| Buttons
|----------------------------------------------------------------------------*/

.jp-FileBrowser-toolbar > .jp-Toolbar-item {
  flex: 0 0 auto;
  padding-left: 0;
  padding-right: 2px;
  align-items: center;
  height: unset;
}

.jp-FileBrowser-toolbar > .jp-Toolbar-item .jp-ToolbarButtonComponent {
  width: 40px;
}

/*-----------------------------------------------------------------------------
| Other styles
|----------------------------------------------------------------------------*/

.jp-FileDialog.jp-mod-conflict input {
  color: var(--jp-error-color1);
}

.jp-FileDialog .jp-new-name-title {
  margin-top: 12px;
}

.jp-LastModified-hidden {
  display: none;
}

.jp-FileSize-hidden {
  display: none;
}

.jp-FileBrowser .lm-AccordionPanel > h3:first-child {
  display: none;
}

/*-----------------------------------------------------------------------------
| DirListing
|----------------------------------------------------------------------------*/

.jp-DirListing {
  flex: 1 1 auto;
  display: flex;
  flex-direction: column;
  outline: 0;
}

.jp-DirListing-header {
  flex: 0 0 auto;
  display: flex;
  flex-direction: row;
  align-items: center;
  overflow: hidden;
  border-top: var(--jp-border-width) solid var(--jp-border-color2);
  border-bottom: var(--jp-border-width) solid var(--jp-border-color1);
  box-shadow: var(--jp-toolbar-box-shadow);
  z-index: 2;
}

.jp-DirListing-headerItem {
  padding: 4px 12px 2px;
  font-weight: 500;
}

.jp-DirListing-headerItem:hover {
  background: var(--jp-layout-color2);
}

.jp-DirListing-headerItem.jp-id-name {
  flex: 1 0 84px;
}

.jp-DirListing-headerItem.jp-id-modified {
  flex: 0 0 112px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-DirListing-headerItem.jp-id-filesize {
  flex: 0 0 75px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
}

.jp-id-narrow {
  display: none;
  flex: 0 0 5px;
  padding: 4px;
  border-left: var(--jp-border-width) solid var(--jp-border-color2);
  text-align: right;
  color: var(--jp-border-color2);
}

.jp-DirListing-narrow .jp-id-narrow {
  display: block;
}

.jp-DirListing-narrow .jp-id-modified,
.jp-DirListing-narrow .jp-DirListing-itemModified {
  display: none;
}

.jp-DirListing-headerItem.jp-mod-selected {
  font-weight: 600;
}

/* increase specificity to override bundled default */
.jp-DirListing-content {
  flex: 1 1 auto;
  margin: 0;
  padding: 0;
  list-style-type: none;
  overflow: auto;
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-content mark {
  color: var(--jp-ui-font-color0);
  background-color: transparent;
  font-weight: bold;
}

.jp-DirListing-content .jp-DirListing-item.jp-mod-selected mark {
  color: var(--jp-ui-inverse-font-color0);
}

/* Style the directory listing content when a user drops a file to upload */
.jp-DirListing.jp-mod-native-drop .jp-DirListing-content {
  outline: 5px dashed rgba(128, 128, 128, 0.5);
  outline-offset: -10px;
  cursor: copy;
}

.jp-DirListing-item {
  display: flex;
  flex-direction: row;
  align-items: center;
  padding: 4px 12px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-DirListing-checkboxWrapper {
  /* Increases hit area of checkbox. */
  padding: 4px;
}

.jp-DirListing-header
  .jp-DirListing-checkboxWrapper
  + .jp-DirListing-headerItem {
  padding-left: 4px;
}

.jp-DirListing-content .jp-DirListing-checkboxWrapper {
  position: relative;
  left: -4px;
  margin: -4px 0 -4px -8px;
}

.jp-DirListing-checkboxWrapper.jp-mod-visible {
  visibility: visible;
}

/* For devices that support hovering, hide checkboxes until hovered, selected...
*/
@media (hover: hover) {
  .jp-DirListing-checkboxWrapper {
    visibility: hidden;
  }

  .jp-DirListing-item:hover .jp-DirListing-checkboxWrapper,
  .jp-DirListing-item.jp-mod-selected .jp-DirListing-checkboxWrapper {
    visibility: visible;
  }
}

.jp-DirListing-item[data-is-dot] {
  opacity: 75%;
}

.jp-DirListing-item.jp-mod-selected {
  color: var(--jp-ui-inverse-font-color1);
  background: var(--jp-brand-color1);
}

.jp-DirListing-item.jp-mod-dropTarget {
  background: var(--jp-brand-color3);
}

.jp-DirListing-item:hover:not(.jp-mod-selected) {
  background: var(--jp-layout-color2);
}

.jp-DirListing-itemIcon {
  flex: 0 0 20px;
  margin-right: 4px;
}

.jp-DirListing-itemText {
  flex: 1 0 64px;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  user-select: none;
}

.jp-DirListing-itemText:focus {
  outline-width: 2px;
  outline-color: var(--jp-inverse-layout-color1);
  outline-style: solid;
  outline-offset: 1px;
}

.jp-DirListing-item.jp-mod-selected .jp-DirListing-itemText:focus {
  outline-color: var(--jp-layout-color1);
}

.jp-DirListing-itemModified {
  flex: 0 0 125px;
  text-align: right;
}

.jp-DirListing-itemFileSize {
  flex: 0 0 90px;
  text-align: right;
}

.jp-DirListing-editor {
  flex: 1 0 64px;
  outline: none;
  border: none;
  color: var(--jp-ui-font-color1);
  background-color: var(--jp-layout-color1);
}

.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon::before {
  color: var(--jp-success-color1);
  content: '\25CF';
  font-size: 8px;
  position: absolute;
  left: -8px;
}

.jp-DirListing-item.jp-mod-running.jp-mod-selected
  .jp-DirListing-itemIcon::before {
  color: var(--jp-ui-inverse-font-color1);
}

.jp-DirListing-item.lm-mod-drag-image,
.jp-DirListing-item.jp-mod-selected.lm-mod-drag-image {
  font-size: var(--jp-ui-font-size1);
  padding-left: 4px;
  margin-left: 4px;
  width: 160px;
  background-color: var(--jp-ui-inverse-font-color2);
  box-shadow: var(--jp-elevation-z2);
  border-radius: 0;
  color: var(--jp-ui-font-color1);
  transform: translateX(-40%) translateY(-58%);
}

.jp-Document {
  min-width: 120px;
  min-height: 120px;
  outline: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Main OutputArea
| OutputArea has a list of Outputs
|----------------------------------------------------------------------------*/

.jp-OutputArea {
  overflow-y: auto;
}

.jp-OutputArea-child {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-OutputPrompt {
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-outprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
  opacity: var(--jp-cell-prompt-opacity);

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-OutputArea-prompt {
  display: table-cell;
  vertical-align: top;
}

.jp-OutputArea-output {
  display: table-cell;
  width: 100%;
  height: auto;
  overflow: auto;
  user-select: text;
  -moz-user-select: text;
  -webkit-user-select: text;
  -ms-user-select: text;
}

.jp-OutputArea .jp-RenderedText {
  padding-left: 1ch;
}

/**
 * Prompt overlay.
 */

.jp-OutputArea-promptOverlay {
  position: absolute;
  top: 0;
  width: var(--jp-cell-prompt-width);
  height: 100%;
  opacity: 0.5;
}

.jp-OutputArea-promptOverlay:hover {
  background: var(--jp-layout-color2);
  box-shadow: inset 0 0 1px var(--jp-inverse-layout-color0);
  cursor: zoom-out;
}

.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay:hover {
  cursor: zoom-in;
}

/**
 * Isolated output.
 */
.jp-OutputArea-output.jp-mod-isolated {
  width: 100%;
  display: block;
}

/*
When drag events occur, `lm-mod-override-cursor` is added to the body.
Because iframes steal all cursor events, the following two rules are necessary
to suppress pointer events while resize drags are occurring. There may be a
better solution to this problem.
*/
body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated {
  position: relative;
}

body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated::before {
  content: '';
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: transparent;
}

/* pre */

.jp-OutputArea-output pre {
  border: none;
  margin: 0;
  padding: 0;
  overflow-x: auto;
  overflow-y: auto;
  word-break: break-all;
  word-wrap: break-word;
  white-space: pre-wrap;
}

/* tables */

.jp-OutputArea-output.jp-RenderedHTMLCommon table {
  margin-left: 0;
  margin-right: 0;
}

/* description lists */

.jp-OutputArea-output dl,
.jp-OutputArea-output dt,
.jp-OutputArea-output dd {
  display: block;
}

.jp-OutputArea-output dl {
  width: 100%;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dt {
  font-weight: bold;
  float: left;
  width: 20%;
  padding: 0;
  margin: 0;
}

.jp-OutputArea-output dd {
  float: left;
  width: 80%;
  padding: 0;
  margin: 0;
}

.jp-TrimmedOutputs pre {
  background: var(--jp-layout-color3);
  font-size: calc(var(--jp-code-font-size) * 1.4);
  text-align: center;
  text-transform: uppercase;
}

/* Hide the gutter in case of
 *  - nested output areas (e.g. in the case of output widgets)
 *  - mirrored output areas
 */
.jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt {
  display: none;
}

/* Hide empty lines in the output area, for instance due to cleared widgets */
.jp-OutputArea-prompt:empty {
  padding: 0;
  border: 0;
}

/*-----------------------------------------------------------------------------
| executeResult is added to any Output-result for the display of the object
| returned by a cell
|----------------------------------------------------------------------------*/

.jp-OutputArea-output.jp-OutputArea-executeResult {
  margin-left: 0;
  width: 100%;
}

/* Text output with the Out[] prompt needs a top padding to match the
 * alignment of the Out[] prompt itself.
 */
.jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output {
  padding-top: var(--jp-code-padding);
  border-top: var(--jp-border-width) solid transparent;
}

/*-----------------------------------------------------------------------------
| The Stdin output
|----------------------------------------------------------------------------*/

.jp-Stdin-prompt {
  color: var(--jp-content-font-color0);
  padding-right: var(--jp-code-padding);
  vertical-align: baseline;
  flex: 0 0 auto;
}

.jp-Stdin-input {
  font-family: var(--jp-code-font-family);
  font-size: inherit;
  color: inherit;
  background-color: inherit;
  width: 42%;
  min-width: 200px;

  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;

  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0 0.25em;
  margin: 0 0.25em;
  flex: 0 0 70%;
}

.jp-Stdin-input::placeholder {
  opacity: 0;
}

.jp-Stdin-input:focus {
  box-shadow: none;
}

.jp-Stdin-input:focus::placeholder {
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Output Area View
|----------------------------------------------------------------------------*/

.jp-LinkedOutputView .jp-OutputArea {
  height: 100%;
  display: block;
}

.jp-LinkedOutputView .jp-OutputArea-output:only-child {
  height: 100%;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

@media print {
  .jp-OutputArea-child {
    break-inside: avoid-page;
  }
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-OutputPrompt {
    display: table-row;
    text-align: left;
  }

  .jp-OutputArea-child .jp-OutputArea-output {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }
}

/* Trimmed outputs warning */
.jp-TrimmedOutputs > a {
  margin: 10px;
  text-decoration: none;
  cursor: pointer;
}

.jp-TrimmedOutputs > a:hover {
  text-decoration: none;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Table of Contents
|----------------------------------------------------------------------------*/

:root {
  --jp-private-toc-active-width: 4px;
}

.jp-TableOfContents {
  display: flex;
  flex-direction: column;
  background: var(--jp-layout-color1);
  color: var(--jp-ui-font-color1);
  font-size: var(--jp-ui-font-size1);
  height: 100%;
}

.jp-TableOfContents-placeholder {
  text-align: center;
}

.jp-TableOfContents-placeholderContent {
  color: var(--jp-content-font-color2);
  padding: 8px;
}

.jp-TableOfContents-placeholderContent > h3 {
  margin-bottom: var(--jp-content-heading-margin-bottom);
}

.jp-TableOfContents .jp-SidePanel-content {
  overflow-y: auto;
}

.jp-TableOfContents-tree {
  margin: 4px;
}

.jp-TableOfContents ol {
  list-style-type: none;
}

/* stylelint-disable-next-line selector-max-type */
.jp-TableOfContents li > ol {
  /* Align left border with triangle icon center */
  padding-left: 11px;
}

.jp-TableOfContents-content {
  /* left margin for the active heading indicator */
  margin: 0 0 0 var(--jp-private-toc-active-width);
  padding: 0;
  background-color: var(--jp-layout-color1);
}

.jp-tocItem {
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.jp-tocItem-heading {
  display: flex;
  cursor: pointer;
}

.jp-tocItem-heading:hover {
  background-color: var(--jp-layout-color2);
}

.jp-tocItem-content {
  display: block;
  padding: 4px 0;
  white-space: nowrap;
  text-overflow: ellipsis;
  overflow-x: hidden;
}

.jp-tocItem-collapser {
  height: 20px;
  margin: 2px 2px 0;
  padding: 0;
  background: none;
  border: none;
  cursor: pointer;
}

.jp-tocItem-collapser:hover {
  background-color: var(--jp-layout-color3);
}

/* Active heading indicator */

.jp-tocItem-heading::before {
  content: ' ';
  background: transparent;
  width: var(--jp-private-toc-active-width);
  height: 24px;
  position: absolute;
  left: 0;
  border-radius: var(--jp-border-radius);
}

.jp-tocItem-heading.jp-tocItem-active::before {
  background-color: var(--jp-brand-color1);
}

.jp-tocItem-heading:hover.jp-tocItem-active::before {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

.jp-Collapser {
  flex: 0 0 var(--jp-cell-collapser-width);
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
  border-radius: var(--jp-border-radius);
  opacity: 1;
}

.jp-Collapser-child {
  display: block;
  width: 100%;
  box-sizing: border-box;

  /* height: 100% doesn't work because the height of its parent is computed from content */
  position: absolute;
  top: 0;
  bottom: 0;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Hiding collapsers in print mode.

Note: input and output wrappers have "display: block" propery in print mode.
*/

@media print {
  .jp-Collapser {
    display: none;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Header/Footer
|----------------------------------------------------------------------------*/

/* Hidden by zero height by default */
.jp-CellHeader,
.jp-CellFooter {
  height: 0;
  width: 100%;
  padding: 0;
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Input
|----------------------------------------------------------------------------*/

/* All input areas */
.jp-InputArea {
  display: table;
  table-layout: fixed;
  width: 100%;
  overflow: hidden;
}

.jp-InputArea-editor {
  display: table-cell;
  overflow: hidden;
  vertical-align: top;

  /* This is the non-active, default styling */
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  background: var(--jp-cell-editor-background);
}

.jp-InputPrompt {
  display: table-cell;
  vertical-align: top;
  width: var(--jp-cell-prompt-width);
  color: var(--jp-cell-inprompt-font-color);
  font-family: var(--jp-cell-prompt-font-family);
  padding: var(--jp-code-padding);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  opacity: var(--jp-cell-prompt-opacity);
  line-height: var(--jp-code-line-height);
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;

  /* Right align prompt text, don't wrap to handle large prompt numbers */
  text-align: right;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;

  /* Disable text selection */
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

/*-----------------------------------------------------------------------------
| Mobile
|----------------------------------------------------------------------------*/
@media only screen and (max-width: 760px) {
  .jp-InputArea-editor {
    display: table-row;
    margin-left: var(--jp-notebook-padding);
  }

  .jp-InputPrompt {
    display: table-row;
    text-align: left;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Placeholder {
  display: table;
  table-layout: fixed;
  width: 100%;
}

.jp-Placeholder-prompt {
  display: table-cell;
  box-sizing: border-box;
}

.jp-Placeholder-content {
  display: table-cell;
  padding: 4px 6px;
  border: 1px solid transparent;
  border-radius: 0;
  background: none;
  box-sizing: border-box;
  cursor: pointer;
}

.jp-Placeholder-contentContainer {
  display: flex;
}

.jp-Placeholder-content:hover,
.jp-InputPlaceholder > .jp-Placeholder-content:hover {
  border-color: var(--jp-layout-color3);
}

.jp-Placeholder-content .jp-MoreHorizIcon {
  width: 32px;
  height: 16px;
  border: 1px solid transparent;
  border-radius: var(--jp-border-radius);
}

.jp-Placeholder-content .jp-MoreHorizIcon:hover {
  border: 1px solid var(--jp-border-color1);
  box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.25);
  background-color: var(--jp-layout-color0);
}

.jp-PlaceholderText {
  white-space: nowrap;
  overflow-x: hidden;
  color: var(--jp-inverse-layout-color3);
  font-family: var(--jp-code-font-family);
}

.jp-InputPlaceholder > .jp-Placeholder-content {
  border-color: var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Private CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-private-cell-scrolling-output-offset: 5px;
}

/*-----------------------------------------------------------------------------
| Cell
|----------------------------------------------------------------------------*/

.jp-Cell {
  padding: var(--jp-cell-padding);
  margin: 0;
  border: none;
  outline: none;
  background: transparent;
}

/*-----------------------------------------------------------------------------
| Common input/output
|----------------------------------------------------------------------------*/

.jp-Cell-inputWrapper,
.jp-Cell-outputWrapper {
  display: flex;
  flex-direction: row;
  padding: 0;
  margin: 0;

  /* Added to reveal the box-shadow on the input and output collapsers. */
  overflow: visible;
}

/* Only input/output areas inside cells */
.jp-Cell-inputArea,
.jp-Cell-outputArea {
  flex: 1 1 auto;
}

/*-----------------------------------------------------------------------------
| Collapser
|----------------------------------------------------------------------------*/

/* Make the output collapser disappear when there is not output, but do so
 * in a manner that leaves it in the layout and preserves its width.
 */
.jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser {
  border: none !important;
  background: transparent !important;
}

.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser {
  min-height: var(--jp-cell-collapser-min-height);
}

/*-----------------------------------------------------------------------------
| Output
|----------------------------------------------------------------------------*/

/* Put a space between input and output when there IS output */
.jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper {
  margin-top: 5px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea {
  overflow-y: auto;
  max-height: 24em;
  margin-left: var(--jp-private-cell-scrolling-output-offset);
  resize: vertical;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea[style*='height'] {
  max-height: unset;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea::after {
  content: ' ';
  box-shadow: inset 0 0 6px 2px rgb(0 0 0 / 30%);
  width: 100%;
  height: 100%;
  position: sticky;
  bottom: 0;
  top: 0;
  margin-top: -50%;
  float: left;
  display: block;
  pointer-events: none;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-child {
  padding-top: 6px;
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt {
  width: calc(
    var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset)
  );
}

.jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-promptOverlay {
  left: calc(-1 * var(--jp-private-cell-scrolling-output-offset));
}

/*-----------------------------------------------------------------------------
| CodeCell
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| MarkdownCell
|----------------------------------------------------------------------------*/

.jp-MarkdownOutput {
  display: table-cell;
  width: 100%;
  margin-top: 0;
  margin-bottom: 0;
  padding-left: var(--jp-code-padding);
}

.jp-MarkdownOutput.jp-RenderedHTMLCommon {
  overflow: auto;
}

/* collapseHeadingButton (show always if hiddenCellsButton is _not_ shown) */
.jp-collapseHeadingButton {
  display: flex;
  min-height: var(--jp-cell-collapser-min-height);
  font-size: var(--jp-code-font-size);
  position: absolute;
  background-color: transparent;
  background-size: 25px;
  background-repeat: no-repeat;
  background-position-x: center;
  background-position-y: top;
  background-image: var(--jp-icon-caret-down);
  right: 0;
  top: 0;
  bottom: 0;
}

.jp-collapseHeadingButton.jp-mod-collapsed {
  background-image: var(--jp-icon-caret-right);
}

/*
 set the container font size to match that of content
 so that the nested collapse buttons have the right size
*/
.jp-MarkdownCell .jp-InputPrompt {
  font-size: var(--jp-content-font-size1);
}

/*
  Align collapseHeadingButton with cell top header
  The font sizes are identical to the ones in packages/rendermime/style/base.css
*/
.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='1'] {
  font-size: var(--jp-content-font-size5);
  background-position-y: calc(0.3 * var(--jp-content-font-size5));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='2'] {
  font-size: var(--jp-content-font-size4);
  background-position-y: calc(0.3 * var(--jp-content-font-size4));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='3'] {
  font-size: var(--jp-content-font-size3);
  background-position-y: calc(0.3 * var(--jp-content-font-size3));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='4'] {
  font-size: var(--jp-content-font-size2);
  background-position-y: calc(0.3 * var(--jp-content-font-size2));
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='5'] {
  font-size: var(--jp-content-font-size1);
  background-position-y: top;
}

.jp-mod-rendered .jp-collapseHeadingButton[data-heading-level='6'] {
  font-size: var(--jp-content-font-size0);
  background-position-y: top;
}

/* collapseHeadingButton (show only on (hover,active) if hiddenCellsButton is shown) */
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-collapseHeadingButton {
  display: none;
}

.jp-Notebook.jp-mod-showHiddenCellsButton
  :is(.jp-MarkdownCell:hover, .jp-mod-active)
  .jp-collapseHeadingButton {
  display: flex;
}

/* showHiddenCellsButton (only show if jp-mod-showHiddenCellsButton is set, which
is a consequence of the showHiddenCellsButton option in Notebook Settings)*/
.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton {
  margin-left: calc(var(--jp-cell-prompt-width) + 2 * var(--jp-code-padding));
  margin-top: var(--jp-code-padding);
  border: 1px solid var(--jp-border-color2);
  background-color: var(--jp-border-color3) !important;
  color: var(--jp-content-font-color0) !important;
  display: flex;
}

.jp-Notebook.jp-mod-showHiddenCellsButton .jp-showHiddenCellsButton:hover {
  background-color: var(--jp-border-color2) !important;
}

.jp-showHiddenCellsButton {
  display: none;
}

/*-----------------------------------------------------------------------------
| Printing
|----------------------------------------------------------------------------*/

/*
Using block instead of flex to allow the use of the break-inside CSS property for
cell outputs.
*/

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

:root {
  --jp-notebook-toolbar-padding: 2px 5px 2px 2px;
}

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-NotebookPanel-toolbar {
  padding: var(--jp-notebook-toolbar-padding);

  /* disable paint containment from lumino 2.0 default strict CSS containment */
  contain: style size !important;
}

.jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused {
  border: none;
  box-shadow: none;
}

.jp-Notebook-toolbarCellTypeDropdown select {
  height: 24px;
  font-size: var(--jp-ui-font-size1);
  line-height: 14px;
  border-radius: 0;
  display: block;
}

.jp-Notebook-toolbarCellTypeDropdown span {
  top: 5px !important;
}

.jp-Toolbar-responsive-popup {
  position: absolute;
  height: fit-content;
  display: flex;
  flex-direction: row;
  flex-wrap: wrap;
  justify-content: flex-end;
  border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color);
  box-shadow: var(--jp-toolbar-box-shadow);
  background: var(--jp-toolbar-background);
  min-height: var(--jp-toolbar-micro-height);
  padding: var(--jp-notebook-toolbar-padding);
  z-index: 1;
  right: 0;
  top: 0;
}

.jp-Toolbar > .jp-Toolbar-responsive-opener {
  margin-left: auto;
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Variables
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------

/*-----------------------------------------------------------------------------
| Styles
|----------------------------------------------------------------------------*/

.jp-Notebook-ExecutionIndicator {
  position: relative;
  display: inline-block;
  height: 100%;
  z-index: 9997;
}

.jp-Notebook-ExecutionIndicator-tooltip {
  visibility: hidden;
  height: auto;
  width: max-content;
  width: -moz-max-content;
  background-color: var(--jp-layout-color2);
  color: var(--jp-ui-font-color1);
  text-align: justify;
  border-radius: 6px;
  padding: 0 5px;
  position: fixed;
  display: table;
}

.jp-Notebook-ExecutionIndicator-tooltip.up {
  transform: translateX(-50%) translateY(-100%) translateY(-32px);
}

.jp-Notebook-ExecutionIndicator-tooltip.down {
  transform: translateX(calc(-100% + 16px)) translateY(5px);
}

.jp-Notebook-ExecutionIndicator-tooltip.hidden {
  display: none;
}

.jp-Notebook-ExecutionIndicator:hover .jp-Notebook-ExecutionIndicator-tooltip {
  visibility: visible;
}

.jp-Notebook-ExecutionIndicator span {
  font-size: var(--jp-ui-font-size1);
  font-family: var(--jp-ui-font-family);
  color: var(--jp-ui-font-color1);
  line-height: 24px;
  display: block;
}

.jp-Notebook-ExecutionIndicator-progress-bar {
  display: flex;
  justify-content: center;
  height: 100%;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

/*
 * Execution indicator
 */
.jp-tocItem-content::after {
  content: '';

  /* Must be identical to form a circle */
  width: 12px;
  height: 12px;
  background: none;
  border: none;
  position: absolute;
  right: 0;
}

.jp-tocItem-content[data-running='0']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background: none;
}

.jp-tocItem-content[data-running='1']::after {
  border-radius: 50%;
  border: var(--jp-border-width) solid var(--jp-inverse-layout-color3);
  background-color: var(--jp-inverse-layout-color3);
}

.jp-tocItem-content[data-running='0'],
.jp-tocItem-content[data-running='1'] {
  margin-right: 12px;
}

/*
 * Copyright (c) Jupyter Development Team.
 * Distributed under the terms of the Modified BSD License.
 */

.jp-Notebook-footer {
  height: 27px;
  margin-left: calc(
    var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
      var(--jp-cell-padding)
  );
  width: calc(
    100% -
      (
        var(--jp-cell-prompt-width) + var(--jp-cell-collapser-width) +
          var(--jp-cell-padding) + var(--jp-cell-padding)
      )
  );
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  color: var(--jp-ui-font-color3);
  margin-top: 6px;
  background: none;
  cursor: pointer;
}

.jp-Notebook-footer:focus {
  border-color: var(--jp-cell-editor-active-border-color);
}

/* For devices that support hovering, hide footer until hover */
@media (hover: hover) {
  .jp-Notebook-footer {
    opacity: 0;
  }

  .jp-Notebook-footer:focus,
  .jp-Notebook-footer:hover {
    opacity: 1;
  }
}

/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| Imports
|----------------------------------------------------------------------------*/

/*-----------------------------------------------------------------------------
| CSS variables
|----------------------------------------------------------------------------*/

:root {
  --jp-side-by-side-output-size: 1fr;
  --jp-side-by-side-resized-cell: var(--jp-side-by-side-output-size);
  --jp-private-notebook-dragImage-width: 304px;
  --jp-private-notebook-dragImage-height: 36px;
  --jp-private-notebook-selected-color: var(--md-blue-400);
  --jp-private-notebook-active-color: var(--md-green-400);
}

/*-----------------------------------------------------------------------------
| Notebook
|----------------------------------------------------------------------------*/

/* stylelint-disable selector-max-class */

.jp-NotebookPanel {
  display: block;
  height: 100%;
}

.jp-NotebookPanel.jp-Document {
  min-width: 240px;
  min-height: 120px;
}

.jp-Notebook {
  padding: var(--jp-notebook-padding);
  outline: none;
  overflow: auto;
  background: var(--jp-layout-color0);
}

.jp-Notebook.jp-mod-scrollPastEnd::after {
  display: block;
  content: '';
  min-height: var(--jp-notebook-scroll-padding);
}

.jp-MainAreaWidget-ContainStrict .jp-Notebook * {
  contain: strict;
}

.jp-Notebook .jp-Cell {
  overflow: visible;
}

.jp-Notebook .jp-Cell .jp-InputPrompt {
  cursor: move;
}

/*-----------------------------------------------------------------------------
| Notebook state related styling
|
| The notebook and cells each have states, here are the possibilities:
|
| - Notebook
|   - Command
|   - Edit
| - Cell
|   - None
|   - Active (only one can be active)
|   - Selected (the cells actions are applied to)
|   - Multiselected (when multiple selected, the cursor)
|   - No outputs
|----------------------------------------------------------------------------*/

/* Command or edit modes */

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

.jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt {
  opacity: var(--jp-cell-prompt-not-active-opacity);
  color: var(--jp-cell-prompt-not-active-font-color);
}

/* cell is active */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser {
  background: var(--jp-brand-color1);
}

/* cell is dirty */
.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt {
  color: var(--jp-warn-color1);
}

.jp-Notebook .jp-Cell.jp-mod-dirty .jp-InputPrompt::before {
  color: var(--jp-warn-color1);
  content: '•';
}

.jp-Notebook .jp-Cell.jp-mod-active.jp-mod-dirty .jp-Collapser {
  background: var(--jp-warn-color1);
}

/* collapser is hovered */
.jp-Notebook .jp-Cell .jp-Collapser:hover {
  box-shadow: var(--jp-elevation-z2);
  background: var(--jp-brand-color1);
  opacity: var(--jp-cell-collapser-not-active-hover-opacity);
}

/* cell is active and collapser is hovered */
.jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover {
  background: var(--jp-brand-color0);
  opacity: 1;
}

/* Command mode */

.jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected {
  background: var(--jp-notebook-multiselected-color);
}

.jp-Notebook.jp-mod-commandMode
  .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) {
  background: transparent;
}

/* Edit mode */

.jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor {
  border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);
  box-shadow: var(--jp-input-box-shadow);
  background-color: var(--jp-cell-editor-active-background);
}

/*-----------------------------------------------------------------------------
| Notebook drag and drop
|----------------------------------------------------------------------------*/

.jp-Notebook-cell.jp-mod-dropSource {
  opacity: 0.5;
}

.jp-Notebook-cell.jp-mod-dropTarget,
.jp-Notebook.jp-mod-commandMode
  .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget {
  border-top-color: var(--jp-private-notebook-selected-color);
  border-top-style: solid;
  border-top-width: 2px;
}

.jp-dragImage {
  display: block;
  flex-direction: row;
  width: var(--jp-private-notebook-dragImage-width);
  height: var(--jp-private-notebook-dragImage-height);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background);
  overflow: visible;
}

.jp-dragImage-singlePrompt {
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

.jp-dragImage .jp-dragImage-content {
  flex: 1 1 auto;
  z-index: 2;
  font-size: var(--jp-code-font-size);
  font-family: var(--jp-code-font-family);
  line-height: var(--jp-code-line-height);
  padding: var(--jp-code-padding);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  background: var(--jp-cell-editor-background-color);
  color: var(--jp-content-font-color3);
  text-align: left;
  margin: 4px 4px 4px 0;
}

.jp-dragImage .jp-dragImage-prompt {
  flex: 0 0 auto;
  min-width: 36px;
  color: var(--jp-cell-inprompt-font-color);
  padding: var(--jp-code-padding);
  padding-left: 12px;
  font-family: var(--jp-cell-prompt-font-family);
  letter-spacing: var(--jp-cell-prompt-letter-spacing);
  line-height: 1.9;
  font-size: var(--jp-code-font-size);
  border: var(--jp-border-width) solid transparent;
}

.jp-dragImage-multipleBack {
  z-index: -1;
  position: absolute;
  height: 32px;
  width: 300px;
  top: 8px;
  left: 8px;
  background: var(--jp-layout-color2);
  border: var(--jp-border-width) solid var(--jp-input-border-color);
  box-shadow: 2px 2px 4px 0 rgba(0, 0, 0, 0.12);
}

/*-----------------------------------------------------------------------------
| Cell toolbar
|----------------------------------------------------------------------------*/

.jp-NotebookTools {
  display: block;
  min-width: var(--jp-sidebar-min-width);
  color: var(--jp-ui-font-color1);
  background: var(--jp-layout-color1);

  /* This is needed so that all font sizing of children done in ems is
    * relative to this base size */
  font-size: var(--jp-ui-font-size1);
  overflow: auto;
}

.jp-ActiveCellTool {
  padding: 12px 0;
  display: flex;
}

.jp-ActiveCellTool-Content {
  flex: 1 1 auto;
}

.jp-ActiveCellTool .jp-ActiveCellTool-CellContent {
  background: var(--jp-cell-editor-background);
  border: var(--jp-border-width) solid var(--jp-cell-editor-border-color);
  border-radius: 0;
  min-height: 29px;
}

.jp-ActiveCellTool .jp-InputPrompt {
  min-width: calc(var(--jp-cell-prompt-width) * 0.75);
}

.jp-ActiveCellTool-CellContent > pre {
  padding: 5px 4px;
  margin: 0;
  white-space: normal;
}

.jp-MetadataEditorTool {
  flex-direction: column;
  padding: 12px 0;
}

.jp-RankedPanel > :not(:first-child) {
  margin-top: 12px;
}

.jp-KeySelector select.jp-mod-styled {
  font-size: var(--jp-ui-font-size1);
  color: var(--jp-ui-font-color0);
  border: var(--jp-border-width) solid var(--jp-border-color1);
}

.jp-KeySelector label,
.jp-MetadataEditorTool label,
.jp-NumberSetter label {
  line-height: 1.4;
}

.jp-NotebookTools .jp-select-wrapper {
  margin-top: 4px;
  margin-bottom: 0;
}

.jp-NumberSetter input {
  width: 100%;
  margin-top: 4px;
}

.jp-NotebookTools .jp-Collapse {
  margin-top: 16px;
}

/*-----------------------------------------------------------------------------
| Presentation Mode (.jp-mod-presentationMode)
|----------------------------------------------------------------------------*/

.jp-mod-presentationMode .jp-Notebook {
  --jp-content-font-size1: var(--jp-content-presentation-font-size1);
  --jp-code-font-size: var(--jp-code-presentation-font-size);
}

.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,
.jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt {
  flex: 0 0 110px;
}

/*-----------------------------------------------------------------------------
| Side-by-side Mode (.jp-mod-sideBySide)
|----------------------------------------------------------------------------*/
.jp-mod-sideBySide.jp-Notebook .jp-Notebook-cell {
  margin-top: 3em;
  margin-bottom: 3em;
  margin-left: 5%;
  margin-right: 5%;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell {
  display: grid;
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-output-size)
    );
  grid-template-rows: auto minmax(0, 1fr) auto;
  grid-template-areas:
    'header header header'
    'input handle output'
    'footer footer footer';
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell.jp-mod-resizedCell {
  grid-template-columns: minmax(0, 1fr) min-content minmax(
      0,
      var(--jp-side-by-side-resized-cell)
    );
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellHeader {
  grid-area: header;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-inputWrapper {
  grid-area: input;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-Cell-outputWrapper {
  /* overwrite the default margin (no vertical separation needed in side by side move */
  margin-top: 0;
  grid-area: output;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellFooter {
  grid-area: footer;
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle {
  grid-area: handle;
  user-select: none;
  display: block;
  height: 100%;
  cursor: ew-resize;
  padding: 0 var(--jp-cell-padding);
}

.jp-mod-sideBySide.jp-Notebook .jp-CodeCell .jp-CellResizeHandle::after {
  content: '';
  display: block;
  background: var(--jp-border-color2);
  height: 100%;
  width: 5px;
}

.jp-mod-sideBySide.jp-Notebook
  .jp-CodeCell.jp-mod-resizedCell
  .jp-CellResizeHandle::after {
  background: var(--jp-border-color0);
}

.jp-CellResizeHandle {
  display: none;
}

/*-----------------------------------------------------------------------------
| Placeholder
|----------------------------------------------------------------------------*/

.jp-Cell-Placeholder {
  padding-left: 55px;
}

.jp-Cell-Placeholder-wrapper {
  background: #fff;
  border: 1px solid;
  border-color: #e5e6e9 #dfe0e4 #d0d1d5;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  margin: 10px 15px;
}

.jp-Cell-Placeholder-wrapper-inner {
  padding: 15px;
  position: relative;
}

.jp-Cell-Placeholder-wrapper-body {
  background-repeat: repeat;
  background-size: 50% auto;
}

.jp-Cell-Placeholder-wrapper-body div {
  background: #f6f7f8;
  background-image: -webkit-linear-gradient(
    left,
    #f6f7f8 0%,
    #edeef1 20%,
    #f6f7f8 40%,
    #f6f7f8 100%
  );
  background-repeat: no-repeat;
  background-size: 800px 104px;
  height: 104px;
  position: absolute;
  right: 15px;
  left: 15px;
  top: 15px;
}

div.jp-Cell-Placeholder-h1 {
  top: 20px;
  height: 20px;
  left: 15px;
  width: 150px;
}

div.jp-Cell-Placeholder-h2 {
  left: 15px;
  top: 50px;
  height: 10px;
  width: 100px;
}

div.jp-Cell-Placeholder-content-1,
div.jp-Cell-Placeholder-content-2,
div.jp-Cell-Placeholder-content-3 {
  left: 15px;
  right: 15px;
  height: 10px;
}

div.jp-Cell-Placeholder-content-1 {
  top: 100px;
}

div.jp-Cell-Placeholder-content-2 {
  top: 120px;
}

div.jp-Cell-Placeholder-content-3 {
  top: 140px;
}

</style>
<style type="text/css">
/*-----------------------------------------------------------------------------
| Copyright (c) Jupyter Development Team.
| Distributed under the terms of the Modified BSD License.
|----------------------------------------------------------------------------*/

/*
The following CSS variables define the main, public API for styling JupyterLab.
These variables should be used by all plugins wherever possible. In other
words, plugins should not define custom colors, sizes, etc unless absolutely
necessary. This enables users to change the visual theme of JupyterLab
by changing these variables.

Many variables appear in an ordered sequence (0,1,2,3). These sequences
are designed to work well together, so for example, `--jp-border-color1` should
be used with `--jp-layout-color1`. The numbers have the following meanings:

* 0: super-primary, reserved for special emphasis
* 1: primary, most important under normal situations
* 2: secondary, next most important under normal situations
* 3: tertiary, next most important under normal situations

Throughout JupyterLab, we are mostly following principles from Google's
Material Design when selecting colors. We are not, however, following
all of MD as it is not optimized for dense, information rich UIs.
*/

:root {
  /* Elevation
   *
   * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here:
   *
   * https://github.com/material-components/material-components-web
   * https://material-components-web.appspot.com/elevation.html
   */

  --jp-shadow-base-lightness: 0;
  --jp-shadow-umbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.2
  );
  --jp-shadow-penumbra-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.14
  );
  --jp-shadow-ambient-color: rgba(
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    var(--jp-shadow-base-lightness),
    0.12
  );
  --jp-elevation-z0: none;
  --jp-elevation-z1: 0 2px 1px -1px var(--jp-shadow-umbra-color),
    0 1px 1px 0 var(--jp-shadow-penumbra-color),
    0 1px 3px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z2: 0 3px 1px -2px var(--jp-shadow-umbra-color),
    0 2px 2px 0 var(--jp-shadow-penumbra-color),
    0 1px 5px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z4: 0 2px 4px -1px var(--jp-shadow-umbra-color),
    0 4px 5px 0 var(--jp-shadow-penumbra-color),
    0 1px 10px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z6: 0 3px 5px -1px var(--jp-shadow-umbra-color),
    0 6px 10px 0 var(--jp-shadow-penumbra-color),
    0 1px 18px 0 var(--jp-shadow-ambient-color);
  --jp-elevation-z8: 0 5px 5px -3px var(--jp-shadow-umbra-color),
    0 8px 10px 1px var(--jp-shadow-penumbra-color),
    0 3px 14px 2px var(--jp-shadow-ambient-color);
  --jp-elevation-z12: 0 7px 8px -4px var(--jp-shadow-umbra-color),
    0 12px 17px 2px var(--jp-shadow-penumbra-color),
    0 5px 22px 4px var(--jp-shadow-ambient-color);
  --jp-elevation-z16: 0 8px 10px -5px var(--jp-shadow-umbra-color),
    0 16px 24px 2px var(--jp-shadow-penumbra-color),
    0 6px 30px 5px var(--jp-shadow-ambient-color);
  --jp-elevation-z20: 0 10px 13px -6px var(--jp-shadow-umbra-color),
    0 20px 31px 3px var(--jp-shadow-penumbra-color),
    0 8px 38px 7px var(--jp-shadow-ambient-color);
  --jp-elevation-z24: 0 11px 15px -7px var(--jp-shadow-umbra-color),
    0 24px 38px 3px var(--jp-shadow-penumbra-color),
    0 9px 46px 8px var(--jp-shadow-ambient-color);

  /* Borders
   *
   * The following variables, specify the visual styling of borders in JupyterLab.
   */

  --jp-border-width: 1px;
  --jp-border-color0: var(--md-grey-400);
  --jp-border-color1: var(--md-grey-400);
  --jp-border-color2: var(--md-grey-300);
  --jp-border-color3: var(--md-grey-200);
  --jp-inverse-border-color: var(--md-grey-600);
  --jp-border-radius: 2px;

  /* UI Fonts
   *
   * The UI font CSS variables are used for the typography all of the JupyterLab
   * user interface elements that are not directly user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-ui-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-ui-font-scale-factor: 1.2;
  --jp-ui-font-size0: 0.83333em;
  --jp-ui-font-size1: 13px; /* Base font size */
  --jp-ui-font-size2: 1.2em;
  --jp-ui-font-size3: 1.44em;
  --jp-ui-font-family: system-ui, -apple-system, blinkmacsystemfont, 'Segoe UI',
    helvetica, arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji',
    'Segoe UI Symbol';

  /*
   * Use these font colors against the corresponding main layout colors.
   * In a light theme, these go from dark to light.
   */

  /* Defaults use Material Design specification */
  --jp-ui-font-color0: rgba(0, 0, 0, 1);
  --jp-ui-font-color1: rgba(0, 0, 0, 0.87);
  --jp-ui-font-color2: rgba(0, 0, 0, 0.54);
  --jp-ui-font-color3: rgba(0, 0, 0, 0.38);

  /*
   * Use these against the brand/accent/warn/error colors.
   * These will typically go from light to darker, in both a dark and light theme.
   */

  --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1);
  --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7);
  --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5);

  /* Content Fonts
   *
   * Content font variables are used for typography of user generated content.
   *
   * The font sizing here is done assuming that the body font size of --jp-content-font-size1
   * is applied to a parent element. When children elements, such as headings, are sized
   * in em all things will be computed relative to that body size.
   */

  --jp-content-line-height: 1.6;
  --jp-content-font-scale-factor: 1.2;
  --jp-content-font-size0: 0.83333em;
  --jp-content-font-size1: 14px; /* Base font size */
  --jp-content-font-size2: 1.2em;
  --jp-content-font-size3: 1.44em;
  --jp-content-font-size4: 1.728em;
  --jp-content-font-size5: 2.0736em;

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-content-presentation-font-size1: 17px;
  --jp-content-heading-line-height: 1;
  --jp-content-heading-margin-top: 1.2em;
  --jp-content-heading-margin-bottom: 0.8em;
  --jp-content-heading-font-weight: 500;

  /* Defaults use Material Design specification */
  --jp-content-font-color0: rgba(0, 0, 0, 1);
  --jp-content-font-color1: rgba(0, 0, 0, 0.87);
  --jp-content-font-color2: rgba(0, 0, 0, 0.54);
  --jp-content-font-color3: rgba(0, 0, 0, 0.38);
  --jp-content-link-color: var(--md-blue-900);
  --jp-content-font-family: system-ui, -apple-system, blinkmacsystemfont,
    'Segoe UI', helvetica, arial, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol';

  /*
   * Code Fonts
   *
   * Code font variables are used for typography of code and other monospaces content.
   */

  --jp-code-font-size: 13px;
  --jp-code-line-height: 1.3077; /* 17px for 13px base */
  --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */
  --jp-code-font-family-default: menlo, consolas, 'DejaVu Sans Mono', monospace;
  --jp-code-font-family: var(--jp-code-font-family-default);

  /* This gives a magnification of about 125% in presentation mode over normal. */
  --jp-code-presentation-font-size: 16px;

  /* may need to tweak cursor width if you change font size */
  --jp-code-cursor-width0: 1.4px;
  --jp-code-cursor-width1: 2px;
  --jp-code-cursor-width2: 4px;

  /* Layout
   *
   * The following are the main layout colors use in JupyterLab. In a light
   * theme these would go from light to dark.
   */

  --jp-layout-color0: white;
  --jp-layout-color1: white;
  --jp-layout-color2: var(--md-grey-200);
  --jp-layout-color3: var(--md-grey-400);
  --jp-layout-color4: var(--md-grey-600);

  /* Inverse Layout
   *
   * The following are the inverse layout colors use in JupyterLab. In a light
   * theme these would go from dark to light.
   */

  --jp-inverse-layout-color0: #111;
  --jp-inverse-layout-color1: var(--md-grey-900);
  --jp-inverse-layout-color2: var(--md-grey-800);
  --jp-inverse-layout-color3: var(--md-grey-700);
  --jp-inverse-layout-color4: var(--md-grey-600);

  /* Brand/accent */

  --jp-brand-color0: var(--md-blue-900);
  --jp-brand-color1: var(--md-blue-700);
  --jp-brand-color2: var(--md-blue-300);
  --jp-brand-color3: var(--md-blue-100);
  --jp-brand-color4: var(--md-blue-50);
  --jp-accent-color0: var(--md-green-900);
  --jp-accent-color1: var(--md-green-700);
  --jp-accent-color2: var(--md-green-300);
  --jp-accent-color3: var(--md-green-100);

  /* State colors (warn, error, success, info) */

  --jp-warn-color0: var(--md-orange-900);
  --jp-warn-color1: var(--md-orange-700);
  --jp-warn-color2: var(--md-orange-300);
  --jp-warn-color3: var(--md-orange-100);
  --jp-error-color0: var(--md-red-900);
  --jp-error-color1: var(--md-red-700);
  --jp-error-color2: var(--md-red-300);
  --jp-error-color3: var(--md-red-100);
  --jp-success-color0: var(--md-green-900);
  --jp-success-color1: var(--md-green-700);
  --jp-success-color2: var(--md-green-300);
  --jp-success-color3: var(--md-green-100);
  --jp-info-color0: var(--md-cyan-900);
  --jp-info-color1: var(--md-cyan-700);
  --jp-info-color2: var(--md-cyan-300);
  --jp-info-color3: var(--md-cyan-100);

  /* Cell specific styles */

  --jp-cell-padding: 5px;
  --jp-cell-collapser-width: 8px;
  --jp-cell-collapser-min-height: 20px;
  --jp-cell-collapser-not-active-hover-opacity: 0.6;
  --jp-cell-editor-background: var(--md-grey-100);
  --jp-cell-editor-border-color: var(--md-grey-300);
  --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-cell-editor-active-background: var(--jp-layout-color0);
  --jp-cell-editor-active-border-color: var(--jp-brand-color1);
  --jp-cell-prompt-width: 64px;
  --jp-cell-prompt-font-family: var(--jp-code-font-family-default);
  --jp-cell-prompt-letter-spacing: 0;
  --jp-cell-prompt-opacity: 1;
  --jp-cell-prompt-not-active-opacity: 0.5;
  --jp-cell-prompt-not-active-font-color: var(--md-grey-700);

  /* A custom blend of MD grey and blue 600
   * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */
  --jp-cell-inprompt-font-color: #307fc1;

  /* A custom blend of MD grey and orange 600
   * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */
  --jp-cell-outprompt-font-color: #bf5b3d;

  /* Notebook specific styles */

  --jp-notebook-padding: 10px;
  --jp-notebook-select-background: var(--jp-layout-color1);
  --jp-notebook-multiselected-color: var(--md-blue-50);

  /* The scroll padding is calculated to fill enough space at the bottom of the
  notebook to show one single-line cell (with appropriate padding) at the top
  when the notebook is scrolled all the way to the bottom. We also subtract one
  pixel so that no scrollbar appears if we have just one single-line cell in the
  notebook. This padding is to enable a 'scroll past end' feature in a notebook.
  */
  --jp-notebook-scroll-padding: calc(
    100% - var(--jp-code-font-size) * var(--jp-code-line-height) -
      var(--jp-code-padding) - var(--jp-cell-padding) - 1px
  );

  /* Rendermime styles */

  --jp-rendermime-error-background: #fdd;
  --jp-rendermime-table-row-background: var(--md-grey-100);
  --jp-rendermime-table-row-hover-background: var(--md-light-blue-50);

  /* Dialog specific styles */

  --jp-dialog-background: rgba(0, 0, 0, 0.25);

  /* Console specific styles */

  --jp-console-padding: 10px;

  /* Toolbar specific styles */

  --jp-toolbar-border-color: var(--jp-border-color1);
  --jp-toolbar-micro-height: 8px;
  --jp-toolbar-background: var(--jp-layout-color1);
  --jp-toolbar-box-shadow: 0 0 2px 0 rgba(0, 0, 0, 0.24);
  --jp-toolbar-header-margin: 4px 4px 0 4px;
  --jp-toolbar-active-background: var(--md-grey-300);

  /* Statusbar specific styles */

  --jp-statusbar-height: 24px;

  /* Input field styles */

  --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300);
  --jp-input-active-background: var(--jp-layout-color1);
  --jp-input-hover-background: var(--jp-layout-color1);
  --jp-input-background: var(--md-grey-100);
  --jp-input-border-color: var(--jp-inverse-border-color);
  --jp-input-active-border-color: var(--jp-brand-color1);
  --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3);

  /* General editor styles */

  --jp-editor-selected-background: #d9d9d9;
  --jp-editor-selected-focused-background: #d7d4f0;
  --jp-editor-cursor-color: var(--jp-ui-font-color0);

  /* Code mirror specific styles */

  --jp-mirror-editor-keyword-color: #008000;
  --jp-mirror-editor-atom-color: #88f;
  --jp-mirror-editor-number-color: #080;
  --jp-mirror-editor-def-color: #00f;
  --jp-mirror-editor-variable-color: var(--md-grey-900);
  --jp-mirror-editor-variable-2-color: rgb(0, 54, 109);
  --jp-mirror-editor-variable-3-color: #085;
  --jp-mirror-editor-punctuation-color: #05a;
  --jp-mirror-editor-property-color: #05a;
  --jp-mirror-editor-operator-color: #a2f;
  --jp-mirror-editor-comment-color: #408080;
  --jp-mirror-editor-string-color: #ba2121;
  --jp-mirror-editor-string-2-color: #708;
  --jp-mirror-editor-meta-color: #a2f;
  --jp-mirror-editor-qualifier-color: #555;
  --jp-mirror-editor-builtin-color: #008000;
  --jp-mirror-editor-bracket-color: #997;
  --jp-mirror-editor-tag-color: #170;
  --jp-mirror-editor-attribute-color: #00c;
  --jp-mirror-editor-header-color: blue;
  --jp-mirror-editor-quote-color: #090;
  --jp-mirror-editor-link-color: #00c;
  --jp-mirror-editor-error-color: #f00;
  --jp-mirror-editor-hr-color: #999;

  /*
    RTC user specific colors.
    These colors are used for the cursor, username in the editor,
    and the icon of the user.
  */

  --jp-collaborator-color1: #ffad8e;
  --jp-collaborator-color2: #dac83d;
  --jp-collaborator-color3: #72dd76;
  --jp-collaborator-color4: #00e4d0;
  --jp-collaborator-color5: #45d4ff;
  --jp-collaborator-color6: #e2b1ff;
  --jp-collaborator-color7: #ff9de6;

  /* Vega extension styles */

  --jp-vega-background: white;

  /* Sidebar-related styles */

  --jp-sidebar-min-width: 250px;

  /* Search-related styles */

  --jp-search-toggle-off-opacity: 0.5;
  --jp-search-toggle-hover-opacity: 0.8;
  --jp-search-toggle-on-opacity: 1;
  --jp-search-selected-match-background-color: rgb(245, 200, 0);
  --jp-search-selected-match-color: black;
  --jp-search-unselected-match-background-color: var(
    --jp-inverse-layout-color0
  );
  --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0);

  /* Icon colors that work well with light or dark backgrounds */
  --jp-icon-contrast-color0: var(--md-purple-600);
  --jp-icon-contrast-color1: var(--md-green-600);
  --jp-icon-contrast-color2: var(--md-pink-600);
  --jp-icon-contrast-color3: var(--md-blue-600);

  /* Button colors */
  --jp-accept-color-normal: var(--md-blue-700);
  --jp-accept-color-hover: var(--md-blue-800);
  --jp-accept-color-active: var(--md-blue-900);
  --jp-warn-color-normal: var(--md-red-700);
  --jp-warn-color-hover: var(--md-red-800);
  --jp-warn-color-active: var(--md-red-900);
  --jp-reject-color-normal: var(--md-grey-600);
  --jp-reject-color-hover: var(--md-grey-700);
  --jp-reject-color-active: var(--md-grey-800);

  /* File or activity icons and switch semantic variables */
  --jp-jupyter-icon-color: #f37626;
  --jp-notebook-icon-color: #f37626;
  --jp-json-icon-color: var(--md-orange-700);
  --jp-console-icon-background-color: var(--md-blue-700);
  --jp-console-icon-color: white;
  --jp-terminal-icon-background-color: var(--md-grey-800);
  --jp-terminal-icon-color: var(--md-grey-200);
  --jp-text-editor-icon-color: var(--md-grey-700);
  --jp-inspector-icon-color: var(--md-grey-700);
  --jp-switch-color: var(--md-grey-400);
  --jp-switch-true-position-color: var(--md-orange-900);
}
</style>
<style type="text/css">
/* Force rendering true colors when outputing to pdf */
* {
  -webkit-print-color-adjust: exact;
}

/* Misc */
a.anchor-link {
  display: none;
}

/* Input area styling */
.jp-InputArea {
  overflow: hidden;
}

.jp-InputArea-editor {
  overflow: hidden;
}

.cm-editor.cm-s-jupyter .highlight pre {
/* weird, but --jp-code-padding defined to be 5px but 4px horizontal padding is hardcoded for pre.cm-line */
  padding: var(--jp-code-padding) 4px;
  margin: 0;

  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
  color: inherit;

}

.jp-OutputArea-output pre {
  line-height: inherit;
  font-family: inherit;
}

.jp-RenderedText pre {
  color: var(--jp-content-font-color1);
  font-size: var(--jp-code-font-size);
}

/* Hiding the collapser by default */
.jp-Collapser {
  display: none;
}

@page {
    margin: 0.5in; /* Margin for each printed piece of paper */
}

@media print {
  .jp-Cell-inputWrapper,
  .jp-Cell-outputWrapper {
    display: block;
  }
}
</style>
<!-- Load mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"> </script>
<!-- MathJax configuration -->
<script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
<!-- End of mathjax configuration --><script type="module">
  document.addEventListener("DOMContentLoaded", async () => {
    const diagrams = document.querySelectorAll(".jp-Mermaid > pre.mermaid");
    // do not load mermaidjs if not needed
    if (!diagrams.length) {
      return;
    }
    const mermaid = (await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.0/mermaid.esm.min.mjs")).default;
    const parser = new DOMParser();

    mermaid.initialize({
      maxTextSize: 100000,
      startOnLoad: false,
      fontFamily: window
        .getComputedStyle(document.body)
        .getPropertyValue("--jp-ui-font-family"),
      theme: document.querySelector("body[data-jp-theme-light='true']")
        ? "default"
        : "dark",
    });

    let _nextMermaidId = 0;

    function makeMermaidImage(svg) {
      const img = document.createElement("img");
      const doc = parser.parseFromString(svg, "image/svg+xml");
      const svgEl = doc.querySelector("svg");
      const { maxWidth } = svgEl?.style || {};
      const firstTitle = doc.querySelector("title");
      const firstDesc = doc.querySelector("desc");

      img.setAttribute("src", `data:image/svg+xml,${encodeURIComponent(svg)}`);
      if (maxWidth) {
        img.width = parseInt(maxWidth);
      }
      if (firstTitle) {
        img.setAttribute("alt", firstTitle.textContent);
      }
      if (firstDesc) {
        const caption = document.createElement("figcaption");
        caption.className = "sr-only";
        caption.textContent = firstDesc.textContent;
        return [img, caption];
      }
      return [img];
    }

    async function makeMermaidError(text) {
      let errorMessage = "";
      try {
        await mermaid.parse(text);
      } catch (err) {
        errorMessage = `${err}`;
      }

      const result = document.createElement("details");
      result.className = 'jp-RenderedMermaid-Details';
      const summary = document.createElement("summary");
      summary.className = 'jp-RenderedMermaid-Summary';
      const pre = document.createElement("pre");
      const code = document.createElement("code");
      code.innerText = text;
      pre.appendChild(code);
      summary.appendChild(pre);
      result.appendChild(summary);

      const warning = document.createElement("pre");
      warning.innerText = errorMessage;
      result.appendChild(warning);
      return [result];
    }

    async function renderOneMarmaid(src) {
      const id = `jp-mermaid-${_nextMermaidId++}`;
      const parent = src.parentNode;
      let raw = src.textContent.trim();
      const el = document.createElement("div");
      el.style.visibility = "hidden";
      document.body.appendChild(el);
      let results = null;
      let output = null;
      try {
        const { svg } = await mermaid.render(id, raw, el);
        results = makeMermaidImage(svg);
        output = document.createElement("figure");
        results.map(output.appendChild, output);
      } catch (err) {
        parent.classList.add("jp-mod-warning");
        results = await makeMermaidError(raw);
        output = results[0];
      } finally {
        el.remove();
      }
      parent.classList.add("jp-RenderedMermaid");
      parent.appendChild(output);
    }

    void Promise.all([...diagrams].map(renderOneMarmaid));
  });
</script>
<style>
  .jp-Mermaid:not(.jp-RenderedMermaid) {
    display: none;
  }

  .jp-RenderedMermaid {
    overflow: auto;
    display: flex;
  }

  .jp-RenderedMermaid.jp-mod-warning {
    width: auto;
    padding: 0.5em;
    margin-top: 0.5em;
    border: var(--jp-border-width) solid var(--jp-warn-color2);
    border-radius: var(--jp-border-radius);
    color: var(--jp-ui-font-color1);
    font-size: var(--jp-ui-font-size1);
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  .jp-RenderedMermaid figure {
    margin: 0;
    overflow: auto;
    max-width: 100%;
  }

  .jp-RenderedMermaid img {
    max-width: 100%;
  }

  .jp-RenderedMermaid-Details > pre {
    margin-top: 1em;
  }

  .jp-RenderedMermaid-Summary {
    color: var(--jp-warn-color2);
  }

  .jp-RenderedMermaid:not(.jp-mod-warning) pre {
    display: none;
  }

  .jp-RenderedMermaid-Summary > pre {
    display: inline-block;
    white-space: normal;
  }
</style>
<!-- End of mermaid configuration --></head>
<body class="jp-Notebook" data-jp-theme-light="true" data-jp-theme-name="JupyterLab Light">
<main><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=6b5599b2-801f-4761-a8f7-f920da044951">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [1]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">ResNet for UDA</span>
<span class="sd">"""</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">funcs</span>
<span class="kn">import</span> <span class="nn">jan</span>
<span class="kn">import</span> <span class="nn">coral</span>
<span class="kn">import</span> <span class="nn">star</span>
<span class="kn">import</span> <span class="nn">mcd</span>
<span class="kn">import</span> <span class="nn">dann</span>
<span class="kn">import</span> <span class="nn">base</span>
<span class="kn">import</span> <span class="nn">plots</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span>

<span class="c1">#%% ResNet block</span>
<span class="k">class</span> <span class="nc">ResidualBlock1D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        A basic residual block for 1D convolutions.</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">identity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">+=</span> <span class="n">identity</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1">#%% Base</span>
<span class="k">class</span> <span class="nc">DeepResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    A deep ResNet classifier for 1D signals.</span>
<span class="sd">    </span>
<span class="sd">    This network consists of:</span>
<span class="sd">      - An initial convolution + BN + ReLU + maxpool block.</span>
<span class="sd">      - Four residual layers (with increasing feature channels).</span>
<span class="sd">      - Global average pooling to obtain a fixed–length feature vector.</span>
<span class="sd">      - A bottleneck fully connected layer mapping to 512–dimensions.</span>
<span class="sd">      - A small classifier head (MLP) mapping to the desired output classes.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DeepResNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to obtain a fixed-length feature vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck fully-connected layer: maps 512-dim to 512-dim features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
        <span class="c1"># Classifier head (MLP)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Creates a sequential layer composed of multiple residual blocks.</span>
<span class="sd">        If the stride is not 1 or the number of channels change,</span>
<span class="sd">        a downsampling layer is used.</span>
<span class="sd">        """</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch_size, 2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 64, L/4)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 64, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 128, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 256, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 512, ?)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># (B, 512)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 512)</span>
        
        <span class="c1"># Classification head</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1">#%% DANN</span>
<span class="k">class</span> <span class="nc">GradReverse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">lambda_</span> <span class="o">=</span> <span class="n">lambda_</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">neg</span><span class="p">()</span> <span class="o">*</span> <span class="n">ctx</span><span class="o">.</span><span class="n">lambda_</span><span class="p">,</span> <span class="kc">None</span>

<span class="k">def</span> <span class="nf">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">GradReverse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DANN_F</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Deep ResNet feature extractor for DANN.</span>
<span class="sd">    </span>
<span class="sd">    This network accepts a 2–channel 1D signal and outputs a 512–dimensional feature vector.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DANN_F</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to produce a fixed-length vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck fully-connected layer (optional, here keeping feature dim at 512)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Create a sequential layer composed of multiple residual blocks.</span>
<span class="sd">        """</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Input x: (batch_size, channels=2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># -&gt; (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># -&gt; (B, 64, L/4)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># -&gt; (B, 64, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># -&gt; (B, 128, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># -&gt; (B, 256, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># -&gt; (B, 512, ?)</span>
        
        <span class="c1"># Global average pooling</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># -&gt; (B, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># -&gt; (B, 512)</span>
        <span class="c1"># Bottleneck transformation</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 512)</span>
        <span class="k">return</span> <span class="n">features</span>

<span class="k">class</span> <span class="nc">DANN_LP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Label predictor network that maps 512–dim features to the desired output classes.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DANN_LP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">DANN_DC</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Domain classifier network for DANN.</span>
<span class="sd">    </span>
<span class="sd">    This network applies a gradient reversal layer (using ReverseLayerF) to the feature vector</span>
<span class="sd">    before classifying it as either source or target.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DANN_DC</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="c1"># Reverse gradient during the backward pass</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">GradReverse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1">#%% CORAL</span>
<span class="k">def</span> <span class="nf">coral_loss</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Computes the CORAL loss between two matrices.</span>
<span class="sd">    Assumes input tensors are of shape (batch_size, feature_dim).</span>
<span class="sd">    """</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">source</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ns</span> <span class="o">=</span> <span class="n">source</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">nt</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Center the features</span>
    <span class="n">source_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">target_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">source_centered</span> <span class="o">=</span> <span class="n">source</span> <span class="o">-</span> <span class="n">source_mean</span>
    <span class="n">target_centered</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">target_mean</span>
    <span class="c1"># Compute covariance matrices</span>
    <span class="n">cov_source</span> <span class="o">=</span> <span class="p">(</span><span class="n">source_centered</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="o">@</span> <span class="n">source_centered</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">ns</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">cov_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">target_centered</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="o">@</span> <span class="n">target_centered</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">nt</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Frobenius norm between covariance matrices (scaled)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">cov_source</span> <span class="o">-</span> <span class="n">cov_target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">d</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
        
<span class="k">class</span> <span class="nc">CORAL_G</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Deep ResNet generator that extracts features at multiple depths for CORAL.</span>
<span class="sd">    </span>
<span class="sd">    Architecture:</span>
<span class="sd">      - An initial convolution + batchnorm + ReLU + maxpool.</span>
<span class="sd">      - Four layers (residual blocks) built with 1D convolutions.</span>
<span class="sd">      - Intermediate features are extracted after layer1 (early), layer2 (middle),</span>
<span class="sd">        and layer3 (late) via global average pooling.</span>
<span class="sd">      - The final layer (layer4) is pooled and passed through a fully-connected</span>
<span class="sd">        bottleneck to obtain a 512–dim feature for classification.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CORAL_G</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolution layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling (adaptive to any sequence length)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck fully-connected layer mapping 512-&gt;512 for classification.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Creates a sequential layer composed of multiple residual blocks.</span>
<span class="sd">        If the stride is not 1 or the channel dimensions differ,</span>
<span class="sd">        a downsampling layer is used.</span>
<span class="sd">        """</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Input x: (batch_size, channels=2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># (B, 64, L/4)</span>
        
        <span class="c1"># Layer 1: Early features</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>           <span class="c1"># (B, 64, L1)</span>
        <span class="n">early</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>      <span class="c1"># (B, 64, 1)</span>
        <span class="n">early</span> <span class="o">=</span> <span class="n">early</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>     <span class="c1"># (B, 64)</span>
        
        <span class="c1"># Layer 2: Middle features</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>          <span class="c1"># (B, 128, L2)</span>
        <span class="n">middle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>     <span class="c1"># (B, 128, 1)</span>
        <span class="n">middle</span> <span class="o">=</span> <span class="n">middle</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># (B, 128)</span>
        
        <span class="c1"># Layer 3: Late features for CORAL loss</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>          <span class="c1"># (B, 256, L3)</span>
        <span class="n">late</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>       <span class="c1"># (B, 256, 1)</span>
        <span class="n">late</span> <span class="o">=</span> <span class="n">late</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>       <span class="c1"># (B, 256)</span>
        
        <span class="c1"># Layer 4: Final block for classification</span>
        <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>          <span class="c1"># (B, 512, L4)</span>
        <span class="n">pooled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>     <span class="c1"># (B, 512, 1)</span>
        <span class="n">pooled</span> <span class="o">=</span> <span class="n">pooled</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># (B, 512)</span>
        <span class="n">final</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">pooled</span><span class="p">)</span>  <span class="c1"># (B, 512)</span>
        
        <span class="c1"># Return multi-level features for deep CORAL</span>
        <span class="k">return</span> <span class="n">early</span><span class="p">,</span> <span class="n">middle</span><span class="p">,</span> <span class="n">late</span><span class="p">,</span> <span class="n">final</span>

<span class="k">class</span> <span class="nc">CORAL_C</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Classifier network that maps the 512–dim bottleneck features to the output classes.</span>
<span class="sd">    </span>
<span class="sd">    Architecture:</span>
<span class="sd">      - A fully-connected layer (512 -&gt; 256) with batch normalization, ReLU, and dropout.</span>
<span class="sd">      - A final fully-connected layer mapping 256 -&gt; output_dim.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CORAL_C</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn_fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">DeepCORAL</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> 
                 <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span><span class="p">,</span> 
                 <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span><span class="p">,</span>
                 <span class="n">class_subset</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">n_runs</span><span class="p">,</span> <span class="n">patience</span><span class="p">,</span> 
                 <span class="n">lambda_coral</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">deep_weights</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        deep_weights: tuple of weights for the CORAL loss at (early, middle, late) layers.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">G</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">S_train_loader</span> <span class="o">=</span> <span class="n">S_train_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">S_val_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_train_loader</span> <span class="o">=</span> <span class="n">T_train_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">T_val_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_subset</span> <span class="o">=</span> <span class="n">class_subset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_epochs</span> <span class="o">=</span> <span class="n">n_epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_runs</span> <span class="o">=</span> <span class="n">n_runs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambda_coral</span> <span class="o">=</span> <span class="n">lambda_coral</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deep_weights</span> <span class="o">=</span> <span class="n">deep_weights</span>
        
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="n">loader</span><span class="p">):</span>
        <span class="n">netG</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">netC</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="c1"># Only need the classification branch (late_fc)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">cls_feat</span> <span class="o">=</span> <span class="n">netG</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">netC</span><span class="p">(</span><span class="n">cls_feat</span><span class="p">)</span>
                <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">best_s_acc_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_t_acc_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_runs</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Deep CORAL Run </span><span class="si">{</span><span class="n">run</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">n_runs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
            <span class="c1"># Instantiate new networks for each run</span>
            <span class="n">netG</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">netC</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">C</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">netG</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">netC</span><span class="o">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
            <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
            <span class="n">best_val_acc</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">patience_counter</span> <span class="o">=</span> <span class="mi">0</span>
            
            <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epochs</span><span class="p">):</span>
                <span class="n">netG</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                <span class="n">netC</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                <span class="c1"># Use zip to iterate over source and target batches in parallel.</span>
                <span class="k">for</span> <span class="p">(</span><span class="n">s_data</span><span class="p">,</span> <span class="n">s_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">t_data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">S_train_loader</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_train_loader</span><span class="p">):</span>
                    <span class="n">s_data</span><span class="p">,</span> <span class="n">s_labels</span> <span class="o">=</span> <span class="n">s_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">s_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">t_data</span> <span class="o">=</span> <span class="n">t_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    
                    <span class="c1"># Forward pass for source: get all features</span>
                    <span class="n">s_early</span><span class="p">,</span> <span class="n">s_middle</span><span class="p">,</span> <span class="n">s_late</span><span class="p">,</span> <span class="n">s_cls</span> <span class="o">=</span> <span class="n">netG</span><span class="p">(</span><span class="n">s_data</span><span class="p">)</span>
                    <span class="c1"># Classification loss on source (using the classification feature)</span>
                    <span class="n">loss_cls</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">netC</span><span class="p">(</span><span class="n">s_cls</span><span class="p">),</span> <span class="n">s_labels</span><span class="p">)</span>
                    
                    <span class="c1"># Forward pass for target: we only need the intermediate features</span>
                    <span class="n">t_early</span><span class="p">,</span> <span class="n">t_middle</span><span class="p">,</span> <span class="n">t_late</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">netG</span><span class="p">(</span><span class="n">t_data</span><span class="p">)</span>
                    
                    <span class="c1"># Compute CORAL loss for each level</span>
                    <span class="n">loss_early</span> <span class="o">=</span> <span class="n">coral_loss</span><span class="p">(</span><span class="n">s_early</span><span class="p">,</span> <span class="n">t_early</span><span class="p">)</span>
                    <span class="n">loss_middle</span> <span class="o">=</span> <span class="n">coral_loss</span><span class="p">(</span><span class="n">s_middle</span><span class="p">,</span> <span class="n">t_middle</span><span class="p">)</span>
                    <span class="n">loss_late</span> <span class="o">=</span> <span class="n">coral_loss</span><span class="p">(</span><span class="n">s_late</span><span class="p">,</span> <span class="n">t_late</span><span class="p">)</span>
                    <span class="n">loss_coral_total</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">deep_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_early</span> <span class="o">+</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">deep_weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_middle</span> <span class="o">+</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">deep_weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_late</span><span class="p">)</span>
                    
                    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">loss_cls</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambda_coral</span> <span class="o">*</span> <span class="n">loss_coral_total</span>
                    
                    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                    <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
                    
                <span class="c1"># End-of-epoch evaluation on source validation set</span>
                <span class="n">s_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S_val_loader</span><span class="p">)</span>
                <span class="n">t_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_val_loader</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: Source Val Acc = </span><span class="si">{</span><span class="n">s_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Target Val Acc = </span><span class="si">{</span><span class="n">t_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                
                <span class="c1"># Early stopping on source validation accuracy</span>
                <span class="k">if</span> <span class="n">s_acc</span> <span class="o">&gt;</span> <span class="n">best_val_acc</span><span class="p">:</span>
                    <span class="n">best_val_acc</span> <span class="o">=</span> <span class="n">s_acc</span>
                    <span class="n">best_model_G</span> <span class="o">=</span> <span class="n">netG</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                    <span class="n">best_model_C</span> <span class="o">=</span> <span class="n">netC</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                    <span class="n">patience_counter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">patience_counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">patience_counter</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">"Early stopping triggered."</span><span class="p">)</span>
                    <span class="k">break</span>
                    
            <span class="c1"># Load best model from this run</span>
            <span class="n">netG</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_model_G</span><span class="p">)</span>
            <span class="n">netC</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">best_model_C</span><span class="p">)</span>
            <span class="n">s_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">S_val_loader</span><span class="p">)</span>
            <span class="n">t_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">netG</span><span class="p">,</span> <span class="n">netC</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">T_val_loader</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Run </span><span class="si">{</span><span class="n">run</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> finished: Best Source Val Acc = </span><span class="si">{</span><span class="n">s_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Target Val Acc = </span><span class="si">{</span><span class="n">t_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
            <span class="n">best_s_acc_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s_acc</span><span class="p">)</span>
            <span class="n">best_t_acc_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_acc</span><span class="p">)</span>
            
        <span class="n">avg_s_acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">best_s_acc_list</span><span class="p">)</span>
        <span class="n">avg_t_acc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">best_t_acc_list</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Deep CORAL: Average Source Val Acc = </span><span class="si">{</span><span class="n">avg_s_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Average Target Val Acc = </span><span class="si">{</span><span class="n">avg_t_acc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">avg_s_acc</span><span class="p">,</span> <span class="n">avg_t_acc</span>

<span class="c1">#%% STAR</span>
<span class="k">class</span> <span class="nc">STAR_G</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Deep ResNet feature extractor for STAR.</span>
<span class="sd">    </span>
<span class="sd">    This network accepts a 2–channel 1D signal and extracts a 512–dimensional</span>
<span class="sd">    feature vector via several residual layers.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">STAR_G</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to obtain a fixed-length feature vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Optional bottleneck fully-connected layer (maps 512-&gt;512)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch_size, channels=2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># (B, 64, L/4)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># (B, 64, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># (B, 128, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># (B, 256, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># (B, 512, ?)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># (B, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># (B, 512)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, 512)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">STAR_C</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Stochastic classifier network for STAR.</span>
<span class="sd">    </span>
<span class="sd">    This network receives the 512–dim features from DeepResNet_STAR_G,</span>
<span class="sd">    applies a fully connected layer with batch normalization and ReLU,</span>
<span class="sd">    and then uses a learned weight distribution (mu2, sigma2) to sample</span>
<span class="sd">    classifier weights. During training, it samples num_classifiers_train classifiers,</span>
<span class="sd">    while during evaluation it can either use only the mean (only_mu=True) or</span>
<span class="sd">    sample num_classifiers_test classifiers.</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">num_classifiers_train</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_classifiers_test</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                 <span class="n">init</span><span class="o">=</span><span class="s1">'kaiming_u'</span><span class="p">,</span> <span class="n">use_init</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">STAR_C</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classifiers_train</span> <span class="o">=</span> <span class="n">num_classifiers_train</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classifiers_test</span> <span class="o">=</span> <span class="n">num_classifiers_test</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>

        <span class="n">function_init</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">'kaiming_u'</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">,</span>
            <span class="s1">'kaiming_n'</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">,</span>
            <span class="s1">'xavier'</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span>
        <span class="p">}</span>

        <span class="c1"># Change input dimension to 512 (from the DeepResNet feature extractor)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1_fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

        <span class="c1"># Learnable parameters for the classifier weight distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">use_init</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mu2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma2</span><span class="p">]:</span>
                <span class="n">function_init</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">](</span><span class="n">item</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_dim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">only_mu</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bn1_fc</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="n">sigma2_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma2</span><span class="p">)</span>
        <span class="n">fc2_distribution</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu2</span><span class="p">,</span> <span class="n">sigma2_pos</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">classifiers</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classifiers_train</span><span class="p">):</span>
                <span class="n">fc2_w</span> <span class="o">=</span> <span class="n">fc2_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>
                <span class="n">classifiers</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">fc2_w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">])</span>

            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">classifier</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">classifier</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">classifier</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">outputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">only_mu</span><span class="p">:</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">)</span>
                <span class="k">return</span> <span class="p">[</span><span class="n">out</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">classifiers</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classifiers_test</span><span class="p">):</span>
                    <span class="n">fc2_w</span> <span class="o">=</span> <span class="n">fc2_distribution</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>
                    <span class="n">classifiers</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">fc2_w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span><span class="p">])</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">classifier</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
                    <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">classifier</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">classifier</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                    <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">outputs</span>

<span class="c1">#%% MCD</span>
<span class="k">class</span> <span class="nc">MCD_G</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MCD_G</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to get a fixed-length feature vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck fully-connected layer mapping 512 -&gt; 512 (optional)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Input x shape: (batch_size, 2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># -&gt; (B, 64, L/2)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># -&gt; (B, 64, L/4)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># -&gt; (B, 64, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># -&gt; (B, 128, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># -&gt; (B, 256, ?)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>     <span class="c1"># -&gt; (B, 512, ?)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># -&gt; (B, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>      <span class="c1"># -&gt; (B, 512)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># -&gt; (B, 512)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">MCD_C</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MCD_C</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Since our feature extractor now outputs a 512-dim vector,</span>
        <span class="c1"># adjust the input dimension accordingly.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lambda_</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="c1"># Optionally apply gradient reversal</span>
        <span class="k">if</span> <span class="n">reverse</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="c1">#%% JAN</span>
<span class="k">class</span> <span class="nc">JAN_G</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">JAN_G</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Initial convolutional block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Residual layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_layer</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        
        <span class="c1"># Global average pooling to obtain a fixed-length feature vector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Bottleneck layer mapping 512 -&gt; 512 dimensions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock1D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch_size, 2, length)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># shape: (batch, 512, 1)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># shape: (batch, 512)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_bottleneck</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># shape: (batch, 512)</span>
        
        <span class="c1"># Debug: Ensure x is a valid tensor</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Generator output is None. Check your forward pass."</span><span class="p">)</span>
        <span class="c1"># You can also uncomment the next line to print the shape during debugging.</span>
        <span class="c1"># print("Generator output shape:", x.shape)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">C_JAN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">C_JAN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">return_intermediate</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">inter</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">inter</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_intermediate</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">inter</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=ddd6acf4-60eb-46de-822d-ac7fb1783d03">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [2]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"/home/ash/ic3/testbed_da/data"</span>

<span class="c1"># Classes in loaded npy files</span>
<span class="n">class_subset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bpsk"</span><span class="p">,</span> <span class="s2">"qpsk"</span><span class="p">,</span> <span class="s2">"16qam"</span><span class="p">,</span> <span class="s2">"8apsk"</span><span class="p">]</span>

<span class="c1"># simulated data</span>
<span class="n">X_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_X.npy"</span><span class="p">)</span>
<span class="n">Y_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_Y.npy"</span><span class="p">)</span>

<span class="c1"># over the air data</span>
<span class="n">X_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_X.npy"</span><span class="p">)</span>
<span class="n">Y_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_Y.npy"</span><span class="p">)</span>

<span class="n">z_val</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_snr</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># (Lists for storing accuracies; you may also want to add lists for deep coral)</span>
<span class="n">t_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_base_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_dann_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_mcd_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_star_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">t_jan_acc</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_snr</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1">SNR level: </span><span class="si">{</span><span class="n">z_val</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="c1"># Filter for SNR level (for simulated data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_sim</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="p">)</span>
    <span class="n">X_s</span> <span class="o">=</span> <span class="n">X_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_s</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># first column: class labels</span>

    <span class="c1"># Filter for SNR level (for over the air data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_ota</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">X_t</span> <span class="o">=</span> <span class="n">X_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Create data loaders</span>
    <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">Y_s</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_t</span><span class="p">,</span> <span class="n">Y_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'Base'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_base</span> <span class="o">=</span> <span class="n">base</span><span class="o">.</span><span class="n">Base</span><span class="p">(</span>
        <span class="n">model_cls</span><span class="o">=</span><span class="n">DeepResNet</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span> 
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span> 
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_base_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_base</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'DANN'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_dann</span> <span class="o">=</span> <span class="n">dann</span><span class="o">.</span><span class="n">DAN</span><span class="p">(</span>
        <span class="n">dann</span><span class="o">.</span><span class="n">DANN</span><span class="p">,</span>
        <span class="n">FA</span><span class="o">=</span><span class="n">DANN_F</span><span class="p">,</span>
        <span class="n">LP</span><span class="o">=</span><span class="n">DANN_LP</span><span class="p">,</span>
        <span class="n">DC</span><span class="o">=</span><span class="n">DANN_DC</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_dann_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_dann</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'Deep CORAL'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_deep</span> <span class="o">=</span> <span class="n">DeepCORAL</span><span class="p">(</span>
        <span class="n">G</span><span class="o">=</span><span class="n">CORAL_G</span><span class="p">,</span> 
        <span class="n">C</span><span class="o">=</span><span class="n">CORAL_C</span><span class="p">,</span> 
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> 
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> 
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span> 
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">lambda_coral</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">deep_weights</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_deep_coral_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_deep</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'STAR'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_star</span> <span class="o">=</span>  <span class="n">star</span><span class="o">.</span><span class="n">Star</span><span class="p">(</span>
        <span class="n">G</span><span class="o">=</span><span class="n">STAR_G</span><span class="p">,</span>
        <span class="n">C</span><span class="o">=</span><span class="n">STAR_C</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>  
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_star_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_star</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'MCD'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_mcd</span> <span class="o">=</span> <span class="n">mcd</span><span class="o">.</span><span class="n">Mcd</span><span class="p">(</span>
        <span class="n">G</span><span class="o">=</span><span class="n">MCD_G</span><span class="p">,</span>
        <span class="n">C</span><span class="o">=</span><span class="n">MCD_C</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>  
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">class_subset</span><span class="o">=</span><span class="n">class_subset</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">5</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_mcd_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_mcd</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">'JAN'</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">t_jan</span> <span class="o">=</span> <span class="n">jan</span><span class="o">.</span><span class="n">Jan</span><span class="p">(</span>
        <span class="n">C</span><span class="o">=</span><span class="n">C_JAN</span><span class="p">,</span>
        <span class="n">G</span><span class="o">=</span><span class="n">JAN_G</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">class_subset</span><span class="p">),</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">S_train_loader</span><span class="o">=</span><span class="n">S_train_loader</span><span class="p">,</span>
        <span class="n">T_train_loader</span><span class="o">=</span><span class="n">T_train_loader</span><span class="p">,</span>
        <span class="n">S_val_loader</span><span class="o">=</span><span class="n">S_val_loader</span><span class="p">,</span>
        <span class="n">T_val_loader</span><span class="o">=</span><span class="n">T_val_loader</span><span class="p">,</span>
        <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
        <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">lambda_jmmd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">n_runs</span><span class="o">=</span><span class="n">n_runs</span>
    <span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="c1">#s_deep_coral_acc.append(s_deep)</span>
    <span class="n">t_jan_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t_jan</span><span class="p">)</span>

    

    <span class="n">z_val</span> <span class="o">+=</span> <span class="mi">4</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>
SNR level: 10
Base

Run 1/3
Epoch 1/50, Train Loss: 0.2876, Train Acc: 0.8988, Val Loss: 0.6662, Val Acc: 0.7410
Epoch 2/50, Train Loss: 0.0394, Train Acc: 0.9882, Val Loss: 1.8437, Val Acc: 0.6379
Epoch 3/50, Train Loss: 0.0192, Train Acc: 0.9945, Val Loss: 0.0091, Val Acc: 0.9982
Epoch 4/50, Train Loss: 0.0216, Train Acc: 0.9936, Val Loss: 0.2437, Val Acc: 0.9155
Epoch 5/50, Train Loss: 0.0227, Train Acc: 0.9927, Val Loss: 0.0123, Val Acc: 0.9952
Epoch 6/50, Train Loss: 0.0147, Train Acc: 0.9963, Val Loss: 0.0078, Val Acc: 0.9970
Epoch 7/50, Train Loss: 0.0213, Train Acc: 0.9937, Val Loss: 1.5503, Val Acc: 0.7116
Epoch 8/50, Train Loss: 0.0111, Train Acc: 0.9981, Val Loss: 0.0255, Val Acc: 0.9904
Epoch 9/50, Train Loss: 0.0109, Train Acc: 0.9975, Val Loss: 2.2404, Val Acc: 0.6553
Epoch 10/50, Train Loss: 0.0067, Train Acc: 0.9982, Val Loss: 0.0080, Val Acc: 0.9976
Epoch 11/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0029, Val Acc: 0.9982
Epoch 12/50, Train Loss: 0.0013, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0019, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0011, Train Acc: 0.9997, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0015, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0023, Train Acc: 0.9991, Val Loss: 0.0022, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0035, Train Acc: 0.9993, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0050, Train Acc: 0.9988, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0021, Val Acc: 0.9982
Epoch 23/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0011, Val Acc: 1.0000
Early stopping!

Run 2/3
Epoch 1/50, Train Loss: 0.3030, Train Acc: 0.9039, Val Loss: 1.0889, Val Acc: 0.6421
Epoch 2/50, Train Loss: 0.0614, Train Acc: 0.9798, Val Loss: 0.6136, Val Acc: 0.7590
Epoch 3/50, Train Loss: 0.0306, Train Acc: 0.9903, Val Loss: 0.0432, Val Acc: 0.9838
Epoch 4/50, Train Loss: 0.0226, Train Acc: 0.9936, Val Loss: 0.6852, Val Acc: 0.7866
Epoch 5/50, Train Loss: 0.0129, Train Acc: 0.9966, Val Loss: 0.0095, Val Acc: 0.9964
Epoch 6/50, Train Loss: 0.0089, Train Acc: 0.9976, Val Loss: 0.0144, Val Acc: 0.9952
Epoch 7/50, Train Loss: 0.0050, Train Acc: 0.9990, Val Loss: 0.0114, Val Acc: 0.9952
Epoch 8/50, Train Loss: 0.0104, Train Acc: 0.9966, Val Loss: 2.1940, Val Acc: 0.7044
Epoch 9/50, Train Loss: 0.0129, Train Acc: 0.9961, Val Loss: 0.0365, Val Acc: 0.9844
Epoch 10/50, Train Loss: 0.0152, Train Acc: 0.9948, Val Loss: 0.3048, Val Acc: 0.9131
Early stopping!

Run 3/3
Epoch 1/50, Train Loss: 0.3225, Train Acc: 0.8874, Val Loss: 1.6049, Val Acc: 0.7440
Epoch 2/50, Train Loss: 0.0518, Train Acc: 0.9837, Val Loss: 0.1712, Val Acc: 0.9382
Epoch 3/50, Train Loss: 0.0163, Train Acc: 0.9960, Val Loss: 0.0203, Val Acc: 0.9946
Epoch 4/50, Train Loss: 0.0184, Train Acc: 0.9946, Val Loss: 0.0076, Val Acc: 0.9964
Epoch 5/50, Train Loss: 0.0352, Train Acc: 0.9894, Val Loss: 0.2200, Val Acc: 0.9263
Epoch 6/50, Train Loss: 0.0106, Train Acc: 0.9967, Val Loss: 0.0103, Val Acc: 0.9970
Epoch 7/50, Train Loss: 0.0094, Train Acc: 0.9970, Val Loss: 0.0717, Val Acc: 0.9814
Epoch 8/50, Train Loss: 0.0134, Train Acc: 0.9943, Val Loss: 0.0519, Val Acc: 0.9814
Epoch 9/50, Train Loss: 0.0082, Train Acc: 0.9972, Val Loss: 0.0041, Val Acc: 0.9988
Epoch 10/50, Train Loss: 0.0121, Train Acc: 0.9964, Val Loss: 0.0194, Val Acc: 0.9940
Epoch 11/50, Train Loss: 0.0035, Train Acc: 0.9990, Val Loss: 0.0030, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0023, Train Acc: 0.9996, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0019, Train Acc: 0.9994, Val Loss: 0.0028, Val Acc: 0.9982
Epoch 14/50, Train Loss: 0.0012, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0013, Train Acc: 0.9997, Val Loss: 0.0033, Val Acc: 0.9976
Epoch 16/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988
Epoch 19/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0038, Train Acc: 0.9996, Val Loss: 0.0025, Val Acc: 0.9982
Epoch 21/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0024, Val Acc: 0.9988
Epoch 22/50, Train Loss: 0.0010, Train Acc: 0.9997, Val Loss: 0.0023, Val Acc: 0.9988
Epoch 23/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9988
Epoch 24/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9982
Early stopping!

Source performance: 97.04 97.49 96.89 96.91
Target performance: 66.37 68.84 65.68 59.40

bpsk: 99.92
qpsk: 58.35
16qam: 4.45
8apsk: 100.00
DANN
Epoch 1/50, Loss: 2.2591, Domain Loss: 1.3775, Class Loss: 0.8816
Epoch 2/50, Loss: 1.6344, Domain Loss: 1.3774, Class Loss: 0.2571
Epoch 3/50, Loss: 1.5083, Domain Loss: 1.3894, Class Loss: 0.1190
Epoch 4/50, Loss: 2.0160, Domain Loss: 1.9284, Class Loss: 0.0875
Epoch 5/50, Loss: 2.9906, Domain Loss: 2.8552, Class Loss: 0.1354
Epoch 6/50, Loss: 5.2653, Domain Loss: 4.7813, Class Loss: 0.4840
Epoch 7/50, Loss: 5.6949, Domain Loss: 5.3192, Class Loss: 0.3757
Epoch 8/50, Loss: 3.2667, Domain Loss: 3.0472, Class Loss: 0.2195
Epoch 9/50, Loss: 3.2296, Domain Loss: 2.9825, Class Loss: 0.2472
Epoch 10/50, Loss: 3.9330, Domain Loss: 3.4413, Class Loss: 0.4916
Epoch 11/50, Loss: 2.2782, Domain Loss: 2.0638, Class Loss: 0.2144
Epoch 12/50, Loss: 3.5459, Domain Loss: 3.0828, Class Loss: 0.4631
Epoch 13/50, Loss: 4.3021, Domain Loss: 3.5818, Class Loss: 0.7203
Epoch 14/50, Loss: 2.2993, Domain Loss: 1.9015, Class Loss: 0.3978
Epoch 15/50, Loss: 1.8151, Domain Loss: 1.4611, Class Loss: 0.3539
Epoch 16/50, Loss: 1.7307, Domain Loss: 1.4021, Class Loss: 0.3286
Epoch 17/50, Loss: 1.6620, Domain Loss: 1.3825, Class Loss: 0.2796
Epoch 18/50, Loss: 1.6482, Domain Loss: 1.4043, Class Loss: 0.2439
Epoch 19/50, Loss: 1.9065, Domain Loss: 1.5440, Class Loss: 0.3625
Epoch 20/50, Loss: 2.0647, Domain Loss: 1.7507, Class Loss: 0.3140
Epoch 21/50, Loss: 4.9832, Domain Loss: 4.6094, Class Loss: 0.3738
Epoch 22/50, Loss: 2.6221, Domain Loss: 2.1892, Class Loss: 0.4329
Epoch 23/50, Loss: 1.8568, Domain Loss: 1.5408, Class Loss: 0.3160
Epoch 24/50, Loss: 1.6451, Domain Loss: 1.4133, Class Loss: 0.2317
Epoch 25/50, Loss: 1.5452, Domain Loss: 1.3730, Class Loss: 0.1722
Epoch 26/50, Loss: 1.5272, Domain Loss: 1.3821, Class Loss: 0.1451
Epoch 27/50, Loss: 1.4970, Domain Loss: 1.3947, Class Loss: 0.1023
Epoch 28/50, Loss: 1.5150, Domain Loss: 1.4247, Class Loss: 0.0903
Epoch 29/50, Loss: 1.5177, Domain Loss: 1.4374, Class Loss: 0.0803
Epoch 30/50, Loss: 1.5268, Domain Loss: 1.4554, Class Loss: 0.0714
Epoch 31/50, Loss: 1.5246, Domain Loss: 1.4400, Class Loss: 0.0846
Epoch 32/50, Loss: 1.4362, Domain Loss: 1.3947, Class Loss: 0.0416
Epoch 33/50, Loss: 1.5701, Domain Loss: 1.4575, Class Loss: 0.1126
Epoch 34/50, Loss: 1.5376, Domain Loss: 1.4335, Class Loss: 0.1041
Epoch 35/50, Loss: 1.5063, Domain Loss: 1.4286, Class Loss: 0.0776
Epoch 36/50, Loss: 1.4803, Domain Loss: 1.4182, Class Loss: 0.0621
Epoch 37/50, Loss: 1.4768, Domain Loss: 1.4367, Class Loss: 0.0401
Epoch 38/50, Loss: 1.4984, Domain Loss: 1.4455, Class Loss: 0.0529
Epoch 39/50, Loss: 1.5197, Domain Loss: 1.4703, Class Loss: 0.0494
Epoch 40/50, Loss: 1.4804, Domain Loss: 1.4366, Class Loss: 0.0438
Epoch 41/50, Loss: 1.4487, Domain Loss: 1.4009, Class Loss: 0.0479
Epoch 42/50, Loss: 1.5219, Domain Loss: 1.4686, Class Loss: 0.0533
Epoch 43/50, Loss: 1.4483, Domain Loss: 1.4168, Class Loss: 0.0314
Epoch 44/50, Loss: 1.4429, Domain Loss: 1.4019, Class Loss: 0.0410
Epoch 45/50, Loss: 1.4493, Domain Loss: 1.3923, Class Loss: 0.0571
Epoch 46/50, Loss: 1.4693, Domain Loss: 1.4195, Class Loss: 0.0498
Epoch 47/50, Loss: 1.4848, Domain Loss: 1.4407, Class Loss: 0.0440
Epoch 48/50, Loss: 1.4640, Domain Loss: 1.4133, Class Loss: 0.0507
Epoch 49/50, Loss: 1.4418, Domain Loss: 1.4205, Class Loss: 0.0212
Epoch 50/50, Loss: 1.4227, Domain Loss: 1.4019, Class Loss: 0.0208
73.14


Epoch 1/50, Loss: 2.2326, Domain Loss: 1.3970, Class Loss: 0.8356
Epoch 2/50, Loss: 1.6429, Domain Loss: 1.3734, Class Loss: 0.2695
Epoch 3/50, Loss: 1.4766, Domain Loss: 1.3499, Class Loss: 0.1267
Epoch 4/50, Loss: 1.4262, Domain Loss: 1.3515, Class Loss: 0.0747
Epoch 5/50, Loss: 1.5051, Domain Loss: 1.4544, Class Loss: 0.0507
Epoch 6/50, Loss: 2.5040, Domain Loss: 2.4017, Class Loss: 0.1022
Epoch 7/50, Loss: 2.4998, Domain Loss: 2.2272, Class Loss: 0.2726
Epoch 8/50, Loss: 2.0297, Domain Loss: 1.7770, Class Loss: 0.2527
Epoch 9/50, Loss: 1.7504, Domain Loss: 1.6108, Class Loss: 0.1397
Epoch 10/50, Loss: 3.1626, Domain Loss: 2.9785, Class Loss: 0.1842
Epoch 11/50, Loss: 3.3210, Domain Loss: 2.9771, Class Loss: 0.3439
Epoch 12/50, Loss: 2.3111, Domain Loss: 2.0746, Class Loss: 0.2365
Epoch 13/50, Loss: 5.4497, Domain Loss: 5.0076, Class Loss: 0.4421
Epoch 14/50, Loss: 8.2163, Domain Loss: 7.7969, Class Loss: 0.4193
Epoch 15/50, Loss: 6.7229, Domain Loss: 6.2214, Class Loss: 0.5015
Epoch 16/50, Loss: 3.2221, Domain Loss: 2.8842, Class Loss: 0.3379
Epoch 17/50, Loss: 1.9817, Domain Loss: 1.7846, Class Loss: 0.1970
Epoch 18/50, Loss: 1.7930, Domain Loss: 1.6516, Class Loss: 0.1414
Epoch 19/50, Loss: 1.8846, Domain Loss: 1.7210, Class Loss: 0.1636
Epoch 20/50, Loss: 1.7223, Domain Loss: 1.6072, Class Loss: 0.1150
Epoch 21/50, Loss: 1.5903, Domain Loss: 1.4425, Class Loss: 0.1477
Epoch 22/50, Loss: 1.4519, Domain Loss: 1.3616, Class Loss: 0.0903
Epoch 23/50, Loss: 1.5144, Domain Loss: 1.3818, Class Loss: 0.1326
Epoch 24/50, Loss: 1.4490, Domain Loss: 1.3788, Class Loss: 0.0702
Epoch 25/50, Loss: 1.4433, Domain Loss: 1.3831, Class Loss: 0.0602
Epoch 26/50, Loss: 1.4912, Domain Loss: 1.4104, Class Loss: 0.0809
Epoch 27/50, Loss: 1.4612, Domain Loss: 1.3881, Class Loss: 0.0731
Epoch 28/50, Loss: 1.4711, Domain Loss: 1.3950, Class Loss: 0.0761
Epoch 29/50, Loss: 1.4430, Domain Loss: 1.3931, Class Loss: 0.0498
Epoch 30/50, Loss: 1.4130, Domain Loss: 1.3911, Class Loss: 0.0219
Epoch 31/50, Loss: 1.4602, Domain Loss: 1.4140, Class Loss: 0.0462
Epoch 32/50, Loss: 1.4569, Domain Loss: 1.4169, Class Loss: 0.0400
Epoch 33/50, Loss: 1.4494, Domain Loss: 1.4159, Class Loss: 0.0335
Epoch 34/50, Loss: 1.4604, Domain Loss: 1.4196, Class Loss: 0.0408
Epoch 35/50, Loss: 1.4456, Domain Loss: 1.4243, Class Loss: 0.0213
Epoch 36/50, Loss: 1.4451, Domain Loss: 1.4136, Class Loss: 0.0314
Epoch 37/50, Loss: 1.4408, Domain Loss: 1.3975, Class Loss: 0.0433
Epoch 38/50, Loss: 1.4041, Domain Loss: 1.3919, Class Loss: 0.0122
Epoch 39/50, Loss: 1.4352, Domain Loss: 1.4044, Class Loss: 0.0308
Epoch 40/50, Loss: 1.4300, Domain Loss: 1.3947, Class Loss: 0.0352
Epoch 41/50, Loss: 1.4121, Domain Loss: 1.3820, Class Loss: 0.0302
Epoch 42/50, Loss: 1.4107, Domain Loss: 1.3888, Class Loss: 0.0219
Epoch 43/50, Loss: 1.4237, Domain Loss: 1.4093, Class Loss: 0.0143
Epoch 44/50, Loss: 1.4115, Domain Loss: 1.3856, Class Loss: 0.0259
Epoch 45/50, Loss: 1.4105, Domain Loss: 1.3968, Class Loss: 0.0137
Epoch 46/50, Loss: 1.4097, Domain Loss: 1.3989, Class Loss: 0.0108
Epoch 47/50, Loss: 1.4131, Domain Loss: 1.4033, Class Loss: 0.0098
Epoch 48/50, Loss: 1.4549, Domain Loss: 1.4178, Class Loss: 0.0371
Epoch 49/50, Loss: 1.4199, Domain Loss: 1.3879, Class Loss: 0.0320
Epoch 50/50, Loss: 1.4286, Domain Loss: 1.4037, Class Loss: 0.0250
77.46


Epoch 1/50, Loss: 2.1709, Domain Loss: 1.3827, Class Loss: 0.7882
Epoch 2/50, Loss: 1.5092, Domain Loss: 1.3387, Class Loss: 0.1705
Epoch 3/50, Loss: 1.5191, Domain Loss: 1.3392, Class Loss: 0.1799
Epoch 4/50, Loss: 1.8524, Domain Loss: 1.7346, Class Loss: 0.1178
Epoch 5/50, Loss: 2.7529, Domain Loss: 2.6425, Class Loss: 0.1103
Epoch 6/50, Loss: 3.4699, Domain Loss: 3.3478, Class Loss: 0.1221
Epoch 7/50, Loss: 2.8498, Domain Loss: 2.6524, Class Loss: 0.1974
Epoch 8/50, Loss: 3.8061, Domain Loss: 3.6101, Class Loss: 0.1960
Epoch 9/50, Loss: 3.6565, Domain Loss: 3.2982, Class Loss: 0.3582
Epoch 10/50, Loss: 2.3860, Domain Loss: 2.1285, Class Loss: 0.2576
Epoch 11/50, Loss: 2.2478, Domain Loss: 2.0033, Class Loss: 0.2444
Epoch 12/50, Loss: 6.8277, Domain Loss: 5.6543, Class Loss: 1.1735
Epoch 13/50, Loss: 6.1992, Domain Loss: 5.4307, Class Loss: 0.7685
Epoch 14/50, Loss: 14.0686, Domain Loss: 13.0359, Class Loss: 1.0327
Epoch 15/50, Loss: 5.2768, Domain Loss: 4.8066, Class Loss: 0.4702
Epoch 16/50, Loss: 2.5595, Domain Loss: 2.0660, Class Loss: 0.4935
Epoch 17/50, Loss: 3.0588, Domain Loss: 2.4263, Class Loss: 0.6325
Epoch 18/50, Loss: 5.7455, Domain Loss: 5.0159, Class Loss: 0.7296
Epoch 19/50, Loss: 6.1082, Domain Loss: 5.4932, Class Loss: 0.6149
Epoch 20/50, Loss: 6.0225, Domain Loss: 5.2929, Class Loss: 0.7296
Epoch 21/50, Loss: 5.4025, Domain Loss: 4.8023, Class Loss: 0.6001
Epoch 22/50, Loss: 3.5533, Domain Loss: 3.0017, Class Loss: 0.5517
Epoch 23/50, Loss: 3.2286, Domain Loss: 2.8461, Class Loss: 0.3826
Epoch 24/50, Loss: 2.8786, Domain Loss: 2.4696, Class Loss: 0.4089
Epoch 25/50, Loss: 2.0239, Domain Loss: 1.5949, Class Loss: 0.4291
Epoch 26/50, Loss: 1.7426, Domain Loss: 1.4302, Class Loss: 0.3125
Epoch 27/50, Loss: 1.6779, Domain Loss: 1.3816, Class Loss: 0.2963
Epoch 28/50, Loss: 1.6833, Domain Loss: 1.3823, Class Loss: 0.3011
Epoch 29/50, Loss: 1.6609, Domain Loss: 1.3665, Class Loss: 0.2944
Epoch 30/50, Loss: 1.6228, Domain Loss: 1.3652, Class Loss: 0.2576
Epoch 31/50, Loss: 1.6449, Domain Loss: 1.3991, Class Loss: 0.2458
Epoch 32/50, Loss: 1.6188, Domain Loss: 1.4138, Class Loss: 0.2049
Epoch 33/50, Loss: 1.6794, Domain Loss: 1.4432, Class Loss: 0.2362
Epoch 34/50, Loss: 1.7001, Domain Loss: 1.4823, Class Loss: 0.2179
Epoch 35/50, Loss: 1.7095, Domain Loss: 1.4990, Class Loss: 0.2105
Epoch 36/50, Loss: 1.8518, Domain Loss: 1.6125, Class Loss: 0.2392
Epoch 37/50, Loss: 1.7417, Domain Loss: 1.5068, Class Loss: 0.2349
Epoch 38/50, Loss: 1.6195, Domain Loss: 1.4634, Class Loss: 0.1561
Epoch 39/50, Loss: 1.5803, Domain Loss: 1.4265, Class Loss: 0.1538
Epoch 40/50, Loss: 1.5595, Domain Loss: 1.4364, Class Loss: 0.1232
Epoch 41/50, Loss: 1.5174, Domain Loss: 1.4156, Class Loss: 0.1018
Epoch 42/50, Loss: 1.5155, Domain Loss: 1.4221, Class Loss: 0.0934
Epoch 43/50, Loss: 1.5438, Domain Loss: 1.4227, Class Loss: 0.1211
Epoch 44/50, Loss: 1.6264, Domain Loss: 1.5115, Class Loss: 0.1149
Epoch 45/50, Loss: 1.5728, Domain Loss: 1.4497, Class Loss: 0.1231
Epoch 46/50, Loss: 1.5906, Domain Loss: 1.5072, Class Loss: 0.0834
Epoch 47/50, Loss: 1.5298, Domain Loss: 1.4174, Class Loss: 0.1124
Epoch 48/50, Loss: 1.7980, Domain Loss: 1.5721, Class Loss: 0.2259
Epoch 49/50, Loss: 1.7157, Domain Loss: 1.4695, Class Loss: 0.2461
Epoch 50/50, Loss: 1.7173, Domain Loss: 1.4641, Class Loss: 0.2532
83.93


Source performance:
85.27 88.23 84.94 83.54 
Target performance:
78.18 78.55 77.75 75.80 

Deep CORALtarget performance: 100.00 71.32 45.55 94.14 
Deep CORAL Run 1/3
Epoch 1: Source Val Acc = 0.8783, Target Val Acc = 0.6499
Epoch 2: Source Val Acc = 0.6757, Target Val Acc = 0.9436
Epoch 3: Source Val Acc = 0.2602, Target Val Acc = 0.4742
Epoch 4: Source Val Acc = 0.7902, Target Val Acc = 0.8747
Epoch 5: Source Val Acc = 0.9940, Target Val Acc = 0.7650
Epoch 6: Source Val Acc = 0.9454, Target Val Acc = 0.7998
Epoch 7: Source Val Acc = 0.9892, Target Val Acc = 0.8088
Epoch 8: Source Val Acc = 0.7956, Target Val Acc = 0.9287
Epoch 9: Source Val Acc = 0.9988, Target Val Acc = 0.7314
Epoch 10: Source Val Acc = 0.9964, Target Val Acc = 0.7950
Epoch 11: Source Val Acc = 0.9934, Target Val Acc = 0.8106
Epoch 12: Source Val Acc = 0.9706, Target Val Acc = 0.8429
Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 0.7704
Epoch 14: Source Val Acc = 0.5665, Target Val Acc = 0.7692
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.5665, Target Val Acc = 0.7692

Deep CORAL Run 2/3
Epoch 1: Source Val Acc = 0.7620, Target Val Acc = 0.8807
Epoch 2: Source Val Acc = 0.7920, Target Val Acc = 0.7224
Epoch 3: Source Val Acc = 0.9892, Target Val Acc = 0.6307
Epoch 4: Source Val Acc = 0.9958, Target Val Acc = 0.7254
Epoch 5: Source Val Acc = 0.6625, Target Val Acc = 0.8261
Epoch 6: Source Val Acc = 0.7728, Target Val Acc = 0.7926
Epoch 7: Source Val Acc = 0.9718, Target Val Acc = 0.8135
Epoch 8: Source Val Acc = 0.9263, Target Val Acc = 0.8231
Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.6847
Epoch 10: Source Val Acc = 0.9958, Target Val Acc = 0.7986
Epoch 11: Source Val Acc = 0.9958, Target Val Acc = 0.7728
Epoch 12: Source Val Acc = 0.7788, Target Val Acc = 0.6757
Epoch 13: Source Val Acc = 0.9011, Target Val Acc = 0.6169
Epoch 14: Source Val Acc = 0.9952, Target Val Acc = 0.6763
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9952, Target Val Acc = 0.6763

Deep CORAL Run 3/3
Epoch 1: Source Val Acc = 0.7626, Target Val Acc = 0.7458
Epoch 2: Source Val Acc = 0.8957, Target Val Acc = 0.7758
Epoch 3: Source Val Acc = 0.7734, Target Val Acc = 0.9113
Epoch 4: Source Val Acc = 0.9940, Target Val Acc = 0.5558
Epoch 5: Source Val Acc = 0.9850, Target Val Acc = 0.5995
Epoch 6: Source Val Acc = 0.9838, Target Val Acc = 0.7602
Epoch 7: Source Val Acc = 0.9916, Target Val Acc = 0.7812
Epoch 8: Source Val Acc = 0.9928, Target Val Acc = 0.6811
Epoch 9: Source Val Acc = 0.9886, Target Val Acc = 0.7680
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.9886, Target Val Acc = 0.7680

Deep CORAL: Average Source Val Acc = 0.8501, Average Target Val Acc = 0.7378
STAR

Run 1/3
Epoch [1/50], Class Loss: 0.7980, Discrepancy Loss: 0.0747
Epoch [2/50], Class Loss: 0.2923, Discrepancy Loss: 0.0461
Epoch [3/50], Class Loss: 0.0793, Discrepancy Loss: 0.0238
Epoch [4/50], Class Loss: 0.0806, Discrepancy Loss: 0.0184
Epoch [5/50], Class Loss: 0.4689, Discrepancy Loss: 0.0369
Epoch [6/50], Class Loss: 0.2024, Discrepancy Loss: 0.0269
Epoch [7/50], Class Loss: 0.0759, Discrepancy Loss: 0.0176
Epoch [8/50], Class Loss: 0.0193, Discrepancy Loss: 0.0105
Epoch [9/50], Class Loss: 0.0098, Discrepancy Loss: 0.0144
Epoch [10/50], Class Loss: 0.0237, Discrepancy Loss: 0.0164
Epoch [11/50], Class Loss: 0.0439, Discrepancy Loss: 0.0096
Epoch [12/50], Class Loss: 0.0136, Discrepancy Loss: 0.0126
Epoch [13/50], Class Loss: 0.0151, Discrepancy Loss: 0.0081
Epoch [14/50], Class Loss: 0.0092, Discrepancy Loss: 0.0132
Epoch [15/50], Class Loss: 0.0071, Discrepancy Loss: 0.0093
Epoch [16/50], Class Loss: 0.0074, Discrepancy Loss: 0.0094
Epoch [17/50], Class Loss: 0.0050, Discrepancy Loss: 0.0091
Epoch [18/50], Class Loss: 0.0100, Discrepancy Loss: 0.0078
Epoch [19/50], Class Loss: 0.0514, Discrepancy Loss: 0.0094
Epoch [20/50], Class Loss: 0.0435, Discrepancy Loss: 0.0148
Epoch [21/50], Class Loss: 0.0089, Discrepancy Loss: 0.0184
Epoch [22/50], Class Loss: 0.0077, Discrepancy Loss: 0.0154
Epoch [23/50], Class Loss: 0.0078, Discrepancy Loss: 0.0127
Epoch [24/50], Class Loss: 0.0973, Discrepancy Loss: 0.0159
Epoch [25/50], Class Loss: 0.0148, Discrepancy Loss: 0.0162
Epoch [26/50], Class Loss: 0.0041, Discrepancy Loss: 0.0127
Epoch [27/50], Class Loss: 0.0159, Discrepancy Loss: 0.0141
Epoch [28/50], Class Loss: 0.0088, Discrepancy Loss: 0.0159
Epoch [29/50], Class Loss: 0.0051, Discrepancy Loss: 0.0111
Epoch [30/50], Class Loss: 0.0064, Discrepancy Loss: 0.0127
Epoch [31/50], Class Loss: 0.0061, Discrepancy Loss: 0.0139
Epoch [32/50], Class Loss: 0.0052, Discrepancy Loss: 0.0123
Epoch [33/50], Class Loss: 0.0034, Discrepancy Loss: 0.0149
Epoch [34/50], Class Loss: 0.0079, Discrepancy Loss: 0.0114
Epoch [35/50], Class Loss: 0.0070, Discrepancy Loss: 0.0119
Epoch [36/50], Class Loss: 0.0047, Discrepancy Loss: 0.0149
Epoch [37/50], Class Loss: 0.0072, Discrepancy Loss: 0.0131
Epoch [38/50], Class Loss: 0.0044, Discrepancy Loss: 0.0121
Epoch [39/50], Class Loss: 0.0045, Discrepancy Loss: 0.0140
Epoch [40/50], Class Loss: 0.0056, Discrepancy Loss: 0.0162
Epoch [41/50], Class Loss: 0.2126, Discrepancy Loss: 0.0134
Epoch [42/50], Class Loss: 0.0077, Discrepancy Loss: 0.0115
Epoch [43/50], Class Loss: 0.0052, Discrepancy Loss: 0.0109
Epoch [44/50], Class Loss: 0.0061, Discrepancy Loss: 0.0138
Epoch [45/50], Class Loss: 0.0053, Discrepancy Loss: 0.0136
Epoch [46/50], Class Loss: 0.0088, Discrepancy Loss: 0.0112
Epoch [47/50], Class Loss: 0.0903, Discrepancy Loss: 0.0121
Epoch [48/50], Class Loss: 0.0156, Discrepancy Loss: 0.0116
Epoch [49/50], Class Loss: 0.1020, Discrepancy Loss: 0.0113
Epoch [50/50], Class Loss: 0.0138, Discrepancy Loss: 0.0117
Source Domain Performance - Accuracy: 95.92%, Precision: 96.03%, Recall: 95.88%, F1 Score: 95.84%
Target Domain Performance - Accuracy: 85.49%, Precision: 88.23%, Recall: 85.28%, F1 Score: 84.33%

Run 2/3
Epoch [1/50], Class Loss: 0.9911, Discrepancy Loss: 0.0979
Epoch [2/50], Class Loss: 0.4773, Discrepancy Loss: 0.0562
Epoch [3/50], Class Loss: 0.2852, Discrepancy Loss: 0.0402
Epoch [4/50], Class Loss: 0.1957, Discrepancy Loss: 0.0251
Epoch [5/50], Class Loss: 0.1368, Discrepancy Loss: 0.0316
Epoch [6/50], Class Loss: 0.0763, Discrepancy Loss: 0.0273
Epoch [7/50], Class Loss: 0.0275, Discrepancy Loss: 0.0194
Epoch [8/50], Class Loss: 0.0391, Discrepancy Loss: 0.0170
Epoch [9/50], Class Loss: 0.1964, Discrepancy Loss: 0.0310
Epoch [10/50], Class Loss: 0.0830, Discrepancy Loss: 0.0302
Epoch [11/50], Class Loss: 0.0158, Discrepancy Loss: 0.0160
Epoch [12/50], Class Loss: 0.0156, Discrepancy Loss: 0.0162
Epoch [13/50], Class Loss: 0.0186, Discrepancy Loss: 0.0167
Epoch [14/50], Class Loss: 0.0316, Discrepancy Loss: 0.0125
Epoch [15/50], Class Loss: 0.0125, Discrepancy Loss: 0.0148
Epoch [16/50], Class Loss: 0.0084, Discrepancy Loss: 0.0127
Epoch [17/50], Class Loss: 0.0056, Discrepancy Loss: 0.0145
Epoch [18/50], Class Loss: 0.0122, Discrepancy Loss: 0.0158
Epoch [19/50], Class Loss: 0.0078, Discrepancy Loss: 0.0117
Epoch [20/50], Class Loss: 0.0067, Discrepancy Loss: 0.0137
Epoch [21/50], Class Loss: 0.0564, Discrepancy Loss: 0.0128
Epoch [22/50], Class Loss: 0.0626, Discrepancy Loss: 0.0192
Epoch [23/50], Class Loss: 0.0121, Discrepancy Loss: 0.0109
Epoch [24/50], Class Loss: 0.0235, Discrepancy Loss: 0.0126
Epoch [25/50], Class Loss: 0.0070, Discrepancy Loss: 0.0137
Epoch [26/50], Class Loss: 0.0072, Discrepancy Loss: 0.0148
Epoch [27/50], Class Loss: 0.0073, Discrepancy Loss: 0.0124
Epoch [28/50], Class Loss: 0.0061, Discrepancy Loss: 0.0116
Epoch [29/50], Class Loss: 0.0076, Discrepancy Loss: 0.0182
Epoch [30/50], Class Loss: 0.0064, Discrepancy Loss: 0.0146
Epoch [31/50], Class Loss: 0.0061, Discrepancy Loss: 0.0116
Epoch [32/50], Class Loss: 0.0175, Discrepancy Loss: 0.0139
Epoch [33/50], Class Loss: 0.0072, Discrepancy Loss: 0.0147
Epoch [34/50], Class Loss: 0.0082, Discrepancy Loss: 0.0120
Epoch [35/50], Class Loss: 0.0285, Discrepancy Loss: 0.0140
Epoch [36/50], Class Loss: 0.0057, Discrepancy Loss: 0.0143
Epoch [37/50], Class Loss: 0.0068, Discrepancy Loss: 0.0120
Epoch [38/50], Class Loss: 0.0066, Discrepancy Loss: 0.0143
Epoch [39/50], Class Loss: 0.0074, Discrepancy Loss: 0.0125
Epoch [40/50], Class Loss: 0.0064, Discrepancy Loss: 0.0117
Epoch [41/50], Class Loss: 0.0070, Discrepancy Loss: 0.0117
Epoch [42/50], Class Loss: 0.0101, Discrepancy Loss: 0.0114
Epoch [43/50], Class Loss: 0.0100, Discrepancy Loss: 0.0136
Epoch [44/50], Class Loss: 0.0088, Discrepancy Loss: 0.0151
Epoch [45/50], Class Loss: 0.0169, Discrepancy Loss: 0.0140
Epoch [46/50], Class Loss: 0.0061, Discrepancy Loss: 0.0137
Epoch [47/50], Class Loss: 0.0969, Discrepancy Loss: 0.0131
Epoch [48/50], Class Loss: 0.0086, Discrepancy Loss: 0.0122
Epoch [49/50], Class Loss: 0.0127, Discrepancy Loss: 0.0148
Epoch [50/50], Class Loss: 0.0074, Discrepancy Loss: 0.0127
Source Domain Performance - Accuracy: 97.84%, Precision: 97.86%, Recall: 97.79%, F1 Score: 97.80%
Target Domain Performance - Accuracy: 85.91%, Precision: 89.14%, Recall: 85.71%, F1 Score: 84.85%

Run 3/3
Epoch [1/50], Class Loss: 1.0231, Discrepancy Loss: 0.0925
Epoch [2/50], Class Loss: 0.1859, Discrepancy Loss: 0.0355
Epoch [3/50], Class Loss: 0.1513, Discrepancy Loss: 0.0276
Epoch [4/50], Class Loss: 0.1595, Discrepancy Loss: 0.0346
Epoch [5/50], Class Loss: 0.0787, Discrepancy Loss: 0.0256
Epoch [6/50], Class Loss: 0.4900, Discrepancy Loss: 0.0545
Epoch [7/50], Class Loss: 0.1336, Discrepancy Loss: 0.0294
Epoch [8/50], Class Loss: 0.1290, Discrepancy Loss: 0.0222
Epoch [9/50], Class Loss: 0.1073, Discrepancy Loss: 0.0257
Epoch [10/50], Class Loss: 0.1356, Discrepancy Loss: 0.0220
Epoch [11/50], Class Loss: 0.0345, Discrepancy Loss: 0.0162
Epoch [12/50], Class Loss: 0.0219, Discrepancy Loss: 0.0152
Epoch [13/50], Class Loss: 0.0184, Discrepancy Loss: 0.0136
Epoch [14/50], Class Loss: 0.0653, Discrepancy Loss: 0.0136
Epoch [15/50], Class Loss: 0.0337, Discrepancy Loss: 0.0190
Epoch [16/50], Class Loss: 0.0236, Discrepancy Loss: 0.0109
Epoch [17/50], Class Loss: 0.0130, Discrepancy Loss: 0.0115
Epoch [18/50], Class Loss: 0.0123, Discrepancy Loss: 0.0093
Epoch [19/50], Class Loss: 0.0197, Discrepancy Loss: 0.0099
Epoch [20/50], Class Loss: 0.0875, Discrepancy Loss: 0.0088
Epoch [21/50], Class Loss: 0.0197, Discrepancy Loss: 0.0156
Epoch [22/50], Class Loss: 0.0466, Discrepancy Loss: 0.0128
Epoch [23/50], Class Loss: 0.0079, Discrepancy Loss: 0.0117
Epoch [24/50], Class Loss: 0.0066, Discrepancy Loss: 0.0119
Epoch [25/50], Class Loss: 0.0092, Discrepancy Loss: 0.0115
Epoch [26/50], Class Loss: 0.0148, Discrepancy Loss: 0.0140
Epoch [27/50], Class Loss: 0.0078, Discrepancy Loss: 0.0118
Epoch [28/50], Class Loss: 0.0190, Discrepancy Loss: 0.0106
Epoch [29/50], Class Loss: 0.0078, Discrepancy Loss: 0.0101
Epoch [30/50], Class Loss: 0.0291, Discrepancy Loss: 0.0105
Epoch [31/50], Class Loss: 0.0134, Discrepancy Loss: 0.0112
Epoch [32/50], Class Loss: 0.0067, Discrepancy Loss: 0.0115
Epoch [33/50], Class Loss: 0.0083, Discrepancy Loss: 0.0105
Epoch [34/50], Class Loss: 0.0071, Discrepancy Loss: 0.0090
Epoch [35/50], Class Loss: 0.0060, Discrepancy Loss: 0.0093
Epoch [36/50], Class Loss: 0.1435, Discrepancy Loss: 0.0107
Epoch [37/50], Class Loss: 0.0075, Discrepancy Loss: 0.0087
Epoch [38/50], Class Loss: 0.0387, Discrepancy Loss: 0.0116
Epoch [39/50], Class Loss: 0.0057, Discrepancy Loss: 0.0102
Epoch [40/50], Class Loss: 0.0052, Discrepancy Loss: 0.0081
Epoch [41/50], Class Loss: 0.0059, Discrepancy Loss: 0.0091
Epoch [42/50], Class Loss: 0.0065, Discrepancy Loss: 0.0109
Epoch [43/50], Class Loss: 0.0100, Discrepancy Loss: 0.0099
Epoch [44/50], Class Loss: 0.0250, Discrepancy Loss: 0.0117
Epoch [45/50], Class Loss: 0.0076, Discrepancy Loss: 0.0091
Epoch [46/50], Class Loss: 0.0105, Discrepancy Loss: 0.0115
Epoch [47/50], Class Loss: 0.0061, Discrepancy Loss: 0.0099
Epoch [48/50], Class Loss: 0.0103, Discrepancy Loss: 0.0086
Epoch [49/50], Class Loss: 0.0371, Discrepancy Loss: 0.0102
Epoch [50/50], Class Loss: 0.0067, Discrepancy Loss: 0.0122
Source Domain Performance - Accuracy: 96.94%, Precision: 96.99%, Recall: 96.92%, F1 Score: 96.88%
Target Domain Performance - Accuracy: 88.43%, Precision: 89.70%, Recall: 88.24%, F1 Score: 87.78%

Source performance: 96.90% 96.96% 96.86% 96.84%
Target performance: 86.61% 89.03% 86.41% 85.65%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 93.93%
16qam: 51.70%
8apsk: 100.00%
MCD

Run 1/3
Epoch [1/50], Class Loss: 0.8307, Discrepancy Loss: 0.0174
Validation Loss: 6.5882
Epoch [2/50], Class Loss: 0.2566, Discrepancy Loss: 0.0061
Validation Loss: 0.2362
Epoch [3/50], Class Loss: 0.1750, Discrepancy Loss: 0.0090
Validation Loss: 0.0300
Epoch [4/50], Class Loss: 0.0702, Discrepancy Loss: 0.0082
Validation Loss: 0.2457
Epoch [5/50], Class Loss: 0.0470, Discrepancy Loss: 0.0062
Validation Loss: 0.1970
Epoch [6/50], Class Loss: 0.0701, Discrepancy Loss: 0.0054
Validation Loss: 0.1957
Epoch [7/50], Class Loss: 0.0368, Discrepancy Loss: 0.0087
Validation Loss: 0.1891
Epoch [8/50], Class Loss: 0.0738, Discrepancy Loss: 0.0096
Validation Loss: 0.5764
Early stopping!
Source Domain Performance - Accuracy: 95.32%, Precision: 95.43%, Recall: 95.22%, F1 Score: 95.22%
Target Domain Performance - Accuracy: 54.44%, Precision: 60.92%, Recall: 53.30%, F1 Score: 44.53%

Run 2/3
Epoch [1/50], Class Loss: 0.9342, Discrepancy Loss: 0.0171
Validation Loss: 12.6384
Epoch [2/50], Class Loss: 0.2479, Discrepancy Loss: 0.0057
Validation Loss: 0.9866
Epoch [3/50], Class Loss: 0.1023, Discrepancy Loss: 0.0047
Validation Loss: 0.0359
Epoch [4/50], Class Loss: 0.0670, Discrepancy Loss: 0.0042
Validation Loss: 0.0510
Epoch [5/50], Class Loss: 0.0861, Discrepancy Loss: 0.0091
Validation Loss: 5.1667
Epoch [6/50], Class Loss: 0.0534, Discrepancy Loss: 0.0086
Validation Loss: 0.0199
Epoch [7/50], Class Loss: 0.0308, Discrepancy Loss: 0.0103
Validation Loss: 1.4757
Epoch [8/50], Class Loss: 0.0792, Discrepancy Loss: 0.0183
Validation Loss: 4.3847
Epoch [9/50], Class Loss: 0.1478, Discrepancy Loss: 0.0069
Validation Loss: 0.4851
Epoch [10/50], Class Loss: 0.0222, Discrepancy Loss: 0.0074
Validation Loss: 0.1537
Epoch [11/50], Class Loss: 0.0041, Discrepancy Loss: 0.0028
Validation Loss: 0.0037
Epoch [12/50], Class Loss: 0.0014, Discrepancy Loss: 0.0027
Validation Loss: 0.0033
Epoch [13/50], Class Loss: 0.0012, Discrepancy Loss: 0.0028
Validation Loss: 0.0031
Epoch [14/50], Class Loss: 0.0013, Discrepancy Loss: 0.0033
Validation Loss: 0.0045
Epoch [15/50], Class Loss: 0.0014, Discrepancy Loss: 0.0033
Validation Loss: 0.0029
Epoch [16/50], Class Loss: 0.0018, Discrepancy Loss: 0.0030
Validation Loss: 0.0031
Epoch [17/50], Class Loss: 0.0010, Discrepancy Loss: 0.0032
Validation Loss: 0.0017
Epoch [18/50], Class Loss: 0.0008, Discrepancy Loss: 0.0036
Validation Loss: 0.0020
Epoch [19/50], Class Loss: 0.0005, Discrepancy Loss: 0.0037
Validation Loss: 0.0015
Epoch [20/50], Class Loss: 0.0013, Discrepancy Loss: 0.0053
Validation Loss: 0.0057
Epoch [21/50], Class Loss: 0.0011, Discrepancy Loss: 0.0052
Validation Loss: 0.0026
Epoch [22/50], Class Loss: 0.0010, Discrepancy Loss: 0.0051
Validation Loss: 0.0017
Epoch [23/50], Class Loss: 0.0010, Discrepancy Loss: 0.0050
Validation Loss: 0.0016
Epoch [24/50], Class Loss: 0.0006, Discrepancy Loss: 0.0051
Validation Loss: 0.0019
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 63.13%, Precision: 63.21%, Recall: 62.33%, F1 Score: 57.22%

Run 3/3
Epoch [1/50], Class Loss: 0.7688, Discrepancy Loss: 0.0271
Validation Loss: 15.5404
Epoch [2/50], Class Loss: 0.1135, Discrepancy Loss: 0.0070
Validation Loss: 2.2751
Epoch [3/50], Class Loss: 0.1008, Discrepancy Loss: 0.0160
Validation Loss: 0.3945
Epoch [4/50], Class Loss: 0.1729, Discrepancy Loss: 0.0088
Validation Loss: 0.0827
Epoch [5/50], Class Loss: 0.0588, Discrepancy Loss: 0.0303
Validation Loss: 0.1135
Epoch [6/50], Class Loss: 0.0360, Discrepancy Loss: 0.0121
Validation Loss: 0.0300
Epoch [7/50], Class Loss: 0.0221, Discrepancy Loss: 0.0087
Validation Loss: 0.7115
Epoch [8/50], Class Loss: 0.0695, Discrepancy Loss: 0.0236
Validation Loss: 0.2298
Epoch [9/50], Class Loss: 0.0288, Discrepancy Loss: 0.0258
Validation Loss: 0.0373
Epoch [10/50], Class Loss: 0.0226, Discrepancy Loss: 0.0122
Validation Loss: 0.0357
Epoch [11/50], Class Loss: 0.0548, Discrepancy Loss: 0.0265
Validation Loss: 0.0090
Epoch [12/50], Class Loss: 0.0281, Discrepancy Loss: 0.1245
Validation Loss: 0.0905
Epoch [13/50], Class Loss: 0.0188, Discrepancy Loss: 0.0714
Validation Loss: 0.0085
Epoch [14/50], Class Loss: 0.0067, Discrepancy Loss: 0.0258
Validation Loss: 0.0066
Epoch [15/50], Class Loss: 0.0058, Discrepancy Loss: 0.0128
Validation Loss: 0.0073
Epoch [16/50], Class Loss: 0.0110, Discrepancy Loss: 0.0134
Validation Loss: 0.0069
Epoch [17/50], Class Loss: 0.0026, Discrepancy Loss: 0.0128
Validation Loss: 0.0055
Epoch [18/50], Class Loss: 0.0024, Discrepancy Loss: 0.0168
Validation Loss: 0.0066
Epoch [19/50], Class Loss: 0.0042, Discrepancy Loss: 0.0145
Validation Loss: 0.0038
Epoch [20/50], Class Loss: 0.0017, Discrepancy Loss: 0.0095
Validation Loss: 0.0047
Epoch [21/50], Class Loss: 0.0018, Discrepancy Loss: 0.0107
Validation Loss: 0.0051
Epoch [22/50], Class Loss: 0.0023, Discrepancy Loss: 0.0102
Validation Loss: 0.0045
Epoch [23/50], Class Loss: 0.0020, Discrepancy Loss: 0.0103
Validation Loss: 0.0056
Epoch [24/50], Class Loss: 0.0024, Discrepancy Loss: 0.0106
Validation Loss: 0.0058
Early stopping!
Source Domain Performance - Accuracy: 99.94%, Precision: 99.94%, Recall: 99.94%, F1 Score: 99.94%
Target Domain Performance - Accuracy: 55.82%, Precision: 60.01%, Recall: 54.74%, F1 Score: 46.89%

Source performance: 98.42% 98.46% 98.39% 98.39%
Target performance: 57.79% 61.38% 56.79% 49.55%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 26.10%
16qam: 1.05%
8apsk: 100.00%
JAN

Run 1/3
Epoch [1/50], Class Loss: 0.4809, JMMD Loss: 0.0751
Validation Loss: 0.4473
Epoch [2/50], Class Loss: 0.2015, JMMD Loss: 0.0757
Validation Loss: 2.9412
Epoch [3/50], Class Loss: 0.1017, JMMD Loss: 0.0720
Validation Loss: 3.3033
Epoch [4/50], Class Loss: 0.0793, JMMD Loss: 0.0635
Validation Loss: 0.0249
Epoch [5/50], Class Loss: 0.0188, JMMD Loss: 0.0531
Validation Loss: 0.1277
Epoch [6/50], Class Loss: 0.0564, JMMD Loss: 0.0739
Validation Loss: 0.0806
Epoch [7/50], Class Loss: 0.0463, JMMD Loss: 0.0657
Validation Loss: 0.1991
Epoch [8/50], Class Loss: 0.0303, JMMD Loss: 0.0717
Validation Loss: 0.0093
Epoch [9/50], Class Loss: 0.0085, JMMD Loss: 0.0600
Validation Loss: 1.2388
Epoch [10/50], Class Loss: 0.0701, JMMD Loss: 0.0627
Validation Loss: 0.6097
Epoch [11/50], Class Loss: 0.0147, JMMD Loss: 0.0704
Validation Loss: 0.0309
Epoch [12/50], Class Loss: 0.0191, JMMD Loss: 0.0678
Validation Loss: 0.0462
Epoch [13/50], Class Loss: 0.0089, JMMD Loss: 0.0718
Validation Loss: 0.0151
Early stopping!
Source Domain Performance - Accuracy: 99.40%, Precision: 99.39%, Recall: 99.39%, F1 Score: 99.39%
Target Domain Performance - Accuracy: 77.04%, Precision: 78.19%, Recall: 76.66%, F1 Score: 73.81%

Run 2/3
Epoch [1/50], Class Loss: 0.4713, JMMD Loss: 0.0930
Validation Loss: 3.4185
Epoch [2/50], Class Loss: 0.2191, JMMD Loss: 0.0999
Validation Loss: 0.3345
Epoch [3/50], Class Loss: 0.1288, JMMD Loss: 0.0704
Validation Loss: 5.1267
Epoch [4/50], Class Loss: 0.0797, JMMD Loss: 0.0898
Validation Loss: 0.4657
Epoch [5/50], Class Loss: 0.0424, JMMD Loss: 0.0755
Validation Loss: 1.8189
Epoch [6/50], Class Loss: 0.0660, JMMD Loss: 0.0925
Validation Loss: 0.0146
Epoch [7/50], Class Loss: 0.0311, JMMD Loss: 0.0865
Validation Loss: 0.0249
Epoch [8/50], Class Loss: 0.0066, JMMD Loss: 0.0699
Validation Loss: 0.1857
Epoch [9/50], Class Loss: 0.0158, JMMD Loss: 0.0714
Validation Loss: 0.0069
Epoch [10/50], Class Loss: 0.0102, JMMD Loss: 0.0637
Validation Loss: 0.7378
Epoch [11/50], Class Loss: 0.0119, JMMD Loss: 0.0653
Validation Loss: 0.0319
Epoch [12/50], Class Loss: 0.0043, JMMD Loss: 0.0668
Validation Loss: 0.0240
Epoch [13/50], Class Loss: 0.0040, JMMD Loss: 0.0664
Validation Loss: 0.0482
Epoch [14/50], Class Loss: 0.0029, JMMD Loss: 0.0642
Validation Loss: 0.0453
Early stopping!
Source Domain Performance - Accuracy: 98.62%, Precision: 98.60%, Recall: 98.62%, F1 Score: 98.59%
Target Domain Performance - Accuracy: 86.27%, Precision: 89.73%, Recall: 86.09%, F1 Score: 85.10%

Run 3/3
Epoch [1/50], Class Loss: 0.4554, JMMD Loss: 0.0839
Validation Loss: 0.7410
Epoch [2/50], Class Loss: 0.1151, JMMD Loss: 0.0642
Validation Loss: 1.0918
Epoch [3/50], Class Loss: 0.0535, JMMD Loss: 0.0815
Validation Loss: 1.9230
Epoch [4/50], Class Loss: 0.0305, JMMD Loss: 0.0826
Validation Loss: 0.5004
Epoch [5/50], Class Loss: 0.0492, JMMD Loss: 0.0622
Validation Loss: 0.1241
Epoch [6/50], Class Loss: 0.0435, JMMD Loss: 0.0591
Validation Loss: 0.0967
Epoch [7/50], Class Loss: 0.1835, JMMD Loss: 0.0869
Validation Loss: 0.1498
Epoch [8/50], Class Loss: 0.0830, JMMD Loss: 0.0772
Validation Loss: 0.1121
Epoch [9/50], Class Loss: 0.0161, JMMD Loss: 0.0698
Validation Loss: 0.0283
Epoch [10/50], Class Loss: 0.0206, JMMD Loss: 0.0712
Validation Loss: 0.0059
Epoch [11/50], Class Loss: 0.0111, JMMD Loss: 0.0694
Validation Loss: 0.0250
Epoch [12/50], Class Loss: 0.0139, JMMD Loss: 0.0688
Validation Loss: 0.0398
Epoch [13/50], Class Loss: 0.0089, JMMD Loss: 0.0727
Validation Loss: 0.0315
Epoch [14/50], Class Loss: 0.0074, JMMD Loss: 0.0739
Validation Loss: 0.0554
Epoch [15/50], Class Loss: 0.0053, JMMD Loss: 0.0799
Validation Loss: 0.0161
Early stopping!
Source Domain Performance - Accuracy: 99.34%, Precision: 99.33%, Recall: 99.33%, F1 Score: 99.33%
Target Domain Performance - Accuracy: 78.90%, Precision: 80.73%, Recall: 78.56%, F1 Score: 76.03%

Source performance: 99.12% 99.11% 99.11% 99.10%
Target performance: 80.74% 82.89% 80.43% 78.31%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 100.00%
  Class 1: 89.78%
  Class 2: 31.96%
  Class 3: 100.00%

SNR level: 14
Base

Run 1/3
Epoch 1/50, Train Loss: 0.4061, Train Acc: 0.8622, Val Loss: 3.5266, Val Acc: 0.5731
Epoch 2/50, Train Loss: 0.0497, Train Acc: 0.9847, Val Loss: 0.1622, Val Acc: 0.9406
Epoch 3/50, Train Loss: 0.0317, Train Acc: 0.9928, Val Loss: 0.0922, Val Acc: 0.9664
Epoch 4/50, Train Loss: 0.0196, Train Acc: 0.9940, Val Loss: 0.2570, Val Acc: 0.8987
Epoch 5/50, Train Loss: 0.0128, Train Acc: 0.9967, Val Loss: 1.8901, Val Acc: 0.6589
Epoch 6/50, Train Loss: 0.0090, Train Acc: 0.9972, Val Loss: 0.0482, Val Acc: 0.9808
Epoch 7/50, Train Loss: 0.0083, Train Acc: 0.9982, Val Loss: 3.4290, Val Acc: 0.6301
Epoch 8/50, Train Loss: 0.0170, Train Acc: 0.9960, Val Loss: 0.0039, Val Acc: 0.9988
Epoch 9/50, Train Loss: 0.0068, Train Acc: 0.9990, Val Loss: 0.0066, Val Acc: 0.9988
Epoch 10/50, Train Loss: 0.0044, Train Acc: 0.9991, Val Loss: 0.0043, Val Acc: 0.9988
Epoch 11/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0020, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0026, Train Acc: 0.9996, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0021, Train Acc: 0.9997, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0013, Val Acc: 0.9994
Epoch 24/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994
Early stopping!

Run 2/3
Epoch 1/50, Train Loss: 0.3510, Train Acc: 0.8811, Val Loss: 1.8390, Val Acc: 0.5354
Epoch 2/50, Train Loss: 0.0577, Train Acc: 0.9819, Val Loss: 0.1080, Val Acc: 0.9628
Epoch 3/50, Train Loss: 0.0367, Train Acc: 0.9882, Val Loss: 0.3241, Val Acc: 0.8765
Epoch 4/50, Train Loss: 0.0148, Train Acc: 0.9961, Val Loss: 0.0093, Val Acc: 0.9982
Epoch 5/50, Train Loss: 0.0168, Train Acc: 0.9952, Val Loss: 0.1775, Val Acc: 0.9365
Epoch 6/50, Train Loss: 0.0157, Train Acc: 0.9948, Val Loss: 0.0174, Val Acc: 0.9946
Epoch 7/50, Train Loss: 0.0078, Train Acc: 0.9975, Val Loss: 1.2012, Val Acc: 0.7920
Epoch 8/50, Train Loss: 0.0104, Train Acc: 0.9972, Val Loss: 0.0054, Val Acc: 0.9976
Epoch 9/50, Train Loss: 0.0047, Train Acc: 0.9984, Val Loss: 0.0293, Val Acc: 0.9910
Epoch 10/50, Train Loss: 0.0165, Train Acc: 0.9949, Val Loss: 0.0050, Val Acc: 0.9988
Epoch 11/50, Train Loss: 0.0035, Train Acc: 0.9993, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 12/50, Train Loss: 0.0010, Train Acc: 1.0000, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0014, Train Acc: 0.9999, Val Loss: 0.0012, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0041, Train Acc: 0.9997, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0017, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0027, Train Acc: 0.9997, Val Loss: 0.0010, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0014, Val Acc: 0.9994
Epoch 19/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0009, Train Acc: 0.9997, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0017, Train Acc: 0.9994, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0007, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 0.9994
Epoch 24/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 0.9994
Epoch 25/50, Train Loss: 0.0011, Train Acc: 0.9999, Val Loss: 0.0006, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0008, Val Acc: 0.9994
Epoch 28/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0008, Val Acc: 0.9994
Epoch 30/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 0.9994
Early stopping!

Run 3/3
Epoch 1/50, Train Loss: 0.3218, Train Acc: 0.8946, Val Loss: 1.6991, Val Acc: 0.6361
Epoch 2/50, Train Loss: 0.0552, Train Acc: 0.9844, Val Loss: 0.4711, Val Acc: 0.7872
Epoch 3/50, Train Loss: 0.0202, Train Acc: 0.9943, Val Loss: 1.6864, Val Acc: 0.6559
Epoch 4/50, Train Loss: 0.0189, Train Acc: 0.9954, Val Loss: 0.0311, Val Acc: 0.9868
Epoch 5/50, Train Loss: 0.0205, Train Acc: 0.9940, Val Loss: 0.5080, Val Acc: 0.8573
Epoch 6/50, Train Loss: 0.0119, Train Acc: 0.9966, Val Loss: 2.1492, Val Acc: 0.5659
Epoch 7/50, Train Loss: 0.0080, Train Acc: 0.9981, Val Loss: 0.0061, Val Acc: 0.9982
Epoch 8/50, Train Loss: 0.0067, Train Acc: 0.9976, Val Loss: 0.3391, Val Acc: 0.8927
Epoch 9/50, Train Loss: 0.0208, Train Acc: 0.9951, Val Loss: 0.6000, Val Acc: 0.8333
Epoch 10/50, Train Loss: 0.0042, Train Acc: 0.9993, Val Loss: 0.0011, Val Acc: 1.0000
Epoch 11/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0009, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0012, Train Acc: 0.9999, Val Loss: 0.0028, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0022, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0010, Train Acc: 0.9999, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9994
Early stopping!

Source performance: 99.94 99.94 99.95 99.94
Target performance: 70.76 84.05 71.18 64.31

bpsk: 100.00
qpsk: 82.51
16qam: 2.21
8apsk: 100.00
DANN
Epoch 1/50, Loss: 2.2247, Domain Loss: 1.3825, Class Loss: 0.8421
Epoch 2/50, Loss: 1.6921, Domain Loss: 1.3226, Class Loss: 0.3694
Epoch 3/50, Loss: 1.5200, Domain Loss: 1.3536, Class Loss: 0.1664
Epoch 4/50, Loss: 2.4223, Domain Loss: 2.2974, Class Loss: 0.1249
Epoch 5/50, Loss: 9.7011, Domain Loss: 9.5074, Class Loss: 0.1936
Epoch 6/50, Loss: 13.1054, Domain Loss: 12.7323, Class Loss: 0.3731
Epoch 7/50, Loss: 5.3534, Domain Loss: 5.1279, Class Loss: 0.2255
Epoch 8/50, Loss: 5.2452, Domain Loss: 4.8515, Class Loss: 0.3937
Epoch 9/50, Loss: 22.3432, Domain Loss: 20.9781, Class Loss: 1.3651
Epoch 10/50, Loss: 42.5459, Domain Loss: 41.5911, Class Loss: 0.9547
Epoch 11/50, Loss: 16.4308, Domain Loss: 15.9170, Class Loss: 0.5138
Epoch 12/50, Loss: 5.3004, Domain Loss: 4.8766, Class Loss: 0.4238
Epoch 13/50, Loss: 7.3353, Domain Loss: 4.9705, Class Loss: 2.3648
Epoch 14/50, Loss: 5.0400, Domain Loss: 4.4245, Class Loss: 0.6154
Epoch 15/50, Loss: 3.5688, Domain Loss: 3.1017, Class Loss: 0.4671
Epoch 16/50, Loss: 3.9687, Domain Loss: 3.4948, Class Loss: 0.4739
Epoch 17/50, Loss: 3.9282, Domain Loss: 3.4612, Class Loss: 0.4670
Epoch 18/50, Loss: 3.5059, Domain Loss: 3.1394, Class Loss: 0.3665
Epoch 19/50, Loss: 4.5830, Domain Loss: 3.8132, Class Loss: 0.7698
Epoch 20/50, Loss: 3.5741, Domain Loss: 2.9780, Class Loss: 0.5961
Epoch 21/50, Loss: 2.7821, Domain Loss: 2.3786, Class Loss: 0.4035
Epoch 22/50, Loss: 2.3616, Domain Loss: 2.0087, Class Loss: 0.3529
Epoch 23/50, Loss: 2.3532, Domain Loss: 1.9833, Class Loss: 0.3699
Epoch 24/50, Loss: 2.3788, Domain Loss: 2.0898, Class Loss: 0.2890
Epoch 25/50, Loss: 2.3857, Domain Loss: 1.9780, Class Loss: 0.4077
Epoch 26/50, Loss: 2.1312, Domain Loss: 1.8293, Class Loss: 0.3019
Epoch 27/50, Loss: 3.3204, Domain Loss: 2.9403, Class Loss: 0.3801
Epoch 28/50, Loss: 2.8458, Domain Loss: 2.5656, Class Loss: 0.2802
Epoch 29/50, Loss: 2.0114, Domain Loss: 1.7792, Class Loss: 0.2322
Epoch 30/50, Loss: 1.7422, Domain Loss: 1.5468, Class Loss: 0.1953
Epoch 31/50, Loss: 1.7585, Domain Loss: 1.5585, Class Loss: 0.2000
Epoch 32/50, Loss: 1.8738, Domain Loss: 1.6716, Class Loss: 0.2022
Epoch 33/50, Loss: 2.0618, Domain Loss: 1.8578, Class Loss: 0.2040
Epoch 34/50, Loss: 2.2502, Domain Loss: 1.9874, Class Loss: 0.2628
Epoch 35/50, Loss: 2.0611, Domain Loss: 1.8397, Class Loss: 0.2214
Epoch 36/50, Loss: 1.8687, Domain Loss: 1.6346, Class Loss: 0.2341
Epoch 37/50, Loss: 1.6473, Domain Loss: 1.4636, Class Loss: 0.1836
Epoch 38/50, Loss: 1.5882, Domain Loss: 1.4482, Class Loss: 0.1399
Epoch 39/50, Loss: 1.5161, Domain Loss: 1.4104, Class Loss: 0.1057
Epoch 40/50, Loss: 1.4843, Domain Loss: 1.3857, Class Loss: 0.0987
Epoch 41/50, Loss: 1.4361, Domain Loss: 1.3753, Class Loss: 0.0608
Epoch 42/50, Loss: 1.4633, Domain Loss: 1.3984, Class Loss: 0.0650
Epoch 43/50, Loss: 1.4599, Domain Loss: 1.3906, Class Loss: 0.0692
Epoch 44/50, Loss: 1.4351, Domain Loss: 1.3872, Class Loss: 0.0479
Epoch 45/50, Loss: 1.4157, Domain Loss: 1.3829, Class Loss: 0.0328
Epoch 46/50, Loss: 1.4163, Domain Loss: 1.3723, Class Loss: 0.0440
Epoch 47/50, Loss: 1.4188, Domain Loss: 1.3876, Class Loss: 0.0312
Epoch 48/50, Loss: 1.4659, Domain Loss: 1.4079, Class Loss: 0.0580
Epoch 49/50, Loss: 1.4454, Domain Loss: 1.3925, Class Loss: 0.0528
Epoch 50/50, Loss: 1.4168, Domain Loss: 1.3829, Class Loss: 0.0339
79.26


Epoch 1/50, Loss: 2.2454, Domain Loss: 1.3611, Class Loss: 0.8843
Epoch 2/50, Loss: 1.6618, Domain Loss: 1.3009, Class Loss: 0.3609
Epoch 3/50, Loss: 1.5942, Domain Loss: 1.3474, Class Loss: 0.2468
Epoch 4/50, Loss: 1.8415, Domain Loss: 1.6894, Class Loss: 0.1521
Epoch 5/50, Loss: 1.6449, Domain Loss: 1.5338, Class Loss: 0.1111
Epoch 6/50, Loss: 1.6561, Domain Loss: 1.5634, Class Loss: 0.0927
Epoch 7/50, Loss: 3.9283, Domain Loss: 3.7800, Class Loss: 0.1483
Epoch 8/50, Loss: 4.1939, Domain Loss: 3.9167, Class Loss: 0.2772
Epoch 9/50, Loss: 4.4693, Domain Loss: 4.1233, Class Loss: 0.3460
Epoch 10/50, Loss: 23.6813, Domain Loss: 22.2164, Class Loss: 1.4649
Epoch 11/50, Loss: 42.4301, Domain Loss: 41.6682, Class Loss: 0.7619
Epoch 12/50, Loss: 25.2097, Domain Loss: 24.5841, Class Loss: 0.6256
Epoch 13/50, Loss: 12.3258, Domain Loss: 11.7400, Class Loss: 0.5857
Epoch 14/50, Loss: 9.8643, Domain Loss: 9.1844, Class Loss: 0.6798
Epoch 15/50, Loss: 9.9488, Domain Loss: 9.0218, Class Loss: 0.9270
Epoch 16/50, Loss: 8.8117, Domain Loss: 8.1962, Class Loss: 0.6156
Epoch 17/50, Loss: 3.6304, Domain Loss: 3.1214, Class Loss: 0.5090
Epoch 18/50, Loss: 2.5891, Domain Loss: 2.1415, Class Loss: 0.4476
Epoch 19/50, Loss: 2.5297, Domain Loss: 2.0548, Class Loss: 0.4749
Epoch 20/50, Loss: 2.4812, Domain Loss: 2.0049, Class Loss: 0.4764
Epoch 21/50, Loss: 2.3721, Domain Loss: 1.9503, Class Loss: 0.4218
Epoch 22/50, Loss: 2.1376, Domain Loss: 1.7290, Class Loss: 0.4086
Epoch 23/50, Loss: 1.9161, Domain Loss: 1.5324, Class Loss: 0.3837
Epoch 24/50, Loss: 1.8204, Domain Loss: 1.4275, Class Loss: 0.3929
Epoch 25/50, Loss: 1.6706, Domain Loss: 1.3405, Class Loss: 0.3301
Epoch 26/50, Loss: 1.7921, Domain Loss: 1.4601, Class Loss: 0.3320
Epoch 27/50, Loss: 2.4354, Domain Loss: 2.0234, Class Loss: 0.4120
Epoch 28/50, Loss: 1.7626, Domain Loss: 1.4977, Class Loss: 0.2649
Epoch 29/50, Loss: 1.7993, Domain Loss: 1.5447, Class Loss: 0.2546
Epoch 30/50, Loss: 1.9461, Domain Loss: 1.6125, Class Loss: 0.3336
Epoch 31/50, Loss: 1.8838, Domain Loss: 1.6060, Class Loss: 0.2779
Epoch 32/50, Loss: 1.7582, Domain Loss: 1.5273, Class Loss: 0.2309
Epoch 33/50, Loss: 1.6953, Domain Loss: 1.4619, Class Loss: 0.2334
Epoch 34/50, Loss: 1.6515, Domain Loss: 1.4730, Class Loss: 0.1785
Epoch 35/50, Loss: 1.6305, Domain Loss: 1.4150, Class Loss: 0.2155
Epoch 36/50, Loss: 1.5837, Domain Loss: 1.4275, Class Loss: 0.1562
Epoch 37/50, Loss: 1.5704, Domain Loss: 1.4264, Class Loss: 0.1440
Epoch 38/50, Loss: 1.5730, Domain Loss: 1.4443, Class Loss: 0.1286
Epoch 39/50, Loss: 1.5066, Domain Loss: 1.4174, Class Loss: 0.0893
Epoch 40/50, Loss: 1.5157, Domain Loss: 1.4242, Class Loss: 0.0916
Epoch 41/50, Loss: 1.6196, Domain Loss: 1.5058, Class Loss: 0.1139
Epoch 42/50, Loss: 1.6478, Domain Loss: 1.5155, Class Loss: 0.1323
Epoch 43/50, Loss: 1.6273, Domain Loss: 1.5107, Class Loss: 0.1166
Epoch 44/50, Loss: 1.6419, Domain Loss: 1.5321, Class Loss: 0.1097
Epoch 45/50, Loss: 1.6653, Domain Loss: 1.5691, Class Loss: 0.0962
Epoch 46/50, Loss: 1.6752, Domain Loss: 1.5908, Class Loss: 0.0843
Epoch 47/50, Loss: 1.6956, Domain Loss: 1.5861, Class Loss: 0.1096
Epoch 48/50, Loss: 1.6394, Domain Loss: 1.5398, Class Loss: 0.0996
Epoch 49/50, Loss: 1.5912, Domain Loss: 1.5064, Class Loss: 0.0849
Epoch 50/50, Loss: 1.5037, Domain Loss: 1.4485, Class Loss: 0.0552
81.53


Epoch 1/50, Loss: 2.2511, Domain Loss: 1.3815, Class Loss: 0.8696
Epoch 2/50, Loss: 1.6021, Domain Loss: 1.3093, Class Loss: 0.2928
Epoch 3/50, Loss: 1.5134, Domain Loss: 1.2888, Class Loss: 0.2246
Epoch 4/50, Loss: 1.6932, Domain Loss: 1.4628, Class Loss: 0.2304
Epoch 5/50, Loss: 7.1592, Domain Loss: 6.9264, Class Loss: 0.2329
Epoch 6/50, Loss: 11.7453, Domain Loss: 11.4915, Class Loss: 0.2539
Epoch 7/50, Loss: 14.0777, Domain Loss: 13.6821, Class Loss: 0.3956
Epoch 8/50, Loss: 13.8806, Domain Loss: 12.2368, Class Loss: 1.6438
Epoch 9/50, Loss: 14.2200, Domain Loss: 13.5656, Class Loss: 0.6543
Epoch 10/50, Loss: 12.8910, Domain Loss: 11.6591, Class Loss: 1.2319
Epoch 11/50, Loss: 20.5432, Domain Loss: 19.2226, Class Loss: 1.3206
Epoch 12/50, Loss: 11.8690, Domain Loss: 11.2131, Class Loss: 0.6558
Epoch 13/50, Loss: 9.2078, Domain Loss: 8.5930, Class Loss: 0.6147
Epoch 14/50, Loss: 13.4251, Domain Loss: 12.8632, Class Loss: 0.5619
Epoch 15/50, Loss: 21.7008, Domain Loss: 20.8598, Class Loss: 0.8410
Epoch 16/50, Loss: 6.3190, Domain Loss: 5.8344, Class Loss: 0.4847
Epoch 17/50, Loss: 6.5888, Domain Loss: 5.3849, Class Loss: 1.2039
Epoch 18/50, Loss: 6.8215, Domain Loss: 5.6082, Class Loss: 1.2133
Epoch 19/50, Loss: 3.4322, Domain Loss: 2.8925, Class Loss: 0.5397
Epoch 20/50, Loss: 2.4808, Domain Loss: 2.0615, Class Loss: 0.4192
Epoch 21/50, Loss: 2.5431, Domain Loss: 2.1271, Class Loss: 0.4159
Epoch 22/50, Loss: 1.8601, Domain Loss: 1.6128, Class Loss: 0.2472
Epoch 23/50, Loss: 1.7776, Domain Loss: 1.5256, Class Loss: 0.2520
Epoch 24/50, Loss: 1.6506, Domain Loss: 1.4552, Class Loss: 0.1954
Epoch 25/50, Loss: 1.6221, Domain Loss: 1.4183, Class Loss: 0.2038
Epoch 26/50, Loss: 1.5908, Domain Loss: 1.4210, Class Loss: 0.1698
Epoch 27/50, Loss: 1.6853, Domain Loss: 1.4895, Class Loss: 0.1958
Epoch 28/50, Loss: 1.6594, Domain Loss: 1.4676, Class Loss: 0.1918
Epoch 29/50, Loss: 1.6082, Domain Loss: 1.4771, Class Loss: 0.1311
Epoch 30/50, Loss: 1.5725, Domain Loss: 1.4004, Class Loss: 0.1721
Epoch 31/50, Loss: 1.5781, Domain Loss: 1.4384, Class Loss: 0.1398
Epoch 32/50, Loss: 1.5589, Domain Loss: 1.4296, Class Loss: 0.1292
Epoch 33/50, Loss: 1.5610, Domain Loss: 1.4280, Class Loss: 0.1330
Epoch 34/50, Loss: 1.5401, Domain Loss: 1.4070, Class Loss: 0.1330
Epoch 35/50, Loss: 1.5621, Domain Loss: 1.4472, Class Loss: 0.1149
Epoch 36/50, Loss: 1.5539, Domain Loss: 1.4425, Class Loss: 0.1114
Epoch 37/50, Loss: 1.5114, Domain Loss: 1.4286, Class Loss: 0.0828
Epoch 38/50, Loss: 1.5642, Domain Loss: 1.4444, Class Loss: 0.1198
Epoch 39/50, Loss: 1.5137, Domain Loss: 1.4351, Class Loss: 0.0786
Epoch 40/50, Loss: 1.5178, Domain Loss: 1.4348, Class Loss: 0.0830
Epoch 41/50, Loss: 1.4994, Domain Loss: 1.4217, Class Loss: 0.0776
Epoch 42/50, Loss: 1.4810, Domain Loss: 1.4180, Class Loss: 0.0630
Epoch 43/50, Loss: 1.5046, Domain Loss: 1.4364, Class Loss: 0.0683
Epoch 44/50, Loss: 1.5605, Domain Loss: 1.4804, Class Loss: 0.0801
Epoch 45/50, Loss: 1.5554, Domain Loss: 1.4834, Class Loss: 0.0720
Epoch 46/50, Loss: 1.5764, Domain Loss: 1.4888, Class Loss: 0.0876
Epoch 47/50, Loss: 1.5582, Domain Loss: 1.4943, Class Loss: 0.0639
Epoch 48/50, Loss: 1.5807, Domain Loss: 1.5272, Class Loss: 0.0536
Epoch 49/50, Loss: 1.6137, Domain Loss: 1.5755, Class Loss: 0.0382
Epoch 50/50, Loss: 1.6916, Domain Loss: 1.6384, Class Loss: 0.0532
78.24


Source performance:
82.25 86.71 82.77 80.86 
Target performance:
79.68 83.43 79.87 78.83 

Per-class target performance: 100.00 76.60 68.79 74.10 Deep CORAL
Deep CORAL Run 1/3
Epoch 1: Source Val Acc = 0.7206, Target Val Acc = 0.5701
Epoch 2: Source Val Acc = 0.8393, Target Val Acc = 0.5869
Epoch 3: Source Val Acc = 0.9616, Target Val Acc = 0.8207
Epoch 4: Source Val Acc = 0.9970, Target Val Acc = 0.7680
Epoch 5: Source Val Acc = 0.7860, Target Val Acc = 0.9215
Epoch 6: Source Val Acc = 0.5084, Target Val Acc = 0.7602
Epoch 7: Source Val Acc = 0.9442, Target Val Acc = 0.8261
Epoch 8: Source Val Acc = 0.9826, Target Val Acc = 0.8207
Epoch 9: Source Val Acc = 0.8891, Target Val Acc = 0.8909
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.8891, Target Val Acc = 0.8909

Deep CORAL Run 2/3
Epoch 1: Source Val Acc = 0.7242, Target Val Acc = 0.6115
Epoch 2: Source Val Acc = 0.7764, Target Val Acc = 0.7392
Epoch 3: Source Val Acc = 0.9562, Target Val Acc = 0.5342
Epoch 4: Source Val Acc = 0.9940, Target Val Acc = 0.6865
Epoch 5: Source Val Acc = 0.9988, Target Val Acc = 0.7566
Epoch 6: Source Val Acc = 0.5210, Target Val Acc = 0.7494
Epoch 7: Source Val Acc = 0.7236, Target Val Acc = 0.8309
Epoch 8: Source Val Acc = 0.9928, Target Val Acc = 0.8495
Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.8297
Epoch 10: Source Val Acc = 0.9982, Target Val Acc = 0.7920
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9982, Target Val Acc = 0.7920

Deep CORAL Run 3/3
Epoch 1: Source Val Acc = 0.7122, Target Val Acc = 0.7926
Epoch 2: Source Val Acc = 0.9173, Target Val Acc = 0.8147
Epoch 3: Source Val Acc = 0.8237, Target Val Acc = 0.8663
Epoch 4: Source Val Acc = 0.6948, Target Val Acc = 0.7842
Epoch 5: Source Val Acc = 0.9808, Target Val Acc = 0.8010
Epoch 6: Source Val Acc = 0.9844, Target Val Acc = 0.6247
Epoch 7: Source Val Acc = 0.7560, Target Val Acc = 0.5504
Epoch 8: Source Val Acc = 0.9952, Target Val Acc = 0.9137
Epoch 9: Source Val Acc = 0.7782, Target Val Acc = 0.8621
Epoch 10: Source Val Acc = 0.9946, Target Val Acc = 0.9029
Epoch 11: Source Val Acc = 0.9898, Target Val Acc = 0.9059
Epoch 12: Source Val Acc = 1.0000, Target Val Acc = 0.7806
Epoch 13: Source Val Acc = 0.9946, Target Val Acc = 0.9005
Epoch 14: Source Val Acc = 0.9994, Target Val Acc = 0.7746
Epoch 15: Source Val Acc = 0.9562, Target Val Acc = 0.5288
Epoch 16: Source Val Acc = 0.9994, Target Val Acc = 0.6235
Epoch 17: Source Val Acc = 0.8555, Target Val Acc = 0.9574
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.8555, Target Val Acc = 0.9574

Deep CORAL: Average Source Val Acc = 0.9143, Average Target Val Acc = 0.8801
STAR

Run 1/3
Epoch [1/50], Class Loss: 0.9483, Discrepancy Loss: 0.0763
Epoch [2/50], Class Loss: 0.2911, Discrepancy Loss: 0.0485
Epoch [3/50], Class Loss: 0.1299, Discrepancy Loss: 0.0380
Epoch [4/50], Class Loss: 0.1891, Discrepancy Loss: 0.0406
Epoch [5/50], Class Loss: 0.1754, Discrepancy Loss: 0.0470
Epoch [6/50], Class Loss: 0.0915, Discrepancy Loss: 0.0386
Epoch [7/50], Class Loss: 0.0741, Discrepancy Loss: 0.0200
Epoch [8/50], Class Loss: 0.0125, Discrepancy Loss: 0.0152
Epoch [9/50], Class Loss: 0.0087, Discrepancy Loss: 0.0137
Epoch [10/50], Class Loss: 0.0286, Discrepancy Loss: 0.0132
Epoch [11/50], Class Loss: 0.0349, Discrepancy Loss: 0.0136
Epoch [12/50], Class Loss: 0.0173, Discrepancy Loss: 0.0177
Epoch [13/50], Class Loss: 0.0036, Discrepancy Loss: 0.0108
Epoch [14/50], Class Loss: 0.0036, Discrepancy Loss: 0.0120
Epoch [15/50], Class Loss: 0.0111, Discrepancy Loss: 0.0106
Epoch [16/50], Class Loss: 0.0125, Discrepancy Loss: 0.0134
Epoch [17/50], Class Loss: 0.0038, Discrepancy Loss: 0.0107
Epoch [18/50], Class Loss: 0.0034, Discrepancy Loss: 0.0102
Epoch [19/50], Class Loss: 0.0021, Discrepancy Loss: 0.0108
Epoch [20/50], Class Loss: 0.0026, Discrepancy Loss: 0.0095
Epoch [21/50], Class Loss: 0.0158, Discrepancy Loss: 0.0110
Epoch [22/50], Class Loss: 0.0055, Discrepancy Loss: 0.0096
Epoch [23/50], Class Loss: 0.0008, Discrepancy Loss: 0.0102
Epoch [24/50], Class Loss: 0.0659, Discrepancy Loss: 0.0094
Epoch [25/50], Class Loss: 0.0032, Discrepancy Loss: 0.0115
Epoch [26/50], Class Loss: 0.0052, Discrepancy Loss: 0.0105
Epoch [27/50], Class Loss: 0.0636, Discrepancy Loss: 0.0091
Epoch [28/50], Class Loss: 0.0016, Discrepancy Loss: 0.0123
Epoch [29/50], Class Loss: 0.0016, Discrepancy Loss: 0.0101
Epoch [30/50], Class Loss: 0.0055, Discrepancy Loss: 0.0098
Epoch [31/50], Class Loss: 0.0155, Discrepancy Loss: 0.0093
Epoch [32/50], Class Loss: 0.0042, Discrepancy Loss: 0.0105
Epoch [33/50], Class Loss: 0.0024, Discrepancy Loss: 0.0106
Epoch [34/50], Class Loss: 0.0023, Discrepancy Loss: 0.0110
Epoch [35/50], Class Loss: 0.0092, Discrepancy Loss: 0.0092
Epoch [36/50], Class Loss: 0.0019, Discrepancy Loss: 0.0088
Epoch [37/50], Class Loss: 0.0052, Discrepancy Loss: 0.0118
Epoch [38/50], Class Loss: 0.0019, Discrepancy Loss: 0.0100
Epoch [39/50], Class Loss: 0.0021, Discrepancy Loss: 0.0102
Epoch [40/50], Class Loss: 0.0151, Discrepancy Loss: 0.0114
Epoch [41/50], Class Loss: 0.0425, Discrepancy Loss: 0.0115
Epoch [42/50], Class Loss: 0.0022, Discrepancy Loss: 0.0124
Epoch [43/50], Class Loss: 0.0027, Discrepancy Loss: 0.0111
Epoch [44/50], Class Loss: 0.0051, Discrepancy Loss: 0.0100
Epoch [45/50], Class Loss: 0.0057, Discrepancy Loss: 0.0099
Epoch [46/50], Class Loss: 0.0061, Discrepancy Loss: 0.0104
Epoch [47/50], Class Loss: 0.0990, Discrepancy Loss: 0.0114
Epoch [48/50], Class Loss: 0.0016, Discrepancy Loss: 0.0108
Epoch [49/50], Class Loss: 0.0371, Discrepancy Loss: 0.0090
Epoch [50/50], Class Loss: 0.0042, Discrepancy Loss: 0.0097
Source Domain Performance - Accuracy: 98.32%, Precision: 98.53%, Recall: 98.26%, F1 Score: 98.35%
Target Domain Performance - Accuracy: 90.41%, Precision: 92.52%, Recall: 90.54%, F1 Score: 90.14%

Run 2/3
Epoch [1/50], Class Loss: 1.4131, Discrepancy Loss: 0.0979
Epoch [2/50], Class Loss: 0.4619, Discrepancy Loss: 0.0849
Epoch [3/50], Class Loss: 0.2663, Discrepancy Loss: 0.0554
Epoch [4/50], Class Loss: 0.1540, Discrepancy Loss: 0.0412
Epoch [5/50], Class Loss: 0.0372, Discrepancy Loss: 0.0295
Epoch [6/50], Class Loss: 0.0657, Discrepancy Loss: 0.0337
Epoch [7/50], Class Loss: 0.0454, Discrepancy Loss: 0.0245
Epoch [8/50], Class Loss: 0.2042, Discrepancy Loss: 0.0330
Epoch [9/50], Class Loss: 0.0503, Discrepancy Loss: 0.0263
Epoch [10/50], Class Loss: 0.0327, Discrepancy Loss: 0.0248
Epoch [11/50], Class Loss: 0.0152, Discrepancy Loss: 0.0213
Epoch [12/50], Class Loss: 0.0197, Discrepancy Loss: 0.0213
Epoch [13/50], Class Loss: 0.0109, Discrepancy Loss: 0.0202
Epoch [14/50], Class Loss: 0.0244, Discrepancy Loss: 0.0195
Epoch [15/50], Class Loss: 0.0157, Discrepancy Loss: 0.0197
Epoch [16/50], Class Loss: 0.0428, Discrepancy Loss: 0.0237
Epoch [17/50], Class Loss: 0.0367, Discrepancy Loss: 0.0245
Epoch [18/50], Class Loss: 0.0070, Discrepancy Loss: 0.0233
Epoch [19/50], Class Loss: 0.0082, Discrepancy Loss: 0.0178
Epoch [20/50], Class Loss: 0.0082, Discrepancy Loss: 0.0200
Epoch [21/50], Class Loss: 0.1621, Discrepancy Loss: 0.0198
Epoch [22/50], Class Loss: 0.0069, Discrepancy Loss: 0.0201
Epoch [23/50], Class Loss: 0.0091, Discrepancy Loss: 0.0188
Epoch [24/50], Class Loss: 0.0052, Discrepancy Loss: 0.0180
Epoch [25/50], Class Loss: 0.0095, Discrepancy Loss: 0.0199
Epoch [26/50], Class Loss: 0.0072, Discrepancy Loss: 0.0201
Epoch [27/50], Class Loss: 0.0319, Discrepancy Loss: 0.0194
Epoch [28/50], Class Loss: 0.0056, Discrepancy Loss: 0.0190
Epoch [29/50], Class Loss: 0.0064, Discrepancy Loss: 0.0191
Epoch [30/50], Class Loss: 0.0173, Discrepancy Loss: 0.0185
Epoch [31/50], Class Loss: 0.0090, Discrepancy Loss: 0.0167
Epoch [32/50], Class Loss: 0.0053, Discrepancy Loss: 0.0176
Epoch [33/50], Class Loss: 0.0028, Discrepancy Loss: 0.0201
Epoch [34/50], Class Loss: 0.0072, Discrepancy Loss: 0.0168
Epoch [35/50], Class Loss: 0.0035, Discrepancy Loss: 0.0188
Epoch [36/50], Class Loss: 0.0162, Discrepancy Loss: 0.0151
Epoch [37/50], Class Loss: 0.0077, Discrepancy Loss: 0.0135
Epoch [38/50], Class Loss: 0.0080, Discrepancy Loss: 0.0176
Epoch [39/50], Class Loss: 0.0032, Discrepancy Loss: 0.0179
Epoch [40/50], Class Loss: 0.0044, Discrepancy Loss: 0.0161
Epoch [41/50], Class Loss: 0.0038, Discrepancy Loss: 0.0186
Epoch [42/50], Class Loss: 0.0060, Discrepancy Loss: 0.0175
Epoch [43/50], Class Loss: 0.0057, Discrepancy Loss: 0.0171
Epoch [44/50], Class Loss: 0.0052, Discrepancy Loss: 0.0184
Epoch [45/50], Class Loss: 0.0131, Discrepancy Loss: 0.0171
Epoch [46/50], Class Loss: 0.0038, Discrepancy Loss: 0.0167
Epoch [47/50], Class Loss: 0.0040, Discrepancy Loss: 0.0205
Epoch [48/50], Class Loss: 0.0044, Discrepancy Loss: 0.0193
Epoch [49/50], Class Loss: 0.0039, Discrepancy Loss: 0.0172
Epoch [50/50], Class Loss: 0.0250, Discrepancy Loss: 0.0154
Source Domain Performance - Accuracy: 99.82%, Precision: 99.83%, Recall: 99.82%, F1 Score: 99.83%
Target Domain Performance - Accuracy: 87.77%, Precision: 90.27%, Recall: 87.94%, F1 Score: 87.36%

Run 3/3
Epoch [1/50], Class Loss: 1.1031, Discrepancy Loss: 0.1141
Epoch [2/50], Class Loss: 0.3835, Discrepancy Loss: 0.0575
Epoch [3/50], Class Loss: 0.3576, Discrepancy Loss: 0.0579
Epoch [4/50], Class Loss: 0.2955, Discrepancy Loss: 0.0690
Epoch [5/50], Class Loss: 0.0794, Discrepancy Loss: 0.0396
Epoch [6/50], Class Loss: 0.0586, Discrepancy Loss: 0.0375
Epoch [7/50], Class Loss: 0.1395, Discrepancy Loss: 0.0268
Epoch [8/50], Class Loss: 0.1601, Discrepancy Loss: 0.0397
Epoch [9/50], Class Loss: 0.0366, Discrepancy Loss: 0.0266
Epoch [10/50], Class Loss: 0.1036, Discrepancy Loss: 0.0351
Epoch [11/50], Class Loss: 0.0366, Discrepancy Loss: 0.0230
Epoch [12/50], Class Loss: 0.0141, Discrepancy Loss: 0.0238
Epoch [13/50], Class Loss: 0.0119, Discrepancy Loss: 0.0235
Epoch [14/50], Class Loss: 0.1305, Discrepancy Loss: 0.0266
Epoch [15/50], Class Loss: 0.0376, Discrepancy Loss: 0.0292
Epoch [16/50], Class Loss: 0.0131, Discrepancy Loss: 0.0242
Epoch [17/50], Class Loss: 0.0067, Discrepancy Loss: 0.0248
Epoch [18/50], Class Loss: 0.0841, Discrepancy Loss: 0.0181
Epoch [19/50], Class Loss: 0.1026, Discrepancy Loss: 0.0241
Epoch [20/50], Class Loss: 0.1297, Discrepancy Loss: 0.0300
Epoch [21/50], Class Loss: 0.0289, Discrepancy Loss: 0.0325
Epoch [22/50], Class Loss: 0.0223, Discrepancy Loss: 0.0263
Epoch [23/50], Class Loss: 0.0210, Discrepancy Loss: 0.0329
Epoch [24/50], Class Loss: 0.0220, Discrepancy Loss: 0.0332
Epoch [25/50], Class Loss: 0.0117, Discrepancy Loss: 0.0288
Epoch [26/50], Class Loss: 0.0231, Discrepancy Loss: 0.0242
Epoch [27/50], Class Loss: 0.0095, Discrepancy Loss: 0.0286
Epoch [28/50], Class Loss: 0.0176, Discrepancy Loss: 0.0247
Epoch [29/50], Class Loss: 0.0114, Discrepancy Loss: 0.0269
Epoch [30/50], Class Loss: 0.0183, Discrepancy Loss: 0.0211
Epoch [31/50], Class Loss: 0.0110, Discrepancy Loss: 0.0276
Epoch [32/50], Class Loss: 0.0070, Discrepancy Loss: 0.0293
Epoch [33/50], Class Loss: 0.0102, Discrepancy Loss: 0.0251
Epoch [34/50], Class Loss: 0.0912, Discrepancy Loss: 0.0263
Epoch [35/50], Class Loss: 0.0213, Discrepancy Loss: 0.0260
Epoch [36/50], Class Loss: 0.0251, Discrepancy Loss: 0.0251
Epoch [37/50], Class Loss: 0.1174, Discrepancy Loss: 0.0290
Epoch [38/50], Class Loss: 0.0105, Discrepancy Loss: 0.0276
Epoch [39/50], Class Loss: 0.0202, Discrepancy Loss: 0.0264
Epoch [40/50], Class Loss: 0.0102, Discrepancy Loss: 0.0260
Epoch [41/50], Class Loss: 0.0075, Discrepancy Loss: 0.0238
Epoch [42/50], Class Loss: 0.0219, Discrepancy Loss: 0.0256
Epoch [43/50], Class Loss: 0.0079, Discrepancy Loss: 0.0291
Epoch [44/50], Class Loss: 0.0243, Discrepancy Loss: 0.0269
Epoch [45/50], Class Loss: 0.0098, Discrepancy Loss: 0.0207
Epoch [46/50], Class Loss: 0.0072, Discrepancy Loss: 0.0282
Epoch [47/50], Class Loss: 0.0140, Discrepancy Loss: 0.0223
Epoch [48/50], Class Loss: 0.0089, Discrepancy Loss: 0.0256
Epoch [49/50], Class Loss: 0.0181, Discrepancy Loss: 0.0250
Epoch [50/50], Class Loss: 0.0060, Discrepancy Loss: 0.0279
Source Domain Performance - Accuracy: 99.10%, Precision: 99.17%, Recall: 99.09%, F1 Score: 99.13%
Target Domain Performance - Accuracy: 87.77%, Precision: 91.47%, Recall: 87.94%, F1 Score: 87.09%

Source performance: 99.08% 99.18% 99.06% 99.10%
Target performance: 88.65% 91.42% 88.81% 88.20%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 98.35%
16qam: 57.37%
8apsk: 99.52%
MCD

Run 1/3
Epoch [1/50], Class Loss: 0.5794, Discrepancy Loss: 0.0166
Validation Loss: 7.3358
Epoch [2/50], Class Loss: 0.0836, Discrepancy Loss: 0.0154
Validation Loss: 22.4868
Epoch [3/50], Class Loss: 0.0630, Discrepancy Loss: 0.0120
Validation Loss: 0.0535
Epoch [4/50], Class Loss: 0.0422, Discrepancy Loss: 0.0104
Validation Loss: 11.1788
Epoch [5/50], Class Loss: 0.0320, Discrepancy Loss: 0.0185
Validation Loss: 0.0887
Epoch [6/50], Class Loss: 0.0353, Discrepancy Loss: 0.0159
Validation Loss: 0.0159
Epoch [7/50], Class Loss: 0.0175, Discrepancy Loss: 0.0160
Validation Loss: 0.0194
Epoch [8/50], Class Loss: 0.0265, Discrepancy Loss: 0.0373
Validation Loss: 0.3120
Epoch [9/50], Class Loss: 0.0344, Discrepancy Loss: 0.0249
Validation Loss: 0.0849
Epoch [10/50], Class Loss: 0.0172, Discrepancy Loss: 0.0375
Validation Loss: 0.3016
Epoch [11/50], Class Loss: 0.0048, Discrepancy Loss: 0.0117
Validation Loss: 0.0150
Epoch [12/50], Class Loss: 0.0022, Discrepancy Loss: 0.0051
Validation Loss: 0.0141
Epoch [13/50], Class Loss: 0.0011, Discrepancy Loss: 0.0059
Validation Loss: 0.0149
Epoch [14/50], Class Loss: 0.0013, Discrepancy Loss: 0.0089
Validation Loss: 0.0136
Epoch [15/50], Class Loss: 0.0014, Discrepancy Loss: 0.0151
Validation Loss: 0.0095
Epoch [16/50], Class Loss: 0.0048, Discrepancy Loss: 0.0297
Validation Loss: 0.0186
Epoch [17/50], Class Loss: 0.0048, Discrepancy Loss: 0.0274
Validation Loss: 0.0092
Epoch [18/50], Class Loss: 0.0038, Discrepancy Loss: 0.0180
Validation Loss: 0.0106
Epoch [19/50], Class Loss: 0.0031, Discrepancy Loss: 0.0156
Validation Loss: 0.0079
Epoch [20/50], Class Loss: 0.0036, Discrepancy Loss: 0.0272
Validation Loss: 0.0108
Epoch [21/50], Class Loss: 0.0059, Discrepancy Loss: 0.0358
Validation Loss: 0.0143
Epoch [22/50], Class Loss: 0.0062, Discrepancy Loss: 0.0340
Validation Loss: 0.0115
Epoch [23/50], Class Loss: 0.0058, Discrepancy Loss: 0.0332
Validation Loss: 0.0111
Epoch [24/50], Class Loss: 0.0063, Discrepancy Loss: 0.0320
Validation Loss: 0.0122
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.89%, Recall: 99.89%, F1 Score: 99.89%
Target Domain Performance - Accuracy: 69.12%, Precision: 76.96%, Recall: 69.56%, F1 Score: 67.75%

Run 2/3
Epoch [1/50], Class Loss: 0.6796, Discrepancy Loss: 0.0203
Validation Loss: 5.7380
Epoch [2/50], Class Loss: 0.0705, Discrepancy Loss: 0.0099
Validation Loss: 0.1048
Epoch [3/50], Class Loss: 0.0427, Discrepancy Loss: 0.0176
Validation Loss: 6.2838
Epoch [4/50], Class Loss: 0.0548, Discrepancy Loss: 0.0390
Validation Loss: 0.2472
Epoch [5/50], Class Loss: 0.0760, Discrepancy Loss: 0.0399
Validation Loss: 1.3314
Epoch [6/50], Class Loss: 0.0195, Discrepancy Loss: 0.0489
Validation Loss: 0.0612
Epoch [7/50], Class Loss: 0.0322, Discrepancy Loss: 0.0610
Validation Loss: 0.0367
Epoch [8/50], Class Loss: 0.0325, Discrepancy Loss: 0.0288
Validation Loss: 0.0100
Epoch [9/50], Class Loss: 0.0362, Discrepancy Loss: 0.0549
Validation Loss: 0.0229
Epoch [10/50], Class Loss: 0.0337, Discrepancy Loss: 0.0907
Validation Loss: 0.0337
Epoch [11/50], Class Loss: 0.0013, Discrepancy Loss: 0.0347
Validation Loss: 0.0026
Epoch [12/50], Class Loss: 0.0009, Discrepancy Loss: 0.0096
Validation Loss: 0.0015
Epoch [13/50], Class Loss: 0.0007, Discrepancy Loss: 0.0097
Validation Loss: 0.0069
Epoch [14/50], Class Loss: 0.0005, Discrepancy Loss: 0.0113
Validation Loss: 0.0028
Epoch [15/50], Class Loss: 0.0004, Discrepancy Loss: 0.0158
Validation Loss: 0.0035
Epoch [16/50], Class Loss: 0.0173, Discrepancy Loss: 0.0208
Validation Loss: 0.0040
Epoch [17/50], Class Loss: 0.0151, Discrepancy Loss: 0.0285
Validation Loss: 0.0081
Early stopping!
Source Domain Performance - Accuracy: 99.88%, Precision: 99.89%, Recall: 99.89%, F1 Score: 99.89%
Target Domain Performance - Accuracy: 68.82%, Precision: 76.90%, Recall: 69.27%, F1 Score: 66.88%

Run 3/3
Epoch [1/50], Class Loss: 0.6410, Discrepancy Loss: 0.0250
Validation Loss: 4.4110
Epoch [2/50], Class Loss: 0.1981, Discrepancy Loss: 0.0136
Validation Loss: 12.7830
Epoch [3/50], Class Loss: 0.0548, Discrepancy Loss: 0.0154
Validation Loss: 2.5948
Epoch [4/50], Class Loss: 0.1176, Discrepancy Loss: 0.0162
Validation Loss: 21.0848
Epoch [5/50], Class Loss: 0.0596, Discrepancy Loss: 0.0084
Validation Loss: 0.0128
Epoch [6/50], Class Loss: 0.0186, Discrepancy Loss: 0.0045
Validation Loss: 0.0204
Epoch [7/50], Class Loss: 0.0242, Discrepancy Loss: 0.0062
Validation Loss: 5.4673
Epoch [8/50], Class Loss: 0.0705, Discrepancy Loss: 0.0089
Validation Loss: 0.0268
Epoch [9/50], Class Loss: 0.0527, Discrepancy Loss: 0.0108
Validation Loss: 0.1877
Epoch [10/50], Class Loss: 0.0479, Discrepancy Loss: 0.0089
Validation Loss: 11.6683
Early stopping!
Source Domain Performance - Accuracy: 50.48%, Precision: 64.10%, Recall: 51.55%, F1 Score: 43.34%
Target Domain Performance - Accuracy: 76.50%, Precision: 87.80%, Recall: 76.50%, F1 Score: 70.10%

Source performance: 83.41% 87.96% 83.77% 81.04%
Target performance: 71.48% 80.56% 71.78% 68.24%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 59.50%
16qam: 58.71%
8apsk: 68.90%
JAN

Run 1/3
Epoch [1/50], Class Loss: 0.3545, JMMD Loss: 0.0706
Validation Loss: 1.5770
Epoch [2/50], Class Loss: 0.1424, JMMD Loss: 0.0789
Validation Loss: 3.6838
Epoch [3/50], Class Loss: 0.0312, JMMD Loss: 0.0885
Validation Loss: 0.0112
Epoch [4/50], Class Loss: 0.0486, JMMD Loss: 0.0913
Validation Loss: 0.0406
Epoch [5/50], Class Loss: 0.0297, JMMD Loss: 0.0925
Validation Loss: 1.1227
Epoch [6/50], Class Loss: 0.0250, JMMD Loss: 0.0908
Validation Loss: 0.0453
Epoch [7/50], Class Loss: 0.0416, JMMD Loss: 0.1242
Validation Loss: 1.0295
Epoch [8/50], Class Loss: 0.0098, JMMD Loss: 0.1040
Validation Loss: 0.0998
Early stopping!
Source Domain Performance - Accuracy: 95.80%, Precision: 96.30%, Recall: 95.78%, F1 Score: 95.90%
Target Domain Performance - Accuracy: 94.90%, Precision: 94.94%, Recall: 94.95%, F1 Score: 94.91%

Run 2/3
Epoch [1/50], Class Loss: 0.3696, JMMD Loss: 0.0545
Validation Loss: 0.7790
Epoch [2/50], Class Loss: 0.0677, JMMD Loss: 0.0915
Validation Loss: 0.1549
Epoch [3/50], Class Loss: 0.0377, JMMD Loss: 0.0985
Validation Loss: 0.4311
Epoch [4/50], Class Loss: 0.1319, JMMD Loss: 0.0578
Validation Loss: 0.3998
Epoch [5/50], Class Loss: 0.0313, JMMD Loss: 0.0725
Validation Loss: 0.0251
Epoch [6/50], Class Loss: 0.0907, JMMD Loss: 0.0979
Validation Loss: 6.9293
Epoch [7/50], Class Loss: 0.2293, JMMD Loss: 0.1129
Validation Loss: 3.1328
Epoch [8/50], Class Loss: 0.0962, JMMD Loss: 0.1154
Validation Loss: 0.1032
Epoch [9/50], Class Loss: 0.0372, JMMD Loss: 0.1167
Validation Loss: 0.0808
Epoch [10/50], Class Loss: 0.0183, JMMD Loss: 0.1000
Validation Loss: 0.1326
Early stopping!
Source Domain Performance - Accuracy: 95.02%, Precision: 95.65%, Recall: 95.32%, F1 Score: 95.26%
Target Domain Performance - Accuracy: 86.63%, Precision: 87.39%, Recall: 86.79%, F1 Score: 86.68%

Run 3/3
Epoch [1/50], Class Loss: 0.3917, JMMD Loss: 0.0798
Validation Loss: 2.1318
Epoch [2/50], Class Loss: 0.1850, JMMD Loss: 0.0945
Validation Loss: 0.0962
Epoch [3/50], Class Loss: 0.0329, JMMD Loss: 0.1010
Validation Loss: 0.2090
Epoch [4/50], Class Loss: 0.0623, JMMD Loss: 0.0893
Validation Loss: 3.4949
Epoch [5/50], Class Loss: 0.0611, JMMD Loss: 0.1038
Validation Loss: 0.5628
Epoch [6/50], Class Loss: 0.1501, JMMD Loss: 0.1048
Validation Loss: 1.1873
Epoch [7/50], Class Loss: 0.0216, JMMD Loss: 0.1022
Validation Loss: 0.1057
Early stopping!
Source Domain Performance - Accuracy: 96.04%, Precision: 96.53%, Recall: 96.00%, F1 Score: 96.13%
Target Domain Performance - Accuracy: 89.27%, Precision: 90.85%, Recall: 89.41%, F1 Score: 89.14%

Source performance: 95.62% 96.16% 95.70% 95.76%
Target performance: 90.27% 91.06% 90.39% 90.24%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 100.00%
  Class 1: 94.72%
  Class 2: 72.58%
  Class 3: 94.24%

SNR level: 18
Base

Run 1/3
Epoch 1/50, Train Loss: 0.5185, Train Acc: 0.7854, Val Loss: 2.1184, Val Acc: 0.5594
Epoch 2/50, Train Loss: 0.1256, Train Acc: 0.9592, Val Loss: 0.0380, Val Acc: 0.9868
Epoch 3/50, Train Loss: 0.0434, Train Acc: 0.9865, Val Loss: 0.9690, Val Acc: 0.7902
Epoch 4/50, Train Loss: 0.0185, Train Acc: 0.9945, Val Loss: 1.3059, Val Acc: 0.7662
Epoch 5/50, Train Loss: 0.0172, Train Acc: 0.9955, Val Loss: 2.8517, Val Acc: 0.7602
Epoch 6/50, Train Loss: 0.0076, Train Acc: 0.9979, Val Loss: 2.4995, Val Acc: 0.6385
Epoch 7/50, Train Loss: 0.0075, Train Acc: 0.9985, Val Loss: 1.3041, Val Acc: 0.7824
Early stopping!

Run 2/3
Epoch 1/50, Train Loss: 0.5514, Train Acc: 0.7683, Val Loss: 1.3401, Val Acc: 0.6547
Epoch 2/50, Train Loss: 0.1033, Train Acc: 0.9651, Val Loss: 0.9866, Val Acc: 0.7506
Epoch 3/50, Train Loss: 0.0559, Train Acc: 0.9835, Val Loss: 0.9515, Val Acc: 0.7584
Epoch 4/50, Train Loss: 0.0234, Train Acc: 0.9936, Val Loss: 0.1486, Val Acc: 0.9418
Epoch 5/50, Train Loss: 0.0245, Train Acc: 0.9924, Val Loss: 0.6273, Val Acc: 0.8669
Epoch 6/50, Train Loss: 0.0349, Train Acc: 0.9910, Val Loss: 0.0507, Val Acc: 0.9814
Epoch 7/50, Train Loss: 0.0108, Train Acc: 0.9970, Val Loss: 0.9645, Val Acc: 0.7548
Epoch 8/50, Train Loss: 0.0026, Train Acc: 0.9999, Val Loss: 0.0196, Val Acc: 0.9934
Epoch 9/50, Train Loss: 0.0063, Train Acc: 0.9987, Val Loss: 0.1040, Val Acc: 0.9616
Epoch 10/50, Train Loss: 0.0032, Train Acc: 0.9993, Val Loss: 0.7455, Val Acc: 0.8333
Epoch 11/50, Train Loss: 0.0020, Train Acc: 0.9996, Val Loss: 0.0051, Val Acc: 0.9982
Epoch 12/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 13/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0020, Val Acc: 0.9994
Epoch 15/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0020, Val Acc: 0.9988
Epoch 18/50, Train Loss: 0.0030, Train Acc: 0.9996, Val Loss: 0.0014, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0017, Train Acc: 0.9997, Val Loss: 0.0026, Val Acc: 0.9994
Epoch 20/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0019, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 22/50, Train Loss: 0.0005, Train Acc: 0.9999, Val Loss: 0.0015, Val Acc: 0.9994
Epoch 23/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0018, Val Acc: 0.9994
Early stopping!

Run 3/3
Epoch 1/50, Train Loss: 0.4530, Train Acc: 0.8260, Val Loss: 0.7225, Val Acc: 0.8207
Epoch 2/50, Train Loss: 0.0942, Train Acc: 0.9702, Val Loss: 0.1252, Val Acc: 0.9586
Epoch 3/50, Train Loss: 0.0314, Train Acc: 0.9918, Val Loss: 0.0264, Val Acc: 0.9898
Epoch 4/50, Train Loss: 0.0160, Train Acc: 0.9961, Val Loss: 0.1343, Val Acc: 0.9574
Epoch 5/50, Train Loss: 0.0058, Train Acc: 0.9985, Val Loss: 1.0733, Val Acc: 0.7290
Epoch 6/50, Train Loss: 0.0213, Train Acc: 0.9930, Val Loss: 0.4770, Val Acc: 0.8663
Epoch 7/50, Train Loss: 0.0191, Train Acc: 0.9943, Val Loss: 0.0324, Val Acc: 0.9892
Epoch 8/50, Train Loss: 0.0118, Train Acc: 0.9966, Val Loss: 0.1529, Val Acc: 0.9472
Early stopping!

Source performance: 90.97 94.42 90.62 88.66
Target performance: 82.29 81.73 81.97 78.10

bpsk: 100.00
qpsk: 99.92
16qam: 67.09
8apsk: 60.86
DANN
Epoch 1/50, Loss: 2.2977, Domain Loss: 1.3732, Class Loss: 0.9245
Epoch 2/50, Loss: 1.8645, Domain Loss: 1.3089, Class Loss: 0.5556
Epoch 3/50, Loss: 1.8766, Domain Loss: 1.3622, Class Loss: 0.5143
Epoch 4/50, Loss: 2.3981, Domain Loss: 1.9366, Class Loss: 0.4615
Epoch 5/50, Loss: 4.3750, Domain Loss: 3.9346, Class Loss: 0.4404
Epoch 6/50, Loss: 2.8872, Domain Loss: 2.4036, Class Loss: 0.4836
Epoch 7/50, Loss: 9.5499, Domain Loss: 8.8962, Class Loss: 0.6537
Epoch 8/50, Loss: 18.6224, Domain Loss: 17.7161, Class Loss: 0.9063
Epoch 9/50, Loss: 20.4417, Domain Loss: 19.4695, Class Loss: 0.9722
Epoch 10/50, Loss: 9.6910, Domain Loss: 8.9815, Class Loss: 0.7095
Epoch 11/50, Loss: 10.2864, Domain Loss: 9.7081, Class Loss: 0.5784
Epoch 12/50, Loss: 3.6280, Domain Loss: 3.0506, Class Loss: 0.5774
Epoch 13/50, Loss: 2.7403, Domain Loss: 2.0670, Class Loss: 0.6733
Epoch 14/50, Loss: 3.2227, Domain Loss: 2.5958, Class Loss: 0.6269
Epoch 15/50, Loss: 3.0394, Domain Loss: 2.4651, Class Loss: 0.5743
Epoch 16/50, Loss: 2.3797, Domain Loss: 1.8568, Class Loss: 0.5229
Epoch 17/50, Loss: 2.0472, Domain Loss: 1.5331, Class Loss: 0.5142
Epoch 18/50, Loss: 1.8659, Domain Loss: 1.4030, Class Loss: 0.4630
Epoch 19/50, Loss: 1.8333, Domain Loss: 1.4021, Class Loss: 0.4311
Epoch 20/50, Loss: 1.8040, Domain Loss: 1.3917, Class Loss: 0.4124
Epoch 21/50, Loss: 1.6940, Domain Loss: 1.3682, Class Loss: 0.3259
Epoch 22/50, Loss: 1.6917, Domain Loss: 1.3690, Class Loss: 0.3227
Epoch 23/50, Loss: 1.6618, Domain Loss: 1.3722, Class Loss: 0.2896
Epoch 24/50, Loss: 1.6562, Domain Loss: 1.3742, Class Loss: 0.2819
Epoch 25/50, Loss: 1.5921, Domain Loss: 1.3645, Class Loss: 0.2276
Epoch 26/50, Loss: 1.6236, Domain Loss: 1.3961, Class Loss: 0.2275
Epoch 27/50, Loss: 1.5695, Domain Loss: 1.3746, Class Loss: 0.1950
Epoch 28/50, Loss: 1.5780, Domain Loss: 1.3818, Class Loss: 0.1962
Epoch 29/50, Loss: 1.5145, Domain Loss: 1.3849, Class Loss: 0.1296
Epoch 30/50, Loss: 1.5066, Domain Loss: 1.3953, Class Loss: 0.1113
Epoch 31/50, Loss: 2.3490, Domain Loss: 1.5481, Class Loss: 0.8009
Epoch 32/50, Loss: 1.8548, Domain Loss: 1.4940, Class Loss: 0.3608
Epoch 33/50, Loss: 1.8007, Domain Loss: 1.4832, Class Loss: 0.3175
Epoch 34/50, Loss: 1.6197, Domain Loss: 1.4081, Class Loss: 0.2115
Epoch 35/50, Loss: 1.5564, Domain Loss: 1.3889, Class Loss: 0.1675
Epoch 36/50, Loss: 1.4881, Domain Loss: 1.3791, Class Loss: 0.1090
Epoch 37/50, Loss: 1.4955, Domain Loss: 1.3966, Class Loss: 0.0989
Epoch 38/50, Loss: 1.4807, Domain Loss: 1.4001, Class Loss: 0.0806
Epoch 39/50, Loss: 1.5305, Domain Loss: 1.3976, Class Loss: 0.1328
Epoch 40/50, Loss: 1.5364, Domain Loss: 1.4150, Class Loss: 0.1215
Epoch 41/50, Loss: 1.4634, Domain Loss: 1.3904, Class Loss: 0.0731
Epoch 42/50, Loss: 1.4775, Domain Loss: 1.3976, Class Loss: 0.0799
Epoch 43/50, Loss: 1.4757, Domain Loss: 1.3935, Class Loss: 0.0822
Epoch 44/50, Loss: 1.4705, Domain Loss: 1.4095, Class Loss: 0.0610
Epoch 45/50, Loss: 1.4860, Domain Loss: 1.4267, Class Loss: 0.0593
Epoch 46/50, Loss: 1.4771, Domain Loss: 1.4250, Class Loss: 0.0520
Epoch 47/50, Loss: 1.4691, Domain Loss: 1.4251, Class Loss: 0.0441
Epoch 48/50, Loss: 1.4884, Domain Loss: 1.4419, Class Loss: 0.0465
Epoch 49/50, Loss: 1.5277, Domain Loss: 1.4544, Class Loss: 0.0733
Epoch 50/50, Loss: 1.4959, Domain Loss: 1.4269, Class Loss: 0.0691
75.90


Epoch 1/50, Loss: 2.3334, Domain Loss: 1.3789, Class Loss: 0.9546
Epoch 2/50, Loss: 1.9300, Domain Loss: 1.3229, Class Loss: 0.6072
Epoch 3/50, Loss: 1.8036, Domain Loss: 1.3210, Class Loss: 0.4826
Epoch 4/50, Loss: 1.8936, Domain Loss: 1.5608, Class Loss: 0.3328
Epoch 5/50, Loss: 3.4362, Domain Loss: 3.1045, Class Loss: 0.3317
Epoch 6/50, Loss: 3.8062, Domain Loss: 3.4925, Class Loss: 0.3136
Epoch 7/50, Loss: 9.0974, Domain Loss: 8.7486, Class Loss: 0.3488
Epoch 8/50, Loss: 3.5563, Domain Loss: 3.3221, Class Loss: 0.2342
Epoch 9/50, Loss: 3.5619, Domain Loss: 2.9994, Class Loss: 0.5626
Epoch 10/50, Loss: 2.7551, Domain Loss: 2.3248, Class Loss: 0.4303
Epoch 11/50, Loss: 1.9114, Domain Loss: 1.6728, Class Loss: 0.2387
Epoch 12/50, Loss: 1.6513, Domain Loss: 1.4697, Class Loss: 0.1816
Epoch 13/50, Loss: 1.8054, Domain Loss: 1.6514, Class Loss: 0.1540
Epoch 14/50, Loss: 1.7702, Domain Loss: 1.6445, Class Loss: 0.1257
Epoch 15/50, Loss: 1.6015, Domain Loss: 1.4839, Class Loss: 0.1176
Epoch 16/50, Loss: 1.6900, Domain Loss: 1.5492, Class Loss: 0.1408
Epoch 17/50, Loss: 1.9294, Domain Loss: 1.8061, Class Loss: 0.1233
Epoch 18/50, Loss: 3.5215, Domain Loss: 3.2991, Class Loss: 0.2224
Epoch 19/50, Loss: 2.6183, Domain Loss: 2.2429, Class Loss: 0.3754
Epoch 20/50, Loss: 2.3121, Domain Loss: 1.8569, Class Loss: 0.4552
Epoch 21/50, Loss: 2.3469, Domain Loss: 1.9304, Class Loss: 0.4165
Epoch 22/50, Loss: 3.2166, Domain Loss: 2.7295, Class Loss: 0.4871
Epoch 23/50, Loss: 2.7245, Domain Loss: 2.3630, Class Loss: 0.3615
Epoch 24/50, Loss: 2.1609, Domain Loss: 1.8416, Class Loss: 0.3193
Epoch 25/50, Loss: 4.3067, Domain Loss: 2.5294, Class Loss: 1.7773
Epoch 26/50, Loss: 4.7716, Domain Loss: 3.2621, Class Loss: 1.5096
Epoch 27/50, Loss: 2.5666, Domain Loss: 1.9607, Class Loss: 0.6058
Epoch 28/50, Loss: 2.7533, Domain Loss: 2.1436, Class Loss: 0.6097
Epoch 29/50, Loss: 2.5857, Domain Loss: 2.0382, Class Loss: 0.5475
Epoch 30/50, Loss: 1.9491, Domain Loss: 1.5383, Class Loss: 0.4108
Epoch 31/50, Loss: 3.0264, Domain Loss: 2.2206, Class Loss: 0.8058
Epoch 32/50, Loss: 2.9641, Domain Loss: 2.3547, Class Loss: 0.6095
Epoch 33/50, Loss: 2.2766, Domain Loss: 1.7121, Class Loss: 0.5645
Epoch 34/50, Loss: 1.9841, Domain Loss: 1.5155, Class Loss: 0.4686
Epoch 35/50, Loss: 2.1821, Domain Loss: 1.6641, Class Loss: 0.5180
Epoch 36/50, Loss: 2.3956, Domain Loss: 1.8639, Class Loss: 0.5318
Epoch 37/50, Loss: 2.0215, Domain Loss: 1.5474, Class Loss: 0.4741
Epoch 38/50, Loss: 2.2030, Domain Loss: 1.6155, Class Loss: 0.5875
Epoch 39/50, Loss: 1.8945, Domain Loss: 1.4294, Class Loss: 0.4651
Epoch 40/50, Loss: 2.2757, Domain Loss: 1.8212, Class Loss: 0.4546
Epoch 41/50, Loss: 2.1386, Domain Loss: 1.7481, Class Loss: 0.3905
Epoch 42/50, Loss: 1.7969, Domain Loss: 1.5430, Class Loss: 0.2538
Epoch 43/50, Loss: 1.6642, Domain Loss: 1.4506, Class Loss: 0.2137
Epoch 44/50, Loss: 1.5956, Domain Loss: 1.4133, Class Loss: 0.1823
Epoch 45/50, Loss: 1.5321, Domain Loss: 1.4061, Class Loss: 0.1260
Epoch 46/50, Loss: 1.5652, Domain Loss: 1.4296, Class Loss: 0.1356
Epoch 47/50, Loss: 1.5411, Domain Loss: 1.4187, Class Loss: 0.1224
Epoch 48/50, Loss: 1.4867, Domain Loss: 1.3976, Class Loss: 0.0891
Epoch 49/50, Loss: 1.5547, Domain Loss: 1.4463, Class Loss: 0.1084
Epoch 50/50, Loss: 1.4835, Domain Loss: 1.3974, Class Loss: 0.0861
79.08


Epoch 1/50, Loss: 2.3068, Domain Loss: 1.3811, Class Loss: 0.9256
Epoch 2/50, Loss: 1.9313, Domain Loss: 1.3336, Class Loss: 0.5978
Epoch 3/50, Loss: 1.7874, Domain Loss: 1.3608, Class Loss: 0.4267
Epoch 4/50, Loss: 1.8194, Domain Loss: 1.5290, Class Loss: 0.2904
Epoch 5/50, Loss: 7.0313, Domain Loss: 6.7593, Class Loss: 0.2720
Epoch 6/50, Loss: 4.7228, Domain Loss: 4.5340, Class Loss: 0.1888
Epoch 7/50, Loss: 3.1158, Domain Loss: 2.9529, Class Loss: 0.1630
Epoch 8/50, Loss: 7.4254, Domain Loss: 7.1086, Class Loss: 0.3168
Epoch 9/50, Loss: 12.5682, Domain Loss: 11.7346, Class Loss: 0.8336
Epoch 10/50, Loss: 26.7108, Domain Loss: 25.7099, Class Loss: 1.0009
Epoch 11/50, Loss: 12.0353, Domain Loss: 11.3684, Class Loss: 0.6670
Epoch 12/50, Loss: 5.6625, Domain Loss: 5.0781, Class Loss: 0.5844
Epoch 13/50, Loss: 3.0534, Domain Loss: 2.4640, Class Loss: 0.5894
Epoch 14/50, Loss: 2.5769, Domain Loss: 2.0368, Class Loss: 0.5400
Epoch 15/50, Loss: 2.2583, Domain Loss: 1.7145, Class Loss: 0.5438
Epoch 16/50, Loss: 2.3464, Domain Loss: 1.8199, Class Loss: 0.5265
Epoch 17/50, Loss: 1.8589, Domain Loss: 1.4373, Class Loss: 0.4216
Epoch 18/50, Loss: 1.9168, Domain Loss: 1.5146, Class Loss: 0.4022
Epoch 19/50, Loss: 2.7695, Domain Loss: 2.2474, Class Loss: 0.5222
Epoch 20/50, Loss: 2.4858, Domain Loss: 2.0368, Class Loss: 0.4490
Epoch 21/50, Loss: 2.7379, Domain Loss: 2.1736, Class Loss: 0.5643
Epoch 22/50, Loss: 2.2267, Domain Loss: 1.7237, Class Loss: 0.5030
Epoch 23/50, Loss: 2.2280, Domain Loss: 1.7461, Class Loss: 0.4819
Epoch 24/50, Loss: 2.2654, Domain Loss: 1.8416, Class Loss: 0.4238
Epoch 25/50, Loss: 2.1027, Domain Loss: 1.6524, Class Loss: 0.4503
Epoch 26/50, Loss: 2.0537, Domain Loss: 1.6400, Class Loss: 0.4137
Epoch 27/50, Loss: 1.8685, Domain Loss: 1.5153, Class Loss: 0.3533
Epoch 28/50, Loss: 1.8450, Domain Loss: 1.4901, Class Loss: 0.3549
Epoch 29/50, Loss: 1.6995, Domain Loss: 1.4049, Class Loss: 0.2945
Epoch 30/50, Loss: 1.7060, Domain Loss: 1.4292, Class Loss: 0.2768
Epoch 31/50, Loss: 1.6935, Domain Loss: 1.3435, Class Loss: 0.3500
Epoch 32/50, Loss: 1.6187, Domain Loss: 1.3669, Class Loss: 0.2519
Epoch 33/50, Loss: 1.7083, Domain Loss: 1.4319, Class Loss: 0.2764
Epoch 34/50, Loss: 1.5954, Domain Loss: 1.4113, Class Loss: 0.1841
Epoch 35/50, Loss: 1.5212, Domain Loss: 1.3935, Class Loss: 0.1277
Epoch 36/50, Loss: 1.4682, Domain Loss: 1.3908, Class Loss: 0.0774
Epoch 37/50, Loss: 1.4552, Domain Loss: 1.3851, Class Loss: 0.0701
Epoch 38/50, Loss: 1.4730, Domain Loss: 1.3681, Class Loss: 0.1049
Epoch 39/50, Loss: 1.5113, Domain Loss: 1.3917, Class Loss: 0.1195
Epoch 40/50, Loss: 1.4290, Domain Loss: 1.3701, Class Loss: 0.0588
Epoch 41/50, Loss: 1.4334, Domain Loss: 1.3713, Class Loss: 0.0621
Epoch 42/50, Loss: 1.4504, Domain Loss: 1.3845, Class Loss: 0.0660
Epoch 43/50, Loss: 1.4739, Domain Loss: 1.4086, Class Loss: 0.0653
Epoch 44/50, Loss: 1.4707, Domain Loss: 1.4149, Class Loss: 0.0558
Epoch 45/50, Loss: 1.4870, Domain Loss: 1.4259, Class Loss: 0.0611
Epoch 46/50, Loss: 1.5062, Domain Loss: 1.4585, Class Loss: 0.0476
Epoch 47/50, Loss: 1.5368, Domain Loss: 1.4427, Class Loss: 0.0941
Epoch 48/50, Loss: 1.4614, Domain Loss: 1.4081, Class Loss: 0.0533
Epoch 49/50, Loss: 1.4770, Domain Loss: 1.4245, Class Loss: 0.0525
Epoch 50/50, Loss: 1.5084, Domain Loss: 1.4444, Class Loss: 0.0639
75.12


Source performance:
85.73 89.05 85.52 84.67 
Target performance:
76.70 79.09 76.33 75.35 

Deep CORALtarget performance: 99.33 91.13 66.00 48.85 
Deep CORAL Run 1/3
Epoch 1: Source Val Acc = 0.7212, Target Val Acc = 0.7278
Epoch 2: Source Val Acc = 0.9598, Target Val Acc = 0.7842
Epoch 3: Source Val Acc = 0.9448, Target Val Acc = 0.8657
Epoch 4: Source Val Acc = 0.5827, Target Val Acc = 0.6313
Epoch 5: Source Val Acc = 0.9586, Target Val Acc = 0.8663
Epoch 6: Source Val Acc = 0.7206, Target Val Acc = 0.5803
Epoch 7: Source Val Acc = 0.9616, Target Val Acc = 0.8034
Epoch 8: Source Val Acc = 0.9700, Target Val Acc = 0.7980
Epoch 9: Source Val Acc = 0.9766, Target Val Acc = 0.8645
Epoch 10: Source Val Acc = 0.9904, Target Val Acc = 0.8645
Epoch 11: Source Val Acc = 0.9988, Target Val Acc = 0.9257
Epoch 12: Source Val Acc = 0.9952, Target Val Acc = 0.8837
Epoch 13: Source Val Acc = 1.0000, Target Val Acc = 0.9454
Epoch 14: Source Val Acc = 0.9994, Target Val Acc = 0.9412
Epoch 15: Source Val Acc = 1.0000, Target Val Acc = 0.9466
Epoch 16: Source Val Acc = 0.9994, Target Val Acc = 0.9436
Epoch 17: Source Val Acc = 1.0000, Target Val Acc = 0.9424
Epoch 18: Source Val Acc = 0.9994, Target Val Acc = 0.9460
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.9994, Target Val Acc = 0.9460

Deep CORAL Run 2/3
Epoch 1: Source Val Acc = 0.7422, Target Val Acc = 0.6433
Epoch 2: Source Val Acc = 0.8219, Target Val Acc = 0.7212
Epoch 3: Source Val Acc = 0.7566, Target Val Acc = 0.7224
Epoch 4: Source Val Acc = 0.6763, Target Val Acc = 0.5084
Epoch 5: Source Val Acc = 0.9371, Target Val Acc = 0.8028
Epoch 6: Source Val Acc = 0.7638, Target Val Acc = 0.7356
Epoch 7: Source Val Acc = 0.9173, Target Val Acc = 0.8321
Epoch 8: Source Val Acc = 0.8867, Target Val Acc = 0.7872
Epoch 9: Source Val Acc = 0.9269, Target Val Acc = 0.8369
Epoch 10: Source Val Acc = 0.9988, Target Val Acc = 0.9113
Epoch 11: Source Val Acc = 0.9790, Target Val Acc = 0.8825
Epoch 12: Source Val Acc = 0.9436, Target Val Acc = 0.8231
Epoch 13: Source Val Acc = 0.6715, Target Val Acc = 0.6571
Epoch 14: Source Val Acc = 0.9982, Target Val Acc = 0.9041
Epoch 15: Source Val Acc = 0.9269, Target Val Acc = 0.8243
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9269, Target Val Acc = 0.8243

Deep CORAL Run 3/3
Epoch 1: Source Val Acc = 0.7098, Target Val Acc = 0.6445
Epoch 2: Source Val Acc = 0.7554, Target Val Acc = 0.6529
Epoch 3: Source Val Acc = 0.7656, Target Val Acc = 0.7374
Epoch 4: Source Val Acc = 0.9400, Target Val Acc = 0.8082
Epoch 5: Source Val Acc = 0.7722, Target Val Acc = 0.7620
Epoch 6: Source Val Acc = 0.9442, Target Val Acc = 0.7806
Epoch 7: Source Val Acc = 0.9868, Target Val Acc = 0.8759
Epoch 8: Source Val Acc = 0.9910, Target Val Acc = 0.8939
Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.8885
Epoch 10: Source Val Acc = 0.8801, Target Val Acc = 0.7830
Epoch 11: Source Val Acc = 0.9982, Target Val Acc = 0.9059
Epoch 12: Source Val Acc = 0.8945, Target Val Acc = 0.8369
Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 0.9053
Epoch 14: Source Val Acc = 0.9994, Target Val Acc = 0.9113
Epoch 15: Source Val Acc = 0.9994, Target Val Acc = 0.9299
Epoch 16: Source Val Acc = 0.5546, Target Val Acc = 0.6457
Epoch 17: Source Val Acc = 0.9706, Target Val Acc = 0.8561
Epoch 18: Source Val Acc = 0.9844, Target Val Acc = 0.8453
Epoch 19: Source Val Acc = 0.9832, Target Val Acc = 0.8891
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.9832, Target Val Acc = 0.8891

Deep CORAL: Average Source Val Acc = 0.9698, Average Target Val Acc = 0.8865
STAR

Run 1/3
Epoch [1/50], Class Loss: 1.5615, Discrepancy Loss: 0.1029
Epoch [2/50], Class Loss: 0.9666, Discrepancy Loss: 0.0818
Epoch [3/50], Class Loss: 0.5711, Discrepancy Loss: 0.0636
Epoch [4/50], Class Loss: 0.3416, Discrepancy Loss: 0.0474
Epoch [5/50], Class Loss: 0.4846, Discrepancy Loss: 0.0524
Epoch [6/50], Class Loss: 0.3425, Discrepancy Loss: 0.0431
Epoch [7/50], Class Loss: 0.1235, Discrepancy Loss: 0.0325
Epoch [8/50], Class Loss: 0.0715, Discrepancy Loss: 0.0218
Epoch [9/50], Class Loss: 0.1945, Discrepancy Loss: 0.0302
Epoch [10/50], Class Loss: 0.0401, Discrepancy Loss: 0.0238
Epoch [11/50], Class Loss: 0.1056, Discrepancy Loss: 0.0292
Epoch [12/50], Class Loss: 0.0179, Discrepancy Loss: 0.0238
Epoch [13/50], Class Loss: 0.0101, Discrepancy Loss: 0.0221
Epoch [14/50], Class Loss: 0.0110, Discrepancy Loss: 0.0170
Epoch [15/50], Class Loss: 0.0071, Discrepancy Loss: 0.0187
Epoch [16/50], Class Loss: 0.0068, Discrepancy Loss: 0.0217
Epoch [17/50], Class Loss: 0.0135, Discrepancy Loss: 0.0164
Epoch [18/50], Class Loss: 0.0203, Discrepancy Loss: 0.0208
Epoch [19/50], Class Loss: 0.0039, Discrepancy Loss: 0.0158
Epoch [20/50], Class Loss: 0.0056, Discrepancy Loss: 0.0165
Epoch [21/50], Class Loss: 0.0051, Discrepancy Loss: 0.0169
Epoch [22/50], Class Loss: 0.0050, Discrepancy Loss: 0.0181
Epoch [23/50], Class Loss: 0.0089, Discrepancy Loss: 0.0157
Epoch [24/50], Class Loss: 0.0035, Discrepancy Loss: 0.0194
Epoch [25/50], Class Loss: 0.0073, Discrepancy Loss: 0.0183
Epoch [26/50], Class Loss: 0.0043, Discrepancy Loss: 0.0158
Epoch [27/50], Class Loss: 0.0058, Discrepancy Loss: 0.0138
Epoch [28/50], Class Loss: 0.0044, Discrepancy Loss: 0.0135
Epoch [29/50], Class Loss: 0.0049, Discrepancy Loss: 0.0149
Epoch [30/50], Class Loss: 0.0030, Discrepancy Loss: 0.0145
Epoch [31/50], Class Loss: 0.0042, Discrepancy Loss: 0.0176
Epoch [32/50], Class Loss: 0.0742, Discrepancy Loss: 0.0132
Epoch [33/50], Class Loss: 0.0128, Discrepancy Loss: 0.0173
Epoch [34/50], Class Loss: 0.0041, Discrepancy Loss: 0.0136
Epoch [35/50], Class Loss: 0.0039, Discrepancy Loss: 0.0158
Epoch [36/50], Class Loss: 0.0024, Discrepancy Loss: 0.0180
Epoch [37/50], Class Loss: 0.0029, Discrepancy Loss: 0.0174
Epoch [38/50], Class Loss: 0.0049, Discrepancy Loss: 0.0169
Epoch [39/50], Class Loss: 0.0022, Discrepancy Loss: 0.0174
Epoch [40/50], Class Loss: 0.0036, Discrepancy Loss: 0.0196
Epoch [41/50], Class Loss: 0.0087, Discrepancy Loss: 0.0153
Epoch [42/50], Class Loss: 0.0174, Discrepancy Loss: 0.0166
Epoch [43/50], Class Loss: 0.0027, Discrepancy Loss: 0.0186
Epoch [44/50], Class Loss: 0.0180, Discrepancy Loss: 0.0158
Epoch [45/50], Class Loss: 0.0029, Discrepancy Loss: 0.0145
Epoch [46/50], Class Loss: 0.0026, Discrepancy Loss: 0.0153
Epoch [47/50], Class Loss: 0.0057, Discrepancy Loss: 0.0177
Epoch [48/50], Class Loss: 0.0077, Discrepancy Loss: 0.0164
Epoch [49/50], Class Loss: 0.1712, Discrepancy Loss: 0.0160
Epoch [50/50], Class Loss: 0.0156, Discrepancy Loss: 0.0167
Source Domain Performance - Accuracy: 98.08%, Precision: 98.14%, Recall: 98.06%, F1 Score: 98.04%
Target Domain Performance - Accuracy: 89.81%, Precision: 90.83%, Recall: 89.39%, F1 Score: 89.15%

Run 2/3
Epoch [1/50], Class Loss: 1.2161, Discrepancy Loss: 0.1141
Epoch [2/50], Class Loss: 0.9240, Discrepancy Loss: 0.0921
Epoch [3/50], Class Loss: 0.6935, Discrepancy Loss: 0.0847
Epoch [4/50], Class Loss: 0.4735, Discrepancy Loss: 0.0785
Epoch [5/50], Class Loss: 0.4633, Discrepancy Loss: 0.0781
Epoch [6/50], Class Loss: 0.3014, Discrepancy Loss: 0.0639
Epoch [7/50], Class Loss: 0.2591, Discrepancy Loss: 0.0551
Epoch [8/50], Class Loss: 0.5624, Discrepancy Loss: 0.0891
Epoch [9/50], Class Loss: 0.1701, Discrepancy Loss: 0.0504
Epoch [10/50], Class Loss: 0.1972, Discrepancy Loss: 0.0452
Epoch [11/50], Class Loss: 0.0686, Discrepancy Loss: 0.0356
Epoch [12/50], Class Loss: 0.0900, Discrepancy Loss: 0.0389
Epoch [13/50], Class Loss: 0.1393, Discrepancy Loss: 0.0326
Epoch [14/50], Class Loss: 0.0552, Discrepancy Loss: 0.0387
Epoch [15/50], Class Loss: 0.0576, Discrepancy Loss: 0.0325
Epoch [16/50], Class Loss: 0.0488, Discrepancy Loss: 0.0389
Epoch [17/50], Class Loss: 0.0403, Discrepancy Loss: 0.0350
Epoch [18/50], Class Loss: 0.0444, Discrepancy Loss: 0.0341
Epoch [19/50], Class Loss: 0.0290, Discrepancy Loss: 0.0289
Epoch [20/50], Class Loss: 0.0706, Discrepancy Loss: 0.0292
Epoch [21/50], Class Loss: 0.0622, Discrepancy Loss: 0.0242
Epoch [22/50], Class Loss: 0.0555, Discrepancy Loss: 0.0289
Epoch [23/50], Class Loss: 0.0252, Discrepancy Loss: 0.0260
Epoch [24/50], Class Loss: 0.0914, Discrepancy Loss: 0.0260
Epoch [25/50], Class Loss: 0.0280, Discrepancy Loss: 0.0270
Epoch [26/50], Class Loss: 0.0261, Discrepancy Loss: 0.0244
Epoch [27/50], Class Loss: 0.0186, Discrepancy Loss: 0.0284
Epoch [28/50], Class Loss: 0.0165, Discrepancy Loss: 0.0263
Epoch [29/50], Class Loss: 0.0175, Discrepancy Loss: 0.0263
Epoch [30/50], Class Loss: 0.0166, Discrepancy Loss: 0.0265
Epoch [31/50], Class Loss: 0.0161, Discrepancy Loss: 0.0240
Epoch [32/50], Class Loss: 0.0182, Discrepancy Loss: 0.0211
Epoch [33/50], Class Loss: 0.0210, Discrepancy Loss: 0.0230
Epoch [34/50], Class Loss: 0.0941, Discrepancy Loss: 0.0234
Epoch [35/50], Class Loss: 0.0205, Discrepancy Loss: 0.0210
Epoch [36/50], Class Loss: 0.0167, Discrepancy Loss: 0.0253
Epoch [37/50], Class Loss: 0.0399, Discrepancy Loss: 0.0247
Epoch [38/50], Class Loss: 0.0197, Discrepancy Loss: 0.0276
Epoch [39/50], Class Loss: 0.0462, Discrepancy Loss: 0.0227
Epoch [40/50], Class Loss: 0.0189, Discrepancy Loss: 0.0192
Epoch [41/50], Class Loss: 0.0141, Discrepancy Loss: 0.0253
Epoch [42/50], Class Loss: 0.0163, Discrepancy Loss: 0.0237
Epoch [43/50], Class Loss: 0.0224, Discrepancy Loss: 0.0213
Epoch [44/50], Class Loss: 0.0258, Discrepancy Loss: 0.0236
Epoch [45/50], Class Loss: 0.0191, Discrepancy Loss: 0.0251
Epoch [46/50], Class Loss: 0.0296, Discrepancy Loss: 0.0240
Epoch [47/50], Class Loss: 0.0625, Discrepancy Loss: 0.0255
Epoch [48/50], Class Loss: 0.0245, Discrepancy Loss: 0.0237
Epoch [49/50], Class Loss: 0.0304, Discrepancy Loss: 0.0234
Epoch [50/50], Class Loss: 0.0390, Discrepancy Loss: 0.0232
Source Domain Performance - Accuracy: 95.98%, Precision: 96.36%, Recall: 95.94%, F1 Score: 95.89%
Target Domain Performance - Accuracy: 87.47%, Precision: 87.15%, Recall: 87.11%, F1 Score: 86.95%

Run 3/3
Epoch [1/50], Class Loss: 1.3587, Discrepancy Loss: 0.1099
Epoch [2/50], Class Loss: 0.7863, Discrepancy Loss: 0.0840
Epoch [3/50], Class Loss: 0.6891, Discrepancy Loss: 0.0844
Epoch [4/50], Class Loss: 0.3147, Discrepancy Loss: 0.0580
Epoch [5/50], Class Loss: 0.8022, Discrepancy Loss: 0.0948
Epoch [6/50], Class Loss: 0.1791, Discrepancy Loss: 0.0574
Epoch [7/50], Class Loss: 0.2340, Discrepancy Loss: 0.0374
Epoch [8/50], Class Loss: 0.2027, Discrepancy Loss: 0.0406
Epoch [9/50], Class Loss: 0.0511, Discrepancy Loss: 0.0279
Epoch [10/50], Class Loss: 0.4341, Discrepancy Loss: 0.0580
Epoch [11/50], Class Loss: 0.2211, Discrepancy Loss: 0.0619
Epoch [12/50], Class Loss: 0.1443, Discrepancy Loss: 0.0479
Epoch [13/50], Class Loss: 0.1449, Discrepancy Loss: 0.0405
Epoch [14/50], Class Loss: 0.0893, Discrepancy Loss: 0.0358
Epoch [15/50], Class Loss: 0.0754, Discrepancy Loss: 0.0330
Epoch [16/50], Class Loss: 0.0863, Discrepancy Loss: 0.0343
Epoch [17/50], Class Loss: 0.0488, Discrepancy Loss: 0.0302
Epoch [18/50], Class Loss: 0.0285, Discrepancy Loss: 0.0306
Epoch [19/50], Class Loss: 0.0426, Discrepancy Loss: 0.0293
Epoch [20/50], Class Loss: 0.0418, Discrepancy Loss: 0.0240
Epoch [21/50], Class Loss: 0.0233, Discrepancy Loss: 0.0243
Epoch [22/50], Class Loss: 0.0455, Discrepancy Loss: 0.0239
Epoch [23/50], Class Loss: 0.0281, Discrepancy Loss: 0.0241
Epoch [24/50], Class Loss: 0.0175, Discrepancy Loss: 0.0220
Epoch [25/50], Class Loss: 0.1011, Discrepancy Loss: 0.0245
Epoch [26/50], Class Loss: 0.0199, Discrepancy Loss: 0.0258
Epoch [27/50], Class Loss: 0.0339, Discrepancy Loss: 0.0226
Epoch [28/50], Class Loss: 0.0210, Discrepancy Loss: 0.0263
Epoch [29/50], Class Loss: 0.0344, Discrepancy Loss: 0.0242
Epoch [30/50], Class Loss: 0.0175, Discrepancy Loss: 0.0258
Epoch [31/50], Class Loss: 0.0248, Discrepancy Loss: 0.0228
Epoch [32/50], Class Loss: 0.0683, Discrepancy Loss: 0.0208
Epoch [33/50], Class Loss: 0.0133, Discrepancy Loss: 0.0231
Epoch [34/50], Class Loss: 0.0130, Discrepancy Loss: 0.0263
Epoch [35/50], Class Loss: 0.0137, Discrepancy Loss: 0.0271
Epoch [36/50], Class Loss: 0.0159, Discrepancy Loss: 0.0245
Epoch [37/50], Class Loss: 0.0181, Discrepancy Loss: 0.0234
Epoch [38/50], Class Loss: 0.0296, Discrepancy Loss: 0.0223
Epoch [39/50], Class Loss: 0.0182, Discrepancy Loss: 0.0212
Epoch [40/50], Class Loss: 0.0187, Discrepancy Loss: 0.0217
Epoch [41/50], Class Loss: 0.0224, Discrepancy Loss: 0.0255
Epoch [42/50], Class Loss: 0.0167, Discrepancy Loss: 0.0304
Epoch [43/50], Class Loss: 0.0224, Discrepancy Loss: 0.0236
Epoch [44/50], Class Loss: 0.0131, Discrepancy Loss: 0.0272
Epoch [45/50], Class Loss: 0.1111, Discrepancy Loss: 0.0213
Epoch [46/50], Class Loss: 0.0141, Discrepancy Loss: 0.0224
Epoch [47/50], Class Loss: 0.0140, Discrepancy Loss: 0.0233
Epoch [48/50], Class Loss: 0.0887, Discrepancy Loss: 0.0258
Epoch [49/50], Class Loss: 0.0100, Discrepancy Loss: 0.0232
Epoch [50/50], Class Loss: 0.0775, Discrepancy Loss: 0.0248
Source Domain Performance - Accuracy: 96.94%, Precision: 97.11%, Recall: 96.90%, F1 Score: 96.89%
Target Domain Performance - Accuracy: 86.45%, Precision: 85.97%, Recall: 86.07%, F1 Score: 85.86%

Source performance: 97.00% 97.21% 96.97% 96.94%
Target performance: 87.91% 87.98% 87.52% 87.32%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 99.43%
16qam: 64.82%
8apsk: 85.84%
MCD

Run 1/3
Epoch [1/50], Class Loss: 1.0264, Discrepancy Loss: 0.0383
Validation Loss: 2.0680
Epoch [2/50], Class Loss: 0.2711, Discrepancy Loss: 0.0313
Validation Loss: 2.6256
Epoch [3/50], Class Loss: 0.1111, Discrepancy Loss: 0.0393
Validation Loss: 0.0843
Epoch [4/50], Class Loss: 0.0757, Discrepancy Loss: 0.0584
Validation Loss: 0.7885
Epoch [5/50], Class Loss: 0.0692, Discrepancy Loss: 0.0381
Validation Loss: 13.3109
Epoch [6/50], Class Loss: 0.0269, Discrepancy Loss: 0.0643
Validation Loss: 0.4425
Epoch [7/50], Class Loss: 0.0297, Discrepancy Loss: 0.1337
Validation Loss: 0.8370
Epoch [8/50], Class Loss: 0.2492, Discrepancy Loss: 0.1291
Validation Loss: 0.9140
Early stopping!
Source Domain Performance - Accuracy: 88.91%, Precision: 92.08%, Recall: 88.84%, F1 Score: 88.17%
Target Domain Performance - Accuracy: 53.78%, Precision: 59.76%, Recall: 52.23%, F1 Score: 44.93%

Run 2/3
Epoch [1/50], Class Loss: 0.9626, Discrepancy Loss: 0.0436
Validation Loss: 4.9556
Epoch [2/50], Class Loss: 0.3253, Discrepancy Loss: 0.0349
Validation Loss: 0.3130
Epoch [3/50], Class Loss: 0.1101, Discrepancy Loss: 0.0322
Validation Loss: 23.5082
Epoch [4/50], Class Loss: 0.1164, Discrepancy Loss: 0.0401
Validation Loss: 4.1869
Epoch [5/50], Class Loss: 0.0754, Discrepancy Loss: 0.0513
Validation Loss: 0.5618
Epoch [6/50], Class Loss: 0.0496, Discrepancy Loss: 0.0634
Validation Loss: 12.6093
Epoch [7/50], Class Loss: 0.0619, Discrepancy Loss: 0.0618
Validation Loss: 4.4936
Early stopping!
Source Domain Performance - Accuracy: 78.36%, Precision: 87.45%, Recall: 78.19%, F1 Score: 72.96%
Target Domain Performance - Accuracy: 65.95%, Precision: 73.57%, Recall: 64.54%, F1 Score: 58.22%

Run 3/3
Epoch [1/50], Class Loss: 0.9989, Discrepancy Loss: 0.0449
Validation Loss: 3.0554
Epoch [2/50], Class Loss: 0.3064, Discrepancy Loss: 0.0453
Validation Loss: 4.0880
Epoch [3/50], Class Loss: 0.1704, Discrepancy Loss: 0.0295
Validation Loss: 1.2366
Epoch [4/50], Class Loss: 0.0801, Discrepancy Loss: 0.0291
Validation Loss: 0.3716
Epoch [5/50], Class Loss: 0.0587, Discrepancy Loss: 0.0285
Validation Loss: 0.3897
Epoch [6/50], Class Loss: 0.0315, Discrepancy Loss: 0.0646
Validation Loss: 1.6742
Epoch [7/50], Class Loss: 0.1256, Discrepancy Loss: 0.0893
Validation Loss: 19.3489
Epoch [8/50], Class Loss: 0.0664, Discrepancy Loss: 0.1090
Validation Loss: 8.9002
Epoch [9/50], Class Loss: 0.1002, Discrepancy Loss: 0.1158
Validation Loss: 0.1143
Epoch [10/50], Class Loss: 0.0627, Discrepancy Loss: 0.1080
Validation Loss: 36.9839
Epoch [11/50], Class Loss: 0.0315, Discrepancy Loss: 0.0555
Validation Loss: 0.0055
Epoch [12/50], Class Loss: 0.0042, Discrepancy Loss: 0.0591
Validation Loss: 6.2343
Epoch [13/50], Class Loss: 0.0198, Discrepancy Loss: 0.1137
Validation Loss: 0.1348
Epoch [14/50], Class Loss: 0.0259, Discrepancy Loss: 0.0980
Validation Loss: 0.5362
Epoch [15/50], Class Loss: 0.0086, Discrepancy Loss: 0.0457
Validation Loss: 0.0291
Epoch [16/50], Class Loss: 0.0122, Discrepancy Loss: 0.0839
Validation Loss: 0.4058
Early stopping!
Source Domain Performance - Accuracy: 91.85%, Precision: 93.48%, Recall: 92.36%, F1 Score: 91.97%
Target Domain Performance - Accuracy: 37.35%, Precision: 55.94%, Recall: 38.37%, F1 Score: 33.02%

Source performance: 86.37% 91.00% 86.46% 84.36%
Target performance: 52.36% 63.09% 51.71% 45.39%

Per-Class Accuracy on Target Domain:
bpsk: 70.11%
qpsk: 31.20%
16qam: 36.01%
8apsk: 69.53%
JAN

Run 1/3
Epoch [1/50], Class Loss: 0.6138, JMMD Loss: 0.0552
Validation Loss: 2.5582
Epoch [2/50], Class Loss: 0.4238, JMMD Loss: 0.0520
Validation Loss: 1.3560
Epoch [3/50], Class Loss: 0.2601, JMMD Loss: 0.0885
Validation Loss: 0.3007
Epoch [4/50], Class Loss: 0.0590, JMMD Loss: 0.0931
Validation Loss: 0.1236
Epoch [5/50], Class Loss: 0.1348, JMMD Loss: 0.0943
Validation Loss: 2.7738
Epoch [6/50], Class Loss: 0.2431, JMMD Loss: 0.1352
Validation Loss: 0.1699
Epoch [7/50], Class Loss: 0.0526, JMMD Loss: 0.1358
Validation Loss: 0.0210
Epoch [8/50], Class Loss: 0.0499, JMMD Loss: 0.1202
Validation Loss: 0.1569
Epoch [9/50], Class Loss: 0.0378, JMMD Loss: 0.1172
Validation Loss: 0.0979
Epoch [10/50], Class Loss: 0.0229, JMMD Loss: 0.1042
Validation Loss: 0.9657
Epoch [11/50], Class Loss: 0.0159, JMMD Loss: 0.1022
Validation Loss: 0.0236
Epoch [12/50], Class Loss: 0.0077, JMMD Loss: 0.1012
Validation Loss: 0.0113
Epoch [13/50], Class Loss: 0.0130, JMMD Loss: 0.1126
Validation Loss: 0.0244
Epoch [14/50], Class Loss: 0.0103, JMMD Loss: 0.1046
Validation Loss: 0.0587
Epoch [15/50], Class Loss: 0.0081, JMMD Loss: 0.1045
Validation Loss: 0.0165
Epoch [16/50], Class Loss: 0.0066, JMMD Loss: 0.0993
Validation Loss: 0.0205
Epoch [17/50], Class Loss: 0.0079, JMMD Loss: 0.0912
Validation Loss: 0.0324
Early stopping!
Source Domain Performance - Accuracy: 98.98%, Precision: 98.98%, Recall: 98.97%, F1 Score: 98.95%
Target Domain Performance - Accuracy: 91.01%, Precision: 90.83%, Recall: 90.86%, F1 Score: 90.73%

Run 2/3
Epoch [1/50], Class Loss: 0.5874, JMMD Loss: 0.0688
Validation Loss: 4.0613
Epoch [2/50], Class Loss: 0.4156, JMMD Loss: 0.0713
Validation Loss: 0.3677
Epoch [3/50], Class Loss: 0.1491, JMMD Loss: 0.1138
Validation Loss: 0.8130
Epoch [4/50], Class Loss: 0.0529, JMMD Loss: 0.1059
Validation Loss: 0.0978
Epoch [5/50], Class Loss: 0.0239, JMMD Loss: 0.0996
Validation Loss: 0.7582
Epoch [6/50], Class Loss: 0.0432, JMMD Loss: 0.0998
Validation Loss: 0.1542
Epoch [7/50], Class Loss: 0.0071, JMMD Loss: 0.0955
Validation Loss: 0.0193
Epoch [8/50], Class Loss: 0.1158, JMMD Loss: 0.0789
Validation Loss: 0.0307
Epoch [9/50], Class Loss: 0.0276, JMMD Loss: 0.0712
Validation Loss: 0.0981
Epoch [10/50], Class Loss: 0.0099, JMMD Loss: 0.0658
Validation Loss: 0.0309
Epoch [11/50], Class Loss: 0.0059, JMMD Loss: 0.0619
Validation Loss: 0.0095
Epoch [12/50], Class Loss: 0.0037, JMMD Loss: 0.0657
Validation Loss: 0.0088
Epoch [13/50], Class Loss: 0.0030, JMMD Loss: 0.0603
Validation Loss: 0.0066
Epoch [14/50], Class Loss: 0.0037, JMMD Loss: 0.0663
Validation Loss: 0.0079
Epoch [15/50], Class Loss: 0.0041, JMMD Loss: 0.0688
Validation Loss: 0.0063
Epoch [16/50], Class Loss: 0.0097, JMMD Loss: 0.0602
Validation Loss: 0.0046
Epoch [17/50], Class Loss: 0.0068, JMMD Loss: 0.0645
Validation Loss: 0.0091
Epoch [18/50], Class Loss: 0.0047, JMMD Loss: 0.0747
Validation Loss: 0.0056
Epoch [19/50], Class Loss: 0.0032, JMMD Loss: 0.0608
Validation Loss: 0.0079
Epoch [20/50], Class Loss: 0.0041, JMMD Loss: 0.0551
Validation Loss: 0.0131
Epoch [21/50], Class Loss: 0.0034, JMMD Loss: 0.0559
Validation Loss: 0.0096
Early stopping!
Source Domain Performance - Accuracy: 99.70%, Precision: 99.69%, Recall: 99.70%, F1 Score: 99.69%
Target Domain Performance - Accuracy: 94.96%, Precision: 94.82%, Recall: 94.82%, F1 Score: 94.80%

Run 3/3
Epoch [1/50], Class Loss: 0.6165, JMMD Loss: 0.0625
Validation Loss: 0.6056
Epoch [2/50], Class Loss: 0.2736, JMMD Loss: 0.0903
Validation Loss: 0.4506
Epoch [3/50], Class Loss: 0.1453, JMMD Loss: 0.1055
Validation Loss: 1.2659
Epoch [4/50], Class Loss: 0.1269, JMMD Loss: 0.1078
Validation Loss: 1.7302
Epoch [5/50], Class Loss: 0.0966, JMMD Loss: 0.0816
Validation Loss: 0.1065
Epoch [6/50], Class Loss: 0.0536, JMMD Loss: 0.1133
Validation Loss: 0.6366
Epoch [7/50], Class Loss: 0.0390, JMMD Loss: 0.1019
Validation Loss: 0.2208
Epoch [8/50], Class Loss: 0.0538, JMMD Loss: 0.0946
Validation Loss: 0.0638
Epoch [9/50], Class Loss: 0.0529, JMMD Loss: 0.1051
Validation Loss: 1.5649
Epoch [10/50], Class Loss: 0.0401, JMMD Loss: 0.0811
Validation Loss: 0.0195
Epoch [11/50], Class Loss: 0.0094, JMMD Loss: 0.0855
Validation Loss: 0.0137
Epoch [12/50], Class Loss: 0.0203, JMMD Loss: 0.0901
Validation Loss: 0.0257
Epoch [13/50], Class Loss: 0.0110, JMMD Loss: 0.1053
Validation Loss: 0.0088
Epoch [14/50], Class Loss: 0.0058, JMMD Loss: 0.0890
Validation Loss: 0.0060
Epoch [15/50], Class Loss: 0.0051, JMMD Loss: 0.0875
Validation Loss: 0.0051
Epoch [16/50], Class Loss: 0.0053, JMMD Loss: 0.0856
Validation Loss: 0.0052
Epoch [17/50], Class Loss: 0.0043, JMMD Loss: 0.0823
Validation Loss: 0.0069
Epoch [18/50], Class Loss: 0.0043, JMMD Loss: 0.0831
Validation Loss: 0.0100
Epoch [19/50], Class Loss: 0.0046, JMMD Loss: 0.0736
Validation Loss: 0.0097
Epoch [20/50], Class Loss: 0.0058, JMMD Loss: 0.0886
Validation Loss: 0.0273
Early stopping!
Source Domain Performance - Accuracy: 98.92%, Precision: 98.92%, Recall: 98.91%, F1 Score: 98.89%
Target Domain Performance - Accuracy: 90.65%, Precision: 90.93%, Recall: 90.31%, F1 Score: 90.19%

Source performance: 99.20% 99.20% 99.19% 99.18%
Target performance: 92.21% 92.19% 92.00% 91.90%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 100.00%
  Class 1: 99.67%
  Class 2: 80.40%
  Class 3: 87.91%

SNR level: 22
Base

Run 1/3
Epoch 1/50, Train Loss: 0.4004, Train Acc: 0.8394, Val Loss: 2.9000, Val Acc: 0.7374
Epoch 2/50, Train Loss: 0.1350, Train Acc: 0.9498, Val Loss: 1.3064, Val Acc: 0.7428
Epoch 3/50, Train Loss: 0.0605, Train Acc: 0.9802, Val Loss: 2.0236, Val Acc: 0.6085
Epoch 4/50, Train Loss: 0.0258, Train Acc: 0.9916, Val Loss: 0.4836, Val Acc: 0.8717
Epoch 5/50, Train Loss: 0.0167, Train Acc: 0.9949, Val Loss: 0.1508, Val Acc: 0.9508
Epoch 6/50, Train Loss: 0.0105, Train Acc: 0.9967, Val Loss: 0.4044, Val Acc: 0.8867
Epoch 7/50, Train Loss: 0.0070, Train Acc: 0.9982, Val Loss: 0.0102, Val Acc: 0.9964
Epoch 8/50, Train Loss: 0.0033, Train Acc: 0.9988, Val Loss: 0.0853, Val Acc: 0.9694
Epoch 9/50, Train Loss: 0.0099, Train Acc: 0.9969, Val Loss: 0.5898, Val Acc: 0.8004
Epoch 10/50, Train Loss: 0.0048, Train Acc: 0.9988, Val Loss: 0.0069, Val Acc: 0.9976
Epoch 11/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0019, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0012, Train Acc: 1.0000, Val Loss: 0.0016, Val Acc: 0.9994
Epoch 13/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0025, Val Acc: 0.9994
Epoch 14/50, Train Loss: 0.0051, Train Acc: 0.9999, Val Loss: 0.0028, Val Acc: 0.9988
Epoch 15/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0018, Val Acc: 0.9994
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9994
Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988
Early stopping!

Run 2/3
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Train Loss: 0.4289, Train Acc: 0.8302, Val Loss: 3.5411, Val Acc: 0.5366
Epoch 2/50, Train Loss: 0.1194, Train Acc: 0.9573, Val Loss: 0.1552, Val Acc: 0.9376
Epoch 3/50, Train Loss: 0.0637, Train Acc: 0.9781, Val Loss: 0.0546, Val Acc: 0.9760
Epoch 4/50, Train Loss: 0.0257, Train Acc: 0.9913, Val Loss: 0.1012, Val Acc: 0.9640
Epoch 5/50, Train Loss: 0.0349, Train Acc: 0.9894, Val Loss: 0.6453, Val Acc: 0.8771
Epoch 6/50, Train Loss: 0.0216, Train Acc: 0.9919, Val Loss: 3.8161, Val Acc: 0.7494
Epoch 7/50, Train Loss: 0.0178, Train Acc: 0.9943, Val Loss: 2.2634, Val Acc: 0.6439
Epoch 8/50, Train Loss: 0.0121, Train Acc: 0.9972, Val Loss: 0.0194, Val Acc: 0.9946
Epoch 9/50, Train Loss: 0.0078, Train Acc: 0.9976, Val Loss: 0.8892, Val Acc: 0.8034
Epoch 10/50, Train Loss: 0.0047, Train Acc: 0.9984, Val Loss: 0.5071, Val Acc: 0.8891
Epoch 11/50, Train Loss: 0.0033, Train Acc: 0.9991, Val Loss: 0.0038, Val Acc: 0.9988
Epoch 12/50, Train Loss: 0.0015, Train Acc: 0.9999, Val Loss: 0.0038, Val Acc: 0.9982
Epoch 13/50, Train Loss: 0.0007, Train Acc: 1.0000, Val Loss: 0.0026, Val Acc: 0.9988
Epoch 14/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9976
Epoch 15/50, Train Loss: 0.0006, Train Acc: 1.0000, Val Loss: 0.0029, Val Acc: 0.9988
Epoch 16/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9988
Epoch 17/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0027, Val Acc: 0.9994
Epoch 18/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0026, Val Acc: 0.9988
Epoch 19/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0026, Val Acc: 0.9982
Epoch 20/50, Train Loss: 0.0005, Train Acc: 1.0000, Val Loss: 0.0025, Val Acc: 0.9994
Epoch 21/50, Train Loss: 0.0009, Train Acc: 0.9999, Val Loss: 0.0031, Val Acc: 0.9988
Early stopping!

Run 3/3
Epoch 1/50, Train Loss: 0.4106, Train Acc: 0.8355, Val Loss: 4.7089, Val Acc: 0.5216
Epoch 2/50, Train Loss: 0.0937, Train Acc: 0.9652, Val Loss: 2.0143, Val Acc: 0.5797
Epoch 3/50, Train Loss: 0.0399, Train Acc: 0.9870, Val Loss: 0.0288, Val Acc: 0.9916
Epoch 4/50, Train Loss: 0.0246, Train Acc: 0.9924, Val Loss: 0.0975, Val Acc: 0.9604
Epoch 5/50, Train Loss: 0.0100, Train Acc: 0.9984, Val Loss: 7.2239, Val Acc: 0.6924
Epoch 6/50, Train Loss: 0.0053, Train Acc: 0.9990, Val Loss: 0.0011, Val Acc: 0.9994
Epoch 7/50, Train Loss: 0.0032, Train Acc: 0.9993, Val Loss: 0.6646, Val Acc: 0.8273
Epoch 8/50, Train Loss: 0.0077, Train Acc: 0.9982, Val Loss: 0.0027, Val Acc: 0.9994
Epoch 9/50, Train Loss: 0.0068, Train Acc: 0.9978, Val Loss: 5.2496, Val Acc: 0.5372
Epoch 10/50, Train Loss: 0.0030, Train Acc: 0.9991, Val Loss: 0.2146, Val Acc: 0.9251
Epoch 11/50, Train Loss: 0.0013, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 12/50, Train Loss: 0.0008, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 13/50, Train Loss: 0.0008, Train Acc: 0.9999, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 14/50, Train Loss: 0.0007, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 15/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 16/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 17/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 18/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 19/50, Train Loss: 0.0034, Train Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000
Epoch 20/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 21/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 22/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 23/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 24/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 25/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 26/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 27/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 28/50, Train Loss: 0.0003, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000
Epoch 29/50, Train Loss: 0.0004, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000
Early stopping!

Source performance: 99.92 99.92 99.92 99.92
Target performance: 79.12 75.84 78.37 73.49

bpsk: 100.00
qpsk: 31.48
16qam: 91.93
8apsk: 90.05
DANN
Epoch 1/50, Loss: 2.3791, Domain Loss: 1.4089, Class Loss: 0.9703
Epoch 2/50, Loss: 1.6830, Domain Loss: 1.2705, Class Loss: 0.4124
Epoch 3/50, Loss: 1.6813, Domain Loss: 1.3294, Class Loss: 0.3519
Epoch 4/50, Loss: 1.9517, Domain Loss: 1.7023, Class Loss: 0.2494
Epoch 5/50, Loss: 7.5092, Domain Loss: 7.2029, Class Loss: 0.3062
Epoch 6/50, Loss: 14.8354, Domain Loss: 13.8066, Class Loss: 1.0288
Epoch 7/50, Loss: 10.4794, Domain Loss: 9.9129, Class Loss: 0.5664
Epoch 8/50, Loss: 26.8167, Domain Loss: 25.8124, Class Loss: 1.0043
Epoch 9/50, Loss: 18.5029, Domain Loss: 17.6177, Class Loss: 0.8852
Epoch 10/50, Loss: 7.4717, Domain Loss: 6.5416, Class Loss: 0.9301
Epoch 11/50, Loss: 3.2571, Domain Loss: 2.2888, Class Loss: 0.9683
Epoch 12/50, Loss: 2.8362, Domain Loss: 1.9838, Class Loss: 0.8524
Epoch 13/50, Loss: 2.2280, Domain Loss: 1.5683, Class Loss: 0.6597
Epoch 14/50, Loss: 2.0675, Domain Loss: 1.5596, Class Loss: 0.5079
Epoch 15/50, Loss: 2.0892, Domain Loss: 1.6120, Class Loss: 0.4772
Epoch 16/50, Loss: 2.9093, Domain Loss: 2.3575, Class Loss: 0.5518
Epoch 17/50, Loss: 2.7628, Domain Loss: 2.3313, Class Loss: 0.4315
Epoch 18/50, Loss: 2.3002, Domain Loss: 1.8728, Class Loss: 0.4274
Epoch 19/50, Loss: 2.3576, Domain Loss: 1.8479, Class Loss: 0.5096
Epoch 20/50, Loss: 2.0712, Domain Loss: 1.6549, Class Loss: 0.4163
Epoch 21/50, Loss: 2.0762, Domain Loss: 1.6773, Class Loss: 0.3989
Epoch 22/50, Loss: 2.1511, Domain Loss: 1.7740, Class Loss: 0.3771
Epoch 23/50, Loss: 2.3657, Domain Loss: 1.8854, Class Loss: 0.4803
Epoch 24/50, Loss: 2.6489, Domain Loss: 2.2311, Class Loss: 0.4178
Epoch 25/50, Loss: 2.5084, Domain Loss: 2.0593, Class Loss: 0.4491
Epoch 26/50, Loss: 2.4433, Domain Loss: 1.9057, Class Loss: 0.5376
Epoch 27/50, Loss: 1.9187, Domain Loss: 1.5307, Class Loss: 0.3880
Epoch 28/50, Loss: 1.9485, Domain Loss: 1.5579, Class Loss: 0.3906
Epoch 29/50, Loss: 1.9671, Domain Loss: 1.5950, Class Loss: 0.3721
Epoch 30/50, Loss: 1.9124, Domain Loss: 1.5353, Class Loss: 0.3771
Epoch 31/50, Loss: 1.8963, Domain Loss: 1.5361, Class Loss: 0.3602
Epoch 32/50, Loss: 1.9411, Domain Loss: 1.5147, Class Loss: 0.4264
Epoch 33/50, Loss: 1.8916, Domain Loss: 1.4691, Class Loss: 0.4224
Epoch 34/50, Loss: 1.8582, Domain Loss: 1.4513, Class Loss: 0.4069
Epoch 35/50, Loss: 1.6996, Domain Loss: 1.3517, Class Loss: 0.3479
Epoch 36/50, Loss: 1.6209, Domain Loss: 1.3054, Class Loss: 0.3154
Epoch 37/50, Loss: 1.7412, Domain Loss: 1.4170, Class Loss: 0.3242
Epoch 38/50, Loss: 1.7478, Domain Loss: 1.4248, Class Loss: 0.3230
Epoch 39/50, Loss: 1.7436, Domain Loss: 1.4340, Class Loss: 0.3095
Epoch 40/50, Loss: 1.9342, Domain Loss: 1.6516, Class Loss: 0.2826
Epoch 41/50, Loss: 1.8146, Domain Loss: 1.5318, Class Loss: 0.2828
Epoch 42/50, Loss: 1.6347, Domain Loss: 1.3276, Class Loss: 0.3071
Epoch 43/50, Loss: 1.8725, Domain Loss: 1.5573, Class Loss: 0.3152
Epoch 44/50, Loss: 2.0130, Domain Loss: 1.7205, Class Loss: 0.2925
Epoch 45/50, Loss: 1.6181, Domain Loss: 1.3726, Class Loss: 0.2455
Epoch 46/50, Loss: 1.9013, Domain Loss: 1.5694, Class Loss: 0.3320
Epoch 47/50, Loss: 1.9644, Domain Loss: 1.6476, Class Loss: 0.3168
Epoch 48/50, Loss: 1.8880, Domain Loss: 1.5622, Class Loss: 0.3258
Epoch 49/50, Loss: 1.9051, Domain Loss: 1.5455, Class Loss: 0.3596
Epoch 50/50, Loss: 2.3708, Domain Loss: 2.0013, Class Loss: 0.3695
50.60


Epoch 1/50, Loss: 2.3334, Domain Loss: 1.3861, Class Loss: 0.9473
Epoch 2/50, Loss: 1.7066, Domain Loss: 1.2964, Class Loss: 0.4102
Epoch 3/50, Loss: 1.6288, Domain Loss: 1.2979, Class Loss: 0.3309
Epoch 4/50, Loss: 1.7638, Domain Loss: 1.4923, Class Loss: 0.2715
Epoch 5/50, Loss: 4.4979, Domain Loss: 4.2691, Class Loss: 0.2288
Epoch 6/50, Loss: 10.2412, Domain Loss: 9.8680, Class Loss: 0.3732
Epoch 7/50, Loss: 9.3799, Domain Loss: 8.8766, Class Loss: 0.5033
Epoch 8/50, Loss: 9.8868, Domain Loss: 9.3762, Class Loss: 0.5105
Epoch 9/50, Loss: 12.9643, Domain Loss: 12.4249, Class Loss: 0.5394
Epoch 10/50, Loss: 14.4442, Domain Loss: 13.8219, Class Loss: 0.6222
Epoch 11/50, Loss: 6.8769, Domain Loss: 6.3538, Class Loss: 0.5231
Epoch 12/50, Loss: 4.5439, Domain Loss: 4.1426, Class Loss: 0.4014
Epoch 13/50, Loss: 2.8182, Domain Loss: 2.2867, Class Loss: 0.5315
Epoch 14/50, Loss: 2.4358, Domain Loss: 1.9948, Class Loss: 0.4410
Epoch 15/50, Loss: 2.0635, Domain Loss: 1.7182, Class Loss: 0.3453
Epoch 16/50, Loss: 2.1551, Domain Loss: 1.7885, Class Loss: 0.3667
Epoch 17/50, Loss: 2.1661, Domain Loss: 1.6861, Class Loss: 0.4800
Epoch 18/50, Loss: 2.1870, Domain Loss: 1.6890, Class Loss: 0.4980
Epoch 19/50, Loss: 2.1327, Domain Loss: 1.6905, Class Loss: 0.4422
Epoch 20/50, Loss: 2.1091, Domain Loss: 1.6672, Class Loss: 0.4419
Epoch 21/50, Loss: 1.7948, Domain Loss: 1.4140, Class Loss: 0.3808
Epoch 22/50, Loss: 1.6631, Domain Loss: 1.3396, Class Loss: 0.3235
Epoch 23/50, Loss: 2.0626, Domain Loss: 1.6423, Class Loss: 0.4203
Epoch 24/50, Loss: 3.8005, Domain Loss: 3.2030, Class Loss: 0.5975
Epoch 25/50, Loss: 3.0671, Domain Loss: 2.6464, Class Loss: 0.4207
Epoch 26/50, Loss: 2.5621, Domain Loss: 2.0193, Class Loss: 0.5428
Epoch 27/50, Loss: 2.2786, Domain Loss: 1.8338, Class Loss: 0.4448
Epoch 28/50, Loss: 2.1957, Domain Loss: 1.7805, Class Loss: 0.4152
Epoch 29/50, Loss: 2.0743, Domain Loss: 1.7246, Class Loss: 0.3498
Epoch 30/50, Loss: 2.0029, Domain Loss: 1.5675, Class Loss: 0.4354
Epoch 31/50, Loss: 1.9303, Domain Loss: 1.5920, Class Loss: 0.3382
Epoch 32/50, Loss: 1.7400, Domain Loss: 1.3697, Class Loss: 0.3703
Epoch 33/50, Loss: 2.1938, Domain Loss: 1.7700, Class Loss: 0.4238
Epoch 34/50, Loss: 1.8524, Domain Loss: 1.6139, Class Loss: 0.2385
Epoch 35/50, Loss: 1.8041, Domain Loss: 1.5553, Class Loss: 0.2488
Epoch 36/50, Loss: 1.8889, Domain Loss: 1.5481, Class Loss: 0.3407
Epoch 37/50, Loss: 1.7228, Domain Loss: 1.5396, Class Loss: 0.1832
Epoch 38/50, Loss: 1.8169, Domain Loss: 1.5289, Class Loss: 0.2880
Epoch 39/50, Loss: 2.3804, Domain Loss: 1.7821, Class Loss: 0.5983
Epoch 40/50, Loss: 2.2317, Domain Loss: 1.7467, Class Loss: 0.4850
Epoch 41/50, Loss: 1.9000, Domain Loss: 1.4969, Class Loss: 0.4031
Epoch 42/50, Loss: 2.1517, Domain Loss: 1.7351, Class Loss: 0.4166
Epoch 43/50, Loss: 1.9521, Domain Loss: 1.6538, Class Loss: 0.2983
Epoch 44/50, Loss: 2.2777, Domain Loss: 1.9118, Class Loss: 0.3659
Epoch 45/50, Loss: 2.1834, Domain Loss: 1.8214, Class Loss: 0.3620
Epoch 46/50, Loss: 2.0504, Domain Loss: 1.7840, Class Loss: 0.2663
Epoch 47/50, Loss: 3.0783, Domain Loss: 2.1168, Class Loss: 0.9615
Epoch 48/50, Loss: 3.4630, Domain Loss: 3.0369, Class Loss: 0.4261
Epoch 49/50, Loss: 3.2482, Domain Loss: 2.9362, Class Loss: 0.3120
Epoch 50/50, Loss: 3.7705, Domain Loss: 3.0401, Class Loss: 0.7304
46.70


</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1/50, Loss: 2.2300, Domain Loss: 1.3667, Class Loss: 0.8634
Epoch 2/50, Loss: 1.6555, Domain Loss: 1.2364, Class Loss: 0.4191
Epoch 3/50, Loss: 1.9736, Domain Loss: 1.6258, Class Loss: 0.3478
Epoch 4/50, Loss: 2.1640, Domain Loss: 1.8500, Class Loss: 0.3140
Epoch 5/50, Loss: 7.1205, Domain Loss: 6.6877, Class Loss: 0.4328
Epoch 6/50, Loss: 7.6948, Domain Loss: 6.6647, Class Loss: 1.0301
Epoch 7/50, Loss: 5.7883, Domain Loss: 5.3354, Class Loss: 0.4529
Epoch 8/50, Loss: 6.4265, Domain Loss: 6.0454, Class Loss: 0.3811
Epoch 9/50, Loss: 2.2409, Domain Loss: 1.8259, Class Loss: 0.4150
Epoch 10/50, Loss: 2.0155, Domain Loss: 1.6981, Class Loss: 0.3174
Epoch 11/50, Loss: 2.1387, Domain Loss: 1.8213, Class Loss: 0.3175
Epoch 12/50, Loss: 3.3505, Domain Loss: 2.7767, Class Loss: 0.5737
Epoch 13/50, Loss: 4.9954, Domain Loss: 4.3333, Class Loss: 0.6622
Epoch 14/50, Loss: 2.4752, Domain Loss: 2.1135, Class Loss: 0.3617
Epoch 15/50, Loss: 2.3101, Domain Loss: 1.9805, Class Loss: 0.3296
Epoch 16/50, Loss: 2.0402, Domain Loss: 1.6766, Class Loss: 0.3636
Epoch 17/50, Loss: 1.9163, Domain Loss: 1.6087, Class Loss: 0.3076
Epoch 18/50, Loss: 1.7469, Domain Loss: 1.5003, Class Loss: 0.2465
Epoch 19/50, Loss: 1.7628, Domain Loss: 1.5442, Class Loss: 0.2186
Epoch 20/50, Loss: 1.6522, Domain Loss: 1.4414, Class Loss: 0.2109
Epoch 21/50, Loss: 1.8329, Domain Loss: 1.5824, Class Loss: 0.2504
Epoch 22/50, Loss: 1.6527, Domain Loss: 1.4475, Class Loss: 0.2052
Epoch 23/50, Loss: 1.6481, Domain Loss: 1.4609, Class Loss: 0.1872
Epoch 24/50, Loss: 1.7055, Domain Loss: 1.4940, Class Loss: 0.2115
Epoch 25/50, Loss: 1.8672, Domain Loss: 1.6395, Class Loss: 0.2277
Epoch 26/50, Loss: 1.5402, Domain Loss: 1.3349, Class Loss: 0.2052
Epoch 27/50, Loss: 2.1349, Domain Loss: 1.8951, Class Loss: 0.2398
Epoch 28/50, Loss: 2.2272, Domain Loss: 1.8291, Class Loss: 0.3981
Epoch 29/50, Loss: 2.7541, Domain Loss: 2.0705, Class Loss: 0.6836
Epoch 30/50, Loss: 2.4608, Domain Loss: 1.8983, Class Loss: 0.5625
Epoch 31/50, Loss: 2.5040, Domain Loss: 2.2142, Class Loss: 0.2898
Epoch 32/50, Loss: 2.3067, Domain Loss: 1.9538, Class Loss: 0.3529
Epoch 33/50, Loss: 2.9717, Domain Loss: 2.3279, Class Loss: 0.6439
Epoch 34/50, Loss: 3.9442, Domain Loss: 3.1505, Class Loss: 0.7936
Epoch 35/50, Loss: 2.7158, Domain Loss: 2.2369, Class Loss: 0.4789
Epoch 36/50, Loss: 2.8438, Domain Loss: 2.3260, Class Loss: 0.5178
Epoch 37/50, Loss: 2.6537, Domain Loss: 2.2011, Class Loss: 0.4526
Epoch 38/50, Loss: 2.2166, Domain Loss: 1.7926, Class Loss: 0.4240
Epoch 39/50, Loss: 2.1411, Domain Loss: 1.8169, Class Loss: 0.3241
Epoch 40/50, Loss: 2.0152, Domain Loss: 1.7021, Class Loss: 0.3132
Epoch 41/50, Loss: 2.1706, Domain Loss: 1.7894, Class Loss: 0.3812
Epoch 42/50, Loss: 2.2382, Domain Loss: 1.8441, Class Loss: 0.3941
Epoch 43/50, Loss: 5.5872, Domain Loss: 4.7574, Class Loss: 0.8298
Epoch 44/50, Loss: 3.9066, Domain Loss: 3.5087, Class Loss: 0.3979
Epoch 45/50, Loss: 5.4711, Domain Loss: 4.9724, Class Loss: 0.4987
Epoch 46/50, Loss: 7.9399, Domain Loss: 7.2536, Class Loss: 0.6862
Epoch 47/50, Loss: 10.9926, Domain Loss: 9.9810, Class Loss: 1.0116
Epoch 48/50, Loss: 14.0803, Domain Loss: 12.6759, Class Loss: 1.4044
Epoch 49/50, Loss: 8.3724, Domain Loss: 7.5988, Class Loss: 0.7736
Epoch 50/50, Loss: 6.6354, Domain Loss: 5.9947, Class Loss: 0.6407
49.94


Source performance:
65.83 68.60 66.08 61.90 
Target performance:
49.08 41.28 49.07 43.34 

Per-class target performance: 99.92 6.92 62.27 27.17 Deep CORAL
Deep CORAL Run 1/3
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="application/vnd.jupyter.stderr" tabindex="0">
<pre>/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>Epoch 1: Source Val Acc = 0.8129, Target Val Acc = 0.4838
Epoch 2: Source Val Acc = 0.7740, Target Val Acc = 0.5713
Epoch 3: Source Val Acc = 0.7392, Target Val Acc = 0.4826
Epoch 4: Source Val Acc = 0.9748, Target Val Acc = 0.6433
Epoch 5: Source Val Acc = 0.8831, Target Val Acc = 0.7794
Epoch 6: Source Val Acc = 0.5132, Target Val Acc = 0.4940
Epoch 7: Source Val Acc = 0.9928, Target Val Acc = 0.8585
Epoch 8: Source Val Acc = 0.9844, Target Val Acc = 0.7944
Epoch 9: Source Val Acc = 0.9826, Target Val Acc = 0.7986
Epoch 10: Source Val Acc = 0.9958, Target Val Acc = 0.9167
Epoch 11: Source Val Acc = 0.9928, Target Val Acc = 0.9323
Epoch 12: Source Val Acc = 0.9994, Target Val Acc = 0.9293
Epoch 13: Source Val Acc = 0.9988, Target Val Acc = 0.8537
Epoch 14: Source Val Acc = 0.8046, Target Val Acc = 0.5815
Epoch 15: Source Val Acc = 0.9928, Target Val Acc = 0.7680
Epoch 16: Source Val Acc = 0.9976, Target Val Acc = 0.9191
Epoch 17: Source Val Acc = 0.9874, Target Val Acc = 0.8393
Early stopping triggered.
Run 1 finished: Best Source Val Acc = 0.9874, Target Val Acc = 0.8393

Deep CORAL Run 2/3
Epoch 1: Source Val Acc = 0.7002, Target Val Acc = 0.5420
Epoch 2: Source Val Acc = 0.7470, Target Val Acc = 0.4814
Epoch 3: Source Val Acc = 0.8135, Target Val Acc = 0.7932
Epoch 4: Source Val Acc = 0.9880, Target Val Acc = 0.7566
Epoch 5: Source Val Acc = 0.6613, Target Val Acc = 0.5845
Epoch 6: Source Val Acc = 0.9958, Target Val Acc = 0.9179
Epoch 7: Source Val Acc = 0.9784, Target Val Acc = 0.9209
Epoch 8: Source Val Acc = 0.9964, Target Val Acc = 0.8597
Epoch 9: Source Val Acc = 0.9982, Target Val Acc = 0.9347
Epoch 10: Source Val Acc = 0.9916, Target Val Acc = 0.9526
Epoch 11: Source Val Acc = 0.7482, Target Val Acc = 0.5522
Epoch 12: Source Val Acc = 0.9970, Target Val Acc = 0.9251
Epoch 13: Source Val Acc = 0.9976, Target Val Acc = 0.8891
Epoch 14: Source Val Acc = 0.9820, Target Val Acc = 0.8447
Early stopping triggered.
Run 2 finished: Best Source Val Acc = 0.9820, Target Val Acc = 0.8447

Deep CORAL Run 3/3
Epoch 1: Source Val Acc = 0.7632, Target Val Acc = 0.5881
Epoch 2: Source Val Acc = 0.8819, Target Val Acc = 0.4898
Epoch 3: Source Val Acc = 0.8489, Target Val Acc = 0.6187
Epoch 4: Source Val Acc = 0.8579, Target Val Acc = 0.6978
Epoch 5: Source Val Acc = 0.6847, Target Val Acc = 0.5683
Epoch 6: Source Val Acc = 0.9616, Target Val Acc = 0.7002
Epoch 7: Source Val Acc = 0.9143, Target Val Acc = 0.7974
Epoch 8: Source Val Acc = 0.9532, Target Val Acc = 0.7194
Epoch 9: Source Val Acc = 0.9682, Target Val Acc = 0.6217
Epoch 10: Source Val Acc = 0.9940, Target Val Acc = 0.7998
Epoch 11: Source Val Acc = 0.9712, Target Val Acc = 0.7122
Epoch 12: Source Val Acc = 0.9988, Target Val Acc = 0.9490
Epoch 13: Source Val Acc = 0.8861, Target Val Acc = 0.7842
Epoch 14: Source Val Acc = 0.8867, Target Val Acc = 0.5671
Epoch 15: Source Val Acc = 0.9994, Target Val Acc = 0.7500
Epoch 16: Source Val Acc = 1.0000, Target Val Acc = 0.7734
Epoch 17: Source Val Acc = 1.0000, Target Val Acc = 0.8195
Epoch 18: Source Val Acc = 1.0000, Target Val Acc = 0.8537
Epoch 19: Source Val Acc = 1.0000, Target Val Acc = 0.8004
Epoch 20: Source Val Acc = 1.0000, Target Val Acc = 0.8927
Epoch 21: Source Val Acc = 0.9994, Target Val Acc = 0.7368
Early stopping triggered.
Run 3 finished: Best Source Val Acc = 0.9994, Target Val Acc = 0.7368

Deep CORAL: Average Source Val Acc = 0.9896, Average Target Val Acc = 0.8070
STAR

Run 1/3
Epoch [1/50], Class Loss: 1.9191, Discrepancy Loss: 0.1202
Epoch [2/50], Class Loss: 0.8203, Discrepancy Loss: 0.0822
Epoch [3/50], Class Loss: 0.6857, Discrepancy Loss: 0.0958
Epoch [4/50], Class Loss: 0.3450, Discrepancy Loss: 0.0767
Epoch [5/50], Class Loss: 0.1800, Discrepancy Loss: 0.0496
Epoch [6/50], Class Loss: 0.4068, Discrepancy Loss: 0.0616
Epoch [7/50], Class Loss: 0.1424, Discrepancy Loss: 0.0396
Epoch [8/50], Class Loss: 0.0342, Discrepancy Loss: 0.0159
Epoch [9/50], Class Loss: 0.0460, Discrepancy Loss: 0.0148
Epoch [10/50], Class Loss: 0.0362, Discrepancy Loss: 0.0152
Epoch [11/50], Class Loss: 0.0059, Discrepancy Loss: 0.0104
Epoch [12/50], Class Loss: 0.0086, Discrepancy Loss: 0.0112
Epoch [13/50], Class Loss: 0.0056, Discrepancy Loss: 0.0081
Epoch [14/50], Class Loss: 0.0029, Discrepancy Loss: 0.0076
Epoch [15/50], Class Loss: 0.0044, Discrepancy Loss: 0.0064
Epoch [16/50], Class Loss: 0.0053, Discrepancy Loss: 0.0074
Epoch [17/50], Class Loss: 0.0234, Discrepancy Loss: 0.0070
Epoch [18/50], Class Loss: 0.0221, Discrepancy Loss: 0.0095
Epoch [19/50], Class Loss: 0.0033, Discrepancy Loss: 0.0097
Epoch [20/50], Class Loss: 0.0044, Discrepancy Loss: 0.0087
Epoch [21/50], Class Loss: 0.0027, Discrepancy Loss: 0.0059
Epoch [22/50], Class Loss: 0.0323, Discrepancy Loss: 0.0062
Epoch [23/50], Class Loss: 0.0020, Discrepancy Loss: 0.0064
Epoch [24/50], Class Loss: 0.0041, Discrepancy Loss: 0.0068
Epoch [25/50], Class Loss: 0.0030, Discrepancy Loss: 0.0065
Epoch [26/50], Class Loss: 0.0017, Discrepancy Loss: 0.0075
Epoch [27/50], Class Loss: 0.0038, Discrepancy Loss: 0.0076
Epoch [28/50], Class Loss: 0.0015, Discrepancy Loss: 0.0055
Epoch [29/50], Class Loss: 0.0029, Discrepancy Loss: 0.0063
Epoch [30/50], Class Loss: 0.0009, Discrepancy Loss: 0.0072
Epoch [31/50], Class Loss: 0.0091, Discrepancy Loss: 0.0064
Epoch [32/50], Class Loss: 0.0012, Discrepancy Loss: 0.0051
Epoch [33/50], Class Loss: 0.0047, Discrepancy Loss: 0.0050
Epoch [34/50], Class Loss: 0.0452, Discrepancy Loss: 0.0066
Epoch [35/50], Class Loss: 0.0023, Discrepancy Loss: 0.0069
Epoch [36/50], Class Loss: 0.0022, Discrepancy Loss: 0.0081
Epoch [37/50], Class Loss: 0.0058, Discrepancy Loss: 0.0061
Epoch [38/50], Class Loss: 0.0027, Discrepancy Loss: 0.0062
Epoch [39/50], Class Loss: 0.0017, Discrepancy Loss: 0.0063
Epoch [40/50], Class Loss: 0.0136, Discrepancy Loss: 0.0047
Epoch [41/50], Class Loss: 0.0011, Discrepancy Loss: 0.0068
Epoch [42/50], Class Loss: 0.0012, Discrepancy Loss: 0.0052
Epoch [43/50], Class Loss: 0.0021, Discrepancy Loss: 0.0054
Epoch [44/50], Class Loss: 0.0013, Discrepancy Loss: 0.0046
Epoch [45/50], Class Loss: 0.0027, Discrepancy Loss: 0.0074
Epoch [46/50], Class Loss: 0.0011, Discrepancy Loss: 0.0085
Epoch [47/50], Class Loss: 0.0039, Discrepancy Loss: 0.0053
Epoch [48/50], Class Loss: 0.0034, Discrepancy Loss: 0.0050
Epoch [49/50], Class Loss: 0.0020, Discrepancy Loss: 0.0052
Epoch [50/50], Class Loss: 0.0014, Discrepancy Loss: 0.0072
Source Domain Performance - Accuracy: 99.28%, Precision: 99.28%, Recall: 99.27%, F1 Score: 99.27%
Target Domain Performance - Accuracy: 95.68%, Precision: 95.67%, Recall: 95.76%, F1 Score: 95.66%

Run 2/3
Epoch [1/50], Class Loss: 1.7752, Discrepancy Loss: 0.1602
Epoch [2/50], Class Loss: 0.9975, Discrepancy Loss: 0.1086
Epoch [3/50], Class Loss: 0.4518, Discrepancy Loss: 0.0904
Epoch [4/50], Class Loss: 0.4017, Discrepancy Loss: 0.0805
Epoch [5/50], Class Loss: 0.1606, Discrepancy Loss: 0.0575
Epoch [6/50], Class Loss: 0.2127, Discrepancy Loss: 0.0458
Epoch [7/50], Class Loss: 0.5507, Discrepancy Loss: 0.0721
Epoch [8/50], Class Loss: 0.5128, Discrepancy Loss: 0.1156
Epoch [9/50], Class Loss: 0.2547, Discrepancy Loss: 0.0448
Epoch [10/50], Class Loss: 0.1635, Discrepancy Loss: 0.0561
Epoch [11/50], Class Loss: 0.0490, Discrepancy Loss: 0.0404
Epoch [12/50], Class Loss: 0.0341, Discrepancy Loss: 0.0388
Epoch [13/50], Class Loss: 0.0451, Discrepancy Loss: 0.0349
Epoch [14/50], Class Loss: 0.0251, Discrepancy Loss: 0.0353
Epoch [15/50], Class Loss: 0.0316, Discrepancy Loss: 0.0291
Epoch [16/50], Class Loss: 0.0254, Discrepancy Loss: 0.0228
Epoch [17/50], Class Loss: 0.0513, Discrepancy Loss: 0.0288
Epoch [18/50], Class Loss: 0.0192, Discrepancy Loss: 0.0267
Epoch [19/50], Class Loss: 0.0409, Discrepancy Loss: 0.0262
Epoch [20/50], Class Loss: 0.0415, Discrepancy Loss: 0.0278
Epoch [21/50], Class Loss: 0.0154, Discrepancy Loss: 0.0293
Epoch [22/50], Class Loss: 0.0096, Discrepancy Loss: 0.0233
Epoch [23/50], Class Loss: 0.0134, Discrepancy Loss: 0.0235
Epoch [24/50], Class Loss: 0.0410, Discrepancy Loss: 0.0243
Epoch [25/50], Class Loss: 0.0550, Discrepancy Loss: 0.0238
Epoch [26/50], Class Loss: 0.0190, Discrepancy Loss: 0.0245
Epoch [27/50], Class Loss: 0.0147, Discrepancy Loss: 0.0226
Epoch [28/50], Class Loss: 0.0162, Discrepancy Loss: 0.0202
Epoch [29/50], Class Loss: 0.0108, Discrepancy Loss: 0.0352
Epoch [30/50], Class Loss: 0.0539, Discrepancy Loss: 0.0263
Epoch [31/50], Class Loss: 0.0110, Discrepancy Loss: 0.0235
Epoch [32/50], Class Loss: 0.0350, Discrepancy Loss: 0.0214
Epoch [33/50], Class Loss: 0.0173, Discrepancy Loss: 0.0241
Epoch [34/50], Class Loss: 0.0132, Discrepancy Loss: 0.0226
Epoch [35/50], Class Loss: 0.0296, Discrepancy Loss: 0.0246
Epoch [36/50], Class Loss: 0.0221, Discrepancy Loss: 0.0256
Epoch [37/50], Class Loss: 0.0148, Discrepancy Loss: 0.0235
Epoch [38/50], Class Loss: 0.0294, Discrepancy Loss: 0.0233
Epoch [39/50], Class Loss: 0.0113, Discrepancy Loss: 0.0210
Epoch [40/50], Class Loss: 0.0105, Discrepancy Loss: 0.0201
Epoch [41/50], Class Loss: 0.0166, Discrepancy Loss: 0.0201
Epoch [42/50], Class Loss: 0.0102, Discrepancy Loss: 0.0219
Epoch [43/50], Class Loss: 0.0109, Discrepancy Loss: 0.0220
Epoch [44/50], Class Loss: 0.0318, Discrepancy Loss: 0.0232
Epoch [45/50], Class Loss: 0.0079, Discrepancy Loss: 0.0284
Epoch [46/50], Class Loss: 0.0124, Discrepancy Loss: 0.0212
Epoch [47/50], Class Loss: 0.0379, Discrepancy Loss: 0.0204
Epoch [48/50], Class Loss: 0.0233, Discrepancy Loss: 0.0221
Epoch [49/50], Class Loss: 0.0348, Discrepancy Loss: 0.0231
Epoch [50/50], Class Loss: 0.0172, Discrepancy Loss: 0.0241
Source Domain Performance - Accuracy: 83.15%, Precision: 89.58%, Recall: 83.52%, F1 Score: 82.93%
Target Domain Performance - Accuracy: 93.59%, Precision: 93.57%, Recall: 93.44%, F1 Score: 93.43%

Run 3/3
Epoch [1/50], Class Loss: 1.6181, Discrepancy Loss: 0.1161
Epoch [2/50], Class Loss: 0.7431, Discrepancy Loss: 0.0928
Epoch [3/50], Class Loss: 0.4689, Discrepancy Loss: 0.0897
Epoch [4/50], Class Loss: 0.1586, Discrepancy Loss: 0.0341
Epoch [5/50], Class Loss: 1.7151, Discrepancy Loss: 0.1162
Epoch [6/50], Class Loss: 0.3443, Discrepancy Loss: 0.0631
Epoch [7/50], Class Loss: 0.1417, Discrepancy Loss: 0.0308
Epoch [8/50], Class Loss: 0.0534, Discrepancy Loss: 0.0208
Epoch [9/50], Class Loss: 0.0483, Discrepancy Loss: 0.0164
Epoch [10/50], Class Loss: 0.0167, Discrepancy Loss: 0.0130
Epoch [11/50], Class Loss: 0.0059, Discrepancy Loss: 0.0079
Epoch [12/50], Class Loss: 0.0079, Discrepancy Loss: 0.0066
Epoch [13/50], Class Loss: 0.0037, Discrepancy Loss: 0.0093
Epoch [14/50], Class Loss: 0.0072, Discrepancy Loss: 0.0066
Epoch [15/50], Class Loss: 0.0049, Discrepancy Loss: 0.0085
Epoch [16/50], Class Loss: 0.0060, Discrepancy Loss: 0.0083
Epoch [17/50], Class Loss: 0.0478, Discrepancy Loss: 0.0060
Epoch [18/50], Class Loss: 0.0200, Discrepancy Loss: 0.0112
Epoch [19/50], Class Loss: 0.0066, Discrepancy Loss: 0.0116
Epoch [20/50], Class Loss: 0.0033, Discrepancy Loss: 0.0080
Epoch [21/50], Class Loss: 0.0640, Discrepancy Loss: 0.0061
Epoch [22/50], Class Loss: 0.0036, Discrepancy Loss: 0.0093
Epoch [23/50], Class Loss: 0.0043, Discrepancy Loss: 0.0086
Epoch [24/50], Class Loss: 0.0050, Discrepancy Loss: 0.0090
Epoch [25/50], Class Loss: 0.0111, Discrepancy Loss: 0.0125
Epoch [26/50], Class Loss: 0.0033, Discrepancy Loss: 0.0088
Epoch [27/50], Class Loss: 0.0132, Discrepancy Loss: 0.0082
Epoch [28/50], Class Loss: 0.0026, Discrepancy Loss: 0.0068
Epoch [29/50], Class Loss: 0.0038, Discrepancy Loss: 0.0074
Epoch [30/50], Class Loss: 0.0041, Discrepancy Loss: 0.0090
Epoch [31/50], Class Loss: 0.0083, Discrepancy Loss: 0.0068
Epoch [32/50], Class Loss: 0.0049, Discrepancy Loss: 0.0085
Epoch [33/50], Class Loss: 0.0080, Discrepancy Loss: 0.0082
Epoch [34/50], Class Loss: 0.0031, Discrepancy Loss: 0.0084
Epoch [35/50], Class Loss: 0.0050, Discrepancy Loss: 0.0088
Epoch [36/50], Class Loss: 0.0285, Discrepancy Loss: 0.0080
Epoch [37/50], Class Loss: 0.0027, Discrepancy Loss: 0.0054
Epoch [38/50], Class Loss: 0.0244, Discrepancy Loss: 0.0078
Epoch [39/50], Class Loss: 0.0033, Discrepancy Loss: 0.0068
Epoch [40/50], Class Loss: 0.0028, Discrepancy Loss: 0.0081
Epoch [41/50], Class Loss: 0.0022, Discrepancy Loss: 0.0074
Epoch [42/50], Class Loss: 0.0030, Discrepancy Loss: 0.0087
Epoch [43/50], Class Loss: 0.0046, Discrepancy Loss: 0.0068
Epoch [44/50], Class Loss: 0.0021, Discrepancy Loss: 0.0092
Epoch [45/50], Class Loss: 0.0154, Discrepancy Loss: 0.0072
Epoch [46/50], Class Loss: 0.0024, Discrepancy Loss: 0.0082
Epoch [47/50], Class Loss: 0.0421, Discrepancy Loss: 0.0094
Epoch [48/50], Class Loss: 0.0151, Discrepancy Loss: 0.0075
Epoch [49/50], Class Loss: 0.0028, Discrepancy Loss: 0.0069
Epoch [50/50], Class Loss: 0.0313, Discrepancy Loss: 0.0073
Source Domain Performance - Accuracy: 99.34%, Precision: 99.35%, Recall: 99.34%, F1 Score: 99.34%
Target Domain Performance - Accuracy: 95.50%, Precision: 95.53%, Recall: 95.49%, F1 Score: 95.45%

Source performance: 93.92% 96.07% 94.05% 93.85%
Target performance: 94.92% 94.92% 94.90% 94.84%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 96.29%
16qam: 87.53%
8apsk: 95.77%
MCD

Run 1/3
Epoch [1/50], Class Loss: 0.9209, Discrepancy Loss: 0.0349
Validation Loss: 7.2625
Epoch [2/50], Class Loss: 0.2429, Discrepancy Loss: 0.0209
Validation Loss: 2.9000
Epoch [3/50], Class Loss: 0.1577, Discrepancy Loss: 0.0273
Validation Loss: 30.9625
Epoch [4/50], Class Loss: 0.2233, Discrepancy Loss: 0.0198
Validation Loss: 0.2145
Epoch [5/50], Class Loss: 0.1127, Discrepancy Loss: 0.0464
Validation Loss: 0.5627
Epoch [6/50], Class Loss: 0.0690, Discrepancy Loss: 0.0611
Validation Loss: 10.8452
Epoch [7/50], Class Loss: 0.0720, Discrepancy Loss: 0.0809
Validation Loss: 8.9702
Epoch [8/50], Class Loss: 0.0324, Discrepancy Loss: 0.1307
Validation Loss: 2.5789
Epoch [9/50], Class Loss: 0.0608, Discrepancy Loss: 0.0782
Validation Loss: 0.2676
Early stopping!
Source Domain Performance - Accuracy: 97.66%, Precision: 97.84%, Recall: 97.74%, F1 Score: 97.69%
Target Domain Performance - Accuracy: 64.03%, Precision: 60.38%, Recall: 62.47%, F1 Score: 56.55%

Run 2/3
Epoch [1/50], Class Loss: 0.6669, Discrepancy Loss: 0.0467
Validation Loss: 3.2970
Epoch [2/50], Class Loss: 0.2576, Discrepancy Loss: 0.0448
Validation Loss: 0.8956
Epoch [3/50], Class Loss: 0.1980, Discrepancy Loss: 0.0534
Validation Loss: 15.0497
Epoch [4/50], Class Loss: 0.1833, Discrepancy Loss: 0.0971
Validation Loss: 7.0993
Epoch [5/50], Class Loss: 0.0418, Discrepancy Loss: 0.0461
Validation Loss: 0.4915
Epoch [6/50], Class Loss: 0.0302, Discrepancy Loss: 0.0564
Validation Loss: 17.4106
Epoch [7/50], Class Loss: 0.0214, Discrepancy Loss: 0.0876
Validation Loss: 1.1315
Epoch [8/50], Class Loss: 0.0350, Discrepancy Loss: 0.1645
Validation Loss: 0.0197
Epoch [9/50], Class Loss: 0.0341, Discrepancy Loss: 0.1492
Validation Loss: 2.4575
Epoch [10/50], Class Loss: 0.0706, Discrepancy Loss: 0.1632
Validation Loss: 42.7194
Epoch [11/50], Class Loss: 0.0271, Discrepancy Loss: 0.0605
Validation Loss: 0.0004
Epoch [12/50], Class Loss: 0.0245, Discrepancy Loss: 0.0618
Validation Loss: 0.5010
Epoch [13/50], Class Loss: 0.0177, Discrepancy Loss: 0.0638
Validation Loss: 0.0044
Epoch [14/50], Class Loss: 0.0184, Discrepancy Loss: 0.0683
Validation Loss: 0.0038
Epoch [15/50], Class Loss: 0.0137, Discrepancy Loss: 0.0399
Validation Loss: 0.0062
Epoch [16/50], Class Loss: 0.0041, Discrepancy Loss: 0.0289
Validation Loss: 0.0021
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 61.45%, Precision: 75.86%, Recall: 60.21%, F1 Score: 55.86%

Run 3/3
Epoch [1/50], Class Loss: 0.7132, Discrepancy Loss: 0.0654
Validation Loss: 4.9055
Epoch [2/50], Class Loss: 0.2590, Discrepancy Loss: 0.0298
Validation Loss: 0.3329
Epoch [3/50], Class Loss: 0.1329, Discrepancy Loss: 0.0229
Validation Loss: 0.1273
Epoch [4/50], Class Loss: 0.1366, Discrepancy Loss: 0.0506
Validation Loss: 1.3642
Epoch [5/50], Class Loss: 0.0500, Discrepancy Loss: 0.0439
Validation Loss: 9.9003
Epoch [6/50], Class Loss: 0.1485, Discrepancy Loss: 0.0639
Validation Loss: 0.3293
Epoch [7/50], Class Loss: 0.0358, Discrepancy Loss: 0.0344
Validation Loss: 0.3906
Epoch [8/50], Class Loss: 0.0231, Discrepancy Loss: 0.0995
Validation Loss: 0.2316
Early stopping!
Source Domain Performance - Accuracy: 97.36%, Precision: 97.48%, Recall: 97.37%, F1 Score: 97.32%
Target Domain Performance - Accuracy: 58.27%, Precision: 83.47%, Recall: 59.24%, F1 Score: 53.56%

Source performance: 98.34% 98.44% 98.37% 98.34%
Target performance: 61.25% 73.24% 60.64% 55.33%

Per-Class Accuracy on Target Domain:
bpsk: 100.00%
qpsk: 37.47%
16qam: 33.09%
8apsk: 72.01%
JAN

Run 1/3
Epoch [1/50], Class Loss: 0.3887, JMMD Loss: 0.1898
Validation Loss: 1.8580
Epoch [2/50], Class Loss: 0.1943, JMMD Loss: 0.1448
Validation Loss: 0.5086
Epoch [3/50], Class Loss: 0.1198, JMMD Loss: 0.1136
Validation Loss: 0.0546
Epoch [4/50], Class Loss: 0.0442, JMMD Loss: 0.0988
Validation Loss: 0.1430
Epoch [5/50], Class Loss: 0.0362, JMMD Loss: 0.1052
Validation Loss: 2.3663
Epoch [6/50], Class Loss: 0.0265, JMMD Loss: 0.0890
Validation Loss: 2.1923
Epoch [7/50], Class Loss: 0.0063, JMMD Loss: 0.0978
Validation Loss: 2.0150
Epoch [8/50], Class Loss: 0.0070, JMMD Loss: 0.0835
Validation Loss: 6.0301
Early stopping!
Source Domain Performance - Accuracy: 55.64%, Precision: 83.94%, Recall: 55.28%, F1 Score: 47.25%
Target Domain Performance - Accuracy: 76.14%, Precision: 86.17%, Recall: 75.66%, F1 Score: 69.84%

Run 2/3
Epoch [1/50], Class Loss: 0.3617, JMMD Loss: 0.2058
Validation Loss: 0.1546
Epoch [2/50], Class Loss: 0.1348, JMMD Loss: 0.1356
Validation Loss: 1.5923
Epoch [3/50], Class Loss: 0.1369, JMMD Loss: 0.1073
Validation Loss: 0.3917
Epoch [4/50], Class Loss: 0.1074, JMMD Loss: 0.1269
Validation Loss: 1.0791
Epoch [5/50], Class Loss: 0.0526, JMMD Loss: 0.0997
Validation Loss: 0.0391
Epoch [6/50], Class Loss: 0.0216, JMMD Loss: 0.0910
Validation Loss: 0.3624
Epoch [7/50], Class Loss: 0.0070, JMMD Loss: 0.0958
Validation Loss: 0.0262
Epoch [8/50], Class Loss: 0.0050, JMMD Loss: 0.0818
Validation Loss: 0.0356
Epoch [9/50], Class Loss: 0.0078, JMMD Loss: 0.0810
Validation Loss: 0.0837
Epoch [10/50], Class Loss: 0.0286, JMMD Loss: 0.0978
Validation Loss: 0.5731
Epoch [11/50], Class Loss: 0.0141, JMMD Loss: 0.0774
Validation Loss: 0.0018
Epoch [12/50], Class Loss: 0.0008, JMMD Loss: 0.0655
Validation Loss: 0.0009
Epoch [13/50], Class Loss: 0.0008, JMMD Loss: 0.0657
Validation Loss: 0.0008
Epoch [14/50], Class Loss: 0.0007, JMMD Loss: 0.0588
Validation Loss: 0.0008
Epoch [15/50], Class Loss: 0.0008, JMMD Loss: 0.0607
Validation Loss: 0.0009
Epoch [16/50], Class Loss: 0.0010, JMMD Loss: 0.0625
Validation Loss: 0.0021
Epoch [17/50], Class Loss: 0.0009, JMMD Loss: 0.0657
Validation Loss: 0.0016
Epoch [18/50], Class Loss: 0.0007, JMMD Loss: 0.0508
Validation Loss: 0.0014
Early stopping!
Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%
Target Domain Performance - Accuracy: 97.42%, Precision: 97.41%, Recall: 97.41%, F1 Score: 97.36%

Run 3/3
Epoch [1/50], Class Loss: 0.3995, JMMD Loss: 0.1963
Validation Loss: 0.1356
Epoch [2/50], Class Loss: 0.1769, JMMD Loss: 0.1297
Validation Loss: 0.1401
Epoch [3/50], Class Loss: 0.0922, JMMD Loss: 0.1012
Validation Loss: 0.0909
Epoch [4/50], Class Loss: 0.1465, JMMD Loss: 0.1302
Validation Loss: 0.2285
Epoch [5/50], Class Loss: 0.1343, JMMD Loss: 0.1027
Validation Loss: 2.8069
Epoch [6/50], Class Loss: 0.1769, JMMD Loss: 0.1181
Validation Loss: 0.0958
Epoch [7/50], Class Loss: 0.0605, JMMD Loss: 0.1167
Validation Loss: 0.4324
Epoch [8/50], Class Loss: 0.0533, JMMD Loss: 0.1150
Validation Loss: 0.1110
Early stopping!
Source Domain Performance - Accuracy: 96.16%, Precision: 96.64%, Recall: 96.29%, F1 Score: 96.19%
Target Domain Performance - Accuracy: 90.95%, Precision: 91.41%, Recall: 90.91%, F1 Score: 90.96%

Source performance: 83.93% 93.53% 83.86% 81.15%
Target performance: 88.17% 91.66% 87.99% 86.05%

Per-Class Accuracy on Target Domain (Mean over runs):
  Class 0: 100.00%
  Class 1: 96.20%
  Class 2: 59.41%
  Class 3: 96.36%
</pre>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell" id="cell-id=35388afa-33e5-4846-86dc-331fc02dda64">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [3]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_snr</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_deep_coral_acc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_base_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Base'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_dann_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'DANN'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_star_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Star'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_mcd_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'D'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'MCD'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'v'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'DCORAL'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_jan_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'x'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'JANN'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'SNR (dB)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Acc (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="jp-Cell-outputWrapper">
<div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser">
</div>
<div class="jp-OutputArea jp-Cell-outputArea">
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedText jp-OutputArea-output" data-mime-type="text/plain" tabindex="0">
<pre>4
4
</pre>
</div>
</div>
<div class="jp-OutputArea-child">
<div class="jp-OutputPrompt jp-OutputArea-prompt"></div>
<div class="jp-RenderedImage jp-OutputArea-output" tabindex="0">
<img alt="No description has been provided for this image" class="" src="data:image/png;base64,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"/>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=ecc02521-9d1d-4884-8afc-a020b324e18b">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Resnet for DANN.</span>
<span class="sd">"""</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"/home/ash/ic3/testbed_da/data"</span>

<span class="c1"># Classes in loaded npy files</span>
<span class="n">class_subset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bpsk"</span><span class="p">,</span> <span class="s2">"qpsk"</span><span class="p">,</span> <span class="s2">"16qam"</span><span class="p">,</span> <span class="s2">"8apsk"</span><span class="p">]</span>

<span class="c1"># simulated data</span>
<span class="n">X_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_X.npy"</span><span class="p">)</span>
<span class="n">Y_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_Y.npy"</span><span class="p">)</span>

<span class="c1"># over the air data</span>
<span class="n">X_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_X.npy"</span><span class="p">)</span>
<span class="n">Y_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_Y.npy"</span><span class="p">)</span>

<span class="n">z_val</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_snr</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># (Lists for storing accuracies; you may also want to add lists for deep coral)</span>
<span class="n">t_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">s_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_snr</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1">SNR level: </span><span class="si">{</span><span class="n">z_val</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="c1"># Filter for SNR level (for simulated data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_sim</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="p">)</span>
    <span class="n">X_s</span> <span class="o">=</span> <span class="n">X_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_s</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># first column: class labels</span>

    <span class="c1"># Filter for SNR level (for over the air data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_ota</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">X_t</span> <span class="o">=</span> <span class="n">X_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Create data loaders</span>
    <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">Y_s</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_t</span><span class="p">,</span> <span class="n">Y_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">z_val</span> <span class="o">+=</span> <span class="mi">4</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_snr</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_deep_coral_acc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="c1">#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')</span>
<span class="c1">#plt.plot(x, s_dann_acc, marker='v', linestyle='-', label='Source')</span>
<span class="c1">#plt.plot(x, t_dann_acc, marker='^', linestyle='-', label='Target')</span>
<span class="c1">#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')</span>
<span class="c1">#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'v'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Target'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Source'</span><span class="p">)</span>
<span class="c1">#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'SNR (dB)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Acc (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=d1525c72-3a7a-4191-bb1d-9af898afff3d">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Deep CORAL + ResNet.</span>
<span class="sd">"""</span>

<span class="c1"># -------------------------</span>
<span class="c1"># Data loading (same as before)</span>
<span class="c1"># -------------------------</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"/home/ash/ic3/testbed_da/data"</span>

<span class="c1"># Classes in loaded npy files</span>
<span class="n">class_subset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bpsk"</span><span class="p">,</span> <span class="s2">"qpsk"</span><span class="p">,</span> <span class="s2">"16qam"</span><span class="p">,</span> <span class="s2">"8apsk"</span><span class="p">]</span>

<span class="c1"># simulated data</span>
<span class="n">X_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_X.npy"</span><span class="p">)</span>
<span class="n">Y_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_Y.npy"</span><span class="p">)</span>

<span class="c1"># over the air data</span>
<span class="n">X_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_X.npy"</span><span class="p">)</span>
<span class="n">Y_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_Y.npy"</span><span class="p">)</span>

<span class="n">z_val</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_snr</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># (Lists for storing accuracies; you may also want to add lists for deep coral)</span>
<span class="n">t_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">s_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_snr</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1">SNR level: </span><span class="si">{</span><span class="n">z_val</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="c1"># Filter for SNR level (for simulated data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_sim</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="p">)</span>
    <span class="n">X_s</span> <span class="o">=</span> <span class="n">X_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_s</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># first column: class labels</span>

    <span class="c1"># Filter for SNR level (for over the air data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_ota</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">X_t</span> <span class="o">=</span> <span class="n">X_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Create data loaders</span>
    <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">Y_s</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_t</span><span class="p">,</span> <span class="n">Y_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">z_val</span> <span class="o">+=</span> <span class="mi">4</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_snr</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_deep_coral_acc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="c1">#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')</span>
<span class="c1">#plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')</span>
<span class="c1">#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')</span>
<span class="c1">#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'v'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Target'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Source'</span><span class="p">)</span>
<span class="c1">#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'SNR (dB)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Acc (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=814f35aa-6165-44a4-8296-2cae8fc363ce">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">STAR + ResNet.</span>
<span class="sd">"""</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"/home/ash/ic3/testbed_da/data"</span>

<span class="c1"># Classes in loaded npy files</span>
<span class="n">class_subset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bpsk"</span><span class="p">,</span> <span class="s2">"qpsk"</span><span class="p">,</span> <span class="s2">"16qam"</span><span class="p">,</span> <span class="s2">"8apsk"</span><span class="p">]</span>

<span class="c1"># simulated data</span>
<span class="n">X_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_X.npy"</span><span class="p">)</span>
<span class="n">Y_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_Y.npy"</span><span class="p">)</span>

<span class="c1"># over the air data</span>
<span class="n">X_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_X.npy"</span><span class="p">)</span>
<span class="n">Y_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_Y.npy"</span><span class="p">)</span>

<span class="n">z_val</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_snr</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># (Lists for storing accuracies; you may also want to add lists for deep coral)</span>
<span class="n">t_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">s_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_snr</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1">SNR level: </span><span class="si">{</span><span class="n">z_val</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="c1"># Filter for SNR level (for simulated data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_sim</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="p">)</span>
    <span class="n">X_s</span> <span class="o">=</span> <span class="n">X_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_s</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># first column: class labels</span>

    <span class="c1"># Filter for SNR level (for over the air data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_ota</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">X_t</span> <span class="o">=</span> <span class="n">X_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Create data loaders</span>
    <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">Y_s</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_t</span><span class="p">,</span> <span class="n">Y_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">z_val</span> <span class="o">+=</span> <span class="mi">4</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_snr</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_deep_coral_acc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="c1">#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')</span>
<span class="c1">#plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')</span>
<span class="c1">#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')</span>
<span class="c1">#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'v'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Target'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Source'</span><span class="p">)</span>
<span class="c1">#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'SNR (dB)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Acc (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=4ab5b42e-8a61-4476-9f9c-15314b3fdf21">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Resnet for MCD</span>
<span class="sd">"""</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"/home/ash/ic3/testbed_da/data"</span>

<span class="c1"># Classes in loaded npy files</span>
<span class="n">class_subset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bpsk"</span><span class="p">,</span> <span class="s2">"qpsk"</span><span class="p">,</span> <span class="s2">"16qam"</span><span class="p">,</span> <span class="s2">"8apsk"</span><span class="p">]</span>

<span class="c1"># simulated data</span>
<span class="n">X_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_X.npy"</span><span class="p">)</span>
<span class="n">Y_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_Y.npy"</span><span class="p">)</span>

<span class="c1"># over the air data</span>
<span class="n">X_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_X.npy"</span><span class="p">)</span>
<span class="n">Y_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_Y.npy"</span><span class="p">)</span>

<span class="n">z_val</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_snr</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># (Lists for storing accuracies; you may also want to add lists for deep coral)</span>
<span class="n">t_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">s_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_snr</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1">SNR level: </span><span class="si">{</span><span class="n">z_val</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="c1"># Filter for SNR level (for simulated data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_sim</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="p">)</span>
    <span class="n">X_s</span> <span class="o">=</span> <span class="n">X_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_s</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># first column: class labels</span>

    <span class="c1"># Filter for SNR level (for over the air data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_ota</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">X_t</span> <span class="o">=</span> <span class="n">X_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Create data loaders</span>
    <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">Y_s</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_t</span><span class="p">,</span> <span class="n">Y_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">z_val</span> <span class="o">+=</span> <span class="mi">4</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_snr</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_deep_coral_acc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="c1">#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')</span>
<span class="c1">#plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')</span>
<span class="c1">#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')</span>
<span class="c1">#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'v'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Target'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Source'</span><span class="p">)</span>
<span class="c1">#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'SNR (dB)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Acc (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs" id="cell-id=7c08b866-4797-445a-94cd-4a0941268e77">
<div class="jp-Cell-inputWrapper" tabindex="0">
<div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser">
</div>
<div class="jp-InputArea jp-Cell-inputArea">
<div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div>
<div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline">
<div class="cm-editor cm-s-jupyter">
<div class="highlight hl-ipython3"><pre><span></span><span class="sd">"""</span>
<span class="sd">Resnet for JAN</span>
<span class="sd">"""</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="s2">"/home/ash/ic3/testbed_da/data"</span>

<span class="c1"># Classes in loaded npy files</span>
<span class="n">class_subset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"bpsk"</span><span class="p">,</span> <span class="s2">"qpsk"</span><span class="p">,</span> <span class="s2">"16qam"</span><span class="p">,</span> <span class="s2">"8apsk"</span><span class="p">]</span>

<span class="c1"># simulated data</span>
<span class="n">X_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_X.npy"</span><span class="p">)</span>
<span class="n">Y_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/sim_Y.npy"</span><span class="p">)</span>

<span class="c1"># over the air data</span>
<span class="n">X_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_X.npy"</span><span class="p">)</span>
<span class="n">Y_ota</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_path</span> <span class="o">+</span> <span class="s2">"/ota_Y.npy"</span><span class="p">)</span>

<span class="n">z_val</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">n_runs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_snr</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># (Lists for storing accuracies; you may also want to add lists for deep coral)</span>
<span class="n">t_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">s_deep_coral_acc</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_snr</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1">SNR level: </span><span class="si">{</span><span class="n">z_val</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="c1"># Filter for SNR level (for simulated data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_sim</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="p">)</span>
    <span class="n">X_s</span> <span class="o">=</span> <span class="n">X_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_sim</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_s</span> <span class="o">=</span> <span class="n">Y_s</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># first column: class labels</span>

    <span class="c1"># Filter for SNR level (for over the air data)</span>
    <span class="n">source_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_ota</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z_val</span><span class="o">+</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">X_t</span> <span class="o">=</span> <span class="n">X_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_ota</span><span class="p">[</span><span class="n">source_mask</span><span class="p">]</span>
    <span class="n">Y_t</span> <span class="o">=</span> <span class="n">Y_t</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Create data loaders</span>
    <span class="n">S_train_loader</span><span class="p">,</span> <span class="n">S_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span> <span class="n">Y_s</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">T_train_loader</span><span class="p">,</span> <span class="n">T_val_loader</span> <span class="o">=</span> <span class="n">funcs</span><span class="o">.</span><span class="n">create_loader</span><span class="p">(</span><span class="n">X_t</span><span class="p">,</span> <span class="n">Y_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">permute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">z_val</span> <span class="o">+=</span> <span class="mi">4</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_snr</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_deep_coral_acc</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="c1">#plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')</span>
<span class="c1">#plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')</span>
<span class="c1">#plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')</span>
<span class="c1">#plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'v'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Target'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s_deep_coral_acc</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'^'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Source'</span><span class="p">)</span>
<span class="c1">#plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'SNR (dB)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Acc (%)'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</main>
</body>
</html>
