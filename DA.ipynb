{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c77d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Over the air domain adaptation.\n",
    "\n",
    "4 Modulations: bpsk, qpsk, 16qam and 16apsk.\n",
    "Source SNR as 24, target from 10 to 22.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import funcs\n",
    "import jan\n",
    "import coral\n",
    "import star\n",
    "import mcd\n",
    "import dann\n",
    "import base\n",
    "import plots\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5022b5-1350-4b3b-8bcc-c653e90518d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load testbed data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"/home/ash/ic3/testbed_da/data\"\n",
    "\n",
    "# Classes\n",
    "class_subset = [\"bpsk\", \"qpsk\", \"16qam\", \"16apsk\"]\n",
    "\n",
    "# Split source, target\n",
    "# try selecting some of the mods, not all\n",
    "X = np.load(file_path + \"/ota_X.npy\")\n",
    "Y = np.load(file_path + \"/ota_Y.npy\")\n",
    "\n",
    "sou_snr = 24\n",
    "tar_snr = 10\n",
    "\n",
    "t_base_acc = []\n",
    "t_dann_acc = []\n",
    "t_star_acc = []\n",
    "t_mcd_acc = []\n",
    "t_coral_acc = []\n",
    "t_jan_acc = []\n",
    "\n",
    "s_base_acc = []\n",
    "s_dann_acc = []\n",
    "s_star_acc = []\n",
    "s_mcd_acc = []\n",
    "s_coral_acc = []\n",
    "s_jan_acc = []\n",
    "\n",
    "n_runs = 3\n",
    "lr = 0.002\n",
    "\n",
    "for i in range(7):    \n",
    "    source_mask = (Y[:, 1] == sou_snr)\n",
    "    target_mask = (Y[:, 1] == tar_snr)\n",
    "    \n",
    "    X_s = X[source_mask]\n",
    "    Y_s = Y[source_mask]\n",
    "    Y_s = Y_s[:,0]\n",
    "    \n",
    "    X_t = X[target_mask]\n",
    "    Y_t = Y[target_mask]\n",
    "    Y_t = Y_t[:,0]\n",
    "\n",
    "    \n",
    "    # Dataloaders\n",
    "    S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, permute=False)\n",
    "    T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, permute=False)\n",
    "\n",
    "    s_base, t_base = base.Base(model_cls=base.CLDNN, device=device, S_train_loader=S_train_loader, \n",
    "                    S_val_loader=S_val_loader, T_val_loader=T_val_loader, class_subset=class_subset, \n",
    "                    n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_base_acc.append(s_base)\n",
    "    t_base_acc.append(t_base)\n",
    "    \n",
    "    s_dan, t_dan = dann.DAN(dann.DANN, FA=dann.CLDNN_FA, LP=dann.CLDNN_LP, DC=dann.CLDNN_DC,\n",
    "                      device=device, S_train_loader=S_train_loader, S_val_loader=S_val_loader,\n",
    "                      T_train_loader=T_train_loader, T_val_loader=T_val_loader,\n",
    "                      class_subset=class_subset, n_classes=len(class_subset), lr=lr,\n",
    "                      n_epochs=25, n_runs=n_runs).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_dann_acc.append(s_dan)\n",
    "    t_dann_acc.append(t_dan)\n",
    "    \n",
    "    s_star, t_star = star.Star(G=star.CLDNN_G, C=star.CLDNN_C, device=device, S_train_loader=S_train_loader, S_val_loader=S_val_loader,  \n",
    "               T_train_loader=T_train_loader, T_val_loader=T_val_loader, class_subset=class_subset,\n",
    "               n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs, patience=5).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_star_acc.append(s_star)\n",
    "    t_star_acc.append(t_star)\n",
    "    \n",
    "    s_mcd, t_mcd = mcd.Mcd(G=mcd.CLDNN_G, C=mcd.CLDNN_C, device=device, S_train_loader=S_train_loader, S_val_loader=S_val_loader,  \n",
    "               T_train_loader=T_train_loader, T_val_loader=T_val_loader, class_subset=class_subset,\n",
    "               n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs, patience=5).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_mcd_acc.append(s_mcd)\n",
    "    t_mcd_acc.append(t_mcd)\n",
    "    \n",
    "    s_coral, t_coral = coral.Coral(G=coral.CLDNN_G, C=coral.CLDNN_C, device=device, S_train_loader=S_train_loader,\n",
    "                           S_val_loader=S_val_loader, T_train_loader=T_train_loader, T_val_loader=T_val_loader,\n",
    "                           class_subset=class_subset, n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs,\n",
    "                           patience=5, lambda_coral=0.5).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_coral_acc.append(s_coral)\n",
    "    t_coral_acc.append(t_coral)\n",
    "\n",
    "    s_jan, t_jan = jan.Jan(num_classes=len(class_subset), device=device, S_train_loader=S_train_loader,\n",
    "                     T_train_loader=T_train_loader, S_val_loader=S_val_loader, T_val_loader=T_val_loader,\n",
    "                     n_epochs=50, lr=lr, lambda_jmmd=0.1, n_runs=n_runs).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_jan_acc.append(s_jan)\n",
    "    t_jan_acc.append(t_jan)\n",
    "    \n",
    "    tar_snr += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75f4d8-b552-4e01-8024-24f80cc6eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 8)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, s_base_acc, marker='o', linestyle='-', label='Base')\n",
    "plt.plot(x, s_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "plt.plot(x, s_star_acc, marker='^', linestyle='--', label='Star')\n",
    "plt.plot(x, s_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, s_coral_acc, marker='v', linestyle='--', label='CORAL')\n",
    "plt.plot(x, s_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.title('Source performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x = np.arange(1, 8)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')\n",
    "plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')\n",
    "plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, t_coral_acc, marker='v', linestyle='--', label='CORAL')\n",
    "plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.title('Target performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2036317-47cb-4da7-9caa-b1f64b1857ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "class CLDNN(nn.Module):\n",
    "    def __init__(self, output_dim=4):\n",
    "        super(CLDNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=8, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=False)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        # After conv1 and pooling: input length 4096 becomes 4089 then 2044 after pooling.\n",
    "        # So the flattened output from the LSTM will be 2044 * 64.\n",
    "        self.fc1 = nn.Linear(2044 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has shape: (batch, 2, 4096)\n",
    "        x = self.conv1(x)      # -> (batch, 64, 4089)\n",
    "        x = self.pool(x)       # -> (batch, 64, 2044)\n",
    "        \n",
    "        # Permute to have sequence first: (batch, 2044, 64)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x, _ = self.lstm1(x)   # -> (batch, 2044, 64)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)   # -> (batch, 2044, 64)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Flatten: (batch, 2044*64)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "#%% DANN\n",
    "from torch.autograd import Function\n",
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None\n",
    "\n",
    "def grad_reverse(x, alpha=1.0):\n",
    "    return ReverseLayerF.apply(x, alpha)\n",
    "class CLDNN_FA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLDNN_FA, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=8, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        return x\n",
    "\n",
    "class CLDNN_LP(nn.Module):\n",
    "    def __init__(self, output_dim=7):\n",
    "        super(CLDNN_LP, self).__init__()\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(2044 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CLDNN_DC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLDNN_DC, self).__init__()\n",
    "        self.fc1 = nn.Linear(2044 * 64, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x, alpha):\n",
    "        x = ReverseLayerF.apply(x, alpha)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# star\n",
    "class STAR_G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STAR_G, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=8, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=False)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class STAR_C(nn.Module):\n",
    "    def __init__(self, output_dim, num_classifiers_train=2, num_classifiers_test=20, init='kaiming_u', use_init=False):\n",
    "        super(STAR_C, self).__init__()\n",
    "        self.num_classifiers_train = num_classifiers_train\n",
    "        self.num_classifiers_test = num_classifiers_test\n",
    "        self.init = init\n",
    "\n",
    "        function_init = {\n",
    "            'kaiming_u': nn.init.kaiming_uniform_,\n",
    "            'kaiming_n': nn.init.kaiming_normal_,\n",
    "            'xavier': nn.init.xavier_normal_\n",
    "        }\n",
    "\n",
    "        self.fc1 = nn.Linear(2044 * 64, 128)\n",
    "        self.bn1_fc = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.mu2 = nn.Parameter(torch.randn(output_dim, 128))\n",
    "        self.sigma2 = nn.Parameter(torch.zeros(output_dim, 128))\n",
    "\n",
    "        if use_init:\n",
    "            all_parameters = [self.mu2, self.sigma2]\n",
    "            for item in all_parameters:\n",
    "                function_init[self.init](item)\n",
    "\n",
    "        self.b2 = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, x, only_mu=True):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.bn1_fc(x))\n",
    "\n",
    "        sigma2_pos = torch.sigmoid(self.sigma2)\n",
    "        fc2_distribution = torch.distributions.Normal(self.mu2, sigma2_pos)\n",
    "\n",
    "        if self.training:\n",
    "            classifiers = []\n",
    "            for _ in range(self.num_classifiers_train):\n",
    "                fc2_w = fc2_distribution.rsample()\n",
    "                classifiers.append([fc2_w, self.b2])\n",
    "\n",
    "            outputs = []\n",
    "            for classifier in classifiers:\n",
    "                out = F.linear(x, classifier[0], classifier[1])\n",
    "                outputs.append(out)\n",
    "\n",
    "            return outputs\n",
    "        else:\n",
    "            if only_mu:\n",
    "                out = F.linear(x, self.mu2, self.b2)\n",
    "                return [out]\n",
    "            else:\n",
    "                classifiers = []\n",
    "                for _ in range(self.num_classifiers_test):\n",
    "                    fc2_w = fc2_distribution.rsample()\n",
    "                    classifiers.append([fc2_w, self.b2])\n",
    "\n",
    "                outputs = []\n",
    "                for classifier in classifiers:\n",
    "                    out = F.linear(x, classifier[0], classifier[1])\n",
    "                    outputs.append(out)\n",
    "                return outputs\n",
    "\n",
    "\n",
    "#%% mcd\n",
    "class GradReverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.lambda_, None\n",
    "\n",
    "def grad_reverse(x, lambda_=1.0):\n",
    "    return GradReverse.apply(x, lambda_)\n",
    "\n",
    "class MCD_G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MCD_G, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=2, out_channels=64, kernel_size=8, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, num_layers=1, batch_first=True, bidirectional=False)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return x\n",
    "    \n",
    "class MCD_C(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(MCD_C, self).__init__()\n",
    "        self.fc1 = nn.Linear(2044 * 64, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x, reverse=False, lambda_=1.0):\n",
    "        if reverse:\n",
    "            x = grad_reverse(x, lambda_)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1624700c-1365-4593-810c-4abec1a6d6b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 3.2449, Discrepancy Loss: 0.0851\n",
      "Validation Loss: 1.2299\n",
      "Epoch [2/50], Class Loss: 1.1931, Discrepancy Loss: 0.0365\n",
      "Validation Loss: 1.0637\n",
      "Epoch [3/50], Class Loss: 1.1135, Discrepancy Loss: 0.0389\n",
      "Validation Loss: 1.0070\n",
      "Epoch [4/50], Class Loss: 1.0245, Discrepancy Loss: 0.0633\n",
      "Validation Loss: 1.1558\n",
      "Epoch [5/50], Class Loss: 1.0137, Discrepancy Loss: 0.1024\n",
      "Validation Loss: 2.2449\n",
      "Epoch [6/50], Class Loss: 0.8495, Discrepancy Loss: 0.0696\n",
      "Validation Loss: 0.8897\n",
      "Epoch [7/50], Class Loss: 0.3697, Discrepancy Loss: 0.0698\n",
      "Validation Loss: 0.1339\n",
      "Epoch [8/50], Class Loss: 0.1969, Discrepancy Loss: 0.1218\n",
      "Validation Loss: 0.0216\n",
      "Epoch [9/50], Class Loss: 0.1892, Discrepancy Loss: 0.0849\n",
      "Validation Loss: 0.1134\n",
      "Epoch [10/50], Class Loss: 0.3929, Discrepancy Loss: 0.0532\n",
      "Validation Loss: 0.0221\n",
      "Epoch [11/50], Class Loss: 0.0118, Discrepancy Loss: 0.0255\n",
      "Validation Loss: 0.0085\n",
      "Epoch [12/50], Class Loss: 0.0057, Discrepancy Loss: 0.0270\n",
      "Validation Loss: 0.0079\n",
      "Epoch [13/50], Class Loss: 0.0066, Discrepancy Loss: 0.0213\n",
      "Validation Loss: 0.0067\n",
      "Epoch [14/50], Class Loss: 0.0091, Discrepancy Loss: 0.0241\n",
      "Validation Loss: 0.0093\n",
      "Epoch [15/50], Class Loss: 0.0138, Discrepancy Loss: 0.0268\n",
      "Validation Loss: 0.0167\n",
      "Epoch [16/50], Class Loss: 0.0108, Discrepancy Loss: 0.0245\n",
      "Validation Loss: 0.0521\n",
      "Epoch [17/50], Class Loss: 0.0067, Discrepancy Loss: 0.0246\n",
      "Validation Loss: 0.0247\n",
      "Epoch [18/50], Class Loss: 0.0218, Discrepancy Loss: 0.0267\n",
      "Validation Loss: 0.0094\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.88%, F1 Score: 99.88%\n",
      "Target Domain Performance - Accuracy: 47.60%, Precision: 24.92%, Recall: 47.84%, F1 Score: 32.27%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 2.2180, Discrepancy Loss: 0.0926\n",
      "Validation Loss: 1.2572\n",
      "Epoch [2/50], Class Loss: 1.0642, Discrepancy Loss: 0.0155\n",
      "Validation Loss: 1.0205\n",
      "Epoch [3/50], Class Loss: 0.9862, Discrepancy Loss: 0.0097\n",
      "Validation Loss: 0.9632\n",
      "Epoch [4/50], Class Loss: 0.9777, Discrepancy Loss: 0.0710\n",
      "Validation Loss: 0.8773\n",
      "Epoch [5/50], Class Loss: 0.7898, Discrepancy Loss: 0.0855\n",
      "Validation Loss: 0.5361\n",
      "Epoch [6/50], Class Loss: 0.6222, Discrepancy Loss: 0.0751\n",
      "Validation Loss: 0.1042\n",
      "Epoch [7/50], Class Loss: 0.0596, Discrepancy Loss: 0.0438\n",
      "Validation Loss: 0.0154\n",
      "Epoch [8/50], Class Loss: 0.0248, Discrepancy Loss: 0.0627\n",
      "Validation Loss: 0.0028\n",
      "Epoch [9/50], Class Loss: 0.0274, Discrepancy Loss: 0.0644\n",
      "Validation Loss: 0.0061\n",
      "Epoch [10/50], Class Loss: 0.0894, Discrepancy Loss: 0.0917\n",
      "Validation Loss: 0.2275\n",
      "Epoch [11/50], Class Loss: 0.0284, Discrepancy Loss: 0.0482\n",
      "Validation Loss: 0.0014\n",
      "Epoch [12/50], Class Loss: 0.0056, Discrepancy Loss: 0.0394\n",
      "Validation Loss: 0.0029\n",
      "Epoch [13/50], Class Loss: 0.0027, Discrepancy Loss: 0.0309\n",
      "Validation Loss: 0.0135\n",
      "Epoch [14/50], Class Loss: 0.0078, Discrepancy Loss: 0.0278\n",
      "Validation Loss: 0.0048\n",
      "Epoch [15/50], Class Loss: 0.0026, Discrepancy Loss: 0.0325\n",
      "Validation Loss: 0.0030\n",
      "Epoch [16/50], Class Loss: 0.0023, Discrepancy Loss: 0.0286\n",
      "Validation Loss: 0.0399\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.40%, Precision: 99.41%, Recall: 99.37%, F1 Score: 99.39%\n",
      "Target Domain Performance - Accuracy: 48.80%, Precision: 24.62%, Recall: 48.97%, F1 Score: 32.64%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 3.0089, Discrepancy Loss: 0.0939\n",
      "Validation Loss: 1.2125\n",
      "Epoch [2/50], Class Loss: 1.2368, Discrepancy Loss: 0.0289\n",
      "Validation Loss: 1.1945\n",
      "Epoch [3/50], Class Loss: 1.0715, Discrepancy Loss: 0.0171\n",
      "Validation Loss: 1.0537\n",
      "Epoch [4/50], Class Loss: 1.0106, Discrepancy Loss: 0.0387\n",
      "Validation Loss: 0.9437\n",
      "Epoch [5/50], Class Loss: 0.8971, Discrepancy Loss: 0.1082\n",
      "Validation Loss: 0.8430\n",
      "Epoch [6/50], Class Loss: 0.4643, Discrepancy Loss: 0.0519\n",
      "Validation Loss: 0.3694\n",
      "Epoch [7/50], Class Loss: 0.2089, Discrepancy Loss: 0.0903\n",
      "Validation Loss: 0.7208\n",
      "Epoch [8/50], Class Loss: 0.2243, Discrepancy Loss: 0.0250\n",
      "Validation Loss: 0.0234\n",
      "Epoch [9/50], Class Loss: 0.0623, Discrepancy Loss: 0.0522\n",
      "Validation Loss: 0.0182\n",
      "Epoch [10/50], Class Loss: 0.0956, Discrepancy Loss: 0.1026\n",
      "Validation Loss: 0.1931\n",
      "Epoch [11/50], Class Loss: 0.0152, Discrepancy Loss: 0.0599\n",
      "Validation Loss: 0.0137\n",
      "Epoch [12/50], Class Loss: 0.0072, Discrepancy Loss: 0.0560\n",
      "Validation Loss: 0.0106\n",
      "Epoch [13/50], Class Loss: 0.0093, Discrepancy Loss: 0.0446\n",
      "Validation Loss: 0.0088\n",
      "Epoch [14/50], Class Loss: 0.0052, Discrepancy Loss: 0.0371\n",
      "Validation Loss: 0.0061\n",
      "Epoch [15/50], Class Loss: 0.0045, Discrepancy Loss: 0.0324\n",
      "Validation Loss: 0.0114\n",
      "Epoch [16/50], Class Loss: 0.0032, Discrepancy Loss: 0.0305\n",
      "Validation Loss: 0.0040\n",
      "Epoch [17/50], Class Loss: 0.0023, Discrepancy Loss: 0.0294\n",
      "Validation Loss: 0.0033\n",
      "Epoch [18/50], Class Loss: 0.0022, Discrepancy Loss: 0.0301\n",
      "Validation Loss: 0.0043\n",
      "Epoch [19/50], Class Loss: 0.0026, Discrepancy Loss: 0.0342\n",
      "Validation Loss: 0.0031\n",
      "Epoch [20/50], Class Loss: 0.0029, Discrepancy Loss: 0.0331\n",
      "Validation Loss: 0.0042\n",
      "Epoch [21/50], Class Loss: 0.0017, Discrepancy Loss: 0.0303\n",
      "Validation Loss: 0.0033\n",
      "Epoch [22/50], Class Loss: 0.0019, Discrepancy Loss: 0.0302\n",
      "Validation Loss: 0.0034\n",
      "Epoch [23/50], Class Loss: 0.0020, Discrepancy Loss: 0.0311\n",
      "Validation Loss: 0.0037\n",
      "Epoch [24/50], Class Loss: 0.0019, Discrepancy Loss: 0.0299\n",
      "Validation Loss: 0.0032\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 100.00%, Precision: 100.00%, Recall: 100.00%, F1 Score: 100.00%\n",
      "Target Domain Performance - Accuracy: 49.88%, Precision: 24.96%, Recall: 50.00%, F1 Score: 33.26%\n",
      "\n",
      "Source performance: 99.76% 99.76% 99.75% 99.76%\n",
      "Target performance: 48.76% 24.83% 48.94% 32.72%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 0.00%\n",
      "16qam: 0.00%\n",
      "16apsk: 95.75%\n",
      "\n",
      "Run 1/3\n",
      "Epoch 1/50, Train Loss: 1.0717, Train Acc: 0.5598, Val Loss: 0.5873, Val Acc: 0.6888\n",
      "Epoch 2/50, Train Loss: 0.5391, Train Acc: 0.7185, Val Loss: 0.6943, Val Acc: 0.6595\n",
      "Epoch 3/50, Train Loss: 0.5236, Train Acc: 0.7379, Val Loss: 0.4893, Val Acc: 0.7428\n",
      "Epoch 4/50, Train Loss: 0.4441, Train Acc: 0.7831, Val Loss: 0.5823, Val Acc: 0.7110\n",
      "Epoch 5/50, Train Loss: 0.3423, Train Acc: 0.8790, Val Loss: 0.0397, Val Acc: 0.9856\n",
      "Epoch 6/50, Train Loss: 0.0743, Train Acc: 0.9760, Val Loss: 0.0111, Val Acc: 0.9970\n",
      "Epoch 7/50, Train Loss: 0.0151, Train Acc: 0.9948, Val Loss: 0.0008, Val Acc: 1.0000\n",
      "Epoch 8/50, Train Loss: 0.0038, Train Acc: 0.9987, Val Loss: 0.0350, Val Acc: 0.9898\n",
      "Epoch 9/50, Train Loss: 0.0214, Train Acc: 0.9942, Val Loss: 0.0372, Val Acc: 0.9850\n",
      "Epoch 10/50, Train Loss: 0.0259, Train Acc: 0.9922, Val Loss: 0.0074, Val Acc: 0.9970\n",
      "Epoch 11/50, Train Loss: 0.0004, Train Acc: 0.9999, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 18/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Run 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.8542, Train Acc: 0.5967, Val Loss: 0.5359, Val Acc: 0.7134\n",
      "Epoch 2/50, Train Loss: 0.5649, Train Acc: 0.7092, Val Loss: 0.7700, Val Acc: 0.6109\n",
      "Epoch 3/50, Train Loss: 0.4989, Train Acc: 0.7493, Val Loss: 0.5137, Val Acc: 0.7530\n",
      "Epoch 4/50, Train Loss: 0.3831, Train Acc: 0.8392, Val Loss: 0.2751, Val Acc: 0.9466\n",
      "Epoch 5/50, Train Loss: 0.3187, Train Acc: 0.9364, Val Loss: 15.5301, Val Acc: 0.4041\n",
      "Epoch 6/50, Train Loss: 0.9723, Train Acc: 0.8863, Val Loss: 0.0084, Val Acc: 0.9976\n",
      "Epoch 7/50, Train Loss: 0.0876, Train Acc: 0.9754, Val Loss: 0.0061, Val Acc: 0.9994\n",
      "Epoch 8/50, Train Loss: 0.0050, Train Acc: 0.9981, Val Loss: 0.0194, Val Acc: 0.9922\n",
      "Epoch 9/50, Train Loss: 0.0018, Train Acc: 0.9994, Val Loss: 0.0039, Val Acc: 0.9970\n",
      "Epoch 10/50, Train Loss: 0.0038, Train Acc: 0.9984, Val Loss: 0.0016, Val Acc: 0.9994\n",
      "Epoch 11/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 18/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 19/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 20/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 21/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 22/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 23/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 24/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 25/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 26/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 27/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 28/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Run 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.0049, Train Acc: 0.5462, Val Loss: 0.5353, Val Acc: 0.7038\n",
      "Epoch 2/50, Train Loss: 0.5434, Train Acc: 0.7166, Val Loss: 0.5100, Val Acc: 0.7320\n",
      "Epoch 3/50, Train Loss: 0.5002, Train Acc: 0.7460, Val Loss: 0.4629, Val Acc: 0.7530\n",
      "Epoch 4/50, Train Loss: 0.4768, Train Acc: 0.7854, Val Loss: 0.4125, Val Acc: 0.8963\n",
      "Epoch 5/50, Train Loss: 0.2219, Train Acc: 0.9234, Val Loss: 0.0968, Val Acc: 0.9646\n",
      "Epoch 6/50, Train Loss: 0.0238, Train Acc: 0.9925, Val Loss: 0.0287, Val Acc: 0.9910\n",
      "Epoch 7/50, Train Loss: 0.0267, Train Acc: 0.9924, Val Loss: 0.0076, Val Acc: 0.9970\n",
      "Epoch 8/50, Train Loss: 0.0289, Train Acc: 0.9948, Val Loss: 0.0050, Val Acc: 0.9982\n",
      "Epoch 9/50, Train Loss: 0.0804, Train Acc: 0.9793, Val Loss: 0.0584, Val Acc: 0.9796\n",
      "Epoch 10/50, Train Loss: 0.0085, Train Acc: 0.9973, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 11/50, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Source performance: 100.00 100.00 100.00 100.00\n",
      "Target performance: 49.80 24.97 49.92 33.24\n",
      "\n",
      "bpsk: 100.00\n",
      "qpsk: 0.00\n",
      "16qam: 0.00\n",
      "16apsk: 99.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 4.3502, Domain Loss: 2.1100, Class Loss: 2.2402\n",
      "Epoch 2/50, Loss: 2.2525, Domain Loss: 1.2679, Class Loss: 0.9846\n",
      "Epoch 3/50, Loss: 1.9047, Domain Loss: 1.3159, Class Loss: 0.5888\n",
      "Epoch 4/50, Loss: 1.7187, Domain Loss: 1.1374, Class Loss: 0.5813\n",
      "Epoch 5/50, Loss: 1.5824, Domain Loss: 1.0378, Class Loss: 0.5446\n",
      "Epoch 6/50, Loss: 1.4486, Domain Loss: 0.9279, Class Loss: 0.5208\n",
      "Epoch 7/50, Loss: 1.3580, Domain Loss: 0.8333, Class Loss: 0.5247\n",
      "Epoch 8/50, Loss: 1.3079, Domain Loss: 0.7564, Class Loss: 0.5515\n",
      "Epoch 9/50, Loss: 1.3369, Domain Loss: 0.7445, Class Loss: 0.5924\n",
      "Epoch 10/50, Loss: 1.4962, Domain Loss: 0.8564, Class Loss: 0.6398\n",
      "Epoch 11/50, Loss: 1.8639, Domain Loss: 1.0964, Class Loss: 0.7675\n",
      "Epoch 12/50, Loss: 1.4568, Domain Loss: 0.8699, Class Loss: 0.5869\n",
      "Epoch 13/50, Loss: 1.3752, Domain Loss: 0.8232, Class Loss: 0.5519\n",
      "Epoch 14/50, Loss: 1.2752, Domain Loss: 0.7733, Class Loss: 0.5018\n",
      "Epoch 15/50, Loss: 1.3074, Domain Loss: 0.7684, Class Loss: 0.5390\n",
      "Epoch 16/50, Loss: 1.8988, Domain Loss: 1.0806, Class Loss: 0.8182\n",
      "Epoch 17/50, Loss: 1.3564, Domain Loss: 0.7952, Class Loss: 0.5612\n",
      "Epoch 18/50, Loss: 1.4988, Domain Loss: 0.9207, Class Loss: 0.5781\n",
      "Epoch 19/50, Loss: 1.3324, Domain Loss: 0.8226, Class Loss: 0.5098\n",
      "Epoch 20/50, Loss: 2.1827, Domain Loss: 1.4323, Class Loss: 0.7503\n",
      "Epoch 21/50, Loss: 3.5811, Domain Loss: 2.1271, Class Loss: 1.4540\n",
      "Epoch 22/50, Loss: 2.3276, Domain Loss: 1.6249, Class Loss: 0.7027\n",
      "Epoch 23/50, Loss: 3.1714, Domain Loss: 2.1001, Class Loss: 1.0713\n",
      "Epoch 24/50, Loss: 3.0266, Domain Loss: 1.8280, Class Loss: 1.1985\n",
      "Epoch 25/50, Loss: 1.6955, Domain Loss: 0.9523, Class Loss: 0.7432\n",
      "Epoch 26/50, Loss: 1.4333, Domain Loss: 0.9070, Class Loss: 0.5263\n",
      "Epoch 27/50, Loss: 1.3605, Domain Loss: 0.8349, Class Loss: 0.5256\n",
      "Epoch 28/50, Loss: 1.3545, Domain Loss: 0.8634, Class Loss: 0.4911\n",
      "Epoch 29/50, Loss: 1.3197, Domain Loss: 0.7912, Class Loss: 0.5285\n",
      "Epoch 30/50, Loss: 1.3780, Domain Loss: 0.8076, Class Loss: 0.5704\n",
      "Epoch 31/50, Loss: 1.3816, Domain Loss: 0.8384, Class Loss: 0.5432\n",
      "Epoch 32/50, Loss: 1.3946, Domain Loss: 0.8653, Class Loss: 0.5293\n",
      "Epoch 33/50, Loss: 1.3806, Domain Loss: 0.8527, Class Loss: 0.5279\n",
      "Epoch 34/50, Loss: 1.3093, Domain Loss: 0.7979, Class Loss: 0.5114\n",
      "Epoch 35/50, Loss: 1.2246, Domain Loss: 0.7513, Class Loss: 0.4733\n",
      "Epoch 36/50, Loss: 1.2095, Domain Loss: 0.7487, Class Loss: 0.4608\n",
      "Epoch 37/50, Loss: 1.2483, Domain Loss: 0.7431, Class Loss: 0.5051\n",
      "Epoch 38/50, Loss: 1.2246, Domain Loss: 0.7645, Class Loss: 0.4602\n",
      "Epoch 39/50, Loss: 1.2060, Domain Loss: 0.7513, Class Loss: 0.4547\n",
      "Epoch 40/50, Loss: 1.2314, Domain Loss: 0.7728, Class Loss: 0.4587\n",
      "Epoch 41/50, Loss: 1.1992, Domain Loss: 0.7392, Class Loss: 0.4601\n",
      "Epoch 42/50, Loss: 1.1782, Domain Loss: 0.7242, Class Loss: 0.4540\n",
      "Epoch 43/50, Loss: 1.2373, Domain Loss: 0.7466, Class Loss: 0.4907\n",
      "Epoch 44/50, Loss: 1.1986, Domain Loss: 0.7313, Class Loss: 0.4674\n",
      "Epoch 45/50, Loss: 1.3337, Domain Loss: 0.7915, Class Loss: 0.5422\n",
      "Epoch 46/50, Loss: 1.4574, Domain Loss: 0.8158, Class Loss: 0.6415\n",
      "Epoch 47/50, Loss: 1.2894, Domain Loss: 0.7829, Class Loss: 0.5065\n",
      "Epoch 48/50, Loss: 1.4596, Domain Loss: 0.8422, Class Loss: 0.6173\n",
      "Epoch 49/50, Loss: 1.3868, Domain Loss: 0.8320, Class Loss: 0.5549\n",
      "Epoch 50/50, Loss: 1.8062, Domain Loss: 1.1237, Class Loss: 0.6826\n",
      "49.52\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 4.4054, Domain Loss: 2.2507, Class Loss: 2.1546\n",
      "Epoch 2/50, Loss: 2.3481, Domain Loss: 1.2788, Class Loss: 1.0693\n",
      "Epoch 3/50, Loss: 1.8073, Domain Loss: 1.1983, Class Loss: 0.6090\n",
      "Epoch 4/50, Loss: 1.7747, Domain Loss: 1.2101, Class Loss: 0.5646\n",
      "Epoch 5/50, Loss: 1.6120, Domain Loss: 1.0091, Class Loss: 0.6029\n",
      "Epoch 6/50, Loss: 1.3904, Domain Loss: 0.8364, Class Loss: 0.5540\n",
      "Epoch 7/50, Loss: 1.3905, Domain Loss: 0.8528, Class Loss: 0.5377\n",
      "Epoch 8/50, Loss: 1.6302, Domain Loss: 0.9889, Class Loss: 0.6412\n",
      "Epoch 9/50, Loss: 1.6598, Domain Loss: 1.0132, Class Loss: 0.6466\n",
      "Epoch 10/50, Loss: 1.4994, Domain Loss: 0.9307, Class Loss: 0.5687\n",
      "Epoch 11/50, Loss: 2.8461, Domain Loss: 1.4663, Class Loss: 1.3798\n",
      "Epoch 12/50, Loss: 2.8931, Domain Loss: 1.6807, Class Loss: 1.2124\n",
      "Epoch 13/50, Loss: 1.9416, Domain Loss: 1.2238, Class Loss: 0.7178\n",
      "Epoch 14/50, Loss: 1.6737, Domain Loss: 1.0411, Class Loss: 0.6327\n",
      "Epoch 15/50, Loss: 1.5466, Domain Loss: 0.9889, Class Loss: 0.5577\n",
      "Epoch 16/50, Loss: 1.4932, Domain Loss: 0.9775, Class Loss: 0.5157\n",
      "Epoch 17/50, Loss: 1.6892, Domain Loss: 1.0997, Class Loss: 0.5895\n",
      "Epoch 18/50, Loss: 1.9090, Domain Loss: 1.1807, Class Loss: 0.7282\n",
      "Epoch 19/50, Loss: 1.9672, Domain Loss: 1.2415, Class Loss: 0.7257\n",
      "Epoch 20/50, Loss: 1.9765, Domain Loss: 1.3642, Class Loss: 0.6123\n",
      "Epoch 21/50, Loss: 1.7590, Domain Loss: 1.2002, Class Loss: 0.5589\n",
      "Epoch 22/50, Loss: 1.9683, Domain Loss: 1.3682, Class Loss: 0.6001\n",
      "Epoch 23/50, Loss: 1.8718, Domain Loss: 1.3088, Class Loss: 0.5631\n",
      "Epoch 24/50, Loss: 1.7331, Domain Loss: 1.1928, Class Loss: 0.5403\n",
      "Epoch 25/50, Loss: 1.6370, Domain Loss: 1.1267, Class Loss: 0.5103\n",
      "Epoch 26/50, Loss: 1.5688, Domain Loss: 1.0659, Class Loss: 0.5028\n",
      "Epoch 27/50, Loss: 1.6233, Domain Loss: 1.0639, Class Loss: 0.5594\n",
      "Epoch 28/50, Loss: 1.7193, Domain Loss: 1.1264, Class Loss: 0.5930\n",
      "Epoch 29/50, Loss: 1.6957, Domain Loss: 1.1417, Class Loss: 0.5540\n",
      "Epoch 30/50, Loss: 1.9366, Domain Loss: 1.3890, Class Loss: 0.5476\n",
      "Epoch 31/50, Loss: 1.8858, Domain Loss: 1.3990, Class Loss: 0.4868\n",
      "Epoch 32/50, Loss: 1.8537, Domain Loss: 1.3979, Class Loss: 0.4558\n",
      "Epoch 33/50, Loss: 1.8462, Domain Loss: 1.3968, Class Loss: 0.4494\n",
      "Epoch 34/50, Loss: 1.8278, Domain Loss: 1.3957, Class Loss: 0.4321\n",
      "Epoch 35/50, Loss: 1.7811, Domain Loss: 1.3942, Class Loss: 0.3869\n",
      "Epoch 36/50, Loss: 1.7835, Domain Loss: 1.3931, Class Loss: 0.3905\n",
      "Epoch 37/50, Loss: 1.8190, Domain Loss: 1.3919, Class Loss: 0.4271\n",
      "Epoch 38/50, Loss: 1.8630, Domain Loss: 1.3869, Class Loss: 0.4762\n",
      "Epoch 39/50, Loss: 1.6989, Domain Loss: 1.3865, Class Loss: 0.3123\n",
      "Epoch 40/50, Loss: 1.6214, Domain Loss: 1.3886, Class Loss: 0.2328\n",
      "Epoch 41/50, Loss: 1.9325, Domain Loss: 1.7194, Class Loss: 0.2132\n",
      "Epoch 42/50, Loss: 1.5715, Domain Loss: 1.3900, Class Loss: 0.1815\n",
      "Epoch 43/50, Loss: 2.0185, Domain Loss: 1.8446, Class Loss: 0.1739\n",
      "Epoch 44/50, Loss: 1.5106, Domain Loss: 1.3900, Class Loss: 0.1206\n",
      "Epoch 45/50, Loss: 1.4916, Domain Loss: 1.3897, Class Loss: 0.1019\n",
      "Epoch 46/50, Loss: 1.5022, Domain Loss: 1.3893, Class Loss: 0.1128\n",
      "Epoch 47/50, Loss: 1.4943, Domain Loss: 1.3889, Class Loss: 0.1055\n",
      "Epoch 48/50, Loss: 1.4815, Domain Loss: 1.3886, Class Loss: 0.0929\n",
      "Epoch 49/50, Loss: 1.4905, Domain Loss: 1.3882, Class Loss: 0.1023\n",
      "Epoch 50/50, Loss: 1.4597, Domain Loss: 1.3880, Class Loss: 0.0716\n",
      "45.92\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 5.2893, Domain Loss: 2.5590, Class Loss: 2.7303\n",
      "Epoch 2/50, Loss: 2.2609, Domain Loss: 1.0675, Class Loss: 1.1934\n",
      "Epoch 3/50, Loss: 1.9063, Domain Loss: 1.1284, Class Loss: 0.7779\n",
      "Epoch 4/50, Loss: 1.6301, Domain Loss: 1.0147, Class Loss: 0.6154\n",
      "Epoch 5/50, Loss: 1.5419, Domain Loss: 0.9556, Class Loss: 0.5863\n",
      "Epoch 6/50, Loss: 1.4614, Domain Loss: 0.9117, Class Loss: 0.5497\n",
      "Epoch 7/50, Loss: 1.3318, Domain Loss: 0.7831, Class Loss: 0.5488\n",
      "Epoch 8/50, Loss: 1.3123, Domain Loss: 0.7813, Class Loss: 0.5310\n",
      "Epoch 9/50, Loss: 1.4340, Domain Loss: 0.8848, Class Loss: 0.5492\n",
      "Epoch 10/50, Loss: 1.4431, Domain Loss: 0.8959, Class Loss: 0.5472\n",
      "Epoch 11/50, Loss: 1.6884, Domain Loss: 1.1279, Class Loss: 0.5606\n",
      "Epoch 12/50, Loss: 4.0543, Domain Loss: 2.4921, Class Loss: 1.5622\n",
      "Epoch 13/50, Loss: 3.0131, Domain Loss: 1.6769, Class Loss: 1.3363\n",
      "Epoch 14/50, Loss: 2.2211, Domain Loss: 1.3719, Class Loss: 0.8492\n",
      "Epoch 15/50, Loss: 3.5359, Domain Loss: 2.8644, Class Loss: 0.6715\n",
      "Epoch 16/50, Loss: 5.9206, Domain Loss: 4.8539, Class Loss: 1.0667\n",
      "Epoch 17/50, Loss: 13.0537, Domain Loss: 10.3007, Class Loss: 2.7530\n",
      "Epoch 18/50, Loss: 4.8979, Domain Loss: 3.3710, Class Loss: 1.5269\n",
      "Epoch 19/50, Loss: 4.3752, Domain Loss: 3.2728, Class Loss: 1.1024\n",
      "Epoch 20/50, Loss: 9.4123, Domain Loss: 8.3935, Class Loss: 1.0188\n",
      "Epoch 21/50, Loss: 6.8910, Domain Loss: 5.7508, Class Loss: 1.1402\n",
      "Epoch 22/50, Loss: 7.3971, Domain Loss: 6.4647, Class Loss: 0.9323\n",
      "Epoch 23/50, Loss: 4.1260, Domain Loss: 3.3081, Class Loss: 0.8179\n",
      "Epoch 24/50, Loss: 2.4385, Domain Loss: 1.8271, Class Loss: 0.6114\n",
      "Epoch 25/50, Loss: 1.8537, Domain Loss: 1.2838, Class Loss: 0.5699\n",
      "Epoch 26/50, Loss: 1.6615, Domain Loss: 1.1099, Class Loss: 0.5516\n",
      "Epoch 27/50, Loss: 1.6027, Domain Loss: 1.0716, Class Loss: 0.5311\n",
      "Epoch 28/50, Loss: 1.4566, Domain Loss: 0.9520, Class Loss: 0.5046\n",
      "Epoch 29/50, Loss: 1.4247, Domain Loss: 0.9178, Class Loss: 0.5069\n",
      "Epoch 30/50, Loss: 1.4208, Domain Loss: 0.9002, Class Loss: 0.5206\n",
      "Epoch 31/50, Loss: 1.3921, Domain Loss: 0.8779, Class Loss: 0.5143\n",
      "Epoch 32/50, Loss: 1.3665, Domain Loss: 0.8417, Class Loss: 0.5248\n",
      "Epoch 33/50, Loss: 1.3472, Domain Loss: 0.8466, Class Loss: 0.5006\n",
      "Epoch 34/50, Loss: 1.2796, Domain Loss: 0.7949, Class Loss: 0.4847\n",
      "Epoch 35/50, Loss: 1.2594, Domain Loss: 0.7865, Class Loss: 0.4729\n",
      "Epoch 36/50, Loss: 1.1999, Domain Loss: 0.7767, Class Loss: 0.4232\n",
      "Epoch 37/50, Loss: 1.2117, Domain Loss: 0.7743, Class Loss: 0.4375\n",
      "Epoch 38/50, Loss: 1.1613, Domain Loss: 0.7816, Class Loss: 0.3797\n",
      "Epoch 39/50, Loss: 1.0956, Domain Loss: 0.7587, Class Loss: 0.3369\n",
      "Epoch 40/50, Loss: 1.0581, Domain Loss: 0.7499, Class Loss: 0.3083\n",
      "Epoch 41/50, Loss: 1.0309, Domain Loss: 0.7564, Class Loss: 0.2745\n",
      "Epoch 42/50, Loss: 1.0409, Domain Loss: 0.7628, Class Loss: 0.2781\n",
      "Epoch 43/50, Loss: 0.9872, Domain Loss: 0.7590, Class Loss: 0.2282\n",
      "Epoch 44/50, Loss: 1.0349, Domain Loss: 0.8090, Class Loss: 0.2259\n",
      "Epoch 45/50, Loss: 0.9741, Domain Loss: 0.7770, Class Loss: 0.1972\n",
      "Epoch 46/50, Loss: 0.9431, Domain Loss: 0.7813, Class Loss: 0.1618\n",
      "Epoch 47/50, Loss: 0.9131, Domain Loss: 0.7531, Class Loss: 0.1600\n",
      "Epoch 48/50, Loss: 1.6413, Domain Loss: 0.8318, Class Loss: 0.8095\n",
      "Epoch 49/50, Loss: 1.2379, Domain Loss: 0.8438, Class Loss: 0.3941\n",
      "Epoch 50/50, Loss: 1.0151, Domain Loss: 0.8042, Class Loss: 0.2109\n",
      "49.46\n",
      "\n",
      "\n",
      "Source performance:\n",
      "89.17 89.15 88.82 88.39 \n",
      "Target performance:\n",
      "48.30 26.24 48.50 33.14 \n",
      "\n",
      "Per-class target performance: 100.00 0.00 0.00 94.00 \n",
      "Run 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Class Loss: 2.7403, Discrepancy Loss: 0.0881\n",
      "Epoch [2/50], Class Loss: 1.2114, Discrepancy Loss: 0.1211\n",
      "Epoch [3/50], Class Loss: 1.0942, Discrepancy Loss: 0.1068\n",
      "Epoch [4/50], Class Loss: 0.9771, Discrepancy Loss: 0.1175\n",
      "Epoch [5/50], Class Loss: 0.9624, Discrepancy Loss: 0.1135\n",
      "Epoch [6/50], Class Loss: 0.8841, Discrepancy Loss: 0.1086\n",
      "Epoch [7/50], Class Loss: 0.3670, Discrepancy Loss: 0.0982\n",
      "Epoch [8/50], Class Loss: 0.4002, Discrepancy Loss: 0.0991\n",
      "Epoch [9/50], Class Loss: 0.4857, Discrepancy Loss: 0.1153\n",
      "Epoch [10/50], Class Loss: 0.1588, Discrepancy Loss: 0.0942\n",
      "Epoch [11/50], Class Loss: 0.2025, Discrepancy Loss: 0.1153\n",
      "Epoch [12/50], Class Loss: 0.0538, Discrepancy Loss: 0.1296\n",
      "Epoch [13/50], Class Loss: 0.0267, Discrepancy Loss: 0.1026\n",
      "Epoch [14/50], Class Loss: 0.0488, Discrepancy Loss: 0.0745\n",
      "Epoch [15/50], Class Loss: 0.0374, Discrepancy Loss: 0.0747\n",
      "Epoch [16/50], Class Loss: 0.0251, Discrepancy Loss: 0.0725\n",
      "Epoch [17/50], Class Loss: 0.0433, Discrepancy Loss: 0.0807\n",
      "Epoch [18/50], Class Loss: 0.0360, Discrepancy Loss: 0.0937\n",
      "Epoch [19/50], Class Loss: 0.0222, Discrepancy Loss: 0.1084\n",
      "Epoch [20/50], Class Loss: 0.0179, Discrepancy Loss: 0.1198\n",
      "Epoch [21/50], Class Loss: 0.0201, Discrepancy Loss: 0.1215\n",
      "Epoch [22/50], Class Loss: 0.0100, Discrepancy Loss: 0.1309\n",
      "Epoch [23/50], Class Loss: 0.0143, Discrepancy Loss: 0.1334\n",
      "Epoch [24/50], Class Loss: 0.0156, Discrepancy Loss: 0.1400\n",
      "Epoch [25/50], Class Loss: 0.0233, Discrepancy Loss: 0.1289\n",
      "Epoch [26/50], Class Loss: 0.0122, Discrepancy Loss: 0.1372\n",
      "Epoch [27/50], Class Loss: 0.0061, Discrepancy Loss: 0.1378\n",
      "Epoch [28/50], Class Loss: 0.0144, Discrepancy Loss: 0.1327\n",
      "Epoch [29/50], Class Loss: 0.0084, Discrepancy Loss: 0.1336\n",
      "Epoch [30/50], Class Loss: 0.0286, Discrepancy Loss: 0.1327\n",
      "Epoch [31/50], Class Loss: 0.0164, Discrepancy Loss: 0.1389\n",
      "Epoch [32/50], Class Loss: 0.0205, Discrepancy Loss: 0.1327\n",
      "Epoch [33/50], Class Loss: 0.0162, Discrepancy Loss: 0.1424\n",
      "Epoch [34/50], Class Loss: 0.0091, Discrepancy Loss: 0.1409\n",
      "Epoch [35/50], Class Loss: 0.0130, Discrepancy Loss: 0.1433\n",
      "Epoch [36/50], Class Loss: 0.0432, Discrepancy Loss: 0.1396\n",
      "Epoch [37/50], Class Loss: 0.0105, Discrepancy Loss: 0.1462\n",
      "Epoch [38/50], Class Loss: 0.0081, Discrepancy Loss: 0.1428\n",
      "Epoch [39/50], Class Loss: 0.0653, Discrepancy Loss: 0.1314\n",
      "Epoch [40/50], Class Loss: 0.0169, Discrepancy Loss: 0.1411\n",
      "Epoch [41/50], Class Loss: 0.0213, Discrepancy Loss: 0.1281\n",
      "Epoch [42/50], Class Loss: 0.0192, Discrepancy Loss: 0.1414\n",
      "Epoch [43/50], Class Loss: 0.0183, Discrepancy Loss: 0.1348\n",
      "Epoch [44/50], Class Loss: 0.0174, Discrepancy Loss: 0.1313\n",
      "Epoch [45/50], Class Loss: 0.0116, Discrepancy Loss: 0.1343\n",
      "Epoch [46/50], Class Loss: 0.0306, Discrepancy Loss: 0.1432\n",
      "Epoch [47/50], Class Loss: 0.0158, Discrepancy Loss: 0.1425\n",
      "Epoch [48/50], Class Loss: 0.0359, Discrepancy Loss: 0.1381\n",
      "Epoch [49/50], Class Loss: 0.0101, Discrepancy Loss: 0.1456\n",
      "Epoch [50/50], Class Loss: 0.0507, Discrepancy Loss: 0.1340\n",
      "Source Domain Performance - Accuracy: 77.70%, Precision: 85.48%, Recall: 77.15%, F1 Score: 73.52%\n",
      "Target Domain Performance - Accuracy: 35.61%, Precision: 24.23%, Recall: 36.73%, F1 Score: 28.91%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 2.9279, Discrepancy Loss: 0.0806\n",
      "Epoch [2/50], Class Loss: 1.2084, Discrepancy Loss: 0.0947\n",
      "Epoch [3/50], Class Loss: 1.0408, Discrepancy Loss: 0.1147\n",
      "Epoch [4/50], Class Loss: 0.9205, Discrepancy Loss: 0.1047\n",
      "Epoch [5/50], Class Loss: 0.8181, Discrepancy Loss: 0.1134\n",
      "Epoch [6/50], Class Loss: 0.4859, Discrepancy Loss: 0.0888\n",
      "Epoch [7/50], Class Loss: 0.0898, Discrepancy Loss: 0.0646\n",
      "Epoch [8/50], Class Loss: 0.0749, Discrepancy Loss: 0.0655\n",
      "Epoch [9/50], Class Loss: 0.0275, Discrepancy Loss: 0.0735\n",
      "Epoch [10/50], Class Loss: 0.0376, Discrepancy Loss: 0.0914\n",
      "Epoch [11/50], Class Loss: 0.0154, Discrepancy Loss: 0.1031\n",
      "Epoch [12/50], Class Loss: 0.0101, Discrepancy Loss: 0.0947\n",
      "Epoch [13/50], Class Loss: 0.0074, Discrepancy Loss: 0.0926\n",
      "Epoch [14/50], Class Loss: 0.0207, Discrepancy Loss: 0.0921\n",
      "Epoch [15/50], Class Loss: 0.0162, Discrepancy Loss: 0.0923\n",
      "Epoch [16/50], Class Loss: 0.0107, Discrepancy Loss: 0.0917\n",
      "Epoch [17/50], Class Loss: 0.0149, Discrepancy Loss: 0.0927\n",
      "Epoch [18/50], Class Loss: 0.0154, Discrepancy Loss: 0.0998\n",
      "Epoch [19/50], Class Loss: 0.0172, Discrepancy Loss: 0.1011\n",
      "Epoch [20/50], Class Loss: 0.0092, Discrepancy Loss: 0.1110\n",
      "Epoch [21/50], Class Loss: 0.0068, Discrepancy Loss: 0.1214\n",
      "Epoch [22/50], Class Loss: 0.0184, Discrepancy Loss: 0.1172\n",
      "Epoch [23/50], Class Loss: 0.0409, Discrepancy Loss: 0.1219\n",
      "Epoch [24/50], Class Loss: 0.0091, Discrepancy Loss: 0.1241\n",
      "Epoch [25/50], Class Loss: 0.0168, Discrepancy Loss: 0.1227\n",
      "Epoch [26/50], Class Loss: 0.0075, Discrepancy Loss: 0.1196\n",
      "Epoch [27/50], Class Loss: 0.0453, Discrepancy Loss: 0.1255\n",
      "Epoch [28/50], Class Loss: 0.0126, Discrepancy Loss: 0.1307\n",
      "Epoch [29/50], Class Loss: 0.0064, Discrepancy Loss: 0.1263\n",
      "Epoch [30/50], Class Loss: 0.0192, Discrepancy Loss: 0.1279\n",
      "Epoch [31/50], Class Loss: 0.0406, Discrepancy Loss: 0.1266\n",
      "Epoch [32/50], Class Loss: 0.0041, Discrepancy Loss: 0.1196\n",
      "Epoch [33/50], Class Loss: 0.0052, Discrepancy Loss: 0.1214\n",
      "Epoch [34/50], Class Loss: 0.0065, Discrepancy Loss: 0.1311\n",
      "Epoch [35/50], Class Loss: 0.0244, Discrepancy Loss: 0.1387\n",
      "Epoch [36/50], Class Loss: 0.0063, Discrepancy Loss: 0.1185\n",
      "Epoch [37/50], Class Loss: 0.0041, Discrepancy Loss: 0.1287\n",
      "Epoch [38/50], Class Loss: 0.0078, Discrepancy Loss: 0.1322\n",
      "Epoch [39/50], Class Loss: 0.0054, Discrepancy Loss: 0.1306\n",
      "Epoch [40/50], Class Loss: 0.0097, Discrepancy Loss: 0.1185\n",
      "Epoch [41/50], Class Loss: 0.0110, Discrepancy Loss: 0.1310\n",
      "Epoch [42/50], Class Loss: 0.0067, Discrepancy Loss: 0.1307\n",
      "Epoch [43/50], Class Loss: 0.0034, Discrepancy Loss: 0.1291\n",
      "Epoch [44/50], Class Loss: 0.0043, Discrepancy Loss: 0.1303\n",
      "Epoch [45/50], Class Loss: 0.0120, Discrepancy Loss: 0.1230\n",
      "Epoch [46/50], Class Loss: 0.0022, Discrepancy Loss: 0.1303\n",
      "Epoch [47/50], Class Loss: 0.0253, Discrepancy Loss: 0.1319\n",
      "Epoch [48/50], Class Loss: 0.0078, Discrepancy Loss: 0.1285\n",
      "Epoch [49/50], Class Loss: 0.0373, Discrepancy Loss: 0.1230\n",
      "Epoch [50/50], Class Loss: 0.0046, Discrepancy Loss: 0.1215\n",
      "Source Domain Performance - Accuracy: 75.78%, Precision: 87.29%, Recall: 75.30%, F1 Score: 67.21%\n",
      "Target Domain Performance - Accuracy: 38.49%, Precision: 44.62%, Recall: 39.25%, F1 Score: 31.49%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 2.7115, Discrepancy Loss: 0.1088\n",
      "Epoch [2/50], Class Loss: 1.3214, Discrepancy Loss: 0.1141\n",
      "Epoch [3/50], Class Loss: 1.0048, Discrepancy Loss: 0.1276\n",
      "Epoch [4/50], Class Loss: 1.0660, Discrepancy Loss: 0.1182\n",
      "Epoch [5/50], Class Loss: 1.0404, Discrepancy Loss: 0.1323\n",
      "Epoch [6/50], Class Loss: 0.9065, Discrepancy Loss: 0.1137\n",
      "Epoch [7/50], Class Loss: 0.7379, Discrepancy Loss: 0.1067\n",
      "Epoch [8/50], Class Loss: 0.4392, Discrepancy Loss: 0.1042\n",
      "Epoch [9/50], Class Loss: 0.0765, Discrepancy Loss: 0.0912\n",
      "Epoch [10/50], Class Loss: 0.1291, Discrepancy Loss: 0.0802\n",
      "Epoch [11/50], Class Loss: 0.1334, Discrepancy Loss: 0.0894\n",
      "Epoch [12/50], Class Loss: 0.0166, Discrepancy Loss: 0.1000\n",
      "Epoch [13/50], Class Loss: 0.0157, Discrepancy Loss: 0.1060\n",
      "Epoch [14/50], Class Loss: 0.0277, Discrepancy Loss: 0.0789\n",
      "Epoch [15/50], Class Loss: 0.0163, Discrepancy Loss: 0.0667\n",
      "Epoch [16/50], Class Loss: 0.0119, Discrepancy Loss: 0.0700\n",
      "Epoch [17/50], Class Loss: 0.0146, Discrepancy Loss: 0.0720\n",
      "Epoch [18/50], Class Loss: 0.0146, Discrepancy Loss: 0.0718\n",
      "Epoch [19/50], Class Loss: 0.0244, Discrepancy Loss: 0.0742\n",
      "Epoch [20/50], Class Loss: 0.0196, Discrepancy Loss: 0.0848\n",
      "Epoch [21/50], Class Loss: 0.0238, Discrepancy Loss: 0.0768\n",
      "Epoch [22/50], Class Loss: 0.0121, Discrepancy Loss: 0.0861\n",
      "Epoch [23/50], Class Loss: 0.0203, Discrepancy Loss: 0.0730\n",
      "Epoch [24/50], Class Loss: 0.0106, Discrepancy Loss: 0.0769\n",
      "Epoch [25/50], Class Loss: 0.0085, Discrepancy Loss: 0.0762\n",
      "Epoch [26/50], Class Loss: 0.0177, Discrepancy Loss: 0.0817\n",
      "Epoch [27/50], Class Loss: 0.0148, Discrepancy Loss: 0.0815\n",
      "Epoch [28/50], Class Loss: 0.0082, Discrepancy Loss: 0.0807\n",
      "Epoch [29/50], Class Loss: 0.0085, Discrepancy Loss: 0.0789\n",
      "Epoch [30/50], Class Loss: 0.0117, Discrepancy Loss: 0.0821\n",
      "Epoch [31/50], Class Loss: 0.0309, Discrepancy Loss: 0.0871\n",
      "Epoch [32/50], Class Loss: 0.0098, Discrepancy Loss: 0.0826\n",
      "Epoch [33/50], Class Loss: 0.0095, Discrepancy Loss: 0.0849\n",
      "Epoch [34/50], Class Loss: 0.0141, Discrepancy Loss: 0.0815\n",
      "Epoch [35/50], Class Loss: 0.0124, Discrepancy Loss: 0.0881\n",
      "Epoch [36/50], Class Loss: 0.0072, Discrepancy Loss: 0.0849\n",
      "Epoch [37/50], Class Loss: 0.0212, Discrepancy Loss: 0.0881\n",
      "Epoch [38/50], Class Loss: 0.0223, Discrepancy Loss: 0.0838\n",
      "Epoch [39/50], Class Loss: 0.0077, Discrepancy Loss: 0.0883\n",
      "Epoch [40/50], Class Loss: 0.0098, Discrepancy Loss: 0.0912\n",
      "Epoch [41/50], Class Loss: 0.0068, Discrepancy Loss: 0.0844\n",
      "Epoch [42/50], Class Loss: 0.0178, Discrepancy Loss: 0.0863\n",
      "Epoch [43/50], Class Loss: 0.0237, Discrepancy Loss: 0.0850\n",
      "Epoch [44/50], Class Loss: 0.0151, Discrepancy Loss: 0.0897\n",
      "Epoch [45/50], Class Loss: 0.0130, Discrepancy Loss: 0.0841\n",
      "Epoch [46/50], Class Loss: 0.0100, Discrepancy Loss: 0.0843\n",
      "Epoch [47/50], Class Loss: 0.0259, Discrepancy Loss: 0.0838\n",
      "Epoch [48/50], Class Loss: 0.0115, Discrepancy Loss: 0.0916\n",
      "Epoch [49/50], Class Loss: 0.0085, Discrepancy Loss: 0.0901\n",
      "Epoch [50/50], Class Loss: 0.0126, Discrepancy Loss: 0.0866\n",
      "Source Domain Performance - Accuracy: 64.99%, Precision: 75.12%, Recall: 64.81%, F1 Score: 57.34%\n",
      "Target Domain Performance - Accuracy: 35.91%, Precision: 23.98%, Recall: 36.10%, F1 Score: 27.54%\n",
      "\n",
      "Source performance: 72.82% 82.63% 72.42% 66.02%\n",
      "Target performance: 36.67% 30.94% 37.36% 29.32%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 91.77%\n",
      "qpsk: 1.98%\n",
      "16qam: 5.28%\n",
      "16apsk: 50.42%\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 2.5165, Discrepancy Loss: 0.0857\n",
      "Validation Loss: 1.1297\n",
      "Epoch [2/50], Class Loss: 1.0949, Discrepancy Loss: 0.0287\n",
      "Validation Loss: 1.0335\n",
      "Epoch [3/50], Class Loss: 0.9828, Discrepancy Loss: 0.0460\n",
      "Validation Loss: 0.8846\n",
      "Epoch [4/50], Class Loss: 0.8967, Discrepancy Loss: 0.0580\n",
      "Validation Loss: 1.1155\n",
      "Epoch [5/50], Class Loss: 0.7415, Discrepancy Loss: 0.0579\n",
      "Validation Loss: 0.6477\n",
      "Epoch [6/50], Class Loss: 0.7441, Discrepancy Loss: 0.1294\n",
      "Validation Loss: 1.1828\n",
      "Epoch [7/50], Class Loss: 0.3975, Discrepancy Loss: 0.0548\n",
      "Validation Loss: 0.1428\n",
      "Epoch [8/50], Class Loss: 0.0748, Discrepancy Loss: 0.0460\n",
      "Validation Loss: 0.0407\n",
      "Epoch [9/50], Class Loss: 0.0733, Discrepancy Loss: 0.0482\n",
      "Validation Loss: 0.0250\n",
      "Epoch [10/50], Class Loss: 0.0844, Discrepancy Loss: 0.0733\n",
      "Validation Loss: 0.0937\n",
      "Epoch [11/50], Class Loss: 0.0221, Discrepancy Loss: 0.0462\n",
      "Validation Loss: 0.0484\n",
      "Epoch [12/50], Class Loss: 0.0132, Discrepancy Loss: 0.0378\n",
      "Validation Loss: 0.0324\n",
      "Epoch [13/50], Class Loss: 0.0215, Discrepancy Loss: 0.0375\n",
      "Validation Loss: 0.0451\n",
      "Epoch [14/50], Class Loss: 0.0091, Discrepancy Loss: 0.0308\n",
      "Validation Loss: 0.0421\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.40%, Precision: 99.40%, Recall: 99.40%, F1 Score: 99.40%\n",
      "Target Domain Performance - Accuracy: 48.26%, Precision: 51.09%, Recall: 47.60%, F1 Score: 33.74%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 2.5514, Discrepancy Loss: 0.0840\n",
      "Validation Loss: 1.2921\n",
      "Epoch [2/50], Class Loss: 1.2301, Discrepancy Loss: 0.0321\n",
      "Validation Loss: 1.0396\n",
      "Epoch [3/50], Class Loss: 1.0295, Discrepancy Loss: 0.0534\n",
      "Validation Loss: 1.0690\n",
      "Epoch [4/50], Class Loss: 0.9394, Discrepancy Loss: 0.0568\n",
      "Validation Loss: 0.8400\n",
      "Epoch [5/50], Class Loss: 0.7963, Discrepancy Loss: 0.0863\n",
      "Validation Loss: 0.4143\n",
      "Epoch [6/50], Class Loss: 0.3716, Discrepancy Loss: 0.0877\n",
      "Validation Loss: 0.2691\n",
      "Epoch [7/50], Class Loss: 0.1639, Discrepancy Loss: 0.0599\n",
      "Validation Loss: 0.0684\n",
      "Epoch [8/50], Class Loss: 0.0935, Discrepancy Loss: 0.0542\n",
      "Validation Loss: 0.0879\n",
      "Epoch [9/50], Class Loss: 0.3825, Discrepancy Loss: 0.0863\n",
      "Validation Loss: 0.0829\n",
      "Epoch [10/50], Class Loss: 0.2219, Discrepancy Loss: 0.0751\n",
      "Validation Loss: 0.0355\n",
      "Epoch [11/50], Class Loss: 0.0140, Discrepancy Loss: 0.0273\n",
      "Validation Loss: 0.0218\n",
      "Epoch [12/50], Class Loss: 0.0077, Discrepancy Loss: 0.0294\n",
      "Validation Loss: 0.0619\n",
      "Epoch [13/50], Class Loss: 0.0206, Discrepancy Loss: 0.0455\n",
      "Validation Loss: 0.1052\n",
      "Epoch [14/50], Class Loss: 0.0143, Discrepancy Loss: 0.0392\n",
      "Validation Loss: 0.0935\n",
      "Epoch [15/50], Class Loss: 0.0141, Discrepancy Loss: 0.0341\n",
      "Validation Loss: 0.0810\n",
      "Epoch [16/50], Class Loss: 0.0087, Discrepancy Loss: 0.0296\n",
      "Validation Loss: 0.0494\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 98.98%, Precision: 98.99%, Recall: 98.98%, F1 Score: 98.97%\n",
      "Target Domain Performance - Accuracy: 50.66%, Precision: 54.51%, Recall: 50.01%, F1 Score: 38.96%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 2.4380, Discrepancy Loss: 0.0970\n",
      "Validation Loss: 1.0541\n",
      "Epoch [2/50], Class Loss: 1.0533, Discrepancy Loss: 0.0277\n",
      "Validation Loss: 0.9517\n",
      "Epoch [3/50], Class Loss: 0.9448, Discrepancy Loss: 0.0415\n",
      "Validation Loss: 0.8675\n",
      "Epoch [4/50], Class Loss: 0.6891, Discrepancy Loss: 0.0488\n",
      "Validation Loss: 0.3712\n",
      "Epoch [5/50], Class Loss: 0.3188, Discrepancy Loss: 0.0905\n",
      "Validation Loss: 0.5082\n",
      "Epoch [6/50], Class Loss: 0.2258, Discrepancy Loss: 0.0452\n",
      "Validation Loss: 0.7197\n",
      "Epoch [7/50], Class Loss: 0.3267, Discrepancy Loss: 0.1052\n",
      "Validation Loss: 0.8710\n",
      "Epoch [8/50], Class Loss: 0.2476, Discrepancy Loss: 0.0712\n",
      "Validation Loss: 0.5832\n",
      "Epoch [9/50], Class Loss: 0.2086, Discrepancy Loss: 0.0430\n",
      "Validation Loss: 0.0189\n",
      "Epoch [10/50], Class Loss: 0.0533, Discrepancy Loss: 0.0341\n",
      "Validation Loss: 0.6851\n",
      "Epoch [11/50], Class Loss: 0.0825, Discrepancy Loss: 0.0411\n",
      "Validation Loss: 0.0870\n",
      "Epoch [12/50], Class Loss: 0.0220, Discrepancy Loss: 0.0393\n",
      "Validation Loss: 0.0670\n",
      "Epoch [13/50], Class Loss: 0.0300, Discrepancy Loss: 0.0430\n",
      "Validation Loss: 0.0237\n",
      "Epoch [14/50], Class Loss: 0.0149, Discrepancy Loss: 0.0395\n",
      "Validation Loss: 0.0505\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.28%, Precision: 99.30%, Recall: 99.28%, F1 Score: 99.29%\n",
      "Target Domain Performance - Accuracy: 51.68%, Precision: 53.68%, Recall: 51.03%, F1 Score: 40.12%\n",
      "\n",
      "Source performance: 99.22% 99.23% 99.22% 99.22%\n",
      "Target performance: 50.20% 53.09% 49.55% 37.61%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 2.05%\n",
      "16qam: 7.77%\n",
      "16apsk: 88.36%\n",
      "\n",
      "Run 1/3\n",
      "Epoch 1/50, Train Loss: 1.0273, Train Acc: 0.5108, Val Loss: 0.6432, Val Acc: 0.6625\n",
      "Epoch 2/50, Train Loss: 0.5581, Train Acc: 0.7124, Val Loss: 0.6267, Val Acc: 0.6942\n",
      "Epoch 3/50, Train Loss: 0.5239, Train Acc: 0.7383, Val Loss: 0.4998, Val Acc: 0.7404\n",
      "Epoch 4/50, Train Loss: 0.4045, Train Acc: 0.8388, Val Loss: 1.5332, Val Acc: 0.6876\n",
      "Epoch 5/50, Train Loss: 0.3165, Train Acc: 0.9541, Val Loss: 0.1399, Val Acc: 0.9574\n",
      "Epoch 6/50, Train Loss: 0.1151, Train Acc: 0.9753, Val Loss: 0.0083, Val Acc: 0.9964\n",
      "Epoch 7/50, Train Loss: 0.0216, Train Acc: 0.9954, Val Loss: 0.1547, Val Acc: 0.9604\n",
      "Epoch 8/50, Train Loss: 0.0435, Train Acc: 0.9876, Val Loss: 0.0117, Val Acc: 0.9958\n",
      "Epoch 9/50, Train Loss: 0.0088, Train Acc: 0.9975, Val Loss: 0.0058, Val Acc: 0.9976\n",
      "Epoch 10/50, Train Loss: 0.0114, Train Acc: 0.9967, Val Loss: 0.0021, Val Acc: 0.9994\n",
      "Epoch 11/50, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0010, Val Acc: 0.9994\n",
      "Epoch 12/50, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 18/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 19/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0004, Val Acc: 1.0000\n",
      "Epoch 20/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 21/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 22/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 23/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 24/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 25/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Run 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.8012, Train Acc: 0.6170, Val Loss: 0.5239, Val Acc: 0.7422\n",
      "Epoch 2/50, Train Loss: 0.5246, Train Acc: 0.7304, Val Loss: 0.4858, Val Acc: 0.7380\n",
      "Epoch 3/50, Train Loss: 0.4730, Train Acc: 0.7693, Val Loss: 0.6831, Val Acc: 0.6847\n",
      "Epoch 4/50, Train Loss: 0.3736, Train Acc: 0.8365, Val Loss: 0.1019, Val Acc: 0.9736\n",
      "Epoch 5/50, Train Loss: 0.1326, Train Acc: 0.9624, Val Loss: 0.0368, Val Acc: 0.9826\n",
      "Epoch 6/50, Train Loss: 0.0692, Train Acc: 0.9778, Val Loss: 0.0035, Val Acc: 0.9988\n",
      "Epoch 7/50, Train Loss: 0.0032, Train Acc: 0.9991, Val Loss: 0.0057, Val Acc: 0.9976\n",
      "Epoch 8/50, Train Loss: 0.0058, Train Acc: 0.9982, Val Loss: 0.0648, Val Acc: 0.9778\n",
      "Epoch 9/50, Train Loss: 0.0024, Train Acc: 0.9991, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 10/50, Train Loss: 0.0008, Train Acc: 0.9996, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 11/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Run 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.1543, Train Acc: 0.5726, Val Loss: 0.5927, Val Acc: 0.6835\n",
      "Epoch 2/50, Train Loss: 0.5602, Train Acc: 0.7074, Val Loss: 0.6961, Val Acc: 0.6175\n",
      "Epoch 3/50, Train Loss: 0.5629, Train Acc: 0.7232, Val Loss: 0.5614, Val Acc: 0.7428\n",
      "Epoch 4/50, Train Loss: 0.4798, Train Acc: 0.7722, Val Loss: 0.4137, Val Acc: 0.8471\n",
      "Epoch 5/50, Train Loss: 0.3051, Train Acc: 0.8973, Val Loss: 0.2501, Val Acc: 0.9221\n",
      "Epoch 6/50, Train Loss: 0.2423, Train Acc: 0.9316, Val Loss: 0.0547, Val Acc: 0.9802\n",
      "Epoch 7/50, Train Loss: 0.0150, Train Acc: 0.9949, Val Loss: 0.0180, Val Acc: 0.9946\n",
      "Epoch 8/50, Train Loss: 0.0077, Train Acc: 0.9976, Val Loss: 0.0013, Val Acc: 0.9994\n",
      "Epoch 9/50, Train Loss: 0.0015, Train Acc: 0.9996, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 10/50, Train Loss: 0.0016, Train Acc: 0.9994, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 11/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 18/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 19/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 20/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 21/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 22/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 23/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 24/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 25/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 26/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 27/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 28/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 29/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 30/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Source performance: 100.00 100.00 100.00 100.00\n",
      "Target performance: 50.74 41.65 50.02 37.66\n",
      "\n",
      "bpsk: 100.00\n",
      "qpsk: 0.00\n",
      "16qam: 0.08\n",
      "16apsk: 100.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 5.0957, Domain Loss: 2.8686, Class Loss: 2.2271\n",
      "Epoch 2/50, Loss: 2.9207, Domain Loss: 1.9920, Class Loss: 0.9287\n",
      "Epoch 3/50, Loss: 2.8300, Domain Loss: 2.1460, Class Loss: 0.6839\n",
      "Epoch 4/50, Loss: 2.1785, Domain Loss: 1.5363, Class Loss: 0.6422\n",
      "Epoch 5/50, Loss: 2.0788, Domain Loss: 1.4554, Class Loss: 0.6234\n",
      "Epoch 6/50, Loss: 1.8381, Domain Loss: 1.2402, Class Loss: 0.5979\n",
      "Epoch 7/50, Loss: 1.7220, Domain Loss: 1.1578, Class Loss: 0.5642\n",
      "Epoch 8/50, Loss: 1.4822, Domain Loss: 0.9510, Class Loss: 0.5312\n",
      "Epoch 9/50, Loss: 1.3927, Domain Loss: 0.8069, Class Loss: 0.5858\n",
      "Epoch 10/50, Loss: 1.3959, Domain Loss: 0.8431, Class Loss: 0.5529\n",
      "Epoch 11/50, Loss: 1.5208, Domain Loss: 0.9274, Class Loss: 0.5934\n",
      "Epoch 12/50, Loss: 1.5356, Domain Loss: 0.9534, Class Loss: 0.5822\n",
      "Epoch 13/50, Loss: 1.8172, Domain Loss: 1.2654, Class Loss: 0.5518\n",
      "Epoch 14/50, Loss: 3.0802, Domain Loss: 2.2450, Class Loss: 0.8352\n",
      "Epoch 15/50, Loss: 2.8943, Domain Loss: 1.7140, Class Loss: 1.1804\n",
      "Epoch 16/50, Loss: 1.8117, Domain Loss: 1.2102, Class Loss: 0.6015\n",
      "Epoch 17/50, Loss: 3.1984, Domain Loss: 1.9871, Class Loss: 1.2113\n",
      "Epoch 18/50, Loss: 3.1440, Domain Loss: 1.9180, Class Loss: 1.2260\n",
      "Epoch 19/50, Loss: 2.2682, Domain Loss: 1.4969, Class Loss: 0.7713\n",
      "Epoch 20/50, Loss: 2.3536, Domain Loss: 1.7208, Class Loss: 0.6328\n",
      "Epoch 21/50, Loss: 2.8642, Domain Loss: 2.2463, Class Loss: 0.6179\n",
      "Epoch 22/50, Loss: 1.8516, Domain Loss: 1.1903, Class Loss: 0.6612\n",
      "Epoch 23/50, Loss: 1.4328, Domain Loss: 0.8672, Class Loss: 0.5656\n",
      "Epoch 24/50, Loss: 1.3151, Domain Loss: 0.7521, Class Loss: 0.5631\n",
      "Epoch 25/50, Loss: 1.4036, Domain Loss: 0.8303, Class Loss: 0.5732\n",
      "Epoch 26/50, Loss: 1.2719, Domain Loss: 0.7463, Class Loss: 0.5256\n",
      "Epoch 27/50, Loss: 1.2722, Domain Loss: 0.7662, Class Loss: 0.5059\n",
      "Epoch 28/50, Loss: 1.2478, Domain Loss: 0.7578, Class Loss: 0.4900\n",
      "Epoch 29/50, Loss: 1.2374, Domain Loss: 0.7298, Class Loss: 0.5076\n",
      "Epoch 30/50, Loss: 1.3335, Domain Loss: 0.8320, Class Loss: 0.5015\n",
      "Epoch 31/50, Loss: 1.3182, Domain Loss: 0.8184, Class Loss: 0.4998\n",
      "Epoch 32/50, Loss: 1.2916, Domain Loss: 0.7555, Class Loss: 0.5361\n",
      "Epoch 33/50, Loss: 1.2649, Domain Loss: 0.7133, Class Loss: 0.5516\n",
      "Epoch 34/50, Loss: 1.2019, Domain Loss: 0.7260, Class Loss: 0.4759\n",
      "Epoch 35/50, Loss: 1.2403, Domain Loss: 0.7561, Class Loss: 0.4842\n",
      "Epoch 36/50, Loss: 1.3151, Domain Loss: 0.8450, Class Loss: 0.4701\n",
      "Epoch 37/50, Loss: 1.2874, Domain Loss: 0.8247, Class Loss: 0.4628\n",
      "Epoch 38/50, Loss: 1.3903, Domain Loss: 0.8732, Class Loss: 0.5171\n",
      "Epoch 39/50, Loss: 1.3648, Domain Loss: 0.8443, Class Loss: 0.5205\n",
      "Epoch 40/50, Loss: 1.2933, Domain Loss: 0.7775, Class Loss: 0.5158\n",
      "Epoch 41/50, Loss: 1.2059, Domain Loss: 0.7124, Class Loss: 0.4934\n",
      "Epoch 42/50, Loss: 1.2256, Domain Loss: 0.7511, Class Loss: 0.4745\n",
      "Epoch 43/50, Loss: 1.2511, Domain Loss: 0.7851, Class Loss: 0.4660\n",
      "Epoch 44/50, Loss: 1.1978, Domain Loss: 0.7482, Class Loss: 0.4496\n",
      "Epoch 45/50, Loss: 1.3087, Domain Loss: 0.7988, Class Loss: 0.5099\n",
      "Epoch 46/50, Loss: 1.3815, Domain Loss: 0.9102, Class Loss: 0.4713\n",
      "Epoch 47/50, Loss: 1.2734, Domain Loss: 0.7841, Class Loss: 0.4893\n",
      "Epoch 48/50, Loss: 1.2414, Domain Loss: 0.7934, Class Loss: 0.4481\n",
      "Epoch 49/50, Loss: 1.2914, Domain Loss: 0.8178, Class Loss: 0.4736\n",
      "Epoch 50/50, Loss: 1.3760, Domain Loss: 0.8825, Class Loss: 0.4934\n",
      "50.72\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 3.6811, Domain Loss: 1.9682, Class Loss: 1.7128\n",
      "Epoch 2/50, Loss: 3.0928, Domain Loss: 2.2654, Class Loss: 0.8273\n",
      "Epoch 3/50, Loss: 2.2066, Domain Loss: 1.6087, Class Loss: 0.5978\n",
      "Epoch 4/50, Loss: 1.9000, Domain Loss: 1.2266, Class Loss: 0.6734\n",
      "Epoch 5/50, Loss: 2.0669, Domain Loss: 1.3910, Class Loss: 0.6759\n",
      "Epoch 6/50, Loss: 1.6420, Domain Loss: 1.0695, Class Loss: 0.5725\n",
      "Epoch 7/50, Loss: 1.3404, Domain Loss: 0.8179, Class Loss: 0.5225\n",
      "Epoch 8/50, Loss: 1.3622, Domain Loss: 0.7983, Class Loss: 0.5639\n",
      "Epoch 9/50, Loss: 1.7346, Domain Loss: 1.0138, Class Loss: 0.7207\n",
      "Epoch 10/50, Loss: 2.1231, Domain Loss: 1.3350, Class Loss: 0.7881\n",
      "Epoch 11/50, Loss: 1.9223, Domain Loss: 1.1611, Class Loss: 0.7612\n",
      "Epoch 12/50, Loss: 3.5512, Domain Loss: 2.1564, Class Loss: 1.3948\n",
      "Epoch 13/50, Loss: 2.5685, Domain Loss: 1.6442, Class Loss: 0.9243\n",
      "Epoch 14/50, Loss: 10.6597, Domain Loss: 7.6007, Class Loss: 3.0589\n",
      "Epoch 15/50, Loss: 3.3275, Domain Loss: 1.7395, Class Loss: 1.5880\n",
      "Epoch 16/50, Loss: 2.6327, Domain Loss: 1.3299, Class Loss: 1.3028\n",
      "Epoch 17/50, Loss: 2.4933, Domain Loss: 1.2576, Class Loss: 1.2357\n",
      "Epoch 18/50, Loss: 2.7286, Domain Loss: 1.3566, Class Loss: 1.3720\n",
      "Epoch 19/50, Loss: 2.6273, Domain Loss: 1.3865, Class Loss: 1.2407\n",
      "Epoch 20/50, Loss: 2.4843, Domain Loss: 1.3865, Class Loss: 1.0978\n",
      "Epoch 21/50, Loss: 2.3125, Domain Loss: 1.3865, Class Loss: 0.9259\n",
      "Epoch 22/50, Loss: 2.1477, Domain Loss: 1.3865, Class Loss: 0.7612\n",
      "Epoch 23/50, Loss: 2.1259, Domain Loss: 1.3865, Class Loss: 0.7394\n",
      "Epoch 24/50, Loss: 1.9688, Domain Loss: 1.3865, Class Loss: 0.5823\n",
      "Epoch 25/50, Loss: 1.9733, Domain Loss: 1.3865, Class Loss: 0.5869\n",
      "Epoch 26/50, Loss: 1.9535, Domain Loss: 1.3865, Class Loss: 0.5670\n",
      "Epoch 27/50, Loss: 1.9197, Domain Loss: 1.3864, Class Loss: 0.5332\n",
      "Epoch 28/50, Loss: 1.8720, Domain Loss: 1.3864, Class Loss: 0.4856\n",
      "Epoch 29/50, Loss: 1.9178, Domain Loss: 1.3864, Class Loss: 0.5313\n",
      "Epoch 30/50, Loss: 1.9091, Domain Loss: 1.3864, Class Loss: 0.5227\n",
      "Epoch 31/50, Loss: 1.8925, Domain Loss: 1.3864, Class Loss: 0.5061\n",
      "Epoch 32/50, Loss: 1.9157, Domain Loss: 1.3864, Class Loss: 0.5293\n",
      "Epoch 33/50, Loss: 1.9030, Domain Loss: 1.3864, Class Loss: 0.5166\n",
      "Epoch 34/50, Loss: 1.9020, Domain Loss: 1.3864, Class Loss: 0.5156\n",
      "Epoch 35/50, Loss: 1.8985, Domain Loss: 1.3864, Class Loss: 0.5122\n",
      "Epoch 36/50, Loss: 1.8968, Domain Loss: 1.3864, Class Loss: 0.5104\n",
      "Epoch 37/50, Loss: 1.8798, Domain Loss: 1.3864, Class Loss: 0.4934\n",
      "Epoch 38/50, Loss: 1.8719, Domain Loss: 1.3863, Class Loss: 0.4855\n",
      "Epoch 39/50, Loss: 1.9260, Domain Loss: 1.3863, Class Loss: 0.5397\n",
      "Epoch 40/50, Loss: 1.9192, Domain Loss: 1.3863, Class Loss: 0.5329\n",
      "Epoch 41/50, Loss: 1.8900, Domain Loss: 1.3863, Class Loss: 0.5036\n",
      "Epoch 42/50, Loss: 1.8790, Domain Loss: 1.3863, Class Loss: 0.4927\n",
      "Epoch 43/50, Loss: 1.8640, Domain Loss: 1.3863, Class Loss: 0.4777\n",
      "Epoch 44/50, Loss: 1.9298, Domain Loss: 1.3863, Class Loss: 0.5435\n",
      "Epoch 45/50, Loss: 1.8861, Domain Loss: 1.3863, Class Loss: 0.4998\n",
      "Epoch 46/50, Loss: 1.8904, Domain Loss: 1.3863, Class Loss: 0.5040\n",
      "Epoch 47/50, Loss: 1.8810, Domain Loss: 1.3863, Class Loss: 0.4946\n",
      "Epoch 48/50, Loss: 1.9015, Domain Loss: 1.3863, Class Loss: 0.5152\n",
      "Epoch 49/50, Loss: 1.8417, Domain Loss: 1.3863, Class Loss: 0.4553\n",
      "Epoch 50/50, Loss: 1.8482, Domain Loss: 1.3863, Class Loss: 0.4619\n",
      "50.48\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 4.5902, Domain Loss: 2.5617, Class Loss: 2.0285\n",
      "Epoch 2/50, Loss: 2.3633, Domain Loss: 1.1560, Class Loss: 1.2073\n",
      "Epoch 3/50, Loss: 1.9864, Domain Loss: 1.2642, Class Loss: 0.7222\n",
      "Epoch 4/50, Loss: 1.7213, Domain Loss: 1.1430, Class Loss: 0.5783\n",
      "Epoch 5/50, Loss: 1.7089, Domain Loss: 1.1412, Class Loss: 0.5676\n",
      "Epoch 6/50, Loss: 1.4720, Domain Loss: 0.9372, Class Loss: 0.5348\n",
      "Epoch 7/50, Loss: 1.3479, Domain Loss: 0.8010, Class Loss: 0.5469\n",
      "Epoch 8/50, Loss: 1.4781, Domain Loss: 0.8814, Class Loss: 0.5967\n",
      "Epoch 9/50, Loss: 1.7362, Domain Loss: 1.0849, Class Loss: 0.6513\n",
      "Epoch 10/50, Loss: 3.6106, Domain Loss: 2.9140, Class Loss: 0.6966\n",
      "Epoch 11/50, Loss: 6.5282, Domain Loss: 5.5973, Class Loss: 0.9310\n",
      "Epoch 12/50, Loss: 4.0809, Domain Loss: 2.5930, Class Loss: 1.4879\n",
      "Epoch 13/50, Loss: 2.2693, Domain Loss: 1.4781, Class Loss: 0.7912\n",
      "Epoch 14/50, Loss: 1.8020, Domain Loss: 1.1862, Class Loss: 0.6159\n",
      "Epoch 15/50, Loss: 1.7311, Domain Loss: 1.1914, Class Loss: 0.5397\n",
      "Epoch 16/50, Loss: 2.2153, Domain Loss: 1.3852, Class Loss: 0.8300\n",
      "Epoch 17/50, Loss: 3.0737, Domain Loss: 1.8624, Class Loss: 1.2112\n",
      "Epoch 18/50, Loss: 2.1005, Domain Loss: 1.2874, Class Loss: 0.8131\n",
      "Epoch 19/50, Loss: 1.7615, Domain Loss: 1.1474, Class Loss: 0.6141\n",
      "Epoch 20/50, Loss: 1.6711, Domain Loss: 1.1391, Class Loss: 0.5320\n",
      "Epoch 21/50, Loss: 1.6389, Domain Loss: 1.1254, Class Loss: 0.5136\n",
      "Epoch 22/50, Loss: 1.7837, Domain Loss: 1.2050, Class Loss: 0.5786\n",
      "Epoch 23/50, Loss: 2.0412, Domain Loss: 1.3075, Class Loss: 0.7336\n",
      "Epoch 24/50, Loss: 2.0109, Domain Loss: 1.2950, Class Loss: 0.7159\n",
      "Epoch 25/50, Loss: 1.7724, Domain Loss: 1.1848, Class Loss: 0.5876\n",
      "Epoch 26/50, Loss: 1.6835, Domain Loss: 1.1671, Class Loss: 0.5164\n",
      "Epoch 27/50, Loss: 1.6444, Domain Loss: 1.1168, Class Loss: 0.5276\n",
      "Epoch 28/50, Loss: 1.6274, Domain Loss: 1.1377, Class Loss: 0.4897\n",
      "Epoch 29/50, Loss: 1.6476, Domain Loss: 1.1398, Class Loss: 0.5078\n",
      "Epoch 30/50, Loss: 1.6525, Domain Loss: 1.1601, Class Loss: 0.4925\n",
      "Epoch 31/50, Loss: 1.7820, Domain Loss: 1.2119, Class Loss: 0.5701\n",
      "Epoch 32/50, Loss: 1.8853, Domain Loss: 1.2497, Class Loss: 0.6357\n",
      "Epoch 33/50, Loss: 1.7991, Domain Loss: 1.2177, Class Loss: 0.5814\n",
      "Epoch 34/50, Loss: 1.7593, Domain Loss: 1.1924, Class Loss: 0.5668\n",
      "Epoch 35/50, Loss: 1.6866, Domain Loss: 1.1457, Class Loss: 0.5409\n",
      "Epoch 36/50, Loss: 1.6546, Domain Loss: 1.1436, Class Loss: 0.5110\n",
      "Epoch 37/50, Loss: 1.6772, Domain Loss: 1.1527, Class Loss: 0.5245\n",
      "Epoch 38/50, Loss: 1.7618, Domain Loss: 1.1804, Class Loss: 0.5815\n",
      "Epoch 39/50, Loss: 1.9804, Domain Loss: 1.2821, Class Loss: 0.6983\n",
      "Epoch 40/50, Loss: 1.9742, Domain Loss: 1.3263, Class Loss: 0.6478\n",
      "Epoch 41/50, Loss: 1.9892, Domain Loss: 1.2966, Class Loss: 0.6926\n",
      "Epoch 42/50, Loss: 1.9906, Domain Loss: 1.3244, Class Loss: 0.6663\n",
      "Epoch 43/50, Loss: 1.8324, Domain Loss: 1.2646, Class Loss: 0.5678\n",
      "Epoch 44/50, Loss: 1.6921, Domain Loss: 1.1996, Class Loss: 0.4926\n",
      "Epoch 45/50, Loss: 1.6248, Domain Loss: 1.1512, Class Loss: 0.4736\n",
      "Epoch 46/50, Loss: 1.6035, Domain Loss: 1.1373, Class Loss: 0.4663\n",
      "Epoch 47/50, Loss: 1.5648, Domain Loss: 1.0859, Class Loss: 0.4789\n",
      "Epoch 48/50, Loss: 1.5566, Domain Loss: 1.0888, Class Loss: 0.4678\n",
      "Epoch 49/50, Loss: 1.5405, Domain Loss: 1.0648, Class Loss: 0.4757\n",
      "Epoch 50/50, Loss: 1.6842, Domain Loss: 1.1385, Class Loss: 0.5458\n",
      "50.36\n",
      "\n",
      "\n",
      "Source performance:\n",
      "74.52 73.90 74.27 71.74 \n",
      "Target performance:\n",
      "50.52 30.68 49.80 36.34 \n",
      "\n",
      "Per-class target performance: 100.00 0.00 0.00 99.21 \n",
      "Run 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Class Loss: 2.9710, Discrepancy Loss: 0.1024\n",
      "Epoch [2/50], Class Loss: 1.3751, Discrepancy Loss: 0.1216\n",
      "Epoch [3/50], Class Loss: 1.0504, Discrepancy Loss: 0.1237\n",
      "Epoch [4/50], Class Loss: 0.8920, Discrepancy Loss: 0.1243\n",
      "Epoch [5/50], Class Loss: 0.7125, Discrepancy Loss: 0.1009\n",
      "Epoch [6/50], Class Loss: 0.1401, Discrepancy Loss: 0.0909\n",
      "Epoch [7/50], Class Loss: 0.1319, Discrepancy Loss: 0.0739\n",
      "Epoch [8/50], Class Loss: 0.0792, Discrepancy Loss: 0.0706\n",
      "Epoch [9/50], Class Loss: 0.0511, Discrepancy Loss: 0.0687\n",
      "Epoch [10/50], Class Loss: 0.4240, Discrepancy Loss: 0.0926\n",
      "Epoch [11/50], Class Loss: 0.0716, Discrepancy Loss: 0.0819\n",
      "Epoch [12/50], Class Loss: 0.0478, Discrepancy Loss: 0.0785\n",
      "Epoch [13/50], Class Loss: 0.0233, Discrepancy Loss: 0.0821\n",
      "Epoch [14/50], Class Loss: 0.0851, Discrepancy Loss: 0.0796\n",
      "Epoch [15/50], Class Loss: 0.0124, Discrepancy Loss: 0.0838\n",
      "Epoch [16/50], Class Loss: 0.0128, Discrepancy Loss: 0.0772\n",
      "Epoch [17/50], Class Loss: 0.0141, Discrepancy Loss: 0.0840\n",
      "Epoch [18/50], Class Loss: 0.0099, Discrepancy Loss: 0.0589\n",
      "Epoch [19/50], Class Loss: 0.0277, Discrepancy Loss: 0.0691\n",
      "Epoch [20/50], Class Loss: 0.0086, Discrepancy Loss: 0.0837\n",
      "Epoch [21/50], Class Loss: 0.0086, Discrepancy Loss: 0.0746\n",
      "Epoch [22/50], Class Loss: 0.0101, Discrepancy Loss: 0.0749\n",
      "Epoch [23/50], Class Loss: 0.0073, Discrepancy Loss: 0.0629\n",
      "Epoch [24/50], Class Loss: 0.0239, Discrepancy Loss: 0.0582\n",
      "Epoch [25/50], Class Loss: 0.0198, Discrepancy Loss: 0.0563\n",
      "Epoch [26/50], Class Loss: 0.0038, Discrepancy Loss: 0.0622\n",
      "Epoch [27/50], Class Loss: 0.0054, Discrepancy Loss: 0.0619\n",
      "Epoch [28/50], Class Loss: 0.0183, Discrepancy Loss: 0.0772\n",
      "Epoch [29/50], Class Loss: 0.0070, Discrepancy Loss: 0.0768\n",
      "Epoch [30/50], Class Loss: 0.0055, Discrepancy Loss: 0.0809\n",
      "Epoch [31/50], Class Loss: 0.0047, Discrepancy Loss: 0.0812\n",
      "Epoch [32/50], Class Loss: 0.0077, Discrepancy Loss: 0.0727\n",
      "Epoch [33/50], Class Loss: 0.0043, Discrepancy Loss: 0.0805\n",
      "Epoch [34/50], Class Loss: 0.0196, Discrepancy Loss: 0.0759\n",
      "Epoch [35/50], Class Loss: 0.0072, Discrepancy Loss: 0.0826\n",
      "Epoch [36/50], Class Loss: 0.0092, Discrepancy Loss: 0.0784\n",
      "Epoch [37/50], Class Loss: 0.0062, Discrepancy Loss: 0.0803\n",
      "Epoch [38/50], Class Loss: 0.0039, Discrepancy Loss: 0.0839\n",
      "Epoch [39/50], Class Loss: 0.0044, Discrepancy Loss: 0.0816\n",
      "Epoch [40/50], Class Loss: 0.0117, Discrepancy Loss: 0.0808\n",
      "Epoch [41/50], Class Loss: 0.0174, Discrepancy Loss: 0.0826\n",
      "Epoch [42/50], Class Loss: 0.0081, Discrepancy Loss: 0.0834\n",
      "Epoch [43/50], Class Loss: 0.0104, Discrepancy Loss: 0.0837\n",
      "Epoch [44/50], Class Loss: 0.0081, Discrepancy Loss: 0.0748\n",
      "Epoch [45/50], Class Loss: 0.0034, Discrepancy Loss: 0.0802\n",
      "Epoch [46/50], Class Loss: 0.0085, Discrepancy Loss: 0.0803\n",
      "Epoch [47/50], Class Loss: 0.0097, Discrepancy Loss: 0.0844\n",
      "Epoch [48/50], Class Loss: 0.0159, Discrepancy Loss: 0.0782\n",
      "Epoch [49/50], Class Loss: 0.0103, Discrepancy Loss: 0.0792\n",
      "Epoch [50/50], Class Loss: 0.0126, Discrepancy Loss: 0.0853\n",
      "Source Domain Performance - Accuracy: 84.23%, Precision: 90.36%, Recall: 84.10%, F1 Score: 82.92%\n",
      "Target Domain Performance - Accuracy: 53.12%, Precision: 50.91%, Recall: 52.42%, F1 Score: 42.29%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 2.9104, Discrepancy Loss: 0.0936\n",
      "Epoch [2/50], Class Loss: 1.2172, Discrepancy Loss: 0.1057\n",
      "Epoch [3/50], Class Loss: 1.0123, Discrepancy Loss: 0.1350\n",
      "Epoch [4/50], Class Loss: 0.9745, Discrepancy Loss: 0.1167\n",
      "Epoch [5/50], Class Loss: 0.9188, Discrepancy Loss: 0.1222\n",
      "Epoch [6/50], Class Loss: 0.8720, Discrepancy Loss: 0.1178\n",
      "Epoch [7/50], Class Loss: 0.5677, Discrepancy Loss: 0.0920\n",
      "Epoch [8/50], Class Loss: 0.1100, Discrepancy Loss: 0.0621\n",
      "Epoch [9/50], Class Loss: 0.0600, Discrepancy Loss: 0.0653\n",
      "Epoch [10/50], Class Loss: 0.0482, Discrepancy Loss: 0.0741\n",
      "Epoch [11/50], Class Loss: 0.0148, Discrepancy Loss: 0.0597\n",
      "Epoch [12/50], Class Loss: 0.0072, Discrepancy Loss: 0.0593\n",
      "Epoch [13/50], Class Loss: 0.0130, Discrepancy Loss: 0.0572\n",
      "Epoch [14/50], Class Loss: 0.0057, Discrepancy Loss: 0.0566\n",
      "Epoch [15/50], Class Loss: 0.0057, Discrepancy Loss: 0.0560\n",
      "Epoch [16/50], Class Loss: 0.0060, Discrepancy Loss: 0.0563\n",
      "Epoch [17/50], Class Loss: 0.0555, Discrepancy Loss: 0.0636\n",
      "Epoch [18/50], Class Loss: 0.0829, Discrepancy Loss: 0.0864\n",
      "Epoch [19/50], Class Loss: 0.0259, Discrepancy Loss: 0.0753\n",
      "Epoch [20/50], Class Loss: 0.0155, Discrepancy Loss: 0.0603\n",
      "Epoch [21/50], Class Loss: 0.0075, Discrepancy Loss: 0.0613\n",
      "Epoch [22/50], Class Loss: 0.0056, Discrepancy Loss: 0.0673\n",
      "Epoch [23/50], Class Loss: 0.0070, Discrepancy Loss: 0.0641\n",
      "Epoch [24/50], Class Loss: 0.0062, Discrepancy Loss: 0.0610\n",
      "Epoch [25/50], Class Loss: 0.0054, Discrepancy Loss: 0.0650\n",
      "Epoch [26/50], Class Loss: 0.0068, Discrepancy Loss: 0.0609\n",
      "Epoch [27/50], Class Loss: 0.0067, Discrepancy Loss: 0.0625\n",
      "Epoch [28/50], Class Loss: 0.0067, Discrepancy Loss: 0.0714\n",
      "Epoch [29/50], Class Loss: 0.0098, Discrepancy Loss: 0.0651\n",
      "Epoch [30/50], Class Loss: 0.0060, Discrepancy Loss: 0.0669\n",
      "Epoch [31/50], Class Loss: 0.0084, Discrepancy Loss: 0.0684\n",
      "Epoch [32/50], Class Loss: 0.0081, Discrepancy Loss: 0.0701\n",
      "Epoch [33/50], Class Loss: 0.0043, Discrepancy Loss: 0.0671\n",
      "Epoch [34/50], Class Loss: 0.0142, Discrepancy Loss: 0.0661\n",
      "Epoch [35/50], Class Loss: 0.0083, Discrepancy Loss: 0.0729\n",
      "Epoch [36/50], Class Loss: 0.0047, Discrepancy Loss: 0.0728\n",
      "Epoch [37/50], Class Loss: 0.0053, Discrepancy Loss: 0.0648\n",
      "Epoch [38/50], Class Loss: 0.0088, Discrepancy Loss: 0.0632\n",
      "Epoch [39/50], Class Loss: 0.0044, Discrepancy Loss: 0.0697\n",
      "Epoch [40/50], Class Loss: 0.0069, Discrepancy Loss: 0.0695\n",
      "Epoch [41/50], Class Loss: 0.0046, Discrepancy Loss: 0.0703\n",
      "Epoch [42/50], Class Loss: 0.0098, Discrepancy Loss: 0.0664\n",
      "Epoch [43/50], Class Loss: 0.0089, Discrepancy Loss: 0.0703\n",
      "Epoch [44/50], Class Loss: 0.0085, Discrepancy Loss: 0.0730\n",
      "Epoch [45/50], Class Loss: 0.0053, Discrepancy Loss: 0.0712\n",
      "Epoch [46/50], Class Loss: 0.0096, Discrepancy Loss: 0.0664\n",
      "Epoch [47/50], Class Loss: 0.0063, Discrepancy Loss: 0.0709\n",
      "Epoch [48/50], Class Loss: 0.0088, Discrepancy Loss: 0.0663\n",
      "Epoch [49/50], Class Loss: 0.0110, Discrepancy Loss: 0.0656\n",
      "Epoch [50/50], Class Loss: 0.0096, Discrepancy Loss: 0.0734\n",
      "Source Domain Performance - Accuracy: 94.72%, Precision: 95.39%, Recall: 94.74%, F1 Score: 94.77%\n",
      "Target Domain Performance - Accuracy: 69.96%, Precision: 69.27%, Recall: 69.54%, F1 Score: 67.32%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 2.9945, Discrepancy Loss: 0.1000\n",
      "Epoch [2/50], Class Loss: 1.3883, Discrepancy Loss: 0.1013\n",
      "Epoch [3/50], Class Loss: 1.1140, Discrepancy Loss: 0.1050\n",
      "Epoch [4/50], Class Loss: 1.0483, Discrepancy Loss: 0.1029\n",
      "Epoch [5/50], Class Loss: 0.7392, Discrepancy Loss: 0.1044\n",
      "Epoch [6/50], Class Loss: 0.4643, Discrepancy Loss: 0.0968\n",
      "Epoch [7/50], Class Loss: 0.1295, Discrepancy Loss: 0.0656\n",
      "Epoch [8/50], Class Loss: 0.2571, Discrepancy Loss: 0.0772\n",
      "Epoch [9/50], Class Loss: 0.0791, Discrepancy Loss: 0.0702\n",
      "Epoch [10/50], Class Loss: 0.0479, Discrepancy Loss: 0.0782\n",
      "Epoch [11/50], Class Loss: 0.0381, Discrepancy Loss: 0.0522\n",
      "Epoch [12/50], Class Loss: 0.0290, Discrepancy Loss: 0.0531\n",
      "Epoch [13/50], Class Loss: 0.0177, Discrepancy Loss: 0.0556\n",
      "Epoch [14/50], Class Loss: 0.0153, Discrepancy Loss: 0.0559\n",
      "Epoch [15/50], Class Loss: 0.0291, Discrepancy Loss: 0.0597\n",
      "Epoch [16/50], Class Loss: 0.0082, Discrepancy Loss: 0.0541\n",
      "Epoch [17/50], Class Loss: 0.0125, Discrepancy Loss: 0.0681\n",
      "Epoch [18/50], Class Loss: 0.0396, Discrepancy Loss: 0.0622\n",
      "Epoch [19/50], Class Loss: 0.0251, Discrepancy Loss: 0.0669\n",
      "Epoch [20/50], Class Loss: 0.0129, Discrepancy Loss: 0.0758\n",
      "Epoch [21/50], Class Loss: 0.0055, Discrepancy Loss: 0.0722\n",
      "Epoch [22/50], Class Loss: 0.0056, Discrepancy Loss: 0.0768\n",
      "Epoch [23/50], Class Loss: 0.0074, Discrepancy Loss: 0.0768\n",
      "Epoch [24/50], Class Loss: 0.0094, Discrepancy Loss: 0.0785\n",
      "Epoch [25/50], Class Loss: 0.0141, Discrepancy Loss: 0.0774\n",
      "Epoch [26/50], Class Loss: 0.0106, Discrepancy Loss: 0.0747\n",
      "Epoch [27/50], Class Loss: 0.0139, Discrepancy Loss: 0.0933\n",
      "Epoch [28/50], Class Loss: 0.0075, Discrepancy Loss: 0.0825\n",
      "Epoch [29/50], Class Loss: 0.0608, Discrepancy Loss: 0.0917\n",
      "Epoch [30/50], Class Loss: 0.0541, Discrepancy Loss: 0.0939\n",
      "Epoch [31/50], Class Loss: 0.0227, Discrepancy Loss: 0.0837\n",
      "Epoch [32/50], Class Loss: 0.0139, Discrepancy Loss: 0.0913\n",
      "Epoch [33/50], Class Loss: 0.0293, Discrepancy Loss: 0.0974\n",
      "Epoch [34/50], Class Loss: 0.0159, Discrepancy Loss: 0.0990\n",
      "Epoch [35/50], Class Loss: 0.0305, Discrepancy Loss: 0.0920\n",
      "Epoch [36/50], Class Loss: 0.0171, Discrepancy Loss: 0.0822\n",
      "Epoch [37/50], Class Loss: 0.0169, Discrepancy Loss: 0.0928\n",
      "Epoch [38/50], Class Loss: 0.0191, Discrepancy Loss: 0.0858\n",
      "Epoch [39/50], Class Loss: 0.0121, Discrepancy Loss: 0.0980\n",
      "Epoch [40/50], Class Loss: 0.0227, Discrepancy Loss: 0.0930\n",
      "Epoch [41/50], Class Loss: 0.0164, Discrepancy Loss: 0.0882\n",
      "Epoch [42/50], Class Loss: 0.0173, Discrepancy Loss: 0.1032\n",
      "Epoch [43/50], Class Loss: 0.0231, Discrepancy Loss: 0.0850\n",
      "Epoch [44/50], Class Loss: 0.0421, Discrepancy Loss: 0.0995\n",
      "Epoch [45/50], Class Loss: 0.0169, Discrepancy Loss: 0.0934\n",
      "Epoch [46/50], Class Loss: 0.0083, Discrepancy Loss: 0.0971\n",
      "Epoch [47/50], Class Loss: 0.0136, Discrepancy Loss: 0.0913\n",
      "Epoch [48/50], Class Loss: 0.0115, Discrepancy Loss: 0.0861\n",
      "Epoch [49/50], Class Loss: 0.0150, Discrepancy Loss: 0.0900\n",
      "Epoch [50/50], Class Loss: 0.0181, Discrepancy Loss: 0.0985\n",
      "Source Domain Performance - Accuracy: 97.36%, Precision: 97.52%, Recall: 97.38%, F1 Score: 97.39%\n",
      "Target Domain Performance - Accuracy: 63.19%, Precision: 64.46%, Recall: 62.57%, F1 Score: 55.71%\n",
      "\n",
      "Source performance: 92.11% 94.42% 92.07% 91.69%\n",
      "Target performance: 62.09% 61.55% 61.51% 55.11%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 99.29%\n",
      "qpsk: 11.41%\n",
      "16qam: 43.67%\n",
      "16apsk: 91.67%\n",
      "\n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 2.2112, Discrepancy Loss: 0.0981\n",
      "Validation Loss: 1.0396\n",
      "Epoch [2/50], Class Loss: 1.1663, Discrepancy Loss: 0.0231\n",
      "Validation Loss: 1.0085\n",
      "Epoch [3/50], Class Loss: 1.0590, Discrepancy Loss: 0.0264\n",
      "Validation Loss: 1.0575\n",
      "Epoch [4/50], Class Loss: 0.9738, Discrepancy Loss: 0.0297\n",
      "Validation Loss: 0.9487\n",
      "Epoch [5/50], Class Loss: 0.6421, Discrepancy Loss: 0.0351\n",
      "Validation Loss: 0.3091\n",
      "Epoch [6/50], Class Loss: 1.3035, Discrepancy Loss: 0.0668\n",
      "Validation Loss: 0.4246\n",
      "Epoch [7/50], Class Loss: 0.5198, Discrepancy Loss: 0.0444\n",
      "Validation Loss: 5.2760\n",
      "Epoch [8/50], Class Loss: 0.6279, Discrepancy Loss: 0.0308\n",
      "Validation Loss: 0.4844\n",
      "Epoch [9/50], Class Loss: 0.1683, Discrepancy Loss: 0.0243\n",
      "Validation Loss: 0.0307\n",
      "Epoch [10/50], Class Loss: 0.0326, Discrepancy Loss: 0.0229\n",
      "Validation Loss: 0.1209\n",
      "Epoch [11/50], Class Loss: 0.0181, Discrepancy Loss: 0.0218\n",
      "Validation Loss: 0.0030\n",
      "Epoch [12/50], Class Loss: 0.0037, Discrepancy Loss: 0.0189\n",
      "Validation Loss: 0.0053\n",
      "Epoch [13/50], Class Loss: 0.0033, Discrepancy Loss: 0.0165\n",
      "Validation Loss: 0.0111\n",
      "Epoch [14/50], Class Loss: 0.0057, Discrepancy Loss: 0.0164\n",
      "Validation Loss: 0.0235\n",
      "Epoch [15/50], Class Loss: 0.0042, Discrepancy Loss: 0.0195\n",
      "Validation Loss: 0.0171\n",
      "Epoch [16/50], Class Loss: 0.0061, Discrepancy Loss: 0.0180\n",
      "Validation Loss: 0.0108\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.82%, Precision: 99.82%, Recall: 99.82%, F1 Score: 99.82%\n",
      "Target Domain Performance - Accuracy: 65.35%, Precision: 74.30%, Recall: 65.25%, F1 Score: 59.47%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 2.4829, Discrepancy Loss: 0.1017\n",
      "Validation Loss: 1.2209\n",
      "Epoch [2/50], Class Loss: 1.0856, Discrepancy Loss: 0.0191\n",
      "Validation Loss: 1.0738\n",
      "Epoch [3/50], Class Loss: 0.9956, Discrepancy Loss: 0.0154\n",
      "Validation Loss: 0.9469\n",
      "Epoch [4/50], Class Loss: 0.9105, Discrepancy Loss: 0.0219\n",
      "Validation Loss: 0.6788\n",
      "Epoch [5/50], Class Loss: 2.2105, Discrepancy Loss: 0.0403\n",
      "Validation Loss: 1.1330\n",
      "Epoch [6/50], Class Loss: 0.3290, Discrepancy Loss: 0.0247\n",
      "Validation Loss: 0.0505\n",
      "Epoch [7/50], Class Loss: 0.0480, Discrepancy Loss: 0.0224\n",
      "Validation Loss: 0.1091\n",
      "Epoch [8/50], Class Loss: 0.3761, Discrepancy Loss: 0.0417\n",
      "Validation Loss: 0.1427\n",
      "Epoch [9/50], Class Loss: 0.2980, Discrepancy Loss: 0.0520\n",
      "Validation Loss: 0.0312\n",
      "Epoch [10/50], Class Loss: 0.1202, Discrepancy Loss: 0.0362\n",
      "Validation Loss: 0.0144\n",
      "Epoch [11/50], Class Loss: 0.0235, Discrepancy Loss: 0.0462\n",
      "Validation Loss: 0.0235\n",
      "Epoch [12/50], Class Loss: 0.0111, Discrepancy Loss: 0.0418\n",
      "Validation Loss: 0.0226\n",
      "Epoch [13/50], Class Loss: 0.0141, Discrepancy Loss: 0.0416\n",
      "Validation Loss: 0.0504\n",
      "Epoch [14/50], Class Loss: 0.0105, Discrepancy Loss: 0.0378\n",
      "Validation Loss: 0.0548\n",
      "Epoch [15/50], Class Loss: 0.0080, Discrepancy Loss: 0.0357\n",
      "Validation Loss: 0.0328\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.58%, Precision: 99.57%, Recall: 99.59%, F1 Score: 99.58%\n",
      "Target Domain Performance - Accuracy: 69.90%, Precision: 78.05%, Recall: 69.79%, F1 Score: 65.62%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 2.4636, Discrepancy Loss: 0.1199\n",
      "Validation Loss: 1.0696\n",
      "Epoch [2/50], Class Loss: 1.0945, Discrepancy Loss: 0.0231\n",
      "Validation Loss: 0.9697\n",
      "Epoch [3/50], Class Loss: 1.0089, Discrepancy Loss: 0.0199\n",
      "Validation Loss: 0.9233\n",
      "Epoch [4/50], Class Loss: 0.6564, Discrepancy Loss: 0.0225\n",
      "Validation Loss: 0.2820\n",
      "Epoch [5/50], Class Loss: 0.1973, Discrepancy Loss: 0.0204\n",
      "Validation Loss: 0.1734\n",
      "Epoch [6/50], Class Loss: 0.1052, Discrepancy Loss: 0.0288\n",
      "Validation Loss: 0.0805\n",
      "Epoch [7/50], Class Loss: 0.2827, Discrepancy Loss: 0.0266\n",
      "Validation Loss: 0.8600\n",
      "Epoch [8/50], Class Loss: 0.0704, Discrepancy Loss: 0.0290\n",
      "Validation Loss: 0.0438\n",
      "Epoch [9/50], Class Loss: 0.1568, Discrepancy Loss: 0.0377\n",
      "Validation Loss: 0.2614\n",
      "Epoch [10/50], Class Loss: 0.0741, Discrepancy Loss: 0.0297\n",
      "Validation Loss: 0.0123\n",
      "Epoch [11/50], Class Loss: 0.0025, Discrepancy Loss: 0.0225\n",
      "Validation Loss: 0.0080\n",
      "Epoch [12/50], Class Loss: 0.0008, Discrepancy Loss: 0.0174\n",
      "Validation Loss: 0.0104\n",
      "Epoch [13/50], Class Loss: 0.0054, Discrepancy Loss: 0.0145\n",
      "Validation Loss: 0.0060\n",
      "Epoch [14/50], Class Loss: 0.0035, Discrepancy Loss: 0.0142\n",
      "Validation Loss: 0.0032\n",
      "Epoch [15/50], Class Loss: 0.0010, Discrepancy Loss: 0.0148\n",
      "Validation Loss: 0.0079\n",
      "Epoch [16/50], Class Loss: 0.0012, Discrepancy Loss: 0.0136\n",
      "Validation Loss: 0.0214\n",
      "Epoch [17/50], Class Loss: 0.0048, Discrepancy Loss: 0.0114\n",
      "Validation Loss: 0.0125\n",
      "Epoch [18/50], Class Loss: 0.0045, Discrepancy Loss: 0.0120\n",
      "Validation Loss: 0.0063\n",
      "Epoch [19/50], Class Loss: 0.0018, Discrepancy Loss: 0.0118\n",
      "Validation Loss: 0.0067\n",
      "Early stopping!\n",
      "Source Domain Performance - Accuracy: 99.88%, Precision: 99.88%, Recall: 99.89%, F1 Score: 99.88%\n",
      "Target Domain Performance - Accuracy: 80.76%, Precision: 84.42%, Recall: 80.70%, F1 Score: 79.93%\n",
      "\n",
      "Source performance: 99.76% 99.76% 99.77% 99.76%\n",
      "Target performance: 72.00% 78.92% 71.91% 68.34%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 99.42%\n",
      "qpsk: 25.62%\n",
      "16qam: 62.99%\n",
      "16apsk: 99.61%\n",
      "\n",
      "Run 1/3\n",
      "Epoch 1/50, Train Loss: 0.9467, Train Acc: 0.5525, Val Loss: 0.5223, Val Acc: 0.7548\n",
      "Epoch 2/50, Train Loss: 0.6203, Train Acc: 0.6867, Val Loss: 0.6418, Val Acc: 0.7152\n",
      "Epoch 3/50, Train Loss: 0.5169, Train Acc: 0.7308, Val Loss: 0.4513, Val Acc: 0.7728\n",
      "Epoch 4/50, Train Loss: 0.4423, Train Acc: 0.8071, Val Loss: 0.2291, Val Acc: 0.9394\n",
      "Epoch 5/50, Train Loss: 0.2371, Train Acc: 0.9364, Val Loss: 0.0751, Val Acc: 0.9670\n",
      "Epoch 6/50, Train Loss: 0.0765, Train Acc: 0.9714, Val Loss: 0.0696, Val Acc: 0.9784\n",
      "Epoch 7/50, Train Loss: 0.0253, Train Acc: 0.9912, Val Loss: 0.0011, Val Acc: 1.0000\n",
      "Epoch 8/50, Train Loss: 0.0039, Train Acc: 0.9987, Val Loss: 0.0093, Val Acc: 0.9958\n",
      "Epoch 9/50, Train Loss: 0.0061, Train Acc: 0.9985, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 10/50, Train Loss: 0.0089, Train Acc: 0.9969, Val Loss: 0.0520, Val Acc: 0.9880\n",
      "Epoch 11/50, Train Loss: 0.0020, Train Acc: 0.9994, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 18/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 19/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 20/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 21/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Run 2/3\n",
      "Epoch 1/50, Train Loss: 0.8935, Train Acc: 0.6083, Val Loss: 0.5624, Val Acc: 0.7206\n",
      "Epoch 2/50, Train Loss: 0.5542, Train Acc: 0.7176, Val Loss: 0.5631, Val Acc: 0.7122\n",
      "Epoch 3/50, Train Loss: 0.4785, Train Acc: 0.7725, Val Loss: 0.4083, Val Acc: 0.7884\n",
      "Epoch 4/50, Train Loss: 0.2884, Train Acc: 0.8931, Val Loss: 0.0538, Val Acc: 0.9844\n",
      "Epoch 5/50, Train Loss: 0.1194, Train Acc: 0.9613, Val Loss: 0.0080, Val Acc: 0.9970\n",
      "Epoch 6/50, Train Loss: 0.0178, Train Acc: 0.9940, Val Loss: 0.0089, Val Acc: 0.9982\n",
      "Epoch 7/50, Train Loss: 0.0052, Train Acc: 0.9984, Val Loss: 0.2064, Val Acc: 0.9478\n",
      "Epoch 8/50, Train Loss: 0.0155, Train Acc: 0.9949, Val Loss: 0.0246, Val Acc: 0.9916\n",
      "Epoch 9/50, Train Loss: 0.0117, Train Acc: 0.9973, Val Loss: 0.0018, Val Acc: 0.9988\n",
      "Epoch 10/50, Train Loss: 0.0012, Train Acc: 0.9996, Val Loss: 0.0005, Val Acc: 1.0000\n",
      "Epoch 11/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 14/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 16/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 18/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 19/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 20/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 21/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Epoch 22/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Run 3/3\n",
      "Epoch 1/50, Train Loss: 0.9157, Train Acc: 0.5754, Val Loss: 0.5379, Val Acc: 0.7308\n",
      "Epoch 2/50, Train Loss: 0.5348, Train Acc: 0.7227, Val Loss: 0.5505, Val Acc: 0.7332\n",
      "Epoch 3/50, Train Loss: 0.4990, Train Acc: 0.7463, Val Loss: 0.5000, Val Acc: 0.7368\n",
      "Epoch 4/50, Train Loss: 0.3724, Train Acc: 0.8506, Val Loss: 0.1421, Val Acc: 0.9664\n",
      "Epoch 5/50, Train Loss: 0.0791, Train Acc: 0.9718, Val Loss: 0.1053, Val Acc: 0.9610\n",
      "Epoch 6/50, Train Loss: 0.0478, Train Acc: 0.9840, Val Loss: 0.0306, Val Acc: 0.9892\n",
      "Epoch 7/50, Train Loss: 0.0191, Train Acc: 0.9933, Val Loss: 0.0041, Val Acc: 0.9994\n",
      "Epoch 8/50, Train Loss: 0.0015, Train Acc: 0.9997, Val Loss: 0.0012, Val Acc: 0.9994\n",
      "Epoch 9/50, Train Loss: 0.0031, Train Acc: 0.9991, Val Loss: 0.0011, Val Acc: 0.9994\n",
      "Epoch 10/50, Train Loss: 0.0237, Train Acc: 0.9925, Val Loss: 0.0004, Val Acc: 1.0000\n",
      "Epoch 11/50, Train Loss: 0.0002, Train Acc: 1.0000, Val Loss: 0.0005, Val Acc: 1.0000\n",
      "Epoch 12/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0006, Val Acc: 1.0000\n",
      "Epoch 13/50, Train Loss: 0.0001, Train Acc: 1.0000, Val Loss: 0.0022, Val Acc: 0.9988\n",
      "Epoch 14/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 15/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0023, Val Acc: 0.9988\n",
      "Epoch 16/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000\n",
      "Epoch 17/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 18/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0001, Val Acc: 1.0000\n",
      "Epoch 19/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0003, Val Acc: 1.0000\n",
      "Epoch 20/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 21/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 22/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Epoch 23/50, Train Loss: 0.0000, Train Acc: 1.0000, Val Loss: 0.0002, Val Acc: 1.0000\n",
      "Early stopping!\n",
      "\n",
      "Source performance: 100.00 100.00 100.00 100.00\n",
      "Target performance: 71.42 82.25 71.31 63.47\n",
      "\n",
      "bpsk: 100.00\n",
      "qpsk: 1.77\n",
      "16qam: 83.93\n",
      "16apsk: 99.54\n",
      "Epoch 1/50, Loss: 5.2415, Domain Loss: 2.5287, Class Loss: 2.7127\n",
      "Epoch 2/50, Loss: 2.4487, Domain Loss: 1.3832, Class Loss: 1.0655\n",
      "Epoch 3/50, Loss: 1.8538, Domain Loss: 1.2536, Class Loss: 0.6002\n",
      "Epoch 4/50, Loss: 1.7684, Domain Loss: 1.2032, Class Loss: 0.5652\n",
      "Epoch 5/50, Loss: 1.8507, Domain Loss: 1.2206, Class Loss: 0.6301\n",
      "Epoch 6/50, Loss: 1.9471, Domain Loss: 1.3334, Class Loss: 0.6138\n",
      "Epoch 7/50, Loss: 1.7961, Domain Loss: 1.2122, Class Loss: 0.5839\n",
      "Epoch 8/50, Loss: 1.7139, Domain Loss: 1.1286, Class Loss: 0.5853\n",
      "Epoch 9/50, Loss: 1.6607, Domain Loss: 1.1210, Class Loss: 0.5396\n",
      "Epoch 10/50, Loss: 1.5887, Domain Loss: 1.0931, Class Loss: 0.4956\n",
      "Epoch 11/50, Loss: 1.5461, Domain Loss: 1.0606, Class Loss: 0.4855\n",
      "Epoch 12/50, Loss: 1.5762, Domain Loss: 1.0777, Class Loss: 0.4985\n",
      "Epoch 13/50, Loss: 1.5519, Domain Loss: 1.0766, Class Loss: 0.4753\n",
      "Epoch 14/50, Loss: 1.6142, Domain Loss: 1.0951, Class Loss: 0.5191\n",
      "Epoch 15/50, Loss: 1.5627, Domain Loss: 1.0551, Class Loss: 0.5075\n",
      "Epoch 16/50, Loss: 1.6338, Domain Loss: 1.1650, Class Loss: 0.4688\n",
      "Epoch 17/50, Loss: 3.1140, Domain Loss: 2.0614, Class Loss: 1.0526\n",
      "Epoch 18/50, Loss: 1.7649, Domain Loss: 1.2097, Class Loss: 0.5552\n",
      "Epoch 19/50, Loss: 1.6767, Domain Loss: 1.1773, Class Loss: 0.4994\n",
      "Epoch 20/50, Loss: 1.6052, Domain Loss: 1.0907, Class Loss: 0.5145\n",
      "Epoch 21/50, Loss: 1.5287, Domain Loss: 1.0642, Class Loss: 0.4645\n",
      "Epoch 22/50, Loss: 1.4777, Domain Loss: 1.0590, Class Loss: 0.4187\n",
      "Epoch 23/50, Loss: 1.5086, Domain Loss: 1.0706, Class Loss: 0.4380\n",
      "Epoch 24/50, Loss: 1.5388, Domain Loss: 1.0921, Class Loss: 0.4468\n",
      "Epoch 25/50, Loss: 1.6322, Domain Loss: 1.2094, Class Loss: 0.4228\n",
      "Epoch 26/50, Loss: 2.0711, Domain Loss: 1.4654, Class Loss: 0.6057\n",
      "Epoch 27/50, Loss: 1.7034, Domain Loss: 1.2597, Class Loss: 0.4437\n",
      "Epoch 28/50, Loss: 1.5792, Domain Loss: 1.1253, Class Loss: 0.4538\n",
      "Epoch 29/50, Loss: 1.6450, Domain Loss: 1.2284, Class Loss: 0.4166\n",
      "Epoch 30/50, Loss: 1.6396, Domain Loss: 1.2642, Class Loss: 0.3754\n",
      "Epoch 31/50, Loss: 1.7820, Domain Loss: 1.3526, Class Loss: 0.4294\n",
      "Epoch 32/50, Loss: 1.5562, Domain Loss: 1.1774, Class Loss: 0.3788\n",
      "Epoch 33/50, Loss: 1.4081, Domain Loss: 1.1235, Class Loss: 0.2846\n",
      "Epoch 34/50, Loss: 1.5185, Domain Loss: 1.1992, Class Loss: 0.3193\n",
      "Epoch 35/50, Loss: 1.3166, Domain Loss: 1.1060, Class Loss: 0.2106\n",
      "Epoch 36/50, Loss: 1.2629, Domain Loss: 1.0958, Class Loss: 0.1670\n",
      "Epoch 37/50, Loss: 1.2409, Domain Loss: 1.0931, Class Loss: 0.1478\n",
      "Epoch 38/50, Loss: 1.3165, Domain Loss: 1.1412, Class Loss: 0.1753\n",
      "Epoch 39/50, Loss: 1.2864, Domain Loss: 1.1615, Class Loss: 0.1248\n",
      "Epoch 40/50, Loss: 1.3794, Domain Loss: 1.1409, Class Loss: 0.2385\n",
      "Epoch 41/50, Loss: 1.2610, Domain Loss: 1.1047, Class Loss: 0.1562\n",
      "Epoch 42/50, Loss: 1.1699, Domain Loss: 1.0881, Class Loss: 0.0818\n",
      "Epoch 43/50, Loss: 1.1289, Domain Loss: 1.0501, Class Loss: 0.0788\n",
      "Epoch 44/50, Loss: 1.1412, Domain Loss: 1.0870, Class Loss: 0.0542\n",
      "Epoch 45/50, Loss: 1.0900, Domain Loss: 1.0454, Class Loss: 0.0446\n",
      "Epoch 46/50, Loss: 1.1389, Domain Loss: 1.0803, Class Loss: 0.0586\n",
      "Epoch 47/50, Loss: 1.1150, Domain Loss: 1.0645, Class Loss: 0.0505\n",
      "Epoch 48/50, Loss: 1.1160, Domain Loss: 1.0744, Class Loss: 0.0416\n",
      "Epoch 49/50, Loss: 1.0756, Domain Loss: 1.0482, Class Loss: 0.0274\n",
      "Epoch 50/50, Loss: 1.1211, Domain Loss: 1.0927, Class Loss: 0.0284\n",
      "60.07\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 4.6308, Domain Loss: 2.5322, Class Loss: 2.0986\n",
      "Epoch 2/50, Loss: 2.3825, Domain Loss: 1.3654, Class Loss: 1.0171\n",
      "Epoch 3/50, Loss: 1.8527, Domain Loss: 1.2436, Class Loss: 0.6091\n",
      "Epoch 4/50, Loss: 1.7786, Domain Loss: 1.2112, Class Loss: 0.5674\n",
      "Epoch 5/50, Loss: 1.6526, Domain Loss: 1.1095, Class Loss: 0.5431\n",
      "Epoch 6/50, Loss: 1.6204, Domain Loss: 1.0763, Class Loss: 0.5441\n",
      "Epoch 7/50, Loss: 1.6265, Domain Loss: 1.0745, Class Loss: 0.5520\n",
      "Epoch 8/50, Loss: 1.8005, Domain Loss: 1.1346, Class Loss: 0.6659\n",
      "Epoch 9/50, Loss: 1.7248, Domain Loss: 1.1654, Class Loss: 0.5593\n",
      "Epoch 10/50, Loss: 1.7707, Domain Loss: 1.1848, Class Loss: 0.5859\n",
      "Epoch 11/50, Loss: 1.9232, Domain Loss: 1.3057, Class Loss: 0.6176\n",
      "Epoch 12/50, Loss: 1.6835, Domain Loss: 1.1760, Class Loss: 0.5074\n",
      "Epoch 13/50, Loss: 1.6757, Domain Loss: 1.1645, Class Loss: 0.5111\n",
      "Epoch 14/50, Loss: 1.7621, Domain Loss: 1.2264, Class Loss: 0.5357\n",
      "Epoch 15/50, Loss: 1.9421, Domain Loss: 1.2481, Class Loss: 0.6940\n",
      "Epoch 16/50, Loss: 1.8902, Domain Loss: 1.2785, Class Loss: 0.6117\n",
      "Epoch 17/50, Loss: 10.6683, Domain Loss: 9.8658, Class Loss: 0.8025\n",
      "Epoch 18/50, Loss: 32.6910, Domain Loss: 29.5818, Class Loss: 3.1093\n",
      "Epoch 19/50, Loss: 29.1618, Domain Loss: 27.7655, Class Loss: 1.3963\n",
      "Epoch 20/50, Loss: 16.8646, Domain Loss: 15.3977, Class Loss: 1.4669\n",
      "Epoch 21/50, Loss: 3.9480, Domain Loss: 2.5103, Class Loss: 1.4377\n",
      "Epoch 22/50, Loss: 3.1313, Domain Loss: 1.7357, Class Loss: 1.3956\n",
      "Epoch 23/50, Loss: 3.2013, Domain Loss: 1.8121, Class Loss: 1.3892\n",
      "Epoch 24/50, Loss: 3.5288, Domain Loss: 2.1397, Class Loss: 1.3891\n",
      "Epoch 25/50, Loss: 3.8888, Domain Loss: 2.5011, Class Loss: 1.3877\n",
      "Epoch 26/50, Loss: 4.2156, Domain Loss: 2.8273, Class Loss: 1.3883\n",
      "Epoch 27/50, Loss: 3.9045, Domain Loss: 2.5200, Class Loss: 1.3844\n",
      "Epoch 28/50, Loss: 3.1743, Domain Loss: 1.7926, Class Loss: 1.3818\n",
      "Epoch 29/50, Loss: 3.4305, Domain Loss: 2.0650, Class Loss: 1.3654\n",
      "Epoch 30/50, Loss: 2.8381, Domain Loss: 1.5564, Class Loss: 1.2817\n",
      "Epoch 31/50, Loss: 2.7545, Domain Loss: 1.5051, Class Loss: 1.2494\n",
      "Epoch 32/50, Loss: 2.6528, Domain Loss: 1.4677, Class Loss: 1.1851\n",
      "Epoch 33/50, Loss: 2.8544, Domain Loss: 1.6145, Class Loss: 1.2399\n",
      "Epoch 34/50, Loss: 5.7315, Domain Loss: 4.1915, Class Loss: 1.5400\n",
      "Epoch 35/50, Loss: 5.5596, Domain Loss: 4.1191, Class Loss: 1.4405\n",
      "Epoch 36/50, Loss: 5.0400, Domain Loss: 3.6434, Class Loss: 1.3966\n",
      "Epoch 37/50, Loss: 5.7713, Domain Loss: 4.3822, Class Loss: 1.3891\n",
      "Epoch 38/50, Loss: 24.1776, Domain Loss: 22.8098, Class Loss: 1.3678\n",
      "Epoch 39/50, Loss: 28.8752, Domain Loss: 27.4937, Class Loss: 1.3815\n",
      "Epoch 40/50, Loss: 9.8342, Domain Loss: 8.4530, Class Loss: 1.3812\n",
      "Epoch 41/50, Loss: 4.9642, Domain Loss: 3.5823, Class Loss: 1.3819\n",
      "Epoch 42/50, Loss: 3.4723, Domain Loss: 2.0987, Class Loss: 1.3736\n",
      "Epoch 43/50, Loss: 2.8334, Domain Loss: 1.4697, Class Loss: 1.3637\n",
      "Epoch 44/50, Loss: 2.6606, Domain Loss: 1.3337, Class Loss: 1.3269\n",
      "Epoch 45/50, Loss: 2.6943, Domain Loss: 1.3131, Class Loss: 1.3811\n",
      "Epoch 46/50, Loss: 2.6249, Domain Loss: 1.3150, Class Loss: 1.3099\n",
      "Epoch 47/50, Loss: 2.5793, Domain Loss: 1.3017, Class Loss: 1.2776\n",
      "Epoch 48/50, Loss: 2.5585, Domain Loss: 1.3224, Class Loss: 1.2361\n",
      "Epoch 49/50, Loss: 2.5422, Domain Loss: 1.3081, Class Loss: 1.2341\n",
      "Epoch 50/50, Loss: 2.4613, Domain Loss: 1.3110, Class Loss: 1.1503\n",
      "24.28\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ash/.conda/envs/torch/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 4.9580, Domain Loss: 2.6379, Class Loss: 2.3201\n",
      "Epoch 2/50, Loss: 2.6003, Domain Loss: 1.4710, Class Loss: 1.1293\n",
      "Epoch 3/50, Loss: 2.0137, Domain Loss: 1.3590, Class Loss: 0.6546\n",
      "Epoch 4/50, Loss: 1.8858, Domain Loss: 1.3269, Class Loss: 0.5590\n",
      "Epoch 5/50, Loss: 1.9346, Domain Loss: 1.3015, Class Loss: 0.6332\n",
      "Epoch 6/50, Loss: 1.8864, Domain Loss: 1.3076, Class Loss: 0.5788\n",
      "Epoch 7/50, Loss: 1.9180, Domain Loss: 1.3641, Class Loss: 0.5539\n",
      "Epoch 8/50, Loss: 1.9428, Domain Loss: 1.4427, Class Loss: 0.5000\n",
      "Epoch 9/50, Loss: 1.8968, Domain Loss: 1.3783, Class Loss: 0.5185\n",
      "Epoch 10/50, Loss: 1.9681, Domain Loss: 1.4341, Class Loss: 0.5340\n",
      "Epoch 11/50, Loss: 2.0713, Domain Loss: 1.5385, Class Loss: 0.5328\n",
      "Epoch 12/50, Loss: 4.0067, Domain Loss: 2.9071, Class Loss: 1.0996\n",
      "Epoch 13/50, Loss: 3.3289, Domain Loss: 2.4674, Class Loss: 0.8615\n",
      "Epoch 14/50, Loss: 2.7131, Domain Loss: 2.1111, Class Loss: 0.6020\n",
      "Epoch 15/50, Loss: 2.4227, Domain Loss: 1.8024, Class Loss: 0.6203\n",
      "Epoch 16/50, Loss: 3.3551, Domain Loss: 2.5348, Class Loss: 0.8203\n",
      "Epoch 17/50, Loss: 3.1120, Domain Loss: 2.0816, Class Loss: 1.0304\n",
      "Epoch 18/50, Loss: 2.4769, Domain Loss: 1.6784, Class Loss: 0.7986\n",
      "Epoch 19/50, Loss: 1.9269, Domain Loss: 1.3380, Class Loss: 0.5889\n",
      "Epoch 20/50, Loss: 1.7737, Domain Loss: 1.2685, Class Loss: 0.5052\n",
      "Epoch 21/50, Loss: 1.7767, Domain Loss: 1.2523, Class Loss: 0.5244\n",
      "Epoch 22/50, Loss: 1.7923, Domain Loss: 1.2900, Class Loss: 0.5023\n",
      "Epoch 23/50, Loss: 1.8077, Domain Loss: 1.3098, Class Loss: 0.4979\n",
      "Epoch 24/50, Loss: 1.7837, Domain Loss: 1.2574, Class Loss: 0.5263\n",
      "Epoch 25/50, Loss: 1.6944, Domain Loss: 1.1756, Class Loss: 0.5188\n",
      "Epoch 26/50, Loss: 1.7913, Domain Loss: 1.2628, Class Loss: 0.5286\n",
      "Epoch 27/50, Loss: 1.7763, Domain Loss: 1.2495, Class Loss: 0.5268\n",
      "Epoch 28/50, Loss: 1.9652, Domain Loss: 1.3496, Class Loss: 0.6156\n",
      "Epoch 29/50, Loss: 1.8230, Domain Loss: 1.2657, Class Loss: 0.5573\n",
      "Epoch 30/50, Loss: 1.8330, Domain Loss: 1.3261, Class Loss: 0.5069\n",
      "Epoch 31/50, Loss: 2.0735, Domain Loss: 1.4959, Class Loss: 0.5776\n",
      "Epoch 32/50, Loss: 1.9424, Domain Loss: 1.4014, Class Loss: 0.5410\n",
      "Epoch 33/50, Loss: 1.9711, Domain Loss: 1.4676, Class Loss: 0.5035\n",
      "Epoch 34/50, Loss: 1.7505, Domain Loss: 1.2627, Class Loss: 0.4877\n",
      "Epoch 35/50, Loss: 1.8522, Domain Loss: 1.2844, Class Loss: 0.5678\n",
      "Epoch 36/50, Loss: 1.9762, Domain Loss: 1.4610, Class Loss: 0.5152\n",
      "Epoch 37/50, Loss: 2.7170, Domain Loss: 1.5787, Class Loss: 1.1382\n",
      "Epoch 38/50, Loss: 2.2507, Domain Loss: 1.3544, Class Loss: 0.8963\n",
      "Epoch 39/50, Loss: 2.0392, Domain Loss: 1.3299, Class Loss: 0.7093\n",
      "Epoch 40/50, Loss: 1.7111, Domain Loss: 1.1755, Class Loss: 0.5356\n",
      "Epoch 41/50, Loss: 1.8149, Domain Loss: 1.2526, Class Loss: 0.5624\n",
      "Epoch 42/50, Loss: 1.7261, Domain Loss: 1.2370, Class Loss: 0.4892\n",
      "Epoch 43/50, Loss: 1.8432, Domain Loss: 1.3119, Class Loss: 0.5313\n",
      "Epoch 44/50, Loss: 1.8453, Domain Loss: 1.3073, Class Loss: 0.5380\n",
      "Epoch 45/50, Loss: 1.9909, Domain Loss: 1.4222, Class Loss: 0.5687\n",
      "Epoch 46/50, Loss: 2.0353, Domain Loss: 1.4942, Class Loss: 0.5411\n",
      "Epoch 47/50, Loss: 2.0724, Domain Loss: 1.4606, Class Loss: 0.6118\n",
      "Epoch 48/50, Loss: 1.9726, Domain Loss: 1.3918, Class Loss: 0.5808\n",
      "Epoch 49/50, Loss: 1.9226, Domain Loss: 1.4006, Class Loss: 0.5220\n",
      "Epoch 50/50, Loss: 1.8999, Domain Loss: 1.4124, Class Loss: 0.4875\n",
      "58.51\n",
      "\n",
      "\n",
      "Source performance:\n",
      "68.76 63.56 68.63 64.61 \n",
      "Target performance:\n",
      "47.62 37.87 47.82 38.32 \n",
      "\n",
      "Per-class target performance: 100.00 0.00 28.70 62.57 \n",
      "Run 1/3\n",
      "Epoch [1/50], Class Loss: 2.6843, Discrepancy Loss: 0.1191\n",
      "Epoch [2/50], Class Loss: 1.1863, Discrepancy Loss: 0.1413\n",
      "Epoch [3/50], Class Loss: 1.0228, Discrepancy Loss: 0.1423\n",
      "Epoch [4/50], Class Loss: 1.1442, Discrepancy Loss: 0.1254\n",
      "Epoch [5/50], Class Loss: 0.9862, Discrepancy Loss: 0.1210\n",
      "Epoch [6/50], Class Loss: 0.8923, Discrepancy Loss: 0.1134\n",
      "Epoch [7/50], Class Loss: 0.8328, Discrepancy Loss: 0.1263\n",
      "Epoch [8/50], Class Loss: 0.7998, Discrepancy Loss: 0.1249\n",
      "Epoch [9/50], Class Loss: 0.3989, Discrepancy Loss: 0.1011\n",
      "Epoch [10/50], Class Loss: 0.2968, Discrepancy Loss: 0.0737\n",
      "Epoch [11/50], Class Loss: 0.0417, Discrepancy Loss: 0.0717\n",
      "Epoch [12/50], Class Loss: 0.0290, Discrepancy Loss: 0.0727\n",
      "Epoch [13/50], Class Loss: 0.0308, Discrepancy Loss: 0.0626\n",
      "Epoch [14/50], Class Loss: 0.0360, Discrepancy Loss: 0.0488\n",
      "Epoch [15/50], Class Loss: 0.0126, Discrepancy Loss: 0.0489\n",
      "Epoch [16/50], Class Loss: 0.0249, Discrepancy Loss: 0.0451\n",
      "Epoch [17/50], Class Loss: 0.0158, Discrepancy Loss: 0.0411\n",
      "Epoch [18/50], Class Loss: 0.0171, Discrepancy Loss: 0.0341\n",
      "Epoch [19/50], Class Loss: 0.0210, Discrepancy Loss: 0.0306\n",
      "Epoch [20/50], Class Loss: 0.0084, Discrepancy Loss: 0.0288\n",
      "Epoch [21/50], Class Loss: 0.0080, Discrepancy Loss: 0.0297\n",
      "Epoch [22/50], Class Loss: 0.0074, Discrepancy Loss: 0.0294\n",
      "Epoch [23/50], Class Loss: 0.0075, Discrepancy Loss: 0.0316\n",
      "Epoch [24/50], Class Loss: 0.0074, Discrepancy Loss: 0.0375\n",
      "Epoch [25/50], Class Loss: 0.0115, Discrepancy Loss: 0.0238\n",
      "Epoch [26/50], Class Loss: 0.0100, Discrepancy Loss: 0.0334\n",
      "Epoch [27/50], Class Loss: 0.0062, Discrepancy Loss: 0.0301\n",
      "Epoch [28/50], Class Loss: 0.0069, Discrepancy Loss: 0.0375\n",
      "Epoch [29/50], Class Loss: 0.0058, Discrepancy Loss: 0.0251\n",
      "Epoch [30/50], Class Loss: 0.0054, Discrepancy Loss: 0.0295\n",
      "Epoch [31/50], Class Loss: 0.0123, Discrepancy Loss: 0.0284\n",
      "Epoch [32/50], Class Loss: 0.0069, Discrepancy Loss: 0.0292\n",
      "Epoch [33/50], Class Loss: 0.0060, Discrepancy Loss: 0.0346\n",
      "Epoch [34/50], Class Loss: 0.0065, Discrepancy Loss: 0.0336\n",
      "Epoch [35/50], Class Loss: 0.0060, Discrepancy Loss: 0.0325\n",
      "Epoch [36/50], Class Loss: 0.0074, Discrepancy Loss: 0.0348\n",
      "Epoch [37/50], Class Loss: 0.0101, Discrepancy Loss: 0.0331\n",
      "Epoch [38/50], Class Loss: 0.0106, Discrepancy Loss: 0.0313\n",
      "Epoch [39/50], Class Loss: 0.0053, Discrepancy Loss: 0.0289\n",
      "Epoch [40/50], Class Loss: 0.0079, Discrepancy Loss: 0.0317\n",
      "Epoch [41/50], Class Loss: 0.0048, Discrepancy Loss: 0.0297\n",
      "Epoch [42/50], Class Loss: 0.0070, Discrepancy Loss: 0.0364\n",
      "Epoch [43/50], Class Loss: 0.0036, Discrepancy Loss: 0.0370\n",
      "Epoch [44/50], Class Loss: 0.0070, Discrepancy Loss: 0.0347\n",
      "Epoch [45/50], Class Loss: 0.0075, Discrepancy Loss: 0.0350\n",
      "Epoch [46/50], Class Loss: 0.0312, Discrepancy Loss: 0.0294\n",
      "Epoch [47/50], Class Loss: 0.0089, Discrepancy Loss: 0.0290\n",
      "Epoch [48/50], Class Loss: 0.0044, Discrepancy Loss: 0.0252\n",
      "Epoch [49/50], Class Loss: 0.0073, Discrepancy Loss: 0.0311\n",
      "Epoch [50/50], Class Loss: 0.0166, Discrepancy Loss: 0.0269\n",
      "Source Domain Performance - Accuracy: 99.58%, Precision: 99.57%, Recall: 99.60%, F1 Score: 99.58%\n",
      "Target Domain Performance - Accuracy: 92.33%, Precision: 93.89%, Recall: 92.30%, F1 Score: 92.18%\n",
      "\n",
      "Run 2/3\n",
      "Epoch [1/50], Class Loss: 2.4047, Discrepancy Loss: 0.1330\n",
      "Epoch [2/50], Class Loss: 1.3538, Discrepancy Loss: 0.1179\n",
      "Epoch [3/50], Class Loss: 1.2452, Discrepancy Loss: 0.1168\n",
      "Epoch [4/50], Class Loss: 1.1610, Discrepancy Loss: 0.1301\n",
      "Epoch [5/50], Class Loss: 1.0178, Discrepancy Loss: 0.1469\n",
      "Epoch [6/50], Class Loss: 0.7814, Discrepancy Loss: 0.1275\n",
      "Epoch [7/50], Class Loss: 0.1652, Discrepancy Loss: 0.0781\n",
      "Epoch [8/50], Class Loss: 0.0651, Discrepancy Loss: 0.0576\n",
      "Epoch [9/50], Class Loss: 0.0581, Discrepancy Loss: 0.0437\n",
      "Epoch [10/50], Class Loss: 0.0369, Discrepancy Loss: 0.0372\n",
      "Epoch [11/50], Class Loss: 0.0101, Discrepancy Loss: 0.0265\n",
      "Epoch [12/50], Class Loss: 0.0083, Discrepancy Loss: 0.0291\n",
      "Epoch [13/50], Class Loss: 0.0161, Discrepancy Loss: 0.0237\n",
      "Epoch [14/50], Class Loss: 0.0224, Discrepancy Loss: 0.0238\n",
      "Epoch [15/50], Class Loss: 0.0182, Discrepancy Loss: 0.0265\n",
      "Epoch [16/50], Class Loss: 0.0035, Discrepancy Loss: 0.0214\n",
      "Epoch [17/50], Class Loss: 0.0052, Discrepancy Loss: 0.0211\n",
      "Epoch [18/50], Class Loss: 0.0086, Discrepancy Loss: 0.0223\n",
      "Epoch [19/50], Class Loss: 0.0077, Discrepancy Loss: 0.0269\n",
      "Epoch [20/50], Class Loss: 0.0096, Discrepancy Loss: 0.0247\n",
      "Epoch [21/50], Class Loss: 0.0036, Discrepancy Loss: 0.0271\n",
      "Epoch [22/50], Class Loss: 0.0024, Discrepancy Loss: 0.0255\n",
      "Epoch [23/50], Class Loss: 0.0046, Discrepancy Loss: 0.0244\n",
      "Epoch [24/50], Class Loss: 0.0117, Discrepancy Loss: 0.0252\n",
      "Epoch [25/50], Class Loss: 0.0113, Discrepancy Loss: 0.0258\n",
      "Epoch [26/50], Class Loss: 0.0048, Discrepancy Loss: 0.0272\n",
      "Epoch [27/50], Class Loss: 0.0084, Discrepancy Loss: 0.0301\n",
      "Epoch [28/50], Class Loss: 0.0059, Discrepancy Loss: 0.0292\n",
      "Epoch [29/50], Class Loss: 0.0033, Discrepancy Loss: 0.0279\n",
      "Epoch [30/50], Class Loss: 0.0070, Discrepancy Loss: 0.0276\n",
      "Epoch [31/50], Class Loss: 0.0076, Discrepancy Loss: 0.0294\n",
      "Epoch [32/50], Class Loss: 0.0257, Discrepancy Loss: 0.0298\n",
      "Epoch [33/50], Class Loss: 0.0114, Discrepancy Loss: 0.0300\n",
      "Epoch [34/50], Class Loss: 0.0121, Discrepancy Loss: 0.0256\n",
      "Epoch [35/50], Class Loss: 0.0066, Discrepancy Loss: 0.0270\n",
      "Epoch [36/50], Class Loss: 0.0148, Discrepancy Loss: 0.0292\n",
      "Epoch [37/50], Class Loss: 0.0074, Discrepancy Loss: 0.0292\n",
      "Epoch [38/50], Class Loss: 0.0114, Discrepancy Loss: 0.0291\n",
      "Epoch [39/50], Class Loss: 0.0050, Discrepancy Loss: 0.0271\n",
      "Epoch [40/50], Class Loss: 0.0115, Discrepancy Loss: 0.0314\n",
      "Epoch [41/50], Class Loss: 0.0086, Discrepancy Loss: 0.0275\n",
      "Epoch [42/50], Class Loss: 0.0061, Discrepancy Loss: 0.0297\n",
      "Epoch [43/50], Class Loss: 0.0148, Discrepancy Loss: 0.0284\n",
      "Epoch [44/50], Class Loss: 0.0068, Discrepancy Loss: 0.0264\n",
      "Epoch [45/50], Class Loss: 0.0056, Discrepancy Loss: 0.0323\n",
      "Epoch [46/50], Class Loss: 0.0054, Discrepancy Loss: 0.0290\n",
      "Epoch [47/50], Class Loss: 0.0083, Discrepancy Loss: 0.0349\n",
      "Epoch [48/50], Class Loss: 0.0037, Discrepancy Loss: 0.0346\n",
      "Epoch [49/50], Class Loss: 0.0053, Discrepancy Loss: 0.0273\n",
      "Epoch [50/50], Class Loss: 0.0046, Discrepancy Loss: 0.0334\n",
      "Source Domain Performance - Accuracy: 99.58%, Precision: 99.59%, Recall: 99.58%, F1 Score: 99.58%\n",
      "Target Domain Performance - Accuracy: 92.99%, Precision: 93.66%, Recall: 92.99%, F1 Score: 93.01%\n",
      "\n",
      "Run 3/3\n",
      "Epoch [1/50], Class Loss: 2.6010, Discrepancy Loss: 0.1108\n",
      "Epoch [2/50], Class Loss: 1.2096, Discrepancy Loss: 0.1346\n",
      "Epoch [3/50], Class Loss: 1.0187, Discrepancy Loss: 0.1197\n",
      "Epoch [4/50], Class Loss: 0.9175, Discrepancy Loss: 0.1247\n",
      "Epoch [5/50], Class Loss: 0.9074, Discrepancy Loss: 0.1224\n",
      "Epoch [6/50], Class Loss: 0.8913, Discrepancy Loss: 0.1295\n",
      "Epoch [7/50], Class Loss: 0.7864, Discrepancy Loss: 0.1426\n",
      "Epoch [8/50], Class Loss: 0.5604, Discrepancy Loss: 0.1063\n",
      "Epoch [9/50], Class Loss: 0.2146, Discrepancy Loss: 0.0783\n",
      "Epoch [10/50], Class Loss: 0.0686, Discrepancy Loss: 0.0565\n",
      "Epoch [11/50], Class Loss: 0.0355, Discrepancy Loss: 0.0500\n",
      "Epoch [12/50], Class Loss: 0.0175, Discrepancy Loss: 0.0445\n",
      "Epoch [13/50], Class Loss: 0.0134, Discrepancy Loss: 0.0423\n",
      "Epoch [14/50], Class Loss: 0.0121, Discrepancy Loss: 0.0281\n",
      "Epoch [15/50], Class Loss: 0.0128, Discrepancy Loss: 0.0202\n",
      "Epoch [16/50], Class Loss: 0.0077, Discrepancy Loss: 0.0251\n",
      "Epoch [17/50], Class Loss: 0.0101, Discrepancy Loss: 0.0271\n",
      "Epoch [18/50], Class Loss: 0.0086, Discrepancy Loss: 0.0311\n",
      "Epoch [19/50], Class Loss: 0.0172, Discrepancy Loss: 0.0375\n",
      "Epoch [20/50], Class Loss: 0.0143, Discrepancy Loss: 0.0345\n",
      "Epoch [21/50], Class Loss: 0.0114, Discrepancy Loss: 0.0198\n",
      "Epoch [22/50], Class Loss: 0.0046, Discrepancy Loss: 0.0276\n",
      "Epoch [23/50], Class Loss: 0.0046, Discrepancy Loss: 0.0245\n",
      "Epoch [24/50], Class Loss: 0.0062, Discrepancy Loss: 0.0278\n",
      "Epoch [25/50], Class Loss: 0.0038, Discrepancy Loss: 0.0275\n",
      "Epoch [26/50], Class Loss: 0.0057, Discrepancy Loss: 0.0327\n",
      "Epoch [27/50], Class Loss: 0.0044, Discrepancy Loss: 0.0316\n",
      "Epoch [28/50], Class Loss: 0.0165, Discrepancy Loss: 0.0292\n",
      "Epoch [29/50], Class Loss: 0.0086, Discrepancy Loss: 0.0322\n",
      "Epoch [30/50], Class Loss: 0.0098, Discrepancy Loss: 0.0288\n",
      "Epoch [31/50], Class Loss: 0.0211, Discrepancy Loss: 0.0336\n",
      "Epoch [32/50], Class Loss: 0.0067, Discrepancy Loss: 0.0289\n",
      "Epoch [33/50], Class Loss: 0.0142, Discrepancy Loss: 0.0399\n",
      "Epoch [34/50], Class Loss: 0.0064, Discrepancy Loss: 0.0370\n",
      "Epoch [35/50], Class Loss: 0.0090, Discrepancy Loss: 0.0328\n",
      "Epoch [36/50], Class Loss: 0.0112, Discrepancy Loss: 0.0333\n",
      "Epoch [37/50], Class Loss: 0.0069, Discrepancy Loss: 0.0274\n",
      "Epoch [38/50], Class Loss: 0.0126, Discrepancy Loss: 0.0327\n",
      "Epoch [39/50], Class Loss: 0.0207, Discrepancy Loss: 0.0345\n",
      "Epoch [40/50], Class Loss: 0.0143, Discrepancy Loss: 0.0341\n",
      "Epoch [41/50], Class Loss: 0.0343, Discrepancy Loss: 0.0356\n",
      "Epoch [42/50], Class Loss: 0.0170, Discrepancy Loss: 0.0413\n",
      "Epoch [43/50], Class Loss: 0.0129, Discrepancy Loss: 0.0345\n",
      "Epoch [44/50], Class Loss: 0.0096, Discrepancy Loss: 0.0369\n",
      "Epoch [45/50], Class Loss: 0.0180, Discrepancy Loss: 0.0328\n",
      "Epoch [46/50], Class Loss: 0.0145, Discrepancy Loss: 0.0320\n",
      "Epoch [47/50], Class Loss: 0.0126, Discrepancy Loss: 0.0329\n",
      "Epoch [48/50], Class Loss: 0.0116, Discrepancy Loss: 0.0359\n",
      "Epoch [49/50], Class Loss: 0.0139, Discrepancy Loss: 0.0334\n",
      "Epoch [50/50], Class Loss: 0.0182, Discrepancy Loss: 0.0386\n",
      "Source Domain Performance - Accuracy: 99.46%, Precision: 99.48%, Recall: 99.45%, F1 Score: 99.46%\n",
      "Target Domain Performance - Accuracy: 94.66%, Precision: 95.26%, Recall: 94.67%, F1 Score: 94.69%\n",
      "\n",
      "Source performance: 99.54% 99.55% 99.54% 99.54%\n",
      "Target performance: 93.33% 94.27% 93.32% 93.29%\n",
      "\n",
      "Per-Class Accuracy on Target Domain:\n",
      "bpsk: 100.00%\n",
      "qpsk: 78.55%\n",
      "16qam: 96.96%\n",
      "16apsk: 97.76%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (4,) and (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     89\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m---> 90\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x, s_base_acc, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     91\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x, s_dann_acc, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDann\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     92\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x, s_star_acc, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.12/site-packages/matplotlib/pyplot.py:3708\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3702\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3706\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3707\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[1;32m   3709\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m   3710\u001b[0m         scalex\u001b[38;5;241m=\u001b[39mscalex,\n\u001b[1;32m   3711\u001b[0m         scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[1;32m   3712\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m   3713\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3714\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.12/site-packages/matplotlib/axes/_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plot_args(\n\u001b[1;32m    297\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[38;5;241m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.12/site-packages/matplotlib/axes/_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (4,) and (3,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load testbed data\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "file_path = \"/home/ash/ic3/testbed_da/data\"\n",
    "\n",
    "# Classes\n",
    "class_subset = [\"bpsk\", \"qpsk\", \"16qam\", \"16apsk\"]\n",
    "\n",
    "# Split source, target\n",
    "# try selecting some of the mods, not all\n",
    "X = np.load(file_path + \"/sim_X.npy\")\n",
    "Y = np.load(file_path + \"/sim_Y.npy\")\n",
    "\n",
    "sou_snr = 22\n",
    "tar_snr = 10\n",
    "\n",
    "t_base_acc = []\n",
    "t_dann_acc = []\n",
    "t_star_acc = []\n",
    "t_mcd_acc = []\n",
    "t_coral_acc = []\n",
    "t_jan_acc = []\n",
    "\n",
    "s_base_acc = []\n",
    "s_dann_acc = []\n",
    "s_star_acc = []\n",
    "s_mcd_acc = []\n",
    "s_coral_acc = []\n",
    "s_jan_acc = []\n",
    "\n",
    "n_runs = 3\n",
    "lr = 0.001\n",
    "n_snr = 4\n",
    "offset_snr = 4\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "for i in range(n_snr-1):    \n",
    "    source_mask = (Y[:, 1] == sou_snr)\n",
    "    target_mask = (Y[:, 1] == tar_snr)\n",
    "    \n",
    "    X_s = X[source_mask]\n",
    "    Y_s = Y[source_mask]\n",
    "    Y_s = Y_s[:,0]\n",
    "    \n",
    "    X_t = X[target_mask]\n",
    "    Y_t = Y[target_mask]\n",
    "    Y_t = Y_t[:,0]\n",
    "\n",
    "    \n",
    "    # Dataloaders\n",
    "    S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, permute=False)\n",
    "    T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, permute=False)\n",
    "\n",
    "    s_mcd, t_mcd = mcd.Mcd(G=MCD_G, C=MCD_C, device=device, S_train_loader=S_train_loader, S_val_loader=S_val_loader,  \n",
    "               T_train_loader=T_train_loader, T_val_loader=T_val_loader, class_subset=class_subset,\n",
    "               n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs, patience=5).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_mcd_acc.append(s_mcd)\n",
    "    t_mcd_acc.append(t_mcd)\n",
    "\n",
    "    s_base, t_base = base.Base(model_cls=CLDNN, device=device, S_train_loader=S_train_loader, \n",
    "                    S_val_loader=S_val_loader, T_val_loader=T_val_loader, class_subset=class_subset, \n",
    "                    n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_base_acc.append(s_base)\n",
    "    t_base_acc.append(t_base)\n",
    "    \n",
    "    s_dan, t_dan = dann.DAN(dann.DANN, FA=CLDNN_FA, LP=CLDNN_LP, DC=CLDNN_DC,\n",
    "                      device=device, S_train_loader=S_train_loader, S_val_loader=S_val_loader,\n",
    "                      T_train_loader=T_train_loader, T_val_loader=T_val_loader,\n",
    "                      class_subset=class_subset, n_classes=len(class_subset), lr=lr,\n",
    "                      n_epochs=50, n_runs=n_runs).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_dann_acc.append(s_dan)\n",
    "    t_dann_acc.append(t_dan)\n",
    "    \n",
    "    s_star, t_star = star.Star(G=STAR_G, C=STAR_C, device=device, S_train_loader=S_train_loader, S_val_loader=S_val_loader,  \n",
    "               T_train_loader=T_train_loader, T_val_loader=T_val_loader, class_subset=class_subset,\n",
    "               n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs, patience=5).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_star_acc.append(s_star)\n",
    "    t_star_acc.append(t_star)\n",
    "\n",
    "    tar_snr += offset_snr\n",
    "\n",
    "x = np.arange(1, 5)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, s_base_acc, marker='o', linestyle='-', label='Base')\n",
    "plt.plot(x, s_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "plt.plot(x, s_star_acc, marker='^', linestyle='--', label='Star')\n",
    "plt.plot(x, s_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.title('Target performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "for i in range(n_snr):    \n",
    "    source_mask = (Y[:, 1] == sou_snr)\n",
    "    target_mask = (Y[:, 1] == tar_snr)\n",
    "    \n",
    "    X_s = X[source_mask]\n",
    "    Y_s = Y[source_mask]\n",
    "    Y_s = Y_s[:,0]\n",
    "    \n",
    "    X_t = X[target_mask]\n",
    "    Y_t = Y[target_mask]\n",
    "    Y_t = Y_t[:,0]\n",
    "\n",
    "    \n",
    "    # Dataloaders\n",
    "    S_train_loader, S_val_loader = funcs.create_loader(X_s, Y_s, permute=False)\n",
    "    T_train_loader, T_val_loader = funcs.create_loader(X_t, Y_t, permute=False)\n",
    "\n",
    "    s_base, t_base = base.Base(model_cls=CLDNN, device=device, S_train_loader=S_train_loader, \n",
    "                    S_val_loader=S_val_loader, T_val_loader=T_val_loader, class_subset=class_subset, \n",
    "                    n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_base_acc.append(s_base)\n",
    "    t_base_acc.append(t_base)\n",
    "    \n",
    "    s_dan, t_dan = dann.DAN(dann.DANN, FA=dann.CLDNN_FA, LP=dann.CLDNN_LP, DC=dann.CLDNN_DC,\n",
    "                      device=device, S_train_loader=S_train_loader, S_val_loader=S_val_loader,\n",
    "                      T_train_loader=T_train_loader, T_val_loader=T_val_loader,\n",
    "                      class_subset=class_subset, n_classes=len(class_subset), lr=lr,\n",
    "                      n_epochs=25, n_runs=n_runs).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_dann_acc.append(s_dan)\n",
    "    t_dann_acc.append(t_dan)\n",
    "    \n",
    "    s_star, t_star = star.Star(G=star.CLDNN_G, C=star.CLDNN_C, device=device, S_train_loader=S_train_loader, S_val_loader=S_val_loader,  \n",
    "               T_train_loader=T_train_loader, T_val_loader=T_val_loader, class_subset=class_subset,\n",
    "               n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs, patience=5).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_star_acc.append(s_star)\n",
    "    t_star_acc.append(t_star)\n",
    "    \n",
    "    s_mcd, t_mcd = mcd.Mcd(G=mcd.CLDNN_G, C=mcd.CLDNN_C, device=device, S_train_loader=S_train_loader, S_val_loader=S_val_loader,  \n",
    "               T_train_loader=T_train_loader, T_val_loader=T_val_loader, class_subset=class_subset,\n",
    "               n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs, patience=5).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_mcd_acc.append(s_mcd)\n",
    "    t_mcd_acc.append(t_mcd)\n",
    "    \n",
    "    s_coral, t_coral = coral.Coral(G=coral.CLDNN_G, C=coral.CLDNN_C, device=device, S_train_loader=S_train_loader,\n",
    "                           S_val_loader=S_val_loader, T_train_loader=T_train_loader, T_val_loader=T_val_loader,\n",
    "                           class_subset=class_subset, n_classes=len(class_subset), lr=lr, n_epochs=50, n_runs=n_runs,\n",
    "                           patience=5, lambda_coral=0.5).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_coral_acc.append(s_coral)\n",
    "    t_coral_acc.append(t_coral)\n",
    "\n",
    "    s_jan, t_jan = jan.Jan(num_classes=len(class_subset), device=device, S_train_loader=S_train_loader,\n",
    "                     T_train_loader=T_train_loader, S_val_loader=S_val_loader, T_val_loader=T_val_loader,\n",
    "                     n_epochs=50, lr=lr, lambda_jmmd=0.1, n_runs=n_runs).run()\n",
    "    torch.cuda.empty_cache()\n",
    "    s_jan_acc.append(s_jan)\n",
    "    t_jan_acc.append(t_jan)\n",
    "    \n",
    "    tar_snr += offset_snr\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9352549d-28ec-4140-aa35-78f2bcffed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1, 4)\n",
    "print(len(s_base_acc))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')\n",
    "plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')\n",
    "plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.title('Target performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04e643-cec3-48d4-bd5c-88fb4a4069ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 8)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, s_base_acc, marker='o', linestyle='-', label='Base')\n",
    "plt.plot(x, s_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "plt.plot(x, s_star_acc, marker='^', linestyle='--', label='Star')\n",
    "plt.plot(x, s_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, s_coral_acc, marker='v', linestyle='--', label='CORAL')\n",
    "plt.plot(x, s_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.title('Source performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x = np.arange(1, 8)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, t_base_acc, marker='o', linestyle='-', label='Base')\n",
    "plt.plot(x, t_dann_acc, marker='s', linestyle='--', label='Dann')\n",
    "plt.plot(x, t_star_acc, marker='^', linestyle='--', label='Star')\n",
    "plt.plot(x, t_mcd_acc, marker='D', linestyle='--', label='MCD')\n",
    "plt.plot(x, t_coral_acc, marker='v', linestyle='--', label='CORAL')\n",
    "plt.plot(x, t_jan_acc, marker='x', linestyle='--', label='JANN')\n",
    "\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Acc (%)')\n",
    "plt.title('Target performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
